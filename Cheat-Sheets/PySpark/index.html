<!DOCTYPE html><html class=no-js lang=en> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="This is a completely open-source platform for maintaining curated list of interview questions and answers for people looking and preparing for data science opportunities." name=description><meta content="Kuldeep Singh Sidhu" name=author><link href=https://singhsidhukuldeep.github.io/Cheat-Sheets/PySpark/ rel=canonical><link href=../Pandas/ rel=prev><link href=../PyTorch/ rel=next><link href=https://repository-images.githubusercontent.com/275878203/13719500-bb75-11ea-8f3a-be2ffb87a6a2 rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.5.50" name=generator><title>PySpark - Data Science Interview preparation</title><link href=../../assets/stylesheets/main.a40c8224.min.css rel=stylesheet><link href=../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href=../../stylesheets/extra.css rel=stylesheet><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-EVGNTG49J7"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-EVGNTG49J7",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-EVGNTG49J7",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../assets/javascripts/glightbox.min.js></script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#pyspark-cheat-sheet>1. Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <!-- Add announcement here, including arbitrary HTML --> <style>
    @keyframes shake {
      0%, 100% { transform: translateX(0); }
      10%, 30%, 50%, 70%, 90% { transform: translateX(-2px); }
      20%, 40%, 60%, 80% { transform: translateX(2px); }
    }
    @keyframes glow {
      0%, 100% { text-shadow: 0 0 5px rgba(255, 165, 0, 0.5); }
      50% { text-shadow: 0 0 20px rgba(255, 165, 0, 0.8), 0 0 30px rgba(255, 140, 0, 0.6); }
    }
    .shake-text {
      display: inline-block;
      animation: shake 3s ease-in-out infinite;
    }
    .glow-link {
      animation: glow 2s ease-in-out infinite;
      font-weight: bold;
    }
  </style> <span class=shake-text>üöÄ <a class=glow-link href=/flashcards>Flashcards</a> feature is live!</span> <meta content=ca-pub-4988388949365963 name=google-adsense-account> <script async crossorigin=anonymous src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4988388949365963"></script> </div> </aside> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav aria-label=Header class="md-header__inner md-grid"> <a aria-label="Data Science Interview preparation" class="md-header__button md-logo" data-md-component=logo href=../.. title="Data Science Interview preparation"> <svg viewbox="0 0 576 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"></path></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Science Interview preparation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> PySpark </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media=(prefers-color-scheme) data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"></path></svg> </label> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=purple data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=deep-purple data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg> </label> <input aria-label="Switch to system preference" class=md-option data-md-color-accent=purple data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=deep-purple data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to system preference"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=Search autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=Search required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=Search class=md-search__options> <a aria-label=Share class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=Share> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=Clear class="md-search__icon md-icon" tabindex=-1 title=Clear type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=Navigation class="md-nav md-nav--primary" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="Data Science Interview preparation" class="md-nav__button md-logo" data-md-component=logo href=../.. title="Data Science Interview preparation"> <svg viewbox="0 0 576 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"></path></svg> </a> Data Science Interview preparation </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=../..> <span class=md-ellipsis> üè° Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_2 type=checkbox> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> üë®üèø‚Äçüè´ Interview Questions </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_2_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> üë®üèø‚Äçüè´ Interview Questions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../flashcards/ class=md-nav__link> <span class=md-ellipsis> üìá Flashcards </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/data-structures-algorithms/ class=md-nav__link> <span class=md-ellipsis> DSA (Data Structures &amp; Algorithms) </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Machine-Learning/ class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/System-design/ class=md-nav__link> <span class=md-ellipsis> System Design </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Natural-Language-Processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing (NLP) </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Probability/ class=md-nav__link> <span class=md-ellipsis> Probability </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/AB-testing/ class=md-nav__link> <span class=md-ellipsis> A/B Testing </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/SQL-Interview-Questions/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Scikit-Learn/ class=md-nav__link> <span class=md-ellipsis> Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/LangChain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/LangGraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Interview-Question-Resources/ class=md-nav__link> <span class=md-ellipsis> Interview Question Resources </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> üìù Cheat Sheets </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> üìù Cheat Sheets </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Django/ class=md-nav__link> <span class=md-ellipsis> Django </span> </a> </li> <li class=md-nav__item> <a href=../Flask/ class=md-nav__link> <span class=md-ellipsis> Flask </span> </a> </li> <li class=md-nav__item> <a href=../Hypothesis-Tests/ class=md-nav__link> <span class=md-ellipsis> Hypothesis Tests </span> </a> </li> <li class=md-nav__item> <a href=../Keras/ class=md-nav__link> <span class=md-ellipsis> Keras </span> </a> </li> <li class=md-nav__item> <a href=../LangChain-LangGraph/ class=md-nav__link> <span class=md-ellipsis> LangChain &amp; LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> PySpark </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> PySpark </span> </a> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#getting-started>1.1 <span class=md-ellipsis> Getting Started </span> </a> <nav aria-label="Getting Started" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pyspark-architecture>1.1.1 <span class=md-ellipsis> PySpark Architecture </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#installation>1.1.2 <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sparksession-recommended-for-spark-20>1.1.3 <span class=md-ellipsis> SparkSession (Recommended for Spark 2.0+) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sparkcontext-legacy-api>1.1.4 <span class=md-ellipsis> SparkContext (Legacy API) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#stopping-spark>1.1.5 <span class=md-ellipsis> Stopping Spark </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-loading>1.2 <span class=md-ellipsis> Data Loading </span> </a> <nav aria-label="Data Loading" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-loading-flow>1.2.1 <span class=md-ellipsis> Data Loading Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-text-files>1.2.2 <span class=md-ellipsis> Loading from Text Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-csv-files>1.2.3 <span class=md-ellipsis> Loading from CSV Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-json-files>1.2.4 <span class=md-ellipsis> Loading from JSON Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-parquet-files>1.2.5 <span class=md-ellipsis> Loading from Parquet Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-orc-files>1.2.6 <span class=md-ellipsis> Loading from ORC Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-avro-files>1.2.7 <span class=md-ellipsis> Loading from Avro Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-jdbc-database>1.2.8 <span class=md-ellipsis> Loading from JDBC Database </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-delta-lake>1.2.9 <span class=md-ellipsis> Loading from Delta Lake </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-other-sources>1.2.10 <span class=md-ellipsis> Loading from Other Sources </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#dataframes>1.3 <span class=md-ellipsis> DataFrames </span> </a> <nav aria-label=DataFrames class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#dataframe-lifecycle>1.3.1 <span class=md-ellipsis> DataFrame Lifecycle </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#creating-dataframes>1.3.2 <span class=md-ellipsis> Creating DataFrames </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#built-in-functions>1.3.3 <span class=md-ellipsis> Built-in Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#user-defined-functions-udfs>1.3.4 <span class=md-ellipsis> User-Defined Functions (UDFs) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#groupby-operations>1.3.5 <span class=md-ellipsis> GroupBy Operations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#window-functions>1.3.6 <span class=md-ellipsis> Window Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sql-queries>1.3.7 <span class=md-ellipsis> SQL Queries </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdds-resilient-distributed-datasets>1.4 <span class=md-ellipsis> RDDs (Resilient Distributed Datasets) </span> </a> <nav aria-label="RDDs (Resilient Distributed Datasets)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rdd-vs-dataframe>1.4.1 <span class=md-ellipsis> RDD vs DataFrame </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#creating-rdds>1.4.2 <span class=md-ellipsis> Creating RDDs </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-transformations-lazy>1.4.3 <span class=md-ellipsis> RDD Transformations (Lazy) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-actions-trigger-computation>1.4.4 <span class=md-ellipsis> RDD Actions (Trigger Computation) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#pair-rdd-operations>1.4.5 <span class=md-ellipsis> Pair RDD Operations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-persistence>1.4.6 <span class=md-ellipsis> RDD Persistence </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-data>1.5 <span class=md-ellipsis> Writing Data </span> </a> <nav aria-label="Writing Data" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#write-modes>1.5.1 <span class=md-ellipsis> Write Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-dataframes>1.5.2 <span class=md-ellipsis> Writing DataFrames </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#write-modes_1>1.5.3 <span class=md-ellipsis> Write Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-rdds>1.5.4 <span class=md-ellipsis> Writing RDDs </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#partitioning-strategies>1.5.5 <span class=md-ellipsis> Partitioning Strategies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-sql>1.6 <span class=md-ellipsis> Spark SQL </span> </a> <nav aria-label="Spark SQL" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#creating-tables>1.6.1 <span class=md-ellipsis> Creating Tables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#inserting-data>1.6.2 <span class=md-ellipsis> Inserting Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#selecting-data>1.6.3 <span class=md-ellipsis> Selecting Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#filtering-data>1.6.4 <span class=md-ellipsis> Filtering Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#aggregating-data>1.6.5 <span class=md-ellipsis> Aggregating Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#joining-tables>1.6.6 <span class=md-ellipsis> Joining Tables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#window-functions-in-sql>1.6.7 <span class=md-ellipsis> Window Functions in SQL </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-mllib>1.7 <span class=md-ellipsis> Spark MLlib </span> </a> <nav aria-label="Spark MLlib" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#ml-pipeline-flow>1.7.1 <span class=md-ellipsis> ML Pipeline Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-preparation>1.7.2 <span class=md-ellipsis> Data Preparation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-extraction>1.7.3 <span class=md-ellipsis> Feature Extraction </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-scaling>1.7.4 <span class=md-ellipsis> Feature Scaling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-selection>1.7.5 <span class=md-ellipsis> Feature Selection </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#classification>1.7.6 <span class=md-ellipsis> Classification </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#regression>1.7.7 <span class=md-ellipsis> Regression </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#clustering>1.7.8 <span class=md-ellipsis> Clustering </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#recommendation>1.7.9 <span class=md-ellipsis> Recommendation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evaluation>1.7.10 <span class=md-ellipsis> Evaluation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cross-validation-and-hyperparameter-tuning>1.7.11 <span class=md-ellipsis> Cross-Validation and Hyperparameter Tuning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ml-pipelines>1.7.12 <span class=md-ellipsis> ML Pipelines </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-persistence>1.7.13 <span class=md-ellipsis> Model Persistence </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#structured-streaming>1.8 <span class=md-ellipsis> Structured Streaming </span> </a> <nav aria-label="Structured Streaming" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#streaming-architecture>1.8.1 <span class=md-ellipsis> Streaming Architecture </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#reading-streaming-data>1.8.2 <span class=md-ellipsis> Reading Streaming Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#stream-processing>1.8.3 <span class=md-ellipsis> Stream Processing </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-streaming-data>1.8.4 <span class=md-ellipsis> Writing Streaming Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#output-modes>1.8.5 <span class=md-ellipsis> Output Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#query-management>1.8.6 <span class=md-ellipsis> Query Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#performance-tuning>1.9 <span class=md-ellipsis> Performance Tuning </span> </a> <nav aria-label="Performance Tuning" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#performance-optimization-flow>1.9.1 <span class=md-ellipsis> Performance Optimization Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-partitioning>1.9.2 <span class=md-ellipsis> Data Partitioning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#caching-and-persistence>1.9.3 <span class=md-ellipsis> Caching and Persistence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#broadcast-variables>1.9.4 <span class=md-ellipsis> Broadcast Variables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#accumulators>1.9.5 <span class=md-ellipsis> Accumulators </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#memory-management>1.9.6 <span class=md-ellipsis> Memory Management </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#shuffle-optimization>1.9.7 <span class=md-ellipsis> Shuffle Optimization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#serialization>1.9.8 <span class=md-ellipsis> Serialization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-skew-handling>1.9.9 <span class=md-ellipsis> Data Skew Handling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#query-optimization>1.9.10 <span class=md-ellipsis> Query Optimization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#monitoring-and-debugging>1.9.11 <span class=md-ellipsis> Monitoring and Debugging </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-issues-and-debugging>1.10 <span class=md-ellipsis> Common Issues and Debugging </span> </a> <nav aria-label="Common Issues and Debugging" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#issue-resolution-flow>1.10.1 <span class=md-ellipsis> Issue Resolution Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#out-of-memory-errors>1.10.2 <span class=md-ellipsis> Out of Memory Errors </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#slow-performance>1.10.3 <span class=md-ellipsis> Slow Performance </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#serialization-errors>1.10.4 <span class=md-ellipsis> Serialization Errors </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-skew>1.10.5 <span class=md-ellipsis> Data Skew </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#connection-issues>1.10.6 <span class=md-ellipsis> Connection Issues </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-error-messages>1.10.7 <span class=md-ellipsis> Common Error Messages </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-configuration>1.11 <span class=md-ellipsis> Spark Configuration </span> </a> <nav aria-label="Spark Configuration" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#configuration-hierarchy>1.11.1 <span class=md-ellipsis> Configuration Hierarchy </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#essential-configurations>1.11.2 <span class=md-ellipsis> Essential Configurations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-configuration-patterns>1.11.3 <span class=md-ellipsis> Common Configuration Patterns </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#view-and-update-configuration>1.11.4 <span class=md-ellipsis> View and Update Configuration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#tips-and-best-practices>1.12 <span class=md-ellipsis> Tips and Best Practices </span> </a> <nav aria-label="Tips and Best Practices" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#development-best-practices>1.12.1 <span class=md-ellipsis> Development Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#performance-best-practices>1.12.2 <span class=md-ellipsis> Performance Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-quality-practices>1.12.3 <span class=md-ellipsis> Data Quality Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#production-checklist>1.12.4 <span class=md-ellipsis> Production Checklist </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security-best-practices>1.12.5 <span class=md-ellipsis> Security Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#key-recommendations>1.12.6 <span class=md-ellipsis> Key Recommendations </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../PyTorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../RegEx/ class=md-nav__link> <span class=md-ellipsis> Regular Expressions (RegEx) </span> </a> </li> <li class=md-nav__item> <a href=../Sk-learn/ class=md-nav__link> <span class=md-ellipsis> Scikit Learn </span> </a> </li> <li class=md-nav__item> <a href=../SQL/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../tensorflow/ class=md-nav__link> <span class=md-ellipsis> TensorFlow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> ‚Äçüéì ML Topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> ‚Äçüéì ML Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Machine-Learning/ARIMA/ class=md-nav__link> <span class=md-ellipsis> ARIMA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Activation%20functions/ class=md-nav__link> <span class=md-ellipsis> Activation functions </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Collaborative%20Filtering/ class=md-nav__link> <span class=md-ellipsis> Collaborative Filtering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Confusion%20Matrix/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/DBSCAN/ class=md-nav__link> <span class=md-ellipsis> DBSCAN </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Decision%20Trees/ class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Gradient%20Boosting/ class=md-nav__link> <span class=md-ellipsis> Gradient Boosting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/K-means%20clustering/ class=md-nav__link> <span class=md-ellipsis> K-means clustering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Linear%20Regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Logistic%20Regression/ class=md-nav__link> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/ class=md-nav__link> <span class=md-ellipsis> Loss Function MAE, RMSE </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Neural%20Networks/ class=md-nav__link> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normal%20Distribution/ class=md-nav__link> <span class=md-ellipsis> Normal Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normalization%20Regularisation/ class=md-nav__link> <span class=md-ellipsis> Normalization Regularisation </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Overfitting%2C%20Underfitting/ class=md-nav__link> <span class=md-ellipsis> Overfitting, Underfitting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/PCA/ class=md-nav__link> <span class=md-ellipsis> PCA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Random%20Forest/ class=md-nav__link> <span class=md-ellipsis> Random Forest </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Support%20Vector%20Machines/ class=md-nav__link> <span class=md-ellipsis> Support Vector Machines </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Unbalanced%2C%20Skewed%20data/ class=md-nav__link> <span class=md-ellipsis> Unbalanced, Skewed data </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/kNN/ class=md-nav__link> <span class=md-ellipsis> kNN </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5 type=checkbox> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> üë®üèæ‚Äçüíª Online Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> üë®üèæ‚Äçüíª Online Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Online-Material/Online-Material-for-Learning/ class=md-nav__link> <span class=md-ellipsis> Online Study Material </span> </a> </li> <li class=md-nav__item> <a href=../../Online-Material/popular-resources/ class=md-nav__link> <span class=md-ellipsis> Popular Blogs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Contribute/ class=md-nav__link> <span class=md-ellipsis> ü§ù Contribute </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#getting-started>1.1 <span class=md-ellipsis> Getting Started </span> </a> <nav aria-label="Getting Started" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#pyspark-architecture>1.1.1 <span class=md-ellipsis> PySpark Architecture </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#installation>1.1.2 <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sparksession-recommended-for-spark-20>1.1.3 <span class=md-ellipsis> SparkSession (Recommended for Spark 2.0+) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sparkcontext-legacy-api>1.1.4 <span class=md-ellipsis> SparkContext (Legacy API) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#stopping-spark>1.1.5 <span class=md-ellipsis> Stopping Spark </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-loading>1.2 <span class=md-ellipsis> Data Loading </span> </a> <nav aria-label="Data Loading" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#data-loading-flow>1.2.1 <span class=md-ellipsis> Data Loading Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-text-files>1.2.2 <span class=md-ellipsis> Loading from Text Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-csv-files>1.2.3 <span class=md-ellipsis> Loading from CSV Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-json-files>1.2.4 <span class=md-ellipsis> Loading from JSON Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-parquet-files>1.2.5 <span class=md-ellipsis> Loading from Parquet Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-orc-files>1.2.6 <span class=md-ellipsis> Loading from ORC Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-avro-files>1.2.7 <span class=md-ellipsis> Loading from Avro Files </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-jdbc-database>1.2.8 <span class=md-ellipsis> Loading from JDBC Database </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-delta-lake>1.2.9 <span class=md-ellipsis> Loading from Delta Lake </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#loading-from-other-sources>1.2.10 <span class=md-ellipsis> Loading from Other Sources </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#dataframes>1.3 <span class=md-ellipsis> DataFrames </span> </a> <nav aria-label=DataFrames class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#dataframe-lifecycle>1.3.1 <span class=md-ellipsis> DataFrame Lifecycle </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#creating-dataframes>1.3.2 <span class=md-ellipsis> Creating DataFrames </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#built-in-functions>1.3.3 <span class=md-ellipsis> Built-in Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#user-defined-functions-udfs>1.3.4 <span class=md-ellipsis> User-Defined Functions (UDFs) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#groupby-operations>1.3.5 <span class=md-ellipsis> GroupBy Operations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#window-functions>1.3.6 <span class=md-ellipsis> Window Functions </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#sql-queries>1.3.7 <span class=md-ellipsis> SQL Queries </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdds-resilient-distributed-datasets>1.4 <span class=md-ellipsis> RDDs (Resilient Distributed Datasets) </span> </a> <nav aria-label="RDDs (Resilient Distributed Datasets)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#rdd-vs-dataframe>1.4.1 <span class=md-ellipsis> RDD vs DataFrame </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#creating-rdds>1.4.2 <span class=md-ellipsis> Creating RDDs </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-transformations-lazy>1.4.3 <span class=md-ellipsis> RDD Transformations (Lazy) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-actions-trigger-computation>1.4.4 <span class=md-ellipsis> RDD Actions (Trigger Computation) </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#pair-rdd-operations>1.4.5 <span class=md-ellipsis> Pair RDD Operations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#rdd-persistence>1.4.6 <span class=md-ellipsis> RDD Persistence </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-data>1.5 <span class=md-ellipsis> Writing Data </span> </a> <nav aria-label="Writing Data" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#write-modes>1.5.1 <span class=md-ellipsis> Write Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-dataframes>1.5.2 <span class=md-ellipsis> Writing DataFrames </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#write-modes_1>1.5.3 <span class=md-ellipsis> Write Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-rdds>1.5.4 <span class=md-ellipsis> Writing RDDs </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#partitioning-strategies>1.5.5 <span class=md-ellipsis> Partitioning Strategies </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-sql>1.6 <span class=md-ellipsis> Spark SQL </span> </a> <nav aria-label="Spark SQL" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#creating-tables>1.6.1 <span class=md-ellipsis> Creating Tables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#inserting-data>1.6.2 <span class=md-ellipsis> Inserting Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#selecting-data>1.6.3 <span class=md-ellipsis> Selecting Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#filtering-data>1.6.4 <span class=md-ellipsis> Filtering Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#aggregating-data>1.6.5 <span class=md-ellipsis> Aggregating Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#joining-tables>1.6.6 <span class=md-ellipsis> Joining Tables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#window-functions-in-sql>1.6.7 <span class=md-ellipsis> Window Functions in SQL </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-mllib>1.7 <span class=md-ellipsis> Spark MLlib </span> </a> <nav aria-label="Spark MLlib" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#ml-pipeline-flow>1.7.1 <span class=md-ellipsis> ML Pipeline Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-preparation>1.7.2 <span class=md-ellipsis> Data Preparation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-extraction>1.7.3 <span class=md-ellipsis> Feature Extraction </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-scaling>1.7.4 <span class=md-ellipsis> Feature Scaling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#feature-selection>1.7.5 <span class=md-ellipsis> Feature Selection </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#classification>1.7.6 <span class=md-ellipsis> Classification </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#regression>1.7.7 <span class=md-ellipsis> Regression </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#clustering>1.7.8 <span class=md-ellipsis> Clustering </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#recommendation>1.7.9 <span class=md-ellipsis> Recommendation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evaluation>1.7.10 <span class=md-ellipsis> Evaluation </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#cross-validation-and-hyperparameter-tuning>1.7.11 <span class=md-ellipsis> Cross-Validation and Hyperparameter Tuning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#ml-pipelines>1.7.12 <span class=md-ellipsis> ML Pipelines </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#model-persistence>1.7.13 <span class=md-ellipsis> Model Persistence </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#structured-streaming>1.8 <span class=md-ellipsis> Structured Streaming </span> </a> <nav aria-label="Structured Streaming" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#streaming-architecture>1.8.1 <span class=md-ellipsis> Streaming Architecture </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#reading-streaming-data>1.8.2 <span class=md-ellipsis> Reading Streaming Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#stream-processing>1.8.3 <span class=md-ellipsis> Stream Processing </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#writing-streaming-data>1.8.4 <span class=md-ellipsis> Writing Streaming Data </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#output-modes>1.8.5 <span class=md-ellipsis> Output Modes </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#query-management>1.8.6 <span class=md-ellipsis> Query Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#performance-tuning>1.9 <span class=md-ellipsis> Performance Tuning </span> </a> <nav aria-label="Performance Tuning" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#performance-optimization-flow>1.9.1 <span class=md-ellipsis> Performance Optimization Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-partitioning>1.9.2 <span class=md-ellipsis> Data Partitioning </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#caching-and-persistence>1.9.3 <span class=md-ellipsis> Caching and Persistence </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#broadcast-variables>1.9.4 <span class=md-ellipsis> Broadcast Variables </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#accumulators>1.9.5 <span class=md-ellipsis> Accumulators </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#memory-management>1.9.6 <span class=md-ellipsis> Memory Management </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#shuffle-optimization>1.9.7 <span class=md-ellipsis> Shuffle Optimization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#serialization>1.9.8 <span class=md-ellipsis> Serialization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-skew-handling>1.9.9 <span class=md-ellipsis> Data Skew Handling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#query-optimization>1.9.10 <span class=md-ellipsis> Query Optimization </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#monitoring-and-debugging>1.9.11 <span class=md-ellipsis> Monitoring and Debugging </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-issues-and-debugging>1.10 <span class=md-ellipsis> Common Issues and Debugging </span> </a> <nav aria-label="Common Issues and Debugging" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#issue-resolution-flow>1.10.1 <span class=md-ellipsis> Issue Resolution Flow </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#out-of-memory-errors>1.10.2 <span class=md-ellipsis> Out of Memory Errors </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#slow-performance>1.10.3 <span class=md-ellipsis> Slow Performance </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#serialization-errors>1.10.4 <span class=md-ellipsis> Serialization Errors </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-skew>1.10.5 <span class=md-ellipsis> Data Skew </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#connection-issues>1.10.6 <span class=md-ellipsis> Connection Issues </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-error-messages>1.10.7 <span class=md-ellipsis> Common Error Messages </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#spark-configuration>1.11 <span class=md-ellipsis> Spark Configuration </span> </a> <nav aria-label="Spark Configuration" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#configuration-hierarchy>1.11.1 <span class=md-ellipsis> Configuration Hierarchy </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#essential-configurations>1.11.2 <span class=md-ellipsis> Essential Configurations </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#common-configuration-patterns>1.11.3 <span class=md-ellipsis> Common Configuration Patterns </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#view-and-update-configuration>1.11.4 <span class=md-ellipsis> View and Update Configuration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#tips-and-best-practices>1.12 <span class=md-ellipsis> Tips and Best Practices </span> </a> <nav aria-label="Tips and Best Practices" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#development-best-practices>1.12.1 <span class=md-ellipsis> Development Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#performance-best-practices>1.12.2 <span class=md-ellipsis> Performance Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#data-quality-practices>1.12.3 <span class=md-ellipsis> Data Quality Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#production-checklist>1.12.4 <span class=md-ellipsis> Production Checklist </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#security-best-practices>1.12.5 <span class=md-ellipsis> Security Best Practices </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#key-recommendations>1.12.6 <span class=md-ellipsis> Key Recommendations </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/edit/master/docs/Cheat-Sheets/PySpark.md title="Edit this page"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <a class="md-content__button md-icon" href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/raw/master/docs/Cheat-Sheets/PySpark.md title="View source of this page"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg> </a> <h1 id=pyspark-cheat-sheet><span class="enumerate-headings-plugin enumerate-heading-plugin">1.</span> PySpark Cheat Sheet</h1> <div class=toc> <ul> <li><a href=#pyspark-cheat-sheet>1. PySpark Cheat Sheet</a><ul> <li><a href=#getting-started>1.1 Getting Started</a><ul> <li><a href=#pyspark-architecture>1.1.1 PySpark Architecture</a></li> <li><a href=#installation>1.1.2 Installation</a></li> <li><a href=#sparksession-recommended-for-spark-20>1.1.3 SparkSession (Recommended for Spark 2.0+)</a></li> <li><a href=#sparkcontext-legacy-api>1.1.4 SparkContext (Legacy API)</a></li> <li><a href=#stopping-spark>1.1.5 Stopping Spark</a></li> </ul> </li> <li><a href=#data-loading>1.2 Data Loading</a><ul> <li><a href=#data-loading-flow>1.2.1 Data Loading Flow</a></li> <li><a href=#loading-from-text-files>1.2.2 Loading from Text Files</a></li> <li><a href=#loading-from-csv-files>1.2.3 Loading from CSV Files</a></li> <li><a href=#loading-from-json-files>1.2.4 Loading from JSON Files</a></li> <li><a href=#loading-from-parquet-files>1.2.5 Loading from Parquet Files</a></li> <li><a href=#loading-from-orc-files>1.2.6 Loading from ORC Files</a></li> <li><a href=#loading-from-avro-files>1.2.7 Loading from Avro Files</a></li> <li><a href=#loading-from-jdbc-database>1.2.8 Loading from JDBC Database</a></li> <li><a href=#loading-from-delta-lake>1.2.9 Loading from Delta Lake</a></li> <li><a href=#loading-from-other-sources>1.2.10 Loading from Other Sources</a></li> </ul> </li> <li><a href=#dataframes>1.3 DataFrames</a><ul> <li><a href=#dataframe-lifecycle>1.3.1 DataFrame Lifecycle</a></li> <li><a href=#creating-dataframes>1.3.2 Creating DataFrames</a></li> <li><a href=#built-in-functions>1.3.3 Built-in Functions</a></li> <li><a href=#user-defined-functions-udfs>1.3.4 User-Defined Functions (UDFs)</a></li> <li><a href=#groupby-operations>1.3.5 GroupBy Operations</a></li> <li><a href=#window-functions>1.3.6 Window Functions</a></li> <li><a href=#sql-queries>1.3.7 SQL Queries</a></li> </ul> </li> <li><a href=#rdds-resilient-distributed-datasets>1.4 RDDs (Resilient Distributed Datasets)</a><ul> <li><a href=#rdd-vs-dataframe>1.4.1 RDD vs DataFrame</a></li> <li><a href=#creating-rdds>1.4.2 Creating RDDs</a></li> <li><a href=#rdd-transformations-lazy>1.4.3 RDD Transformations (Lazy)</a></li> <li><a href=#rdd-actions-trigger-computation>1.4.4 RDD Actions (Trigger Computation)</a></li> <li><a href=#pair-rdd-operations>1.4.5 Pair RDD Operations</a></li> <li><a href=#rdd-persistence>1.4.6 RDD Persistence</a></li> </ul> </li> <li><a href=#writing-data>1.5 Writing Data</a><ul> <li><a href=#write-modes>1.5.1 Write Modes</a></li> <li><a href=#writing-dataframes>1.5.2 Writing DataFrames</a></li> <li><a href=#write-modes_1>1.5.3 Write Modes</a></li> <li><a href=#writing-rdds>1.5.4 Writing RDDs</a></li> <li><a href=#partitioning-strategies>1.5.5 Partitioning Strategies</a></li> </ul> </li> <li><a href=#spark-sql>1.6 Spark SQL</a><ul> <li><a href=#creating-tables>1.6.1 Creating Tables</a></li> <li><a href=#inserting-data>1.6.2 Inserting Data</a></li> <li><a href=#selecting-data>1.6.3 Selecting Data</a></li> <li><a href=#filtering-data>1.6.4 Filtering Data</a></li> <li><a href=#aggregating-data>1.6.5 Aggregating Data</a></li> <li><a href=#joining-tables>1.6.6 Joining Tables</a></li> <li><a href=#window-functions-in-sql>1.6.7 Window Functions in SQL</a></li> </ul> </li> <li><a href=#spark-mllib>1.7 Spark MLlib</a><ul> <li><a href=#ml-pipeline-flow>1.7.1 ML Pipeline Flow</a></li> <li><a href=#data-preparation>1.7.2 Data Preparation</a></li> <li><a href=#feature-extraction>1.7.3 Feature Extraction</a></li> <li><a href=#feature-scaling>1.7.4 Feature Scaling</a></li> <li><a href=#feature-selection>1.7.5 Feature Selection</a></li> <li><a href=#classification>1.7.6 Classification</a></li> <li><a href=#regression>1.7.7 Regression</a></li> <li><a href=#clustering>1.7.8 Clustering</a></li> <li><a href=#recommendation>1.7.9 Recommendation</a></li> <li><a href=#evaluation>1.7.10 Evaluation</a></li> <li><a href=#cross-validation-and-hyperparameter-tuning>1.7.11 Cross-Validation and Hyperparameter Tuning</a></li> <li><a href=#ml-pipelines>1.7.12 ML Pipelines</a></li> <li><a href=#model-persistence>1.7.13 Model Persistence</a></li> </ul> </li> <li><a href=#structured-streaming>1.8 Structured Streaming</a><ul> <li><a href=#streaming-architecture>1.8.1 Streaming Architecture</a></li> <li><a href=#reading-streaming-data>1.8.2 Reading Streaming Data</a></li> <li><a href=#stream-processing>1.8.3 Stream Processing</a></li> <li><a href=#writing-streaming-data>1.8.4 Writing Streaming Data</a></li> <li><a href=#output-modes>1.8.5 Output Modes</a></li> <li><a href=#query-management>1.8.6 Query Management</a></li> </ul> </li> <li><a href=#performance-tuning>1.9 Performance Tuning</a><ul> <li><a href=#performance-optimization-flow>1.9.1 Performance Optimization Flow</a></li> <li><a href=#data-partitioning>1.9.2 Data Partitioning</a></li> <li><a href=#caching-and-persistence>1.9.3 Caching and Persistence</a></li> <li><a href=#broadcast-variables>1.9.4 Broadcast Variables</a></li> <li><a href=#accumulators>1.9.5 Accumulators</a></li> <li><a href=#memory-management>1.9.6 Memory Management</a></li> <li><a href=#shuffle-optimization>1.9.7 Shuffle Optimization</a></li> <li><a href=#serialization>1.9.8 Serialization</a></li> <li><a href=#data-skew-handling>1.9.9 Data Skew Handling</a></li> <li><a href=#query-optimization>1.9.10 Query Optimization</a></li> <li><a href=#monitoring-and-debugging>1.9.11 Monitoring and Debugging</a></li> </ul> </li> <li><a href=#common-issues-and-debugging>1.10 Common Issues and Debugging</a><ul> <li><a href=#issue-resolution-flow>1.10.1 Issue Resolution Flow</a></li> <li><a href=#out-of-memory-errors>1.10.2 Out of Memory Errors</a></li> <li><a href=#slow-performance>1.10.3 Slow Performance</a></li> <li><a href=#serialization-errors>1.10.4 Serialization Errors</a></li> <li><a href=#data-skew>1.10.5 Data Skew</a></li> <li><a href=#connection-issues>1.10.6 Connection Issues</a></li> <li><a href=#common-error-messages>1.10.7 Common Error Messages</a></li> </ul> </li> <li><a href=#spark-configuration>1.11 Spark Configuration</a><ul> <li><a href=#configuration-hierarchy>1.11.1 Configuration Hierarchy</a></li> <li><a href=#essential-configurations>1.11.2 Essential Configurations</a></li> <li><a href=#common-configuration-patterns>1.11.3 Common Configuration Patterns</a></li> <li><a href=#view-and-update-configuration>1.11.4 View and Update Configuration</a></li> </ul> </li> <li><a href=#tips-and-best-practices>1.12 Tips and Best Practices</a><ul> <li><a href=#development-best-practices>1.12.1 Development Best Practices</a></li> <li><a href=#performance-best-practices>1.12.2 Performance Best Practices</a></li> <li><a href=#data-quality-practices>1.12.3 Data Quality Practices</a></li> <li><a href=#production-checklist>1.12.4 Production Checklist</a></li> <li><a href=#security-best-practices>1.12.5 Security Best Practices</a></li> <li><a href=#key-recommendations>1.12.6 Key Recommendations</a></li> </ul> </li> </ul> </li> </ul> </div> <p>This cheat sheet provides an exhaustive overview of the PySpark API, covering essential concepts, code snippets, and best practices for efficient data processing and machine learning with Apache Spark. It aims to be a one-stop reference for common tasks.</p> <h2 id=getting-started><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1</span> Getting Started</h2> <h3 id=pyspark-architecture><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.1</span> PySpark Architecture</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              Driver Program                      ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
    ‚îÇ  ‚îÇ        SparkContext/SparkSession         ‚îÇ   ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   Cluster Manager   ‚îÇ
           ‚îÇ  (YARN/Mesos/K8s)   ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì             ‚Üì             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Worker  ‚îÇ  ‚îÇ Worker  ‚îÇ  ‚îÇ Worker  ‚îÇ
    ‚îÇ  Node   ‚îÇ  ‚îÇ  Node   ‚îÇ  ‚îÇ  Node   ‚îÇ
    ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ  ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
    ‚îÇ‚îÇExecutor‚îÇ‚îÇ ‚îÇ‚îÇExecutor‚îÇ‚îÇ ‚îÇ‚îÇExecutor‚îÇ‚îÇ
    ‚îÇ‚îÇ Cache  ‚îÇ‚îÇ  ‚îÇ‚îÇ Cache  ‚îÇ‚îÇ  ‚îÇ‚îÇ Cache  ‚îÇ‚îÇ
    ‚îÇ‚îÇ Tasks  ‚îÇ‚îÇ  ‚îÇ‚îÇ Tasks  ‚îÇ‚îÇ  ‚îÇ‚îÇ Tasks  ‚îÇ‚îÇ
    ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ  ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=installation><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.2</span> Installation</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic installation</span>
pip<span class=w> </span>install<span class=w> </span>pyspark

<span class=c1># With specific version</span>
pip<span class=w> </span>install<span class=w> </span><span class=nv>pyspark</span><span class=o>==</span><span class=m>3</span>.5.0

<span class=c1># With additional dependencies</span>
pip<span class=w> </span>install<span class=w> </span>pyspark<span class=o>[</span>sql,ml,streaming<span class=o>]</span>
</code></pre></div> <p>Using virtual environment:</p> <div class=highlight><pre><span></span><code><span class=c1># Create and activate virtual environment</span>
python<span class=w> </span>-m<span class=w> </span>venv<span class=w> </span>venv
<span class=nb>source</span><span class=w> </span>venv/bin/activate<span class=w>  </span><span class=c1># On Linux/macOS</span>
venv<span class=se>\S</span>cripts<span class=se>\a</span>ctivate<span class=w>     </span><span class=c1># On Windows</span>

<span class=c1># Install PySpark</span>
pip<span class=w> </span>install<span class=w> </span>pyspark
</code></pre></div> <h3 id=sparksession-recommended-for-spark-20><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.3</span> SparkSession (Recommended for Spark 2.0+)</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkSession</span>

<span class=c1># Basic session</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"MyPySparkApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># With configurations</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"MyPySparkApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"2g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"200"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Access SparkContext from SparkSession</span>
<span class=n>sc</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span>

<span class=c1># Get Spark version</span>
<span class=nb>print</span><span class=p>(</span><span class=n>spark</span><span class=o>.</span><span class=n>version</span><span class=p>)</span>
</code></pre></div> <h3 id=sparkcontext-legacy-api><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.4</span> SparkContext (Legacy API)</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkContext</span><span class=p>,</span> <span class=n>SparkConf</span>

<span class=c1># Configuration</span>
<span class=n>conf</span> <span class=o>=</span> <span class=n>SparkConf</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>setAppName</span><span class=p>(</span><span class=s2>"MyPySparkApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>setMaster</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span>

<span class=c1># Create SparkContext</span>
<span class=n>sc</span> <span class=o>=</span> <span class=n>SparkContext</span><span class=p>(</span><span class=n>conf</span><span class=o>=</span><span class=n>conf</span><span class=p>)</span>

<span class=c1># Create SparkSession from existing context</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkSession</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=p>(</span><span class=n>sparkContext</span><span class=o>=</span><span class=n>sc</span><span class=p>)</span>
</code></pre></div> <h3 id=stopping-spark><span class="enumerate-headings-plugin enumerate-heading-plugin">1.1.5</span> Stopping Spark</h3> <div class=highlight><pre><span></span><code><span class=c1># Stop SparkSession (also stops SparkContext)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>

<span class=c1># Stop SparkContext only</span>
<span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</code></pre></div> <h2 id=data-loading><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2</span> Data Loading</h2> <h3 id=data-loading-flow><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.1</span> Data Loading Flow</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Data Sources ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                          ‚îÇ
    ‚Üì                ‚Üì            ‚Üì            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Text  ‚îÇ      ‚îÇ CSV  ‚îÇ    ‚îÇ JSON   ‚îÇ   ‚îÇ  JDBC    ‚îÇ
‚îÇ Files  ‚îÇ      ‚îÇFiles ‚îÇ    ‚îÇ Files  ‚îÇ   ‚îÇDatabase  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ               ‚îÇ           ‚îÇ             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  Spark Reader  ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   DataFrame    ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=loading-from-text-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.2</span> Loading from Text Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Using SparkContext (RDD)</span>
<span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"path/to/textfile.txt"</span><span class=p>)</span>
<span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"hdfs://path/to/textfile.txt"</span><span class=p>)</span>  <span class=c1># HDFS</span>
<span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"s3a://bucket/path/file.txt"</span><span class=p>)</span>   <span class=c1># S3</span>

<span class=c1># Multiple files</span>
<span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"path/to/directory/*.txt"</span><span class=p>)</span>

<span class=c1># Using SparkSession (DataFrame)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=s2>"path/to/textfile.txt"</span><span class=p>)</span>

<span class=c1># Read multiple text files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>text</span><span class=p>([</span><span class=s2>"file1.txt"</span><span class=p>,</span> <span class=s2>"file2.txt"</span><span class=p>])</span>
</code></pre></div> <h3 id=loading-from-csv-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.3</span> Loading from CSV Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic CSV read</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=s2>"path/to/file.csv"</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>inferSchema</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># With detailed options</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span>
    <span class=s2>"path/to/file.csv"</span><span class=p>,</span>
    <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>inferSchema</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>sep</span><span class=o>=</span><span class=s2>","</span><span class=p>,</span>
    <span class=n>quote</span><span class=o>=</span><span class=s1>'"'</span><span class=p>,</span>
    <span class=n>escape</span><span class=o>=</span><span class=s2>"</span><span class=se>\\</span><span class=s2>"</span><span class=p>,</span>
    <span class=n>nullValue</span><span class=o>=</span><span class=s2>"NULL"</span><span class=p>,</span>
    <span class=n>dateFormat</span><span class=o>=</span><span class=s2>"yyyy-MM-dd"</span><span class=p>,</span>
    <span class=n>timestampFormat</span><span class=o>=</span><span class=s2>"yyyy-MM-dd HH:mm:ss"</span>
<span class=p>)</span>

<span class=c1># Alternative syntax</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"csv"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"header"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"inferSchema"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"mode"</span><span class=p>,</span> <span class=s2>"DROPMALFORMED"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/file.csv"</span><span class=p>)</span>

<span class=c1># With explicit schema</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StructType</span><span class=p>,</span> <span class=n>StructField</span><span class=p>,</span> <span class=n>StringType</span><span class=p>,</span> <span class=n>IntegerType</span>

<span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>True</span><span class=p>),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"age"</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>True</span><span class=p>),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"city"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>True</span><span class=p>)</span>
<span class=p>])</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=s2>"path/to/file.csv"</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>schema</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Read multiple CSV files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=s2>"path/to/directory/*.csv"</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>inferSchema</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-json-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.4</span> Loading from JSON Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic JSON read</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>json</span><span class=p>(</span><span class=s2>"path/to/file.json"</span><span class=p>)</span>

<span class=c1># With options</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"json"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"multiLine"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"mode"</span><span class=p>,</span> <span class=s2>"PERMISSIVE"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/file.json"</span><span class=p>)</span>

<span class=c1># Read JSON lines (one JSON object per line)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>json</span><span class=p>(</span><span class=s2>"path/to/jsonlines.jsonl"</span><span class=p>)</span>

<span class=c1># With schema</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>json</span><span class=p>(</span><span class=s2>"path/to/file.json"</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>schema</span><span class=p>)</span>

<span class=c1># Read multiple JSON files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>json</span><span class=p>(</span><span class=s2>"path/to/directory/*.json"</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-parquet-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.5</span> Loading from Parquet Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic Parquet read (preserves schema automatically)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/file.parquet"</span><span class=p>)</span>

<span class=c1># Read multiple Parquet files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/directory/*.parquet"</span><span class=p>)</span>

<span class=c1># Read partitioned data</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/partitioned_data"</span><span class=p>)</span>

<span class=c1># With merge schema (for schema evolution)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"mergeSchema"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/file.parquet"</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-orc-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.6</span> Loading from ORC Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic ORC read</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>orc</span><span class=p>(</span><span class=s2>"path/to/file.orc"</span><span class=p>)</span>

<span class=c1># With options</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"orc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"mergeSchema"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/file.orc"</span><span class=p>)</span>

<span class=c1># Read multiple ORC files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>orc</span><span class=p>(</span><span class=s2>"path/to/directory/*.orc"</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-avro-files><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.7</span> Loading from Avro Files</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic Avro read</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"avro"</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/file.avro"</span><span class=p>)</span>

<span class=c1># Read multiple Avro files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"avro"</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/directory/*.avro"</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-jdbc-database><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.8</span> Loading from JDBC Database</h3> <div class=highlight><pre><span></span><code><span class=c1># Basic JDBC read</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/mydatabase"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"mytable"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"driver"</span><span class=p>,</span> <span class=s2>"org.postgresql.Driver"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># With query</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/mydatabase"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"query"</span><span class=p>,</span> <span class=s2>"SELECT * FROM mytable WHERE age &gt; 25"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># With partitioning for parallel reads</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:mysql://localhost:3306/mydb"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"mytable"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"partitionColumn"</span><span class=p>,</span> <span class=s2>"id"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"lowerBound"</span><span class=p>,</span> <span class=s2>"1"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"upperBound"</span><span class=p>,</span> <span class=s2>"100000"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"numPartitions"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># Common JDBC URLs</span>
<span class=c1># PostgreSQL: jdbc:postgresql://host:5432/database</span>
<span class=c1># MySQL: jdbc:mysql://host:3306/database</span>
<span class=c1># SQL Server: jdbc:sqlserver://host:1433;databaseName=database</span>
<span class=c1># Oracle: jdbc:oracle:thin:@host:1521:database</span>
</code></pre></div> <h3 id=loading-from-delta-lake><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.9</span> Loading from Delta Lake</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>delta.tables</span><span class=w> </span><span class=kn>import</span> <span class=n>DeltaTable</span>

<span class=c1># Read Delta table</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>

<span class=c1># Alternative: Use DeltaTable</span>
<span class=n>deltaTable</span> <span class=o>=</span> <span class=n>DeltaTable</span><span class=o>.</span><span class=n>forPath</span><span class=p>(</span><span class=n>spark</span><span class=p>,</span> <span class=s2>"path/to/delta_table"</span><span class=p>)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>deltaTable</span><span class=o>.</span><span class=n>toDF</span><span class=p>()</span>

<span class=c1># Read specific version (time travel)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"versionAsOf"</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>

<span class=c1># Read at specific timestamp</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"timestampAsOf"</span><span class=p>,</span> <span class=s2>"2023-01-01 00:00:00"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>
</code></pre></div> <h3 id=loading-from-other-sources><span class="enumerate-headings-plugin enumerate-heading-plugin">1.2.10</span> Loading from Other Sources</h3> <div class=highlight><pre><span></span><code><span class=c1># Hive table</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>table</span><span class=p>(</span><span class=s2>"database.tablename"</span><span class=p>)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT * FROM database.tablename"</span><span class=p>)</span>

<span class=c1># From Pandas DataFrame</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=n>pandas_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span><span class=s2>"name"</span><span class=p>:</span> <span class=p>[</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=s2>"Bob"</span><span class=p>],</span> <span class=s2>"age"</span><span class=p>:</span> <span class=p>[</span><span class=mi>25</span><span class=p>,</span> <span class=mi>30</span><span class=p>]})</span>
<span class=n>spark_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>pandas_df</span><span class=p>)</span>

<span class=c1># From Python collections</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[(</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=mi>25</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=mi>30</span><span class=p>)]</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=p>[</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>])</span>

<span class=c1># From RDD</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>([(</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=mi>25</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=mi>30</span><span class=p>)])</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>rdd</span><span class=p>,</span> <span class=p>[</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>])</span>
</code></pre></div> <h2 id=dataframes><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3</span> DataFrames</h2> <h3 id=dataframe-lifecycle><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.1</span> DataFrame Lifecycle</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Data Source  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Load Data  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Transform    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
           ‚îÇ            ‚îÇ
           ‚Üì            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ   Action     ‚îÇ    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
           ‚îÇ            ‚îÇ
           ‚Üì (Trigger)  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ   Execute    ‚îÇ    ‚îÇ
    ‚îÇ  Lazy Eval   ‚îÇ    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
           ‚îÇ            ‚îÇ
           ‚Üì            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ   Result     ‚îÇ    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                        ‚îÇ
           More Transforms? ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=creating-dataframes><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.2</span> Creating DataFrames</h3> <p>From tuple list:</p> <div class=highlight><pre><span></span><code><span class=c1># With column names</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[(</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=mi>30</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=mi>25</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Charlie"</span><span class=p>,</span> <span class=mi>35</span><span class=p>)]</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=p>[</span><span class=s2>"Name"</span><span class=p>,</span> <span class=s2>"Age"</span><span class=p>])</span>

<span class=c1># Inferred schema</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[(</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=mi>30</span><span class=p>,</span> <span class=s2>"Engineer"</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=s2>"Doctor"</span><span class=p>)]</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p>From RDD:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic RDD to DataFrame</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[(</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=mi>30</span><span class=p>),</span> <span class=p>(</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=mi>25</span><span class=p>)]</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>parallelize</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>rdd</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=p>[</span><span class=s2>"Name"</span><span class=p>,</span> <span class=s2>"Age"</span><span class=p>])</span>

<span class=c1># With explicit schema</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StructType</span><span class=p>,</span> <span class=n>StructField</span><span class=p>,</span> <span class=n>StringType</span><span class=p>,</span> <span class=n>IntegerType</span>

<span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"Name"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"Age"</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=p>])</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>rdd</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>schema</span><span class=p>)</span>
</code></pre></div> <p>From list of dictionaries:</p> <div class=highlight><pre><span></span><code><span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
    <span class=p>{</span><span class=s2>"Name"</span><span class=p>:</span> <span class=s2>"Alice"</span><span class=p>,</span> <span class=s2>"Age"</span><span class=p>:</span> <span class=mi>30</span><span class=p>,</span> <span class=s2>"City"</span><span class=p>:</span> <span class=s2>"NYC"</span><span class=p>},</span>
    <span class=p>{</span><span class=s2>"Name"</span><span class=p>:</span> <span class=s2>"Bob"</span><span class=p>,</span> <span class=s2>"Age"</span><span class=p>:</span> <span class=mi>25</span><span class=p>,</span> <span class=s2>"City"</span><span class=p>:</span> <span class=s2>"LA"</span><span class=p>}</span>
<span class=p>]</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</code></pre></div> <p>From Pandas DataFrame:</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>

<span class=c1># Convert Pandas to Spark DataFrame</span>
<span class=n>pandas_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s2>"Name"</span><span class=p>:</span> <span class=p>[</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=s2>"Bob"</span><span class=p>],</span>
    <span class=s2>"Age"</span><span class=p>:</span> <span class=p>[</span><span class=mi>30</span><span class=p>,</span> <span class=mi>25</span><span class=p>]</span>
<span class=p>})</span>
<span class=n>spark_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>pandas_df</span><span class=p>)</span>

<span class=c1># Convert Spark to Pandas DataFrame</span>
<span class=n>pandas_df</span> <span class=o>=</span> <span class=n>spark_df</span><span class=o>.</span><span class=n>toPandas</span><span class=p>()</span>
</code></pre></div> <p>From Row objects:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>Row</span>

<span class=c1># Create Row objects</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
    <span class=n>Row</span><span class=p>(</span><span class=n>Name</span><span class=o>=</span><span class=s2>"Alice"</span><span class=p>,</span> <span class=n>Age</span><span class=o>=</span><span class=mi>30</span><span class=p>),</span>
    <span class=n>Row</span><span class=p>(</span><span class=n>Name</span><span class=o>=</span><span class=s2>"Bob"</span><span class=p>,</span> <span class=n>Age</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>
<span class=p>]</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</code></pre></div> <p>Empty DataFrame with schema:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StructType</span><span class=p>,</span> <span class=n>StructField</span><span class=p>,</span> <span class=n>StringType</span><span class=p>,</span> <span class=n>IntegerType</span>

<span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"Name"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>True</span><span class=p>),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"Age"</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>True</span><span class=p>)</span>
<span class=p>])</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>([],</span> <span class=n>schema</span><span class=p>)</span>

<span class=c1>### DataFrame Operations</span>

<span class=n>Basic</span> <span class=n>inspection</span><span class=p>:</span>

<span class=err>```</span><span class=n>python</span>
<span class=c1># Display rows</span>
<span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>                    <span class=c1># Show first 20 rows</span>
<span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>                   <span class=c1># Show first 5 rows</span>
<span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>      <span class=c1># Show without truncating</span>
<span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>vertical</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>       <span class=c1># Vertical format</span>

<span class=c1># Schema and structure</span>
<span class=n>df</span><span class=o>.</span><span class=n>printSchema</span><span class=p>()</span>             <span class=c1># Print schema tree</span>
<span class=n>df</span><span class=o>.</span><span class=n>schema</span>                    <span class=c1># Get schema object</span>
<span class=n>df</span><span class=o>.</span><span class=n>columns</span>                   <span class=c1># Get column names list</span>
<span class=n>df</span><span class=o>.</span><span class=n>dtypes</span>                    <span class=c1># Get column types [(name, type)]</span>

<span class=c1># Statistics</span>
<span class=n>df</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>                   <span class=c1># Count rows</span>
<span class=n>df</span><span class=o>.</span><span class=n>describe</span><span class=p>()</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>         <span class=c1># Summary statistics</span>
<span class=n>df</span><span class=o>.</span><span class=n>summary</span><span class=p>(</span><span class=s2>"count"</span><span class=p>,</span> <span class=s2>"mean"</span><span class=p>,</span> <span class=s2>"stddev"</span><span class=p>,</span> <span class=s2>"min"</span><span class=p>,</span> <span class=s2>"max"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Sample data</span>
<span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>                   <span class=c1># Return first 5 rows as list</span>
<span class=n>df</span><span class=o>.</span><span class=n>first</span><span class=p>()</span>                   <span class=c1># Return first row</span>
<span class=n>df</span><span class=o>.</span><span class=n>take</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>                   <span class=c1># Return first 5 rows as list</span>
<span class=n>df</span><span class=o>.</span><span class=n>tail</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>                   <span class=c1># Return last 5 rows (use carefully)</span>
</code></pre></div> <p>Column operations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>lit</span>

<span class=c1># Select columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>name</span><span class=p>,</span> <span class=n>df</span><span class=o>.</span><span class=n>age</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>))</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Add new column</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"age_plus_10"</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>+</span> <span class=mi>10</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"full_name"</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>"first_name"</span><span class=p>)</span> <span class=o>+</span> <span class=s2>" "</span> <span class=o>+</span> <span class=n>col</span><span class=p>(</span><span class=s2>"last_name"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"status"</span><span class=p>,</span> <span class=n>lit</span><span class=p>(</span><span class=s2>"active"</span><span class=p>))</span>  <span class=c1># Add constant value</span>

<span class=c1># Rename columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumnRenamed</span><span class=p>(</span><span class=s2>"old_name"</span><span class=p>,</span> <span class=s2>"new_name"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>toDF</span><span class=p>(</span><span class=s2>"new_name1"</span><span class=p>,</span> <span class=s2>"new_name2"</span><span class=p>)</span>  <span class=c1># Rename all columns</span>

<span class=c1># Drop columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>"column1"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>"column1"</span><span class=p>,</span> <span class=s2>"column2"</span><span class=p>)</span>

<span class=c1># Cast column type</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"age"</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=s2>"string"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"price"</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>"price"</span><span class=p>)</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=s2>"double"</span><span class=p>))</span>
</code></pre></div> <p>Filtering and conditions:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>col</span>

<span class=c1># Filter rows</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s2>"age"</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span>

<span class=c1># Multiple conditions</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>((</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span> <span class=o>&amp;</span> <span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"city"</span><span class=p>)</span> <span class=o>==</span> <span class=s2>"NYC"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>((</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>20</span><span class=p>)</span> <span class=o>|</span> <span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>60</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=o>~</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"status"</span><span class=p>)</span> <span class=o>==</span> <span class=s2>"inactive"</span><span class=p>))</span>

<span class=c1># String operations</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>"A"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)</span><span class=o>.</span><span class=n>endswith</span><span class=p>(</span><span class=s2>"son"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=s2>"li"</span><span class=p>))</span>

<span class=c1># Null checks</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>isNull</span><span class=p>())</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>isNotNull</span><span class=p>())</span>

<span class=c1># In operator</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"city"</span><span class=p>)</span><span class=o>.</span><span class=n>isin</span><span class=p>([</span><span class=s2>"NYC"</span><span class=p>,</span> <span class=s2>"LA"</span><span class=p>,</span> <span class=s2>"SF"</span><span class=p>]))</span>
</code></pre></div> <p>Sorting and ordering:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>desc</span><span class=p>,</span> <span class=n>asc</span>

<span class=c1># Sort ascending</span>
<span class=n>df</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>asc</span><span class=p>())</span>

<span class=c1># Sort descending</span>
<span class=n>df</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span>
<span class=n>df</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span>

<span class=c1># Multiple columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>orderBy</span><span class=p>([</span><span class=s2>"age"</span><span class=p>,</span> <span class=s2>"name"</span><span class=p>])</span>
<span class=n>df</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>(),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)</span><span class=o>.</span><span class=n>asc</span><span class=p>())</span>
</code></pre></div> <p>Aggregations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>count</span><span class=p>,</span> <span class=nb>sum</span><span class=p>,</span> <span class=n>avg</span><span class=p>,</span> <span class=nb>min</span><span class=p>,</span> <span class=nb>max</span><span class=p>,</span> <span class=n>countDistinct</span>

<span class=c1># Group by</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>),</span> <span class=nb>max</span><span class=p>(</span><span class=s2>"age"</span><span class=p>))</span>

<span class=c1># Multiple aggregations</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span>
    <span class=n>count</span><span class=p>(</span><span class=s2>"*"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"total_employees"</span><span class=p>),</span>
    <span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"avg_salary"</span><span class=p>),</span>
    <span class=nb>max</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"max_salary"</span><span class=p>),</span>
    <span class=nb>min</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"min_salary"</span><span class=p>)</span>
<span class=p>)</span>

<span class=c1># Without groupBy (global aggregation)</span>
<span class=n>df</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>),</span> <span class=nb>max</span><span class=p>(</span><span class=s2>"age"</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>),</span> <span class=n>count</span><span class=p>(</span><span class=s2>"*"</span><span class=p>))</span>

<span class=c1># Distinct count</span>
<span class=n>df</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>countDistinct</span><span class=p>(</span><span class=s2>"department"</span><span class=p>))</span>
</code></pre></div> <p>Joins:</p> <div class=highlight><pre><span></span><code>    Join Types Flow:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   df1    ‚îÇ        ‚îÇ   df2    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  Join Operation ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ           ‚îÇ           ‚îÇ
        ‚Üì           ‚Üì           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ inner ‚îÇ  ‚îÇ  left  ‚îÇ  ‚îÇright ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ outer  ‚îÇ  ‚îÇouter ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   full   ‚îÇ
              ‚îÇ  outer   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Inner join (default)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=s2>"id"</span><span class=p>)</span>  <span class=c1># If column name is same</span>

<span class=c1># Left outer join</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"left"</span><span class=p>)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"left_outer"</span><span class=p>)</span>

<span class=c1># Right outer join</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"right"</span><span class=p>)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"right_outer"</span><span class=p>)</span>

<span class=c1># Full outer join</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"full"</span><span class=p>)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"full_outer"</span><span class=p>)</span>

<span class=c1># Left semi join (like SQL IN)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"left_semi"</span><span class=p>)</span>

<span class=c1># Left anti join (like SQL NOT IN)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>,</span> <span class=s2>"left_anti"</span><span class=p>)</span>

<span class=c1># Cross join (Cartesian product)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>crossJoin</span><span class=p>(</span><span class=n>df2</span><span class=p>)</span>

<span class=c1># Multiple join conditions</span>
<span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2</span><span class=p>,</span> <span class=p>(</span><span class=n>df1</span><span class=o>.</span><span class=n>id</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>id</span><span class=p>)</span> <span class=o>&amp;</span> <span class=p>(</span><span class=n>df1</span><span class=o>.</span><span class=n>date</span> <span class=o>==</span> <span class=n>df2</span><span class=o>.</span><span class=n>date</span><span class=p>))</span>
</code></pre></div> <p>Set operations:</p> <div class=highlight><pre><span></span><code><span class=c1># Union (combines all rows, includes duplicates)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>union</span><span class=p>(</span><span class=n>df2</span><span class=p>)</span>

<span class=c1># Union by name (matches by column name)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>unionByName</span><span class=p>(</span><span class=n>df2</span><span class=p>)</span>

<span class=c1># Intersection (rows in both)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>intersect</span><span class=p>(</span><span class=n>df2</span><span class=p>)</span>

<span class=c1># Subtract (rows in df1 but not in df2)</span>
<span class=n>df1</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>df2</span><span class=p>)</span>

<span class=c1># Distinct (remove duplicates)</span>
<span class=n>df</span><span class=o>.</span><span class=n>distinct</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>dropDuplicates</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>dropDuplicates</span><span class=p>([</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>])</span>  <span class=c1># Based on specific columns</span>
</code></pre></div> <p>Advanced operations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>StorageLevel</span>

<span class=c1># Sampling</span>
<span class=n>df</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>withReplacement</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>fraction</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>  <span class=c1># 30% sample</span>

<span class=c1># Random split</span>
<span class=n>train_df</span><span class=p>,</span> <span class=n>test_df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>randomSplit</span><span class=p>([</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>],</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Limit</span>
<span class=n>df</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Pivoting</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span><span class=o>.</span><span class=n>pivot</span><span class=p>(</span><span class=s2>"category"</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=s2>"amount"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span><span class=o>.</span><span class=n>pivot</span><span class=p>(</span><span class=s2>"category"</span><span class=p>,</span> <span class=p>[</span><span class=s2>"A"</span><span class=p>,</span> <span class=s2>"B"</span><span class=p>])</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=s2>"amount"</span><span class=p>)</span>

<span class=c1># Rollup (hierarchical aggregation)</span>
<span class=n>df</span><span class=o>.</span><span class=n>rollup</span><span class=p>(</span><span class=s2>"country"</span><span class=p>,</span> <span class=s2>"city"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=s2>"sales"</span><span class=p>))</span>

<span class=c1># Cube (multi-dimensional aggregation)</span>
<span class=n>df</span><span class=o>.</span><span class=n>cube</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"quarter"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=s2>"revenue"</span><span class=p>))</span>

<span class=c1># Caching</span>
<span class=n>df</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>                              <span class=c1># Cache in memory</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_AND_DISK</span><span class=p>)</span>  <span class=c1># Custom storage</span>
<span class=n>df</span><span class=o>.</span><span class=n>unpersist</span><span class=p>()</span>                          <span class=c1># Remove from cache</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>()</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>                    <span class=c1># Cache and trigger computation</span>
</code></pre></div> <p>Handling nulls:</p> <div class=highlight><pre><span></span><code><span class=c1># Drop rows with nulls</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>drop</span><span class=p>()</span>                      <span class=c1># Drop if any null</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>how</span><span class=o>=</span><span class=s2>"all"</span><span class=p>)</span>             <span class=c1># Drop if all nulls</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=p>[</span><span class=s2>"age"</span><span class=p>,</span> <span class=s2>"name"</span><span class=p>])</span> <span class=c1># Drop if null in specific columns</span>

<span class=c1># Fill nulls</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>fill</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>                     <span class=c1># Fill all nulls with 0</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>fill</span><span class=p>({</span><span class=s2>"age"</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>"name"</span><span class=p>:</span> <span class=s2>"Unknown"</span><span class=p>})</span>  <span class=c1># Fill by column</span>
<span class=n>df</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>                      <span class=c1># Alternative syntax</span>

<span class=c1># Replace values</span>
<span class=n>df</span><span class=o>.</span><span class=n>na</span><span class=o>.</span><span class=n>replace</span><span class=p>([</span><span class=s2>"old1"</span><span class=p>,</span> <span class=s2>"old2"</span><span class=p>],</span> <span class=p>[</span><span class=s2>"new1"</span><span class=p>,</span> <span class=s2>"new2"</span><span class=p>])</span>
<span class=n>df</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>to_replace</span><span class=o>=</span><span class=p>{</span><span class=s2>"old"</span><span class=p>:</span> <span class=s2>"new"</span><span class=p>},</span> <span class=n>subset</span><span class=o>=</span><span class=p>[</span><span class=s2>"column"</span><span class=p>])</span>
</code></pre></div> <h3 id=built-in-functions><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.3</span> Built-in Functions</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>col</span><span class=p>,</span> <span class=n>lit</span><span class=p>,</span> <span class=n>when</span><span class=p>,</span> <span class=n>coalesce</span><span class=p>,</span> <span class=n>concat</span><span class=p>,</span> <span class=n>concat_ws</span><span class=p>,</span>
    <span class=n>lower</span><span class=p>,</span> <span class=n>upper</span><span class=p>,</span> <span class=n>trim</span><span class=p>,</span> <span class=n>length</span><span class=p>,</span> <span class=n>substring</span><span class=p>,</span> <span class=n>regexp_replace</span><span class=p>,</span>
    <span class=n>split</span><span class=p>,</span> <span class=n>explode</span><span class=p>,</span> <span class=n>array_contains</span><span class=p>,</span>
    <span class=n>year</span><span class=p>,</span> <span class=n>month</span><span class=p>,</span> <span class=n>dayofmonth</span><span class=p>,</span> <span class=n>current_date</span><span class=p>,</span> <span class=n>current_timestamp</span><span class=p>,</span>
    <span class=n>datediff</span><span class=p>,</span> <span class=n>date_add</span><span class=p>,</span> <span class=n>date_sub</span><span class=p>,</span> <span class=n>to_date</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>,</span>
    <span class=nb>round</span><span class=p>,</span> <span class=n>ceil</span><span class=p>,</span> <span class=n>floor</span><span class=p>,</span> <span class=nb>abs</span><span class=p>,</span> <span class=n>sqrt</span><span class=p>,</span> <span class=nb>pow</span><span class=p>,</span>
    <span class=n>md5</span><span class=p>,</span> <span class=n>sha1</span><span class=p>,</span> <span class=n>sha2</span><span class=p>,</span> <span class=n>base64</span><span class=p>,</span> <span class=n>unbase64</span>
<span class=p>)</span>

<span class=c1># String functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"lower_name"</span><span class=p>,</span> <span class=n>lower</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper_name"</span><span class=p>,</span> <span class=n>upper</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"trimmed"</span><span class=p>,</span> <span class=n>trim</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"name_length"</span><span class=p>,</span> <span class=n>length</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"first_3"</span><span class=p>,</span> <span class=n>substring</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>),</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"cleaned"</span><span class=p>,</span> <span class=n>regexp_replace</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"text"</span><span class=p>),</span> <span class=s2>"[^a-zA-Z]"</span><span class=p>,</span> <span class=s2>""</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"full_name"</span><span class=p>,</span> <span class=n>concat</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"first"</span><span class=p>),</span> <span class=n>lit</span><span class=p>(</span><span class=s2>" "</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"last"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"full_name"</span><span class=p>,</span> <span class=n>concat_ws</span><span class=p>(</span><span class=s2>" "</span><span class=p>,</span> <span class=n>col</span><span class=p>(</span><span class=s2>"first"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"last"</span><span class=p>)))</span>

<span class=c1># Array functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"words"</span><span class=p>,</span> <span class=n>split</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"sentence"</span><span class=p>),</span> <span class=s2>" "</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"word"</span><span class=p>,</span> <span class=n>explode</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"words"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"has_item"</span><span class=p>,</span> <span class=n>array_contains</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"items"</span><span class=p>),</span> <span class=s2>"target"</span><span class=p>))</span>

<span class=c1># Date functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=n>year</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"month"</span><span class=p>,</span> <span class=n>month</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"day"</span><span class=p>,</span> <span class=n>dayofmonth</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"today"</span><span class=p>,</span> <span class=n>current_date</span><span class=p>())</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"now"</span><span class=p>,</span> <span class=n>current_timestamp</span><span class=p>())</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"days_diff"</span><span class=p>,</span> <span class=n>datediff</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"end_date"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"start_date"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"next_week"</span><span class=p>,</span> <span class=n>date_add</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"date"</span><span class=p>),</span> <span class=mi>7</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"last_week"</span><span class=p>,</span> <span class=n>date_sub</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"date"</span><span class=p>),</span> <span class=mi>7</span><span class=p>))</span>

<span class=c1># Math functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"rounded"</span><span class=p>,</span> <span class=nb>round</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>),</span> <span class=mi>2</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"ceiling"</span><span class=p>,</span> <span class=n>ceil</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"floor"</span><span class=p>,</span> <span class=n>floor</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"absolute"</span><span class=p>,</span> <span class=nb>abs</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"square_root"</span><span class=p>,</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)))</span>

<span class=c1># Conditional operations</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"category"</span><span class=p>,</span>
    <span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>18</span><span class=p>,</span> <span class=s2>"minor"</span><span class=p>)</span>
    <span class=o>.</span><span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>65</span><span class=p>,</span> <span class=s2>"adult"</span><span class=p>)</span>
    <span class=o>.</span><span class=n>otherwise</span><span class=p>(</span><span class=s2>"senior"</span><span class=p>)</span>
<span class=p>)</span>

<span class=c1># Null handling</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"filled"</span><span class=p>,</span> <span class=n>coalesce</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"col1"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"col2"</span><span class=p>),</span> <span class=n>lit</span><span class=p>(</span><span class=s2>"default"</span><span class=p>)))</span>
</code></pre></div> <h3 id=user-defined-functions-udfs><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.4</span> User-Defined Functions (UDFs)</h3> <div class=highlight><pre><span></span><code>    UDF Processing Flow:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Python UDF    ‚îÇ
    ‚îÇ  (Slow)       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì Serialization
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  JVM Executor ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì Row by Row
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Result       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Pandas UDF (Faster):

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Pandas UDF    ‚îÇ
    ‚îÇ (Vectorized)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì Apache Arrow
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  JVM Executor ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì Batch Processing
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Result       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Standard Python UDF:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>udf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StringType</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>,</span> <span class=n>DoubleType</span>

<span class=c1># Simple UDF</span>
<span class=k>def</span><span class=w> </span><span class=nf>to_upper</span><span class=p>(</span><span class=n>s</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>s</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>s</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>
    <span class=k>return</span> <span class=kc>None</span>

<span class=n>to_upper_udf</span> <span class=o>=</span> <span class=n>udf</span><span class=p>(</span><span class=n>to_upper</span><span class=p>,</span> <span class=n>StringType</span><span class=p>())</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper_name"</span><span class=p>,</span> <span class=n>to_upper_udf</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>

<span class=c1># UDF with decorator</span>
<span class=nd>@udf</span><span class=p>(</span><span class=n>returnType</span><span class=o>=</span><span class=n>IntegerType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>calculate_age</span><span class=p>(</span><span class=n>birth_year</span><span class=p>):</span>
    <span class=kn>from</span><span class=w> </span><span class=nn>datetime</span><span class=w> </span><span class=kn>import</span> <span class=n>datetime</span>
    <span class=n>current_year</span> <span class=o>=</span> <span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>year</span>
    <span class=k>return</span> <span class=n>current_year</span> <span class=o>-</span> <span class=n>birth_year</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"age"</span><span class=p>,</span> <span class=n>calculate_age</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"birth_year"</span><span class=p>)))</span>

<span class=c1># UDF with multiple arguments</span>
<span class=nd>@udf</span><span class=p>(</span><span class=n>returnType</span><span class=o>=</span><span class=n>DoubleType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>calculate_discount</span><span class=p>(</span><span class=n>price</span><span class=p>,</span> <span class=n>discount_pct</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>price</span> <span class=ow>and</span> <span class=n>discount_pct</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>price</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>discount_pct</span> <span class=o>/</span> <span class=mi>100</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>price</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"final_price"</span><span class=p>,</span> <span class=n>calculate_discount</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"price"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"discount"</span><span class=p>)))</span>
</code></pre></div> <p>Pandas UDF (Vectorized, faster):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>pandas_udf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StringType</span><span class=p>,</span> <span class=n>DoubleType</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>

<span class=c1># Series to Series</span>
<span class=nd>@pandas_udf</span><span class=p>(</span><span class=n>StringType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>to_upper_pandas</span><span class=p>(</span><span class=n>series</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>:</span>
    <span class=k>return</span> <span class=n>series</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper_name"</span><span class=p>,</span> <span class=n>to_upper_pandas</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>

<span class=c1># Multiple columns</span>
<span class=nd>@pandas_udf</span><span class=p>(</span><span class=n>DoubleType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>calculate_discount</span><span class=p>(</span><span class=n>price</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>,</span> <span class=n>discount</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>:</span>
    <span class=k>return</span> <span class=n>price</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>discount</span> <span class=o>/</span> <span class=mi>100</span><span class=p>)</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"final_price"</span><span class=p>,</span> <span class=n>calculate_discount</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"price"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"discount"</span><span class=p>)))</span>

<span class=c1># Iterator of Series (for large data)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>Iterator</span>

<span class=nd>@pandas_udf</span><span class=p>(</span><span class=n>DoubleType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>complex_calculation</span><span class=p>(</span><span class=n>iterator</span><span class=p>:</span> <span class=n>Iterator</span><span class=p>[</span><span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Iterator</span><span class=p>[</span><span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>]:</span>
    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>iterator</span><span class=p>:</span>
        <span class=c1># Process batch</span>
        <span class=k>yield</span> <span class=n>batch</span> <span class=o>*</span> <span class=mi>2</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"doubled"</span><span class=p>,</span> <span class=n>complex_calculation</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)))</span>
</code></pre></div> <p>Best practices for UDFs:</p> <div class=highlight><pre><span></span><code><span class=c1># Register UDF for SQL use</span>
<span class=n>spark</span><span class=o>.</span><span class=n>udf</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=s2>"to_upper_sql"</span><span class=p>,</span> <span class=n>to_upper</span><span class=p>,</span> <span class=n>StringType</span><span class=p>())</span>
<span class=n>df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>"my_table"</span><span class=p>)</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT to_upper_sql(name) FROM my_table"</span><span class=p>)</span>

<span class=c1># Use built-in functions when possible (much faster)</span>
<span class=c1># Instead of UDF for upper:</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper_name"</span><span class=p>,</span> <span class=n>upper</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>  <span class=c1># Preferred</span>

<span class=c1># Cache if using UDF multiple times</span>
<span class=n>df_cached</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>
<span class=n>df_cached</span> <span class=o>=</span> <span class=n>df_cached</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"col1"</span><span class=p>,</span> <span class=n>my_udf</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"x"</span><span class=p>)))</span>
<span class=n>df_cached</span> <span class=o>=</span> <span class=n>df_cached</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"col2"</span><span class=p>,</span> <span class=n>my_udf</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"y"</span><span class=p>)))</span>
</code></pre></div> <h3 id=groupby-operations><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.5</span> GroupBy Operations</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>count</span><span class=p>,</span> <span class=nb>sum</span><span class=p>,</span> <span class=n>avg</span><span class=p>,</span> <span class=nb>max</span><span class=p>,</span> <span class=nb>min</span><span class=p>,</span> <span class=n>mean</span><span class=p>,</span> <span class=n>stddev</span><span class=p>,</span>
    <span class=n>collect_list</span><span class=p>,</span> <span class=n>collect_set</span><span class=p>,</span> <span class=n>countDistinct</span>
<span class=p>)</span>

<span class=c1># Single column grouping</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>

<span class=c1># Multiple column grouping</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>,</span> <span class=s2>"city"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># Multiple aggregations</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span>
    <span class=n>count</span><span class=p>(</span><span class=s2>"*"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"total_employees"</span><span class=p>),</span>
    <span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"avg_salary"</span><span class=p>),</span>
    <span class=nb>max</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"max_salary"</span><span class=p>),</span>
    <span class=nb>min</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"min_salary"</span><span class=p>),</span>
    <span class=nb>sum</span><span class=p>(</span><span class=s2>"bonus"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"total_bonus"</span><span class=p>),</span>
    <span class=n>countDistinct</span><span class=p>(</span><span class=s2>"employee_id"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"unique_employees"</span><span class=p>)</span>
<span class=p>)</span>

<span class=c1># Collect values into list/set</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span>
    <span class=n>collect_list</span><span class=p>(</span><span class=s2>"employee_name"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"employees"</span><span class=p>),</span>
    <span class=n>collect_set</span><span class=p>(</span><span class=s2>"skill"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"unique_skills"</span><span class=p>)</span>
<span class=p>)</span>

<span class=c1># Group and filter</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"avg_salary"</span><span class=p>))</span> \
    <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"avg_salary"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>50000</span><span class=p>)</span>
</code></pre></div> <h3 id=window-functions><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.6</span> Window Functions</h3> <div class=highlight><pre><span></span><code>    Window Function Partitioning:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       Full DataFrame               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
         Partition By
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ            ‚îÇ
    ‚Üì            ‚Üì            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Dept A  ‚îÇ  ‚îÇ Dept B  ‚îÇ  ‚îÇ Dept C  ‚îÇ
‚îÇ Window  ‚îÇ  ‚îÇ Window  ‚îÇ  ‚îÇ Window  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ            ‚îÇ
  Order By     Order By     Order By
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚Üì            ‚Üì            ‚Üì
  Ranking     Ranking     Ranking
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>Window</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>row_number</span><span class=p>,</span> <span class=n>rank</span><span class=p>,</span> <span class=n>dense_rank</span><span class=p>,</span> <span class=n>percent_rank</span><span class=p>,</span>
    <span class=n>lag</span><span class=p>,</span> <span class=n>lead</span><span class=p>,</span> <span class=n>first</span><span class=p>,</span> <span class=n>last</span><span class=p>,</span>
    <span class=nb>sum</span><span class=p>,</span> <span class=n>avg</span><span class=p>,</span> <span class=nb>max</span><span class=p>,</span> <span class=nb>min</span><span class=p>,</span> <span class=n>count</span>
<span class=p>)</span>

<span class=c1># Define window specifications</span>
<span class=c1># Partition by department, order by salary descending</span>
<span class=n>windowSpec</span> <span class=o>=</span> <span class=n>Window</span><span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span>

<span class=c1># Ranking functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"row_num"</span><span class=p>,</span> <span class=n>row_number</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"rank"</span><span class=p>,</span> <span class=n>rank</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"dense_rank"</span><span class=p>,</span> <span class=n>dense_rank</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"percent_rank"</span><span class=p>,</span> <span class=n>percent_rank</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>

<span class=c1># Analytic functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"prev_salary"</span><span class=p>,</span> <span class=n>lag</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"next_salary"</span><span class=p>,</span> <span class=n>lead</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"first_salary"</span><span class=p>,</span> <span class=n>first</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"last_salary"</span><span class=p>,</span> <span class=n>last</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>

<span class=c1># Aggregate window functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"dept_total_salary"</span><span class=p>,</span> <span class=nb>sum</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"dept_avg_salary"</span><span class=p>,</span> <span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"dept_max_salary"</span><span class=p>,</span> <span class=nb>max</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span>

<span class=c1># Running totals</span>
<span class=n>runningTotalWindow</span> <span class=o>=</span> <span class=n>Window</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>rowsBetween</span><span class=p>(</span><span class=n>Window</span><span class=o>.</span><span class=n>unboundedPreceding</span><span class=p>,</span> <span class=n>Window</span><span class=o>.</span><span class=n>currentRow</span><span class=p>)</span>

<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"running_total"</span><span class=p>,</span> <span class=nb>sum</span><span class=p>(</span><span class=s2>"amount"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>runningTotalWindow</span><span class=p>))</span>

<span class=c1># Moving average (last 3 rows)</span>
<span class=n>movingAvgWindow</span> <span class=o>=</span> <span class=n>Window</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>rowsBetween</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>  <span class=c1># 2 rows before and current row</span>

<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"moving_avg"</span><span class=p>,</span> <span class=n>avg</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>movingAvgWindow</span><span class=p>))</span>

<span class=c1># Range-based window (value-based, not row-based)</span>
<span class=n>rangeWindow</span> <span class=o>=</span> <span class=n>Window</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>rangeBetween</span><span class=p>(</span><span class=o>-</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>  <span class=c1># Within 1000 of current salary</span>

<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"peers_count"</span><span class=p>,</span> <span class=n>count</span><span class=p>(</span><span class=s2>"*"</span><span class=p>)</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>rangeWindow</span><span class=p>))</span>

<span class=c1># No partitioning (entire dataset)</span>
<span class=n>globalWindow</span> <span class=o>=</span> <span class=n>Window</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"global_rank"</span><span class=p>,</span> <span class=n>rank</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>globalWindow</span><span class=p>))</span>

<span class=c1># Top N per group using window functions</span>
<span class=n>windowSpec</span> <span class=o>=</span> <span class=n>Window</span><span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"department"</span><span class=p>)</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span>
<span class=n>top_earners</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"rank"</span><span class=p>,</span> <span class=n>rank</span><span class=p>()</span><span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=n>windowSpec</span><span class=p>))</span> \
    <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"rank"</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mi>3</span><span class=p>)</span>
</code></pre></div> <h3 id=sql-queries><span class="enumerate-headings-plugin enumerate-heading-plugin">1.3.7</span> SQL Queries</h3> <p>Register DataFrame as a temporary view:</p> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>"my_table"</span><span class=p>)</span>
</code></pre></div> <p>Run SQL queries:</p> <div class=highlight><pre><span></span><code><span class=n>result_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT Name, Age FROM my_table WHERE Age &gt; 25"</span><span class=p>)</span>
<span class=n>result_df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h2 id=rdds-resilient-distributed-datasets><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4</span> RDDs (Resilient Distributed Datasets)</h2> <h3 id=rdd-vs-dataframe><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.1</span> RDD vs DataFrame</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                RDD                      ‚îÇ
    ‚îÇ  ‚Ä¢ Low-level API                        ‚îÇ
    ‚îÇ  ‚Ä¢ No schema                            ‚îÇ
    ‚îÇ  ‚Ä¢ Type-safe (in Scala)                 ‚îÇ
    ‚îÇ  ‚Ä¢ No optimization                      ‚îÇ
    ‚îÇ  ‚Ä¢ Manual optimization needed           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    VS
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ             DataFrame                   ‚îÇ
    ‚îÇ  ‚Ä¢ High-level API                       ‚îÇ
    ‚îÇ  ‚Ä¢ Schema-based                         ‚îÇ
    ‚îÇ  ‚Ä¢ Catalyst optimizer                   ‚îÇ
    ‚îÇ  ‚Ä¢ Tungsten execution                   ‚îÇ
    ‚îÇ  ‚Ä¢ Better performance                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=creating-rdds><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.2</span> Creating RDDs</h3> <div class=highlight><pre><span></span><code><span class=c1># From Python collection</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>]</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=c1># With specific number of partitions</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>numSlices</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>

<span class=c1># From text file</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"path/to/file.txt"</span><span class=p>)</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>"path/to/directory/*.txt"</span><span class=p>)</span>

<span class=c1># From whole text files (returns filename, content pairs)</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>wholeTextFiles</span><span class=p>(</span><span class=s2>"path/to/directory"</span><span class=p>)</span>

<span class=c1># From sequence file</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>sequenceFile</span><span class=p>(</span><span class=s2>"path/to/file"</span><span class=p>)</span>

<span class=c1># Empty RDD</span>
<span class=n>empty_rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>emptyRDD</span><span class=p>()</span>

<span class=c1># From range</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</code></pre></div> <h3 id=rdd-transformations-lazy><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.3</span> RDD Transformations (Lazy)</h3> <div class=highlight><pre><span></span><code>    Transformation Pipeline:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   RDD    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì map()
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Transformed‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì filter()
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Filtered  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì reduce() (Action)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Execute   ‚îÇ
    ‚îÇ  Pipeline  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Basic transformations:</p> <div class=highlight><pre><span></span><code><span class=c1># Map: Apply function to each element</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>  <span class=c1># Create pairs</span>

<span class=c1># FlatMap: Map and flatten results</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>flatMap</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>split</span><span class=p>())</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>flatMap</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=nb>range</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>

<span class=c1># Filter: Keep elements matching condition</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>&gt;</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>

<span class=c1># MapPartitions: Apply function to each partition</span>
<span class=k>def</span><span class=w> </span><span class=nf>process_partition</span><span class=p>(</span><span class=n>iterator</span><span class=p>):</span>
    <span class=k>yield</span> <span class=nb>sum</span><span class=p>(</span><span class=n>iterator</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>mapPartitions</span><span class=p>(</span><span class=n>process_partition</span><span class=p>)</span>

<span class=c1># MapPartitionsWithIndex: Include partition index</span>
<span class=k>def</span><span class=w> </span><span class=nf>process_with_index</span><span class=p>(</span><span class=n>index</span><span class=p>,</span> <span class=n>iterator</span><span class=p>):</span>
    <span class=k>yield</span> <span class=p>(</span><span class=n>index</span><span class=p>,</span> <span class=nb>sum</span><span class=p>(</span><span class=n>iterator</span><span class=p>))</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>mapPartitionsWithIndex</span><span class=p>(</span><span class=n>process_with_index</span><span class=p>)</span>

<span class=c1># Distinct: Remove duplicates</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>distinct</span><span class=p>()</span>

<span class=c1># Sample: Random sample</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>withReplacement</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>fraction</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Union: Combine RDDs</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>union</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>

<span class=c1># Intersection: Common elements</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>intersection</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>

<span class=c1># Subtract: Elements in rdd1 but not rdd2</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>subtract</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>

<span class=c1># Cartesian: Cartesian product</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>cartesian</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>
</code></pre></div> <p>Sorting and partitioning:</p> <div class=highlight><pre><span></span><code><span class=c1># Sort by value</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>sortBy</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Repartition: Change number of partitions (shuffle)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Coalesce: Reduce partitions (no shuffle)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>coalesce</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Partition by custom function</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=n>numPartitions</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>partitionFunc</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>%</span> <span class=mi>10</span><span class=p>)</span>

<span class=c1># Zip: Combine element-wise</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>zip</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>  <span class=c1># Same partitioning required</span>

<span class=c1># ZipWithIndex: Add indices</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>zipWithIndex</span><span class=p>()</span>

<span class=c1># ZipWithUniqueId: Add unique IDs</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>zipWithUniqueId</span><span class=p>()</span>
</code></pre></div> <h3 id=rdd-actions-trigger-computation><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.4</span> RDD Actions (Trigger Computation)</h3> <div class=highlight><pre><span></span><code><span class=c1># Collect: Return all elements (use carefully!)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>collect</span><span class=p>()</span>

<span class=c1># Count: Number of elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># First: First element</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>first</span><span class=p>()</span>

<span class=c1># Take: First N elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>take</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>

<span class=c1># Top: Top N elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>top</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>

<span class=c1># TakeOrdered: N smallest elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>takeOrdered</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>takeOrdered</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=o>-</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Largest</span>

<span class=c1># TakeSample: Random sample</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>takeSample</span><span class=p>(</span><span class=n>withReplacement</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>num</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Reduce: Aggregate elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>reduce</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>reduce</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>x</span> <span class=k>if</span> <span class=n>x</span> <span class=o>&gt;</span> <span class=n>y</span> <span class=k>else</span> <span class=n>y</span><span class=p>)</span>  <span class=c1># Max</span>

<span class=c1># Fold: Like reduce with initial value</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>fold</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># Aggregate: Different types for accumulator and elements</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>aggregate</span><span class=p>(</span>
    <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=c1># Initial value</span>
    <span class=k>lambda</span> <span class=n>acc</span><span class=p>,</span> <span class=n>value</span><span class=p>:</span> <span class=p>(</span><span class=n>acc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>value</span><span class=p>,</span> <span class=n>acc</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span>  <span class=c1># Seq op</span>
    <span class=k>lambda</span> <span class=n>acc1</span><span class=p>,</span> <span class=n>acc2</span><span class=p>:</span> <span class=p>(</span><span class=n>acc1</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>acc2</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>acc1</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>acc2</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># Comb op</span>
<span class=p>)</span>

<span class=c1># Foreach: Apply function to each element (no return)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>foreach</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=nb>print</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>

<span class=c1># ForeachPartition: Apply function to each partition</span>
<span class=k>def</span><span class=w> </span><span class=nf>print_partition</span><span class=p>(</span><span class=n>iterator</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>iterator</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>item</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>foreachPartition</span><span class=p>(</span><span class=n>print_partition</span><span class=p>)</span>

<span class=c1># CountByValue: Count occurrences of each value</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>countByValue</span><span class=p>()</span>

<span class=c1># SaveAsTextFile: Save to files</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsTextFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># SaveAsSequenceFile: Save as Hadoop SequenceFile</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsSequenceFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <h3 id=pair-rdd-operations><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.5</span> Pair RDD Operations</h3> <div class=highlight><pre><span></span><code><span class=c1># Create pair RDD</span>
<span class=n>words</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>([</span><span class=s2>"hello"</span><span class=p>,</span> <span class=s2>"world"</span><span class=p>,</span> <span class=s2>"hello"</span><span class=p>])</span>
<span class=n>pairs</span> <span class=o>=</span> <span class=n>words</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>

<span class=c1># ReduceByKey: Combine values for each key</span>
<span class=n>word_counts</span> <span class=o>=</span> <span class=n>pairs</span><span class=o>.</span><span class=n>reduceByKey</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># GroupByKey: Group all values for each key (avoid if possible)</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>groupByKey</span><span class=p>()</span><span class=o>.</span><span class=n>mapValues</span><span class=p>(</span><span class=nb>list</span><span class=p>)</span>

<span class=c1># AggregateByKey: Aggregate with initial value</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>aggregateByKey</span><span class=p>(</span>
    <span class=mi>0</span><span class=p>,</span>  <span class=c1># Initial value</span>
    <span class=k>lambda</span> <span class=n>acc</span><span class=p>,</span> <span class=n>value</span><span class=p>:</span> <span class=n>acc</span> <span class=o>+</span> <span class=n>value</span><span class=p>,</span>  <span class=c1># Seq func</span>
    <span class=k>lambda</span> <span class=n>acc1</span><span class=p>,</span> <span class=n>acc2</span><span class=p>:</span> <span class=n>acc1</span> <span class=o>+</span> <span class=n>acc2</span>   <span class=c1># Comb func</span>
<span class=p>)</span>

<span class=c1># FoldByKey: Fold with initial value</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>foldByKey</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># CombineByKey: Most general aggregation</span>
<span class=k>def</span><span class=w> </span><span class=nf>create_combiner</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

<span class=k>def</span><span class=w> </span><span class=nf>merge_value</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=p>(</span><span class=n>acc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>x</span><span class=p>,</span> <span class=n>acc</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>

<span class=k>def</span><span class=w> </span><span class=nf>merge_combiners</span><span class=p>(</span><span class=n>acc1</span><span class=p>,</span> <span class=n>acc2</span><span class=p>):</span>
    <span class=k>return</span> <span class=p>(</span><span class=n>acc1</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>acc2</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>acc1</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>acc2</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>

<span class=n>pairs</span><span class=o>.</span><span class=n>combineByKey</span><span class=p>(</span><span class=n>create_combiner</span><span class=p>,</span> <span class=n>merge_value</span><span class=p>,</span> <span class=n>merge_combiners</span><span class=p>)</span>

<span class=c1># MapValues: Transform only values</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>mapValues</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span>

<span class=c1># FlatMapValues: FlatMap only values</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>flatMapValues</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=nb>range</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>

<span class=c1># Keys: Get all keys</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span>

<span class=c1># Values: Get all values</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>values</span><span class=p>()</span>

<span class=c1># SortByKey: Sort by key</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>sortByKey</span><span class=p>(</span><span class=n>ascending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Join: Inner join</span>
<span class=n>rdd1</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>([(</span><span class=s2>"a"</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=p>(</span><span class=s2>"b"</span><span class=p>,</span> <span class=mi>2</span><span class=p>)])</span>
<span class=n>rdd2</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>([(</span><span class=s2>"a"</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=p>(</span><span class=s2>"c"</span><span class=p>,</span> <span class=mi>4</span><span class=p>)])</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>  <span class=c1># Result: [("a", (1, 3))]</span>

<span class=c1># LeftOuterJoin</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>leftOuterJoin</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>  <span class=c1># Result: [("a", (1, Some(3))), ("b", (2, None))]</span>

<span class=c1># RightOuterJoin</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>rightOuterJoin</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>  <span class=c1># Result: [("a", (Some(1), 3)), ("c", (None, 4))]</span>

<span class=c1># FullOuterJoin</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>fullOuterJoin</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>

<span class=c1># Cogroup: Group from multiple RDDs</span>
<span class=n>rdd1</span><span class=o>.</span><span class=n>cogroup</span><span class=p>(</span><span class=n>rdd2</span><span class=p>)</span>

<span class=c1># Pair RDD actions</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>countByKey</span><span class=p>()</span>        <span class=c1># Count values per key</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>collectAsMap</span><span class=p>()</span>      <span class=c1># Collect as dictionary</span>
<span class=n>pairs</span><span class=o>.</span><span class=n>lookup</span><span class=p>(</span><span class=s2>"hello"</span><span class=p>)</span>     <span class=c1># Get all values for key</span>
</code></pre></div> <h3 id=rdd-persistence><span class="enumerate-headings-plugin enumerate-heading-plugin">1.4.6</span> RDD Persistence</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>StorageLevel</span>

<span class=c1># Cache in memory</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>  <span class=c1># Same as persist(StorageLevel.MEMORY_ONLY)</span>

<span class=c1># Persist with storage level</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_AND_DISK</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_ONLY</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>DISK_ONLY</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_AND_DISK_SER</span><span class=p>)</span>  <span class=c1># Serialized</span>

<span class=c1># Check if persisted</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>is_cached</span>

<span class=c1># Unpersist</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>unpersist</span><span class=p>()</span>

<span class=c1># Get number of partitions</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>getNumPartitions</span><span class=p>()</span>

<span class=c1># Checkpoint (for lineage truncation)</span>
<span class=n>sc</span><span class=o>.</span><span class=n>setCheckpointDir</span><span class=p>(</span><span class=s2>"path/to/checkpoint"</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>checkpoint</span><span class=p>()</span>
</code></pre></div> <h2 id=writing-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5</span> Writing Data</h2> <h3 id=write-modes><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5.1</span> Write Modes</h3> <div class=highlight><pre><span></span><code>    Write Mode Decision Tree:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Target Exists?  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì         ‚Üì
      Yes        No
        ‚îÇ         ‚îÇ
        ‚Üì         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Mode? ‚îÇ  ‚îÇ Create ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                    ‚îÇ
    ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇoverwrite ‚îÇ      ‚îÇ   append   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                    ‚îÇ
    ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Replace  ‚îÇ      ‚îÇ  Add New   ‚îÇ
‚îÇ   All    ‚îÇ      ‚îÇ   Rows     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    ‚Üì error         ‚Üì ignore
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Throw   ‚îÇ   ‚îÇ  Skip if   ‚îÇ
‚îÇ  Error   ‚îÇ   ‚îÇ  Exists    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=writing-dataframes><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5.2</span> Writing DataFrames</h3> <p>CSV files:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With options</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"csv"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"header"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"sep"</span><span class=p>,</span> <span class=s2>","</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"quote"</span><span class=p>,</span> <span class=s1>'"'</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"escape"</span><span class=p>,</span> <span class=s2>"</span><span class=se>\\</span><span class=s2>"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"compression"</span><span class=p>,</span> <span class=s2>"gzip"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dateFormat"</span><span class=p>,</span> <span class=s2>"yyyy-MM-dd"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Single file output</span>
<span class=n>df</span><span class=o>.</span><span class=n>coalesce</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <p>JSON files:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>json</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With options</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"json"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"compression"</span><span class=p>,</span> <span class=s2>"gzip"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dateFormat"</span><span class=p>,</span> <span class=s2>"yyyy-MM-dd"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <p>Parquet files (recommended for Spark):</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With compression</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"parquet"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"compression"</span><span class=p>,</span> <span class=s2>"snappy"</span><span class=p>)</span>  <span class=c1># gzip, snappy, lzo, none</span>
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Partitioned write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With bucketing</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>bucketBy</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=s2>"user_id"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>sortBy</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"user_events"</span><span class=p>)</span>
</code></pre></div> <p>ORC files:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>orc</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With compression</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"orc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"compression"</span><span class=p>,</span> <span class=s2>"snappy"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <p>Avro files:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"avro"</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># With compression</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"avro"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"compression"</span><span class=p>,</span> <span class=s2>"snappy"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>
</code></pre></div> <p>JDBC database:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/mydatabase"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"mytable"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"driver"</span><span class=p>,</span> <span class=s2>"org.postgresql.Driver"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>()</span>

<span class=c1># With batching for better performance</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/mydatabase"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"mytable"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"batchsize"</span><span class=p>,</span> <span class=s2>"10000"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"isolationLevel"</span><span class=p>,</span> <span class=s2>"NONE"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>()</span>

<span class=c1># Truncate and insert</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/mydatabase"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"mytable"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"user"</span><span class=p>,</span> <span class=s2>"myuser"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"password"</span><span class=p>,</span> <span class=s2>"mypassword"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"truncate"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>()</span>
</code></pre></div> <p>Delta Lake:</p> <div class=highlight><pre><span></span><code><span class=c1># Basic write</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>

<span class=c1># Append mode</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>

<span class=c1># With partitioning</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"date"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/delta_table"</span><span class=p>)</span>

<span class=c1># Write as managed table</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"delta"</span><span class=p>)</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"my_delta_table"</span><span class=p>)</span>

<span class=c1># Merge (upsert) operation</span>
<span class=kn>from</span><span class=w> </span><span class=nn>delta.tables</span><span class=w> </span><span class=kn>import</span> <span class=n>DeltaTable</span>

<span class=n>deltaTable</span> <span class=o>=</span> <span class=n>DeltaTable</span><span class=o>.</span><span class=n>forPath</span><span class=p>(</span><span class=n>spark</span><span class=p>,</span> <span class=s2>"path/to/delta_table"</span><span class=p>)</span>
<span class=n>deltaTable</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"target"</span><span class=p>)</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span>
    <span class=n>df</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"source"</span><span class=p>),</span>
    <span class=s2>"target.id = source.id"</span>
<span class=p>)</span><span class=o>.</span><span class=n>whenMatchedUpdate</span><span class=p>(</span><span class=nb>set</span><span class=o>=</span><span class=p>{</span>
    <span class=s2>"value"</span><span class=p>:</span> <span class=s2>"source.value"</span><span class=p>,</span>
    <span class=s2>"updated_at"</span><span class=p>:</span> <span class=s2>"source.updated_at"</span>
<span class=p>})</span><span class=o>.</span><span class=n>whenNotMatchedInsert</span><span class=p>(</span><span class=n>values</span><span class=o>=</span><span class=p>{</span>
    <span class=s2>"id"</span><span class=p>:</span> <span class=s2>"source.id"</span><span class=p>,</span>
    <span class=s2>"value"</span><span class=p>:</span> <span class=s2>"source.value"</span><span class=p>,</span>
    <span class=s2>"updated_at"</span><span class=p>:</span> <span class=s2>"source.updated_at"</span>
<span class=p>})</span><span class=o>.</span><span class=n>execute</span><span class=p>()</span>
</code></pre></div> <p>Hive tables:</p> <div class=highlight><pre><span></span><code><span class=c1># Save as managed table</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"database.tablename"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># Save as external table</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"path"</span><span class=p>,</span> <span class=s2>"hdfs://path/to/data"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"database.tablename"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># Partitioned table</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"database.tablename"</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>"overwrite"</span><span class=p>)</span>

<span class=c1># Insert into existing table</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>insertInto</span><span class=p>(</span><span class=s2>"database.tablename"</span><span class=p>,</span> <span class=n>overwrite</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <h3 id=write-modes_1><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5.3</span> Write Modes</h3> <div class=highlight><pre><span></span><code><span class=c1># Overwrite: Replace existing data</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"overwrite"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Append: Add to existing data</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Error (default): Throw error if exists</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"error"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"errorifexists"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Ignore: Skip if exists</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"ignore"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <h3 id=writing-rdds><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5.4</span> Writing RDDs</h3> <div class=highlight><pre><span></span><code><span class=c1># Save as text file</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsTextFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># With compression</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsTextFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>,</span> <span class=n>compressionCodecClass</span><span class=o>=</span><span class=s2>"org.apache.hadoop.io.compress.GzipCodec"</span><span class=p>)</span>

<span class=c1># Save as pickle file</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsPickleFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Save as sequence file (Hadoop format)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsSequenceFile</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Save as Hadoop file with custom format</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>saveAsHadoopFile</span><span class=p>(</span>
    <span class=s2>"path/to/output"</span><span class=p>,</span>
    <span class=s2>"org.apache.hadoop.mapred.TextOutputFormat"</span>
<span class=p>)</span>

<span class=c1># Convert to DataFrame and write</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>toDF</span><span class=p>([</span><span class=s2>"column1"</span><span class=p>,</span> <span class=s2>"column2"</span><span class=p>])</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <h3 id=partitioning-strategies><span class="enumerate-headings-plugin enumerate-heading-plugin">1.5.5</span> Partitioning Strategies</h3> <div class=highlight><pre><span></span><code><span class=c1># Partition by column values</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>,</span> <span class=s2>"day"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Control number of output files</span>
<span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Single output file (use carefully!)</span>
<span class=n>df</span><span class=o>.</span><span class=n>coalesce</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>

<span class=c1># Partition and control file size</span>
<span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"path/to/output"</span><span class=p>)</span>
</code></pre></div> <h2 id=spark-sql><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6</span> Spark SQL</h2> <h3 id=creating-tables><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.1</span> Creating Tables</h3> <p>From DataFrame:</p> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>"my_table"</span><span class=p>)</span>
</code></pre></div> <p>Using SQL:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"CREATE TABLE my_table (name STRING, age INT) USING parquet"</span><span class=p>)</span>
</code></pre></div> <h3 id=inserting-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.2</span> Inserting Data</h3> <p>From DataFrame:</p> <div class=highlight><pre><span></span><code><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>insertInto</span><span class=p>(</span><span class=s2>"my_table"</span><span class=p>)</span>
</code></pre></div> <p>Using SQL:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"INSERT INTO my_table VALUES ('Alice', 30)"</span><span class=p>)</span>
</code></pre></div> <h3 id=selecting-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.3</span> Selecting Data</h3> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT * FROM my_table"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=filtering-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.4</span> Filtering Data</h3> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT * FROM my_table WHERE age &gt; 25"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=aggregating-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.5</span> Aggregating Data</h3> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT name, AVG(age) FROM my_table GROUP BY name"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=joining-tables><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.6</span> Joining Tables</h3> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT * FROM table1 JOIN table2 ON table1.key = table2.key"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=window-functions-in-sql><span class="enumerate-headings-plugin enumerate-heading-plugin">1.6.7</span> Window Functions in SQL</h3> <div class=highlight><pre><span></span><code><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"""</span>
<span class=s2>SELECT</span>
<span class=s2>    name,</span>
<span class=s2>    age,</span>
<span class=s2>    RANK() OVER (ORDER BY age DESC) as age_rank</span>
<span class=s2>FROM</span>
<span class=s2>    my_table</span>
<span class=s2>"""</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h2 id=spark-mllib><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7</span> Spark MLlib</h2> <h3 id=ml-pipeline-flow><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.1</span> ML Pipeline Flow</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Raw Data    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ StringIndexer ‚îÇ  (Categorical ‚Üí Numeric)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇVectorAssembler‚îÇ  (Combine Features)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ StandardScaler‚îÇ  (Normalize)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Train/Test   ‚îÇ
    ‚îÇ     Split     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì       ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇTrain ‚îÇ ‚îÇ Test ‚îÇ
    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
       ‚îÇ         ‚îÇ
       ‚Üì         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇModel ‚îÇ    ‚îÇ
    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
       ‚îÇ        ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚Üí Predictions
</code></pre></div> <h3 id=data-preparation><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.2</span> Data Preparation</h3> <p>Feature transformers:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.feature</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>StringIndexer</span><span class=p>,</span> <span class=n>VectorAssembler</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>,</span>
    <span class=n>MinMaxScaler</span><span class=p>,</span> <span class=n>Normalizer</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>,</span>
    <span class=n>Tokenizer</span><span class=p>,</span> <span class=n>StopWordsRemover</span><span class=p>,</span> <span class=n>CountVectorizer</span><span class=p>,</span>
    <span class=n>IDF</span><span class=p>,</span> <span class=n>PCA</span><span class=p>,</span> <span class=n>Imputer</span>
<span class=p>)</span>

<span class=c1># StringIndexer: Convert categories to indices</span>
<span class=n>indexer</span> <span class=o>=</span> <span class=n>StringIndexer</span><span class=p>(</span><span class=n>inputCol</span><span class=o>=</span><span class=s2>"category"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"categoryIndex"</span><span class=p>)</span>
<span class=n>indexed_df</span> <span class=o>=</span> <span class=n>indexer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># Handle unseen labels</span>
<span class=n>indexer</span> <span class=o>=</span> <span class=n>StringIndexer</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"category"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"categoryIndex"</span><span class=p>,</span>
    <span class=n>handleInvalid</span><span class=o>=</span><span class=s2>"keep"</span>  <span class=c1># "skip" or "error"</span>
<span class=p>)</span>

<span class=c1># OneHotEncoder: Convert indices to binary vectors</span>
<span class=n>encoder</span> <span class=o>=</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>inputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"categoryIndex"</span><span class=p>],</span> <span class=n>outputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"categoryVec"</span><span class=p>])</span>
<span class=n>encoded_df</span> <span class=o>=</span> <span class=n>encoder</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>indexed_df</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>indexed_df</span><span class=p>)</span>

<span class=c1># VectorAssembler: Combine features into vector</span>
<span class=n>assembler</span> <span class=o>=</span> <span class=n>VectorAssembler</span><span class=p>(</span>
    <span class=n>inputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"feature1"</span><span class=p>,</span> <span class=s2>"feature2"</span><span class=p>,</span> <span class=s2>"feature3"</span><span class=p>],</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>handleInvalid</span><span class=o>=</span><span class=s2>"skip"</span>  <span class=c1># "keep" or "error"</span>
<span class=p>)</span>
<span class=n>output_df</span> <span class=o>=</span> <span class=n>assembler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># Imputer: Fill missing values</span>
<span class=n>imputer</span> <span class=o>=</span> <span class=n>Imputer</span><span class=p>(</span>
    <span class=n>inputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"age"</span><span class=p>,</span> <span class=s2>"income"</span><span class=p>],</span>
    <span class=n>outputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"age_imputed"</span><span class=p>,</span> <span class=s2>"income_imputed"</span><span class=p>],</span>
    <span class=n>strategy</span><span class=o>=</span><span class=s2>"mean"</span>  <span class=c1># "median" or "mode"</span>
<span class=p>)</span>
<span class=n>imputer_model</span> <span class=o>=</span> <span class=n>imputer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
<span class=n>imputed_df</span> <span class=o>=</span> <span class=n>imputer_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># StandardScaler: Standardize features (mean=0, std=1)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"scaledFeatures"</span><span class=p>,</span>
    <span class=n>withStd</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>withMean</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>
<span class=n>scaler_model</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
<span class=n>scaled_df</span> <span class=o>=</span> <span class=n>scaler_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># MinMaxScaler: Scale to range [0, 1]</span>
<span class=n>min_max_scaler</span> <span class=o>=</span> <span class=n>MinMaxScaler</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"scaledFeatures"</span><span class=p>,</span>
    <span class=nb>min</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span>
    <span class=nb>max</span><span class=o>=</span><span class=mf>1.0</span>
<span class=p>)</span>
<span class=n>min_max_model</span> <span class=o>=</span> <span class=n>min_max_scaler</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
<span class=n>scaled_df</span> <span class=o>=</span> <span class=n>min_max_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># Normalizer: Normalize to unit norm</span>
<span class=n>normalizer</span> <span class=o>=</span> <span class=n>Normalizer</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"normFeatures"</span><span class=p>,</span>
    <span class=n>p</span><span class=o>=</span><span class=mf>2.0</span>  <span class=c1># L2 norm (p=1.0 for L1)</span>
<span class=p>)</span>
<span class=n>normalized_df</span> <span class=o>=</span> <span class=n>normalizer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># PCA: Dimensionality reduction</span>
<span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>  <span class=c1># Number of principal components</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"pcaFeatures"</span>
<span class=p>)</span>
<span class=n>pca_model</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
<span class=n>pca_df</span> <span class=o>=</span> <span class=n>pca_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</code></pre></div> <p>Text processing:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.feature</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>Tokenizer</span><span class=p>,</span> <span class=n>RegexTokenizer</span><span class=p>,</span> <span class=n>StopWordsRemover</span><span class=p>,</span>
    <span class=n>CountVectorizer</span><span class=p>,</span> <span class=n>HashingTF</span><span class=p>,</span> <span class=n>IDF</span><span class=p>,</span> <span class=n>Word2Vec</span><span class=p>,</span> <span class=n>NGram</span>
<span class=p>)</span>

<span class=c1># Tokenizer: Split text into words</span>
<span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>inputCol</span><span class=o>=</span><span class=s2>"text"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"words"</span><span class=p>)</span>
<span class=n>tokenized_df</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

<span class=c1># RegexTokenizer: Tokenize with regex</span>
<span class=n>regex_tokenizer</span> <span class=o>=</span> <span class=n>RegexTokenizer</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"text"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"words"</span><span class=p>,</span>
    <span class=n>pattern</span><span class=o>=</span><span class=s2>"</span><span class=se>\\</span><span class=s2>W"</span>  <span class=c1># Split on non-word characters</span>
<span class=p>)</span>

<span class=c1># StopWordsRemover: Remove common words</span>
<span class=n>remover</span> <span class=o>=</span> <span class=n>StopWordsRemover</span><span class=p>(</span><span class=n>inputCol</span><span class=o>=</span><span class=s2>"words"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"filtered"</span><span class=p>)</span>
<span class=n>removed_df</span> <span class=o>=</span> <span class=n>remover</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>tokenized_df</span><span class=p>)</span>

<span class=c1># CountVectorizer: Bag of words</span>
<span class=n>cv</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"filtered"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"rawFeatures"</span><span class=p>,</span>
    <span class=n>vocabSize</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span>
    <span class=n>minDF</span><span class=o>=</span><span class=mf>2.0</span>  <span class=c1># Minimum document frequency</span>
<span class=p>)</span>
<span class=n>cv_model</span> <span class=o>=</span> <span class=n>cv</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>removed_df</span><span class=p>)</span>
<span class=n>cv_df</span> <span class=o>=</span> <span class=n>cv_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>removed_df</span><span class=p>)</span>

<span class=c1># TF-IDF: Term frequency - inverse document frequency</span>
<span class=c1># First HashingTF</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.feature</span><span class=w> </span><span class=kn>import</span> <span class=n>HashingTF</span>
<span class=n>hashing_tf</span> <span class=o>=</span> <span class=n>HashingTF</span><span class=p>(</span><span class=n>inputCol</span><span class=o>=</span><span class=s2>"filtered"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"rawFeatures"</span><span class=p>,</span> <span class=n>numFeatures</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
<span class=n>tf_df</span> <span class=o>=</span> <span class=n>hashing_tf</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>removed_df</span><span class=p>)</span>

<span class=c1># Then IDF</span>
<span class=n>idf</span> <span class=o>=</span> <span class=n>IDF</span><span class=p>(</span><span class=n>inputCol</span><span class=o>=</span><span class=s2>"rawFeatures"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>)</span>
<span class=n>idf_model</span> <span class=o>=</span> <span class=n>idf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tf_df</span><span class=p>)</span>
<span class=n>tfidf_df</span> <span class=o>=</span> <span class=n>idf_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>tf_df</span><span class=p>)</span>

<span class=c1># Word2Vec: Word embeddings</span>
<span class=n>word2vec</span> <span class=o>=</span> <span class=n>Word2Vec</span><span class=p>(</span>
    <span class=n>vectorSize</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>minCount</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"filtered"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"features"</span>
<span class=p>)</span>
<span class=n>w2v_model</span> <span class=o>=</span> <span class=n>word2vec</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>removed_df</span><span class=p>)</span>
<span class=n>w2v_df</span> <span class=o>=</span> <span class=n>w2v_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>removed_df</span><span class=p>)</span>

<span class=c1># NGram: Generate n-grams</span>
<span class=n>ngram</span> <span class=o>=</span> <span class=n>NGram</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>inputCol</span><span class=o>=</span><span class=s2>"words"</span><span class=p>,</span> <span class=n>outputCol</span><span class=o>=</span><span class=s2>"ngrams"</span><span class=p>)</span>
<span class=n>ngram_df</span> <span class=o>=</span> <span class=n>ngram</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>tokenized_df</span><span class=p>)</span>
</code></pre></div> <h3 id=feature-extraction><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.3</span> Feature Extraction</h3> <ul> <li><code>Tokenizer</code>: Splits strings into words.</li> <li><code>StopWordsRemover</code>: Removes stop words.</li> <li><code>CountVectorizer</code>: Converts text documents to vectors of term counts.</li> <li><code>IDF</code>: Computes Inverse Document Frequency.</li> <li><code>Word2Vec</code>: Learns vector representations of words.</li> <li><code>NGram</code>: Generates n-grams from input sequences.</li> </ul> <h3 id=feature-scaling><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.4</span> Feature Scaling</h3> <ul> <li><code>StandardScaler</code>: Standardizes features by removing the mean and scaling to unit variance.</li> <li><code>MinMaxScaler</code>: Transforms features by scaling each feature to a given range.</li> <li><code>MaxAbsScaler</code>: Scales each feature to the [-1, 1] range by dividing through the largest maximum absolute value in each feature.</li> <li><code>Normalizer</code>: Normalizes each sample to unit norm.</li> </ul> <h3 id=feature-selection><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.5</span> Feature Selection</h3> <ul> <li><code>VectorSlicer</code>: Creates a new feature vector by selecting a subset of features from an existing vector.</li> <li><code>RFormula</code>: Implements the R formula string syntax for selecting features.</li> <li><code>PCA</code>: Reduces the dimensionality of feature vectors using Principal Component Analysis.</li> </ul> <h3 id=classification><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.6</span> Classification</h3> <div class=highlight><pre><span></span><code>    Classification Model Selection:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Problem Type?   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 ‚îÇ
    ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Binary  ‚îÇ      ‚îÇ  Multi  ‚îÇ
‚îÇ  Class  ‚îÇ      ‚îÇ  Class  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                ‚îÇ
     ‚Üì                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Linear Separable?        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì          ‚Üì
  Yes         No
    ‚îÇ          ‚îÇ
    ‚Üì          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇLogistic‚îÇ  ‚îÇRandom    ‚îÇ
‚îÇRegress ‚îÇ  ‚îÇForest/   ‚îÇ
‚îÇ        ‚îÇ  ‚îÇGBT/SVM   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Logistic Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=c1># Binary classification</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>regParam</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>        <span class=c1># Regularization parameter</span>
    <span class=n>elasticNetParam</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span>  <span class=c1># 0=L2, 1=L1, 0.5=mix</span>
    <span class=n>threshold</span><span class=o>=</span><span class=mf>0.5</span>         <span class=c1># Classification threshold</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Multi-class classification</span>
<span class=n>lr_multi</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>family</span><span class=o>=</span><span class=s2>"multinomial"</span>
<span class=p>)</span>

<span class=c1># Access model parameters</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Coefficients: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>coefficients</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Intercept: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>intercept</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Summary</span>
<span class=n>training_summary</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>summary</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Accuracy: </span><span class=si>{</span><span class=n>training_summary</span><span class=o>.</span><span class=n>accuracy</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Area under ROC: </span><span class=si>{</span><span class=n>training_summary</span><span class=o>.</span><span class=n>areaUnderROC</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Decision Tree:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>

<span class=n>dt</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>           <span class=c1># Maximum depth</span>
    <span class=n>maxBins</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>           <span class=c1># Number of bins for continuous features</span>
    <span class=n>impurity</span><span class=o>=</span><span class=s2>"gini"</span><span class=p>,</span>      <span class=c1># "gini" or "entropy"</span>
    <span class=n>minInstancesPerNode</span><span class=o>=</span><span class=mi>1</span> <span class=c1># Minimum instances per node</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>dt</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Tree structure</span>
<span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>toDebugString</span><span class=p>)</span>
</code></pre></div> <p>Random Forest:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>numTrees</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>         <span class=c1># Number of trees</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>maxBins</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
    <span class=n>subsamplingRate</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>  <span class=c1># Fraction of training data</span>
    <span class=n>featureSubsetStrategy</span><span class=o>=</span><span class=s2>"auto"</span><span class=p>,</span>  <span class=c1># "all", "sqrt", "log2", "onethird"</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>rf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Access individual trees</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Number of trees: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>numTrees</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Total nodes: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>totalNumNodes</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Gradient-Boosted Trees (GBT):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>GBTClassifier</span>

<span class=n>gbt</span> <span class=o>=</span> <span class=n>GBTClassifier</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>           <span class=c1># Number of boosting iterations</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>stepSize</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>         <span class=c1># Learning rate</span>
    <span class=n>subsamplingRate</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>
    <span class=n>featureSubsetStrategy</span><span class=o>=</span><span class=s2>"all"</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>gbt</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Naive Bayes:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>NaiveBayes</span>

<span class=n>nb</span> <span class=o>=</span> <span class=n>NaiveBayes</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>smoothing</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>        <span class=c1># Laplace smoothing</span>
    <span class=n>modelType</span><span class=o>=</span><span class=s2>"multinomial"</span>  <span class=c1># "multinomial", "bernoulli", "gaussian"</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>nb</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <p>Linear Support Vector Machine:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearSVC</span>

<span class=n>svm</span> <span class=o>=</span> <span class=n>LinearSVC</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>regParam</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
    <span class=n>threshold</span><span class=o>=</span><span class=mf>0.0</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>svm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <p>Multilayer Perceptron:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>MultilayerPerceptronClassifier</span>

<span class=c1># Define network architecture</span>
<span class=c1># [input_size, hidden_layer1, hidden_layer2, ..., output_size]</span>
<span class=n>layers</span> <span class=o>=</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>  <span class=c1># 10 features, 2 hidden layers, 3 classes</span>

<span class=n>mlp</span> <span class=o>=</span> <span class=n>MultilayerPerceptronClassifier</span><span class=p>(</span>
    <span class=n>layers</span><span class=o>=</span><span class=n>layers</span><span class=p>,</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>blockSize</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>        <span class=c1># Block size for stacking input data</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>mlp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <h3 id=regression><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.7</span> Regression</h3> <p>Linear Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span>

<span class=n>lr</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>regParam</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>        <span class=c1># L2 regularization</span>
    <span class=n>elasticNetParam</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span>  <span class=c1># 0=L2 (Ridge), 1=L1 (Lasso)</span>
    <span class=n>standardization</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>fitIntercept</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Model statistics</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Coefficients: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>coefficients</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Intercept: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>intercept</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"RMSE: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>rootMeanSquaredError</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"R2: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>r2</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Mean Absolute Error: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>meanAbsoluteError</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Decision Tree Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeRegressor</span>

<span class=n>dt</span> <span class=o>=</span> <span class=n>DecisionTreeRegressor</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>maxBins</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
    <span class=n>minInstancesPerNode</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
    <span class=n>varianceCol</span><span class=o>=</span><span class=s2>"variance"</span>  <span class=c1># Output variance column</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>dt</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Random Forest Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestRegressor</span>

<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>numTrees</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>maxBins</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
    <span class=n>subsamplingRate</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>
    <span class=n>featureSubsetStrategy</span><span class=o>=</span><span class=s2>"auto"</span><span class=p>,</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>rf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Gradient-Boosted Trees Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>GBTRegressor</span>

<span class=n>gbt</span> <span class=o>=</span> <span class=n>GBTRegressor</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
    <span class=n>maxDepth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>stepSize</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>lossType</span><span class=o>=</span><span class=s2>"squared"</span><span class=p>,</span>   <span class=c1># "squared", "absolute"</span>
    <span class=n>subsamplingRate</span><span class=o>=</span><span class=mf>1.0</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>gbt</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Feature importances: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>featureImportances</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Generalized Linear Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>GeneralizedLinearRegression</span>

<span class=n>glr</span> <span class=o>=</span> <span class=n>GeneralizedLinearRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>family</span><span class=o>=</span><span class=s2>"gaussian"</span><span class=p>,</span>     <span class=c1># "gaussian", "binomial", "poisson", "gamma"</span>
    <span class=n>link</span><span class=o>=</span><span class=s2>"identity"</span><span class=p>,</span>       <span class=c1># Link function</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>regParam</span><span class=o>=</span><span class=mf>0.01</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>glr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <p>Isotonic Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.regression</span><span class=w> </span><span class=kn>import</span> <span class=n>IsotonicRegression</span>

<span class=n>iso</span> <span class=o>=</span> <span class=n>IsotonicRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>isotonic</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># True for ascending, False for descending</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>iso</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <h3 id=clustering><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.8</span> Clustering</h3> <p>K-Means:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.clustering</span><span class=w> </span><span class=kn>import</span> <span class=n>KMeans</span>

<span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"cluster"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>                  <span class=c1># Number of clusters</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>initMode</span><span class=o>=</span><span class=s2>"k-means||"</span>  <span class=c1># "k-means||" or "random"</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=c1># Cluster centers</span>
<span class=n>centers</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>clusterCenters</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Cluster Centers: </span><span class=si>{</span><span class=n>centers</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Within set sum of squared errors</span>
<span class=n>wssse</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>trainingCost</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"WSSSE: </span><span class=si>{</span><span class=n>wssse</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Cluster sizes</span>
<span class=n>cluster_sizes</span> <span class=o>=</span> <span class=n>predictions</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"cluster"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p>Bisecting K-Means:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.clustering</span><span class=w> </span><span class=kn>import</span> <span class=n>BisectingKMeans</span>

<span class=n>bkm</span> <span class=o>=</span> <span class=n>BisectingKMeans</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
    <span class=n>minDivisibleClusterSize</span><span class=o>=</span><span class=mf>1.0</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>bkm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</code></pre></div> <p>Gaussian Mixture Model:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.clustering</span><span class=w> </span><span class=kn>import</span> <span class=n>GaussianMixture</span>

<span class=n>gmm</span> <span class=o>=</span> <span class=n>GaussianMixture</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>                  <span class=c1># Number of mixture components</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>probabilityCol</span><span class=o>=</span><span class=s2>"probability"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"cluster"</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>gmm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=c1># Mixture weights</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Weights: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>weights</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Gaussian parameters</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Gaussians: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>gaussiansDF</span><span class=o>.</span><span class=n>show</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Latent Dirichlet Allocation (LDA):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.clustering</span><span class=w> </span><span class=kn>import</span> <span class=n>LDA</span>

<span class=n>lda</span> <span class=o>=</span> <span class=n>LDA</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>                 <span class=c1># Number of topics</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>optimizer</span><span class=o>=</span><span class=s2>"online"</span><span class=p>,</span>   <span class=c1># "online" or "em"</span>
    <span class=n>learningOffset</span><span class=o>=</span><span class=mf>1024.0</span><span class=p>,</span>
    <span class=n>learningDecay</span><span class=o>=</span><span class=mf>0.51</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>lda</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=c1># Topics</span>
<span class=n>topics</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>describeTopics</span><span class=p>(</span><span class=n>maxTermsPerTopic</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>topics</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Transform documents</span>
<span class=n>transformed</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</code></pre></div> <h3 id=recommendation><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.9</span> Recommendation</h3> <div class=highlight><pre><span></span><code>    Collaborative Filtering Flow:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ User-Item Matrix   ‚îÇ
    ‚îÇ  (Sparse Ratings)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Matrix Factorization‚îÇ
    ‚îÇ        (ALS)          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ        ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚Üì                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User   ‚îÇ      ‚îÇ   Item   ‚îÇ
‚îÇ Factors  ‚îÇ      ‚îÇ Factors  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì (Dot Product)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Predicted Ratings   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Alternating Least Squares (ALS):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.recommendation</span><span class=w> </span><span class=kn>import</span> <span class=n>ALS</span>

<span class=n>als</span> <span class=o>=</span> <span class=n>ALS</span><span class=p>(</span>
    <span class=n>userCol</span><span class=o>=</span><span class=s2>"userId"</span><span class=p>,</span>
    <span class=n>itemCol</span><span class=o>=</span><span class=s2>"movieId"</span><span class=p>,</span>
    <span class=n>ratingCol</span><span class=o>=</span><span class=s2>"rating"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>regParam</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>        <span class=c1># Regularization parameter</span>
    <span class=n>rank</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>              <span class=c1># Number of latent factors</span>
    <span class=n>coldStartStrategy</span><span class=o>=</span><span class=s2>"drop"</span><span class=p>,</span>  <span class=c1># "drop" or "nan"</span>
    <span class=n>nonnegative</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>    <span class=c1># Force non-negative factors</span>
    <span class=n>implicitPrefs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1># True for implicit feedback</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span>             <span class=c1># Confidence parameter (implicit)</span>
<span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>als</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Get user factors</span>
<span class=n>user_factors</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>userFactors</span>
<span class=n>user_factors</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Get item factors</span>
<span class=n>item_factors</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>itemFactors</span>
<span class=n>item_factors</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Recommend top N items for all users</span>
<span class=n>user_recs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>recommendForAllUsers</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
<span class=n>user_recs</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Recommend top N users for all items</span>
<span class=n>item_recs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>recommendForAllItems</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Recommend for specific users</span>
<span class=n>specific_users</span> <span class=o>=</span> <span class=n>training_data</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"userId"</span><span class=p>)</span><span class=o>.</span><span class=n>distinct</span><span class=p>()</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
<span class=n>user_subset_recs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>recommendForUserSubset</span><span class=p>(</span><span class=n>specific_users</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

<span class=c1># Handle cold start</span>
<span class=n>predictions_with_nan</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>setColdStartStrategy</span><span class=p>(</span><span class=s2>"nan"</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <h3 id=evaluation><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.10</span> Evaluation</h3> <p>Binary Classification:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>BinaryClassificationEvaluator</span>

<span class=c1># ROC-AUC</span>
<span class=n>evaluator_roc</span> <span class=o>=</span> <span class=n>BinaryClassificationEvaluator</span><span class=p>(</span>
    <span class=n>rawPredictionCol</span><span class=o>=</span><span class=s2>"rawPrediction"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"areaUnderROC"</span>
<span class=p>)</span>
<span class=n>auc</span> <span class=o>=</span> <span class=n>evaluator_roc</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Area Under ROC: </span><span class=si>{</span><span class=n>auc</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># PR-AUC</span>
<span class=n>evaluator_pr</span> <span class=o>=</span> <span class=n>BinaryClassificationEvaluator</span><span class=p>(</span>
    <span class=n>rawPredictionCol</span><span class=o>=</span><span class=s2>"rawPrediction"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"areaUnderPR"</span>
<span class=p>)</span>
<span class=n>pr_auc</span> <span class=o>=</span> <span class=n>evaluator_pr</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Area Under PR: </span><span class=si>{</span><span class=n>pr_auc</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Multiclass Classification:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>MulticlassClassificationEvaluator</span>

<span class=c1># Accuracy</span>
<span class=n>accuracy_eval</span> <span class=o>=</span> <span class=n>MulticlassClassificationEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"accuracy"</span>
<span class=p>)</span>
<span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Accuracy: </span><span class=si>{</span><span class=n>accuracy</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># F1 Score</span>
<span class=n>f1_eval</span> <span class=o>=</span> <span class=n>MulticlassClassificationEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"f1"</span>
<span class=p>)</span>
<span class=n>f1</span> <span class=o>=</span> <span class=n>f1_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"F1 Score: </span><span class=si>{</span><span class=n>f1</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Weighted Precision</span>
<span class=n>precision_eval</span> <span class=o>=</span> <span class=n>MulticlassClassificationEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"weightedPrecision"</span>
<span class=p>)</span>
<span class=n>precision</span> <span class=o>=</span> <span class=n>precision_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># Weighted Recall</span>
<span class=n>recall_eval</span> <span class=o>=</span> <span class=n>MulticlassClassificationEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"weightedRecall"</span>
<span class=p>)</span>
<span class=n>recall</span> <span class=o>=</span> <span class=n>recall_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># Confusion Matrix</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.mllib.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>MulticlassMetrics</span>
<span class=n>metrics</span> <span class=o>=</span> <span class=n>MulticlassMetrics</span><span class=p>(</span><span class=n>predictions</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"prediction"</span><span class=p>,</span> <span class=s2>"label"</span><span class=p>)</span><span class=o>.</span><span class=n>rdd</span><span class=p>)</span>
<span class=n>confusion_matrix</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>confusionMatrix</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Confusion Matrix:</span><span class=se>\n</span><span class=si>{</span><span class=n>confusion_matrix</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Regression:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>RegressionEvaluator</span>

<span class=c1># RMSE</span>
<span class=n>rmse_eval</span> <span class=o>=</span> <span class=n>RegressionEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"rmse"</span>
<span class=p>)</span>
<span class=n>rmse</span> <span class=o>=</span> <span class=n>rmse_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"RMSE: </span><span class=si>{</span><span class=n>rmse</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># MSE</span>
<span class=n>mse_eval</span> <span class=o>=</span> <span class=n>RegressionEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"mse"</span>
<span class=p>)</span>
<span class=n>mse</span> <span class=o>=</span> <span class=n>mse_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># MAE</span>
<span class=n>mae_eval</span> <span class=o>=</span> <span class=n>RegressionEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"mae"</span>
<span class=p>)</span>
<span class=n>mae</span> <span class=o>=</span> <span class=n>mae_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># R2</span>
<span class=n>r2_eval</span> <span class=o>=</span> <span class=n>RegressionEvaluator</span><span class=p>(</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"r2"</span>
<span class=p>)</span>
<span class=n>r2</span> <span class=o>=</span> <span class=n>r2_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"R2: </span><span class=si>{</span><span class=n>r2</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Clustering:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>ClusteringEvaluator</span>

<span class=c1># Silhouette Score</span>
<span class=n>evaluator</span> <span class=o>=</span> <span class=n>ClusteringEvaluator</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"prediction"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"silhouette"</span><span class=p>,</span>
    <span class=n>distanceMeasure</span><span class=o>=</span><span class=s2>"squaredEuclidean"</span>  <span class=c1># or "cosine"</span>
<span class=p>)</span>
<span class=n>silhouette</span> <span class=o>=</span> <span class=n>evaluator</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Silhouette Score: </span><span class=si>{</span><span class=n>silhouette</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Ranking (Recommendation):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>RankingEvaluator</span>

<span class=c1># Precision at K</span>
<span class=n>precision_eval</span> <span class=o>=</span> <span class=n>RankingEvaluator</span><span class=p>(</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"recommendations"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"items"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"precisionAtK"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>precision_at_k</span> <span class=o>=</span> <span class=n>precision_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># Mean Average Precision</span>
<span class=n>map_eval</span> <span class=o>=</span> <span class=n>RankingEvaluator</span><span class=p>(</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"recommendations"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"items"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"meanAveragePrecision"</span>
<span class=p>)</span>
<span class=n>map_score</span> <span class=o>=</span> <span class=n>map_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>

<span class=c1># NDCG</span>
<span class=n>ndcg_eval</span> <span class=o>=</span> <span class=n>RankingEvaluator</span><span class=p>(</span>
    <span class=n>predictionCol</span><span class=o>=</span><span class=s2>"recommendations"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"items"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"ndcgAtK"</span><span class=p>,</span>
    <span class=n>k</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>ndcg</span> <span class=o>=</span> <span class=n>ndcg_eval</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</code></pre></div> <h3 id=cross-validation-and-hyperparameter-tuning><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.11</span> Cross-Validation and Hyperparameter Tuning</h3> <div class=highlight><pre><span></span><code>    Cross-Validation Flow:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Training Data‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Split into K folds
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Fold1‚îÇFold2‚îÇ...  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì For each param combo
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Train on K-1     ‚îÇ
    ‚îÇ Validate on 1    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Repeat K times
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Average metrics  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Best Parameters  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>K-Fold Cross-Validation:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.tuning</span><span class=w> </span><span class=kn>import</span> <span class=n>CrossValidator</span><span class=p>,</span> <span class=n>ParamGridBuilder</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.evaluation</span><span class=w> </span><span class=kn>import</span> <span class=n>BinaryClassificationEvaluator</span>

<span class=c1># Create estimator</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>featuresCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span> <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>)</span>

<span class=c1># Build parameter grid</span>
<span class=n>paramGrid</span> <span class=o>=</span> <span class=n>ParamGridBuilder</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>regParam</span><span class=p>,</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.001</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>elasticNetParam</span><span class=p>,</span> <span class=p>[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>maxIter</span><span class=p>,</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>build</span><span class=p>()</span>

<span class=c1># Create evaluator</span>
<span class=n>evaluator</span> <span class=o>=</span> <span class=n>BinaryClassificationEvaluator</span><span class=p>(</span>
    <span class=n>rawPredictionCol</span><span class=o>=</span><span class=s2>"rawPrediction"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>metricName</span><span class=o>=</span><span class=s2>"areaUnderROC"</span>
<span class=p>)</span>

<span class=c1># Cross validator</span>
<span class=n>crossval</span> <span class=o>=</span> <span class=n>CrossValidator</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span>
    <span class=n>estimatorParamMaps</span><span class=o>=</span><span class=n>paramGrid</span><span class=p>,</span>
    <span class=n>evaluator</span><span class=o>=</span><span class=n>evaluator</span><span class=p>,</span>
    <span class=n>numFolds</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>           <span class=c1># Number of folds</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>parallelism</span><span class=o>=</span><span class=mi>2</span>         <span class=c1># Number of parallel fits</span>
<span class=p>)</span>

<span class=c1># Fit and get best model</span>
<span class=n>cvModel</span> <span class=o>=</span> <span class=n>crossval</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>best_model</span> <span class=o>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>bestModel</span>

<span class=c1># Make predictions</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Get best parameters</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Best Params: </span><span class=si>{</span><span class=n>best_model</span><span class=o>.</span><span class=n>extractParamMap</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Get average metrics per parameter combination</span>
<span class=n>avg_metrics</span> <span class=o>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>avgMetrics</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Average Metrics: </span><span class=si>{</span><span class=n>avg_metrics</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <p>Train-Validation Split:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.tuning</span><span class=w> </span><span class=kn>import</span> <span class=n>TrainValidationSplit</span>

<span class=c1># Faster than CrossValidator (single split)</span>
<span class=n>tvs</span> <span class=o>=</span> <span class=n>TrainValidationSplit</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span>
    <span class=n>estimatorParamMaps</span><span class=o>=</span><span class=n>paramGrid</span><span class=p>,</span>
    <span class=n>evaluator</span><span class=o>=</span><span class=n>evaluator</span><span class=p>,</span>
    <span class=n>trainRatio</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>       <span class=c1># 80% for training, 20% for validation</span>
    <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>parallelism</span><span class=o>=</span><span class=mi>2</span>
<span class=p>)</span>

<span class=n>tvsModel</span> <span class=o>=</span> <span class=n>tvs</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>best_model</span> <span class=o>=</span> <span class=n>tvsModel</span><span class=o>.</span><span class=n>bestModel</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>tvsModel</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <h3 id=ml-pipelines><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.12</span> ML Pipelines</h3> <div class=highlight><pre><span></span><code>    Pipeline Architecture:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Raw Data   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Stage 1
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Transformer ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Stage 2
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Transformer ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Stage 3 (Final)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Estimator   ‚îÇ
    ‚îÇ  (Model)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Predictions  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Basic Pipeline:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.feature</span><span class=w> </span><span class=kn>import</span> <span class=n>StringIndexer</span><span class=p>,</span> <span class=n>VectorAssembler</span><span class=p>,</span> <span class=n>StandardScaler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=c1># Stage 1: Index categorical column</span>
<span class=n>indexer</span> <span class=o>=</span> <span class=n>StringIndexer</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"category"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"categoryIndex"</span><span class=p>,</span>
    <span class=n>handleInvalid</span><span class=o>=</span><span class=s2>"keep"</span>
<span class=p>)</span>

<span class=c1># Stage 2: Assemble features</span>
<span class=n>assembler</span> <span class=o>=</span> <span class=n>VectorAssembler</span><span class=p>(</span>
    <span class=n>inputCols</span><span class=o>=</span><span class=p>[</span><span class=s2>"categoryIndex"</span><span class=p>,</span> <span class=s2>"feature1"</span><span class=p>,</span> <span class=s2>"feature2"</span><span class=p>],</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>handleInvalid</span><span class=o>=</span><span class=s2>"skip"</span>
<span class=p>)</span>

<span class=c1># Stage 3: Scale features</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>(</span>
    <span class=n>inputCol</span><span class=o>=</span><span class=s2>"features"</span><span class=p>,</span>
    <span class=n>outputCol</span><span class=o>=</span><span class=s2>"scaledFeatures"</span><span class=p>,</span>
    <span class=n>withStd</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>withMean</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>

<span class=c1># Stage 4: Train model</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span>
    <span class=n>featuresCol</span><span class=o>=</span><span class=s2>"scaledFeatures"</span><span class=p>,</span>
    <span class=n>labelCol</span><span class=o>=</span><span class=s2>"label"</span><span class=p>,</span>
    <span class=n>maxIter</span><span class=o>=</span><span class=mi>100</span>
<span class=p>)</span>

<span class=c1># Create pipeline</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>(</span><span class=n>stages</span><span class=o>=</span><span class=p>[</span><span class=n>indexer</span><span class=p>,</span> <span class=n>assembler</span><span class=p>,</span> <span class=n>scaler</span><span class=p>,</span> <span class=n>lr</span><span class=p>])</span>

<span class=c1># Fit pipeline</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>

<span class=c1># Transform test data</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Access stages</span>
<span class=n>stages</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>stages</span>
<span class=n>lr_model</span> <span class=o>=</span> <span class=n>stages</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>  <span class=c1># Get the trained LogisticRegression model</span>
</code></pre></div> <p>Pipeline with Cross-Validation:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.tuning</span><span class=w> </span><span class=kn>import</span> <span class=n>CrossValidator</span><span class=p>,</span> <span class=n>ParamGridBuilder</span>

<span class=c1># Create pipeline</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>(</span><span class=n>stages</span><span class=o>=</span><span class=p>[</span><span class=n>indexer</span><span class=p>,</span> <span class=n>assembler</span><span class=p>,</span> <span class=n>scaler</span><span class=p>,</span> <span class=n>lr</span><span class=p>])</span>

<span class=c1># Parameter grid for the entire pipeline</span>
<span class=n>paramGrid</span> <span class=o>=</span> <span class=n>ParamGridBuilder</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>regParam</span><span class=p>,</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>lr</span><span class=o>.</span><span class=n>elasticNetParam</span><span class=p>,</span> <span class=p>[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>addGrid</span><span class=p>(</span><span class=n>scaler</span><span class=o>.</span><span class=n>withMean</span><span class=p>,</span> <span class=p>[</span><span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>])</span> \
    <span class=o>.</span><span class=n>build</span><span class=p>()</span>

<span class=c1># Cross-validation</span>
<span class=n>crossval</span> <span class=o>=</span> <span class=n>CrossValidator</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>pipeline</span><span class=p>,</span>
    <span class=n>estimatorParamMaps</span><span class=o>=</span><span class=n>paramGrid</span><span class=p>,</span>
    <span class=n>evaluator</span><span class=o>=</span><span class=n>evaluator</span><span class=p>,</span>
    <span class=n>numFolds</span><span class=o>=</span><span class=mi>3</span>
<span class=p>)</span>

<span class=c1># Fit and get best pipeline</span>
<span class=n>cvModel</span> <span class=o>=</span> <span class=n>crossval</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>training_data</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>cvModel</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <h3 id=model-persistence><span class="enumerate-headings-plugin enumerate-heading-plugin">1.7.13</span> Model Persistence</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml</span><span class=w> </span><span class=kn>import</span> <span class=n>PipelineModel</span>

<span class=c1># Save model</span>
<span class=n>model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/model"</span><span class=p>)</span>

<span class=c1># Load model</span>
<span class=n>loaded_model</span> <span class=o>=</span> <span class=n>PipelineModel</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/model"</span><span class=p>)</span>

<span class=c1># Use loaded model</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>loaded_model</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>

<span class=c1># Save specific estimator</span>
<span class=n>lr_model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"path/to/lr_model"</span><span class=p>)</span>

<span class=c1># Load specific estimator</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.ml.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegressionModel</span>
<span class=n>loaded_lr</span> <span class=o>=</span> <span class=n>LogisticRegressionModel</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/lr_model"</span><span class=p>)</span>

<span class=c1># Save to HDFS</span>
<span class=n>model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"hdfs://namenode:8020/models/my_model"</span><span class=p>)</span>

<span class=c1># Save to S3</span>
<span class=n>model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>"s3a://bucket-name/models/my_model"</span><span class=p>)</span>
</code></pre></div> <h2 id=structured-streaming><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8</span> Structured Streaming</h2> <h3 id=streaming-architecture><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.1</span> Streaming Architecture</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Data Source    ‚îÇ
    ‚îÇ (Kafka/Socket) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ Continuous
             ‚Üì Stream
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Spark Streaming‚îÇ
    ‚îÇ   DataFrame    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚Üì Transform
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Aggregations   ‚îÇ
    ‚îÇ   Windows      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚Üì Output
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Sink          ‚îÇ
    ‚îÇ (Files/DB)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Checkpoint    ‚îÇ
    ‚îÇ  (Recovery)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=reading-streaming-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.2</span> Reading Streaming Data</h3> <p>From socket:</p> <div class=highlight><pre><span></span><code><span class=c1># Read from TCP socket</span>
<span class=n>lines</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"socket"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"host"</span><span class=p>,</span> <span class=s2>"localhost"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"port"</span><span class=p>,</span> <span class=mi>9999</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>
</code></pre></div> <p>From files:</p> <div class=highlight><pre><span></span><code><span class=c1># CSV files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"csv"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"header"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"inferSchema"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>schema</span><span class=p>(</span><span class=n>predefined_schema</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/directory"</span><span class=p>)</span>

<span class=c1># JSON files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"json"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"maxFilesPerTrigger"</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/directory"</span><span class=p>)</span>

<span class=c1># Parquet files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"parquet"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>schema</span><span class=p>(</span><span class=n>predefined_schema</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/directory"</span><span class=p>)</span>

<span class=c1># Text files</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"text"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>"path/to/directory"</span><span class=p>)</span>
</code></pre></div> <p>From Kafka:</p> <div class=highlight><pre><span></span><code><span class=c1># Read from Kafka topic</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"kafka"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"kafka.bootstrap.servers"</span><span class=p>,</span> <span class=s2>"localhost:9092"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"subscribe"</span><span class=p>,</span> <span class=s2>"topic1"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"startingOffsets"</span><span class=p>,</span> <span class=s2>"earliest"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># Multiple topics</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"kafka"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"kafka.bootstrap.servers"</span><span class=p>,</span> <span class=s2>"localhost:9092"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"subscribe"</span><span class=p>,</span> <span class=s2>"topic1,topic2,topic3"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># Topic pattern</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"kafka"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"kafka.bootstrap.servers"</span><span class=p>,</span> <span class=s2>"localhost:9092"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"subscribePattern"</span><span class=p>,</span> <span class=s2>"topic.*"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># Parse Kafka message</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>from_json</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.types</span><span class=w> </span><span class=kn>import</span> <span class=n>StructType</span><span class=p>,</span> <span class=n>StructField</span><span class=p>,</span> <span class=n>StringType</span>

<span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>()),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"age"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>())</span>
<span class=p>])</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>selectExpr</span><span class=p>(</span><span class=s2>"CAST(value AS STRING)"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>from_json</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>),</span> <span class=n>schema</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"data"</span><span class=p>))</span> \
    <span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"data.*"</span><span class=p>)</span>
</code></pre></div> <h3 id=stream-processing><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.3</span> Stream Processing</h3> <p>Basic transformations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>explode</span><span class=p>,</span> <span class=n>split</span><span class=p>,</span> <span class=n>col</span><span class=p>,</span> <span class=n>length</span>

<span class=c1># Split and explode</span>
<span class=n>words</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>explode</span><span class=p>(</span><span class=n>split</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"value"</span><span class=p>),</span> <span class=s2>" "</span><span class=p>))</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"word"</span><span class=p>))</span>

<span class=c1># Filter</span>
<span class=n>filtered</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span>

<span class=c1># Select</span>
<span class=n>selected</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>)</span>

<span class=c1># With new columns</span>
<span class=n>enhanced</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"word_length"</span><span class=p>,</span> <span class=n>length</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"word"</span><span class=p>)))</span>
</code></pre></div> <p>Aggregations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>count</span><span class=p>,</span> <span class=n>avg</span><span class=p>,</span> <span class=nb>sum</span><span class=p>,</span> <span class=nb>max</span><span class=p>,</span> <span class=nb>min</span>

<span class=c1># Group by and count</span>
<span class=n>wordCounts</span> <span class=o>=</span> <span class=n>words</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"word"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># Multiple aggregations</span>
<span class=n>stats</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"category"</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span>
    <span class=n>count</span><span class=p>(</span><span class=s2>"*"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"count"</span><span class=p>),</span>
    <span class=n>avg</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"avg_value"</span><span class=p>),</span>
    <span class=nb>max</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"max_value"</span><span class=p>)</span>
<span class=p>)</span>
</code></pre></div> <p>Window operations:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>window</span><span class=p>,</span> <span class=n>col</span>

<span class=c1># Tumbling window (non-overlapping)</span>
<span class=n>windowedCounts</span> <span class=o>=</span> <span class=n>df</span> \
    <span class=o>.</span><span class=n>withWatermark</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>,</span> <span class=s2>"10 minutes"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>groupBy</span><span class=p>(</span>
        <span class=n>window</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>),</span> <span class=s2>"10 minutes"</span><span class=p>),</span>
        <span class=n>col</span><span class=p>(</span><span class=s2>"category"</span><span class=p>)</span>
    <span class=p>)</span> \
    <span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># Sliding window (overlapping)</span>
<span class=n>slidingCounts</span> <span class=o>=</span> <span class=n>df</span> \
    <span class=o>.</span><span class=n>withWatermark</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>,</span> <span class=s2>"10 minutes"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>groupBy</span><span class=p>(</span>
        <span class=n>window</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>),</span> <span class=s2>"10 minutes"</span><span class=p>,</span> <span class=s2>"5 minutes"</span><span class=p>),</span>  <span class=c1># window, slide</span>
        <span class=n>col</span><span class=p>(</span><span class=s2>"category"</span><span class=p>)</span>
    <span class=p>)</span> \
    <span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># Session window (event-based gaps)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>session_window</span>
<span class=n>sessionCounts</span> <span class=o>=</span> <span class=n>df</span> \
    <span class=o>.</span><span class=n>withWatermark</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>,</span> <span class=s2>"10 minutes"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>groupBy</span><span class=p>(</span>
        <span class=n>session_window</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>),</span> <span class=s2>"5 minutes"</span><span class=p>),</span>  <span class=c1># gap duration</span>
        <span class=n>col</span><span class=p>(</span><span class=s2>"user_id"</span><span class=p>)</span>
    <span class=p>)</span> \
    <span class=o>.</span><span class=n>count</span><span class=p>()</span>
</code></pre></div> <p>Watermarking (handling late data):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>window</span><span class=p>,</span> <span class=n>col</span>

<span class=c1># Define watermark (data older than 10 minutes will be dropped)</span>
<span class=n>df_with_watermark</span> <span class=o>=</span> <span class=n>df</span> \
    <span class=o>.</span><span class=n>withWatermark</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>,</span> <span class=s2>"10 minutes"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>groupBy</span><span class=p>(</span>
        <span class=n>window</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"timestamp"</span><span class=p>),</span> <span class=s2>"5 minutes"</span><span class=p>),</span>
        <span class=n>col</span><span class=p>(</span><span class=s2>"device_id"</span><span class=p>)</span>
    <span class=p>)</span> \
    <span class=o>.</span><span class=n>count</span><span class=p>()</span>
</code></pre></div> <h3 id=writing-streaming-data><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.4</span> Writing Streaming Data</h3> <div class=highlight><pre><span></span><code>    Output Modes:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Query Type?     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                          ‚îÇ
    ‚Üì (With Aggregation)       ‚Üì (No Aggregation)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇcomplete‚îÇ                 ‚îÇ append ‚îÇ
‚îÇ  OR    ‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ update ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Console output:</p> <div class=highlight><pre><span></span><code><span class=c1># Display results in console</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>wordCounts</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"complete"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"console"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"truncate"</span><span class=p>,</span> <span class=s2>"false"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"numRows"</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=n>query</span><span class=o>.</span><span class=n>awaitTermination</span><span class=p>()</span>
</code></pre></div> <p>File output:</p> <div class=highlight><pre><span></span><code><span class=c1># Parquet</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"parquet"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"path"</span><span class=p>,</span> <span class=s2>"path/to/output"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"checkpointLocation"</span><span class=p>,</span> <span class=s2>"path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># CSV</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"csv"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"path"</span><span class=p>,</span> <span class=s2>"path/to/output"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"checkpointLocation"</span><span class=p>,</span> <span class=s2>"path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"header"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># Partitioned output</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"parquet"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"path"</span><span class=p>,</span> <span class=s2>"path/to/output"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"checkpointLocation"</span><span class=p>,</span> <span class=s2>"path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</code></pre></div> <p>Kafka output:</p> <div class=highlight><pre><span></span><code><span class=c1># Write to Kafka</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>to_json</span><span class=p>,</span> <span class=n>struct</span>

<span class=n>kafka_df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span>
    <span class=n>col</span><span class=p>(</span><span class=s2>"key"</span><span class=p>),</span>
    <span class=n>to_json</span><span class=p>(</span><span class=n>struct</span><span class=p>(</span><span class=s2>"*"</span><span class=p>))</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>"value"</span><span class=p>)</span>
<span class=p>)</span>

<span class=n>query</span> <span class=o>=</span> <span class=n>kafka_df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"kafka"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"kafka.bootstrap.servers"</span><span class=p>,</span> <span class=s2>"localhost:9092"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"topic"</span><span class=p>,</span> <span class=s2>"output_topic"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"checkpointLocation"</span><span class=p>,</span> <span class=s2>"path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</code></pre></div> <p>Memory output (for debugging):</p> <div class=highlight><pre><span></span><code><span class=c1># Store in memory table</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>wordCounts</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"complete"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"memory"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>queryName</span><span class=p>(</span><span class=s2>"word_counts"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># Query the memory table</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>"SELECT * FROM word_counts"</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p>ForeachBatch (custom logic):</p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>process_batch</span><span class=p>(</span><span class=n>batch_df</span><span class=p>,</span> <span class=n>batch_id</span><span class=p>):</span>
    <span class=c1># Custom processing for each micro-batch</span>
    <span class=n>batch_df</span><span class=o>.</span><span class=n>write</span> \
        <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/db"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"dbtable"</span><span class=p>,</span> <span class=s2>"my_table"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>save</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Processed batch </span><span class=si>{</span><span class=n>batch_id</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"update"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>foreachBatch</span><span class=p>(</span><span class=n>process_batch</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</code></pre></div> <p>Foreach (row-by-row):</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.streaming</span><span class=w> </span><span class=kn>import</span> <span class=n>ForeachWriter</span>

<span class=k>class</span><span class=w> </span><span class=nc>MyForeachWriter</span><span class=p>(</span><span class=n>ForeachWriter</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=nf>open</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>partition_id</span><span class=p>,</span> <span class=n>epoch_id</span><span class=p>):</span>
        <span class=c1># Open connection</span>
        <span class=k>return</span> <span class=kc>True</span>

    <span class=k>def</span><span class=w> </span><span class=nf>process</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>row</span><span class=p>):</span>
        <span class=c1># Process each row</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>row</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>close</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>error</span><span class=p>):</span>
        <span class=c1># Close connection</span>
        <span class=k>pass</span>

<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>foreach</span><span class=p>(</span><span class=n>MyForeachWriter</span><span class=p>())</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>
</code></pre></div> <h3 id=output-modes><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.5</span> Output Modes</h3> <div class=highlight><pre><span></span><code><span class=c1># Append: Only new rows (no aggregation or with watermark)</span>
<span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span>

<span class=c1># Complete: All rows every time (aggregations only)</span>
<span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"complete"</span><span class=p>)</span>

<span class=c1># Update: Only updated rows (aggregations only)</span>
<span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"update"</span><span class=p>)</span>
</code></pre></div> <h3 id=query-management><span class="enumerate-headings-plugin enumerate-heading-plugin">1.8.6</span> Query Management</h3> <div class=highlight><pre><span></span><code><span class=c1># Start query</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"complete"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"console"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># Wait for termination</span>
<span class=n>query</span><span class=o>.</span><span class=n>awaitTermination</span><span class=p>()</span>

<span class=c1># Wait with timeout</span>
<span class=n>query</span><span class=o>.</span><span class=n>awaitTermination</span><span class=p>(</span><span class=n>timeout</span><span class=o>=</span><span class=mi>60</span><span class=p>)</span>  <span class=c1># 60 seconds</span>

<span class=c1># Stop query</span>
<span class=n>query</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>

<span class=c1># Check status</span>
<span class=nb>print</span><span class=p>(</span><span class=n>query</span><span class=o>.</span><span class=n>status</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>query</span><span class=o>.</span><span class=n>lastProgress</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>query</span><span class=o>.</span><span class=n>recentProgress</span><span class=p>)</span>

<span class=c1># Get active streams</span>
<span class=n>active_streams</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>streams</span><span class=o>.</span><span class=n>active</span>
<span class=k>for</span> <span class=n>stream</span> <span class=ow>in</span> <span class=n>active_streams</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>stream</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>stream</span><span class=o>.</span><span class=n>name</span><span class=p>)</span>

<span class=c1># Stop all streams</span>
<span class=k>for</span> <span class=n>stream</span> <span class=ow>in</span> <span class=n>spark</span><span class=o>.</span><span class=n>streams</span><span class=o>.</span><span class=n>active</span><span class=p>:</span>
    <span class=n>stream</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>

<span class=c1># Query with trigger</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>outputMode</span><span class=p>(</span><span class=s2>"complete"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"console"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>trigger</span><span class=p>(</span><span class=n>processingTime</span><span class=o>=</span><span class=s2>"10 seconds"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># Available triggers:</span>
<span class=c1># .trigger(processingTime="10 seconds")  # Micro-batch every 10 sec</span>
<span class=c1># .trigger(once=True)                    # Process once and stop</span>
<span class=c1># .trigger(continuous="1 second")         # Continuous processing</span>
</code></pre></div> <h2 id=performance-tuning><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9</span> Performance Tuning</h2> <h3 id=performance-optimization-flow><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.1</span> Performance Optimization Flow</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Identify        ‚îÇ
    ‚îÇ  Bottleneck      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 ‚îÇ
    ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Spark   ‚îÇ     ‚îÇ Monitor  ‚îÇ
‚îÇ  UI     ‚îÇ     ‚îÇ Metrics  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                    ‚îÇ
    ‚Üì                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Memory  ‚îÇ        ‚îÇ Shuffle  ‚îÇ
‚îÇ Issue   ‚îÇ        ‚îÇ Issue    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                   ‚îÇ
     ‚Üì                   ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cache   ‚îÇ        ‚îÇPartition ‚îÇ
‚îÇOptimize ‚îÇ        ‚îÇOptimize  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Re-measure    ‚îÇ
    ‚îÇ  Performance   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=data-partitioning><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.2</span> Data Partitioning</h3> <div class=highlight><pre><span></span><code><span class=c1># Check current partitions</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Number of partitions: </span><span class=si>{</span><span class=n>df</span><span class=o>.</span><span class=n>rdd</span><span class=o>.</span><span class=n>getNumPartitions</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Repartition (increases or decreases, causes shuffle)</span>
<span class=n>df_repartitioned</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span>
<span class=n>df_repartitioned</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=s2>"user_id"</span><span class=p>)</span>  <span class=c1># By column</span>

<span class=c1># Coalesce (only decreases, no shuffle)</span>
<span class=n>df_coalesced</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>coalesce</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Optimal partition size: 128MB - 1GB per partition</span>
<span class=c1># Rule of thumb: 2-3x number of cores</span>

<span class=c1># Partition by column (for joins and aggregations)</span>
<span class=n>df_partitioned</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=s2>"date"</span><span class=p>,</span> <span class=s2>"category"</span><span class=p>)</span>

<span class=c1># Check partition distribution</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=n>spark_partition_id</span><span class=p>())</span><span class=o>.</span><span class=n>count</span><span class=p>()</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Custom partitioner for RDDs</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>HashPartitioner</span>
<span class=n>rdd</span> <span class=o>=</span> <span class=n>rdd</span><span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>key</span><span class=p>:</span> <span class=nb>hash</span><span class=p>(</span><span class=n>key</span><span class=p>)</span> <span class=o>%</span> <span class=mi>100</span><span class=p>)</span>
</code></pre></div> <h3 id=caching-and-persistence><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.3</span> Caching and Persistence</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>StorageLevel</span>

<span class=c1># Cache in memory (default: MEMORY_AND_DISK)</span>
<span class=n>df_cached</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>

<span class=c1># Persist with storage level</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_ONLY</span><span class=p>)</span>        <span class=c1># Memory only</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_AND_DISK</span><span class=p>)</span>    <span class=c1># Memory, overflow to disk</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>DISK_ONLY</span><span class=p>)</span>          <span class=c1># Disk only</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>MEMORY_AND_DISK_SER</span><span class=p>)</span> <span class=c1># Serialized in memory</span>
<span class=n>df</span><span class=o>.</span><span class=n>persist</span><span class=p>(</span><span class=n>StorageLevel</span><span class=o>.</span><span class=n>OFF_HEAP</span><span class=p>)</span>           <span class=c1># Off-heap memory</span>

<span class=c1># Unpersist</span>
<span class=n>df</span><span class=o>.</span><span class=n>unpersist</span><span class=p>()</span>

<span class=c1># Check if cached</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>is_cached</span><span class=p>)</span>

<span class=c1># Best practices:</span>
<span class=c1># 1. Cache DataFrames used multiple times</span>
<span class=c1># 2. Cache after expensive operations (joins, aggregations)</span>
<span class=c1># 3. Unpersist when no longer needed</span>
<span class=c1># 4. Cache before count() to trigger computation</span>

<span class=c1># Example</span>
<span class=n>df_filtered</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>
<span class=n>df_filtered</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>  <span class=c1># Trigger caching</span>
<span class=n>result1</span> <span class=o>=</span> <span class=n>df_filtered</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"city"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>
<span class=n>result2</span> <span class=o>=</span> <span class=n>df_filtered</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"country"</span><span class=p>)</span><span class=o>.</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>
</code></pre></div> <h3 id=broadcast-variables><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.4</span> Broadcast Variables</h3> <div class=highlight><pre><span></span><code><span class=c1># Broadcast small datasets (&lt; 2GB) to all executors</span>
<span class=n>small_data</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_lookup_dict</span><span class=p>)</span>

<span class=c1># Use in RDD operations</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>small_data</span><span class=o>.</span><span class=n>value</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>

<span class=c1># Use in DataFrame operations</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>udf</span><span class=p>,</span> <span class=n>broadcast</span>

<span class=c1># Broadcast join (for small tables)</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>large_df</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_df</span><span class=p>),</span> <span class=s2>"key"</span><span class=p>)</span>

<span class=c1># Manual broadcast</span>
<span class=n>broadcast_var</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_df</span><span class=o>.</span><span class=n>collect</span><span class=p>())</span>

<span class=c1># Unpersist broadcast variable</span>
<span class=n>broadcast_var</span><span class=o>.</span><span class=n>unpersist</span><span class=p>()</span>

<span class=c1># Best practices:</span>
<span class=c1># - Broadcast tables &lt; 10MB automatically</span>
<span class=c1># - Manually broadcast for tables up to 2GB</span>
<span class=c1># - Use for dimension tables in star schema joins</span>
</code></pre></div> <h3 id=accumulators><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.5</span> Accumulators</h3> <div class=highlight><pre><span></span><code><span class=c1># Create accumulator</span>
<span class=n>counter</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>accumulator</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
<span class=n>sum_accumulator</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>accumulator</span><span class=p>(</span><span class=mf>0.0</span><span class=p>)</span>

<span class=c1># Use in operations</span>
<span class=k>def</span><span class=w> </span><span class=nf>process_row</span><span class=p>(</span><span class=n>row</span><span class=p>):</span>
    <span class=n>counter</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>row</span>

<span class=n>rdd</span><span class=o>.</span><span class=n>foreach</span><span class=p>(</span><span class=n>process_row</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Processed </span><span class=si>{</span><span class=n>counter</span><span class=o>.</span><span class=n>value</span><span class=si>}</span><span class=s2> rows"</span><span class=p>)</span>

<span class=c1># Custom accumulator</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.accumulators</span><span class=w> </span><span class=kn>import</span> <span class=n>AccumulatorParam</span>

<span class=k>class</span><span class=w> </span><span class=nc>SetAccumulatorParam</span><span class=p>(</span><span class=n>AccumulatorParam</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=nf>zero</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>value</span><span class=p>):</span>
        <span class=k>return</span> <span class=nb>set</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>addInPlace</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>val1</span><span class=p>,</span> <span class=n>val2</span><span class=p>):</span>
        <span class=n>val1</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>val2</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>val1</span>

<span class=n>set_accumulator</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>accumulator</span><span class=p>(</span><span class=nb>set</span><span class=p>(),</span> <span class=n>SetAccumulatorParam</span><span class=p>())</span>

<span class=c1># Note: Only use accumulators in actions, not transformations</span>
</code></pre></div> <h3 id=memory-management><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.6</span> Memory Management</h3> <div class=highlight><pre><span></span><code><span class=c1># Check memory usage</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>_conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>_conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>)</span>

<span class=c1># Memory tuning configurations:</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"8g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.memory.fraction"</span><span class=p>,</span> <span class=s2>"0.8"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.memory.storageFraction"</span><span class=p>,</span> <span class=s2>"0.5"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memoryOverhead"</span><span class=p>,</span> <span class=s2>"1g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Memory regions:</span>
<span class=c1># - Execution: 60% of spark.memory.fraction (shuffles, joins, sorts)</span>
<span class=c1># - Storage: 40% of spark.memory.fraction (cache, broadcasts)</span>
<span class=c1># - User: 1 - spark.memory.fraction (user objects)</span>

<span class=c1># Best practices:</span>
<span class=c1># 1. executor.memory = 4-8GB optimal</span>
<span class=c1># 2. Too large executors cause GC issues</span>
<span class=c1># 3. Monitor via Spark UI &gt; Executors tab</span>
</code></pre></div> <h3 id=shuffle-optimization><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.7</span> Shuffle Optimization</h3> <div class=highlight><pre><span></span><code><span class=c1># Control shuffle partitions</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"200"</span><span class=p>)</span>

<span class=c1># Default: 200 (often too many for small data)</span>
<span class=c1># Rule: Set to ~2x number of cores for small data</span>
<span class=c1># For large data: Aim for 128MB-1GB per partition</span>

<span class=c1># Reduce shuffles:</span>
<span class=c1># 1. Use broadcast joins for small tables</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>large_df</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_df</span><span class=p>),</span> <span class=s2>"key"</span><span class=p>)</span>

<span class=c1># 2. Pre-partition before multiple operations</span>
<span class=n>df_partitioned</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>
<span class=n>result1</span> <span class=o>=</span> <span class=n>df_partitioned</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
<span class=n>result2</span> <span class=o>=</span> <span class=n>df_partitioned</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>avg</span><span class=p>()</span>

<span class=c1># 3. Avoid wide transformations when possible</span>
<span class=c1># Wide: groupBy, join, distinct, repartition</span>
<span class=c1># Narrow: filter, select, map</span>

<span class=c1># 4. Coalesce writer partitions</span>
<span class=n>df</span><span class=o>.</span><span class=n>coalesce</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"output"</span><span class=p>)</span>

<span class=c1># Adaptive Query Execution (Spark 3.0+)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.coalescePartitions.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
</code></pre></div> <h3 id=serialization><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.8</span> Serialization</h3> <div class=highlight><pre><span></span><code><span class=c1># Use Kryo serialization (faster than Java serialization)</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.serializer"</span><span class=p>,</span> <span class=s2>"org.apache.spark.serializer.KryoSerializer"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.kryoserializer.buffer.max"</span><span class=p>,</span> <span class=s2>"512m"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Register custom classes</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkConf</span>

<span class=n>conf</span> <span class=o>=</span> <span class=n>SparkConf</span><span class=p>()</span>
<span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.kryo.registrationRequired"</span><span class=p>,</span> <span class=s2>"false"</span><span class=p>)</span>
<span class=n>conf</span><span class=o>.</span><span class=n>registerKryoClasses</span><span class=p>([</span><span class=n>MyClass1</span><span class=p>,</span> <span class=n>MyClass2</span><span class=p>])</span>
</code></pre></div> <h3 id=data-skew-handling><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.9</span> Data Skew Handling</h3> <div class=highlight><pre><span></span><code><span class=c1># Detect skew</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"count"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Solution 1: Salting (add random prefix)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>rand</span><span class=p>,</span> <span class=n>floor</span>

<span class=n>df_salted</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"salt"</span><span class=p>,</span> <span class=p>(</span><span class=n>rand</span><span class=p>()</span> <span class=o>*</span> <span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=s2>"int"</span><span class=p>))</span>
<span class=n>df_salted</span> <span class=o>=</span> <span class=n>df_salted</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"salted_key"</span><span class=p>,</span> 
    <span class=n>concat</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"key"</span><span class=p>),</span> <span class=n>lit</span><span class=p>(</span><span class=s2>"_"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"salt"</span><span class=p>)))</span>

<span class=c1># Join with salted keys</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>df1_salted</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>df2_salted</span><span class=p>,</span> <span class=s2>"salted_key"</span><span class=p>)</span>

<span class=c1># Solution 2: Broadcast the skewed side</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>df1</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>df2</span><span class=p>),</span> <span class=s2>"key"</span><span class=p>)</span>

<span class=c1># Solution 3: Adaptive Query Execution</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
</code></pre></div> <h3 id=query-optimization><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.10</span> Query Optimization</h3> <div class=highlight><pre><span></span><code><span class=c1># Use DataFrames over RDDs (Catalyst optimizer)</span>
<span class=c1># Good: df.filter(col("age") &gt; 25)</span>
<span class=c1># Avoid: rdd.filter(lambda x: x[0] &gt; 25)</span>

<span class=c1># Filter early</span>
<span class=n>df_filtered</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"city"</span><span class=p>)</span>  <span class=c1># Good</span>
<span class=c1># Avoid: df.select("name", "city").filter(col("age") &gt; 25)</span>

<span class=c1># Select only needed columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"col1"</span><span class=p>,</span> <span class=s2>"col2"</span><span class=p>)</span>  <span class=c1># Good</span>
<span class=c1># Avoid: df.select("*")</span>

<span class=c1># Use built-in functions over UDFs</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper"</span><span class=p>,</span> <span class=n>upper</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>  <span class=c1># Good</span>
<span class=c1># Avoid: df.withColumn("upper", my_udf(col("name")))</span>

<span class=c1># Predicate pushdown</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"data"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"year"</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2023</span><span class=p>)</span>  <span class=c1># Pushed to storage</span>

<span class=c1># Column pruning</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"data"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>)</span>  <span class=c1># Only read needed columns</span>

<span class=c1># Explain query plan</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"extended"</span><span class=p>)</span>  <span class=c1># More details</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"cost"</span><span class=p>)</span>      <span class=c1># With statistics</span>
</code></pre></div> <h3 id=monitoring-and-debugging><span class="enumerate-headings-plugin enumerate-heading-plugin">1.9.11</span> Monitoring and Debugging</h3> <div class=highlight><pre><span></span><code><span class=c1># Spark UI: http://localhost:4040</span>

<span class=c1># Show execution plan</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>()</span>

<span class=c1># Show physical plan</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"formatted"</span><span class=p>)</span>

<span class=c1># Enable event logs</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.dir"</span><span class=p>,</span> <span class=s2>"file:///tmp/spark-events"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Log level</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>setLogLevel</span><span class=p>(</span><span class=s2>"WARN"</span><span class=p>)</span>  <span class=c1># ERROR, WARN, INFO, DEBUG</span>

<span class=c1># Checkpoint for long lineages</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>setCheckpointDir</span><span class=p>(</span><span class=s2>"path/to/checkpoint"</span><span class=p>)</span>
<span class=n>df</span><span class=o>.</span><span class=n>checkpoint</span><span class=p>()</span>  <span class=c1># Truncate lineage</span>
</code></pre></div> <h2 id=common-issues-and-debugging><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10</span> Common Issues and Debugging</h2> <h3 id=issue-resolution-flow><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.1</span> Issue Resolution Flow</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Error?    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                        ‚îÇ
    ‚Üì                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OOM    ‚îÇ            ‚îÇ Slow     ‚îÇ
‚îÇ Error   ‚îÇ            ‚îÇ Perform  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                      ‚îÇ
     ‚Üì                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇIncrease ‚îÇ            ‚îÇCheck     ‚îÇ
‚îÇ Memory  ‚îÇ            ‚îÇSpark UI  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                      ‚îÇ
     ‚Üì Still Fails?         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇOptimize ‚îÇ            ‚îÇOptimize  ‚îÇ
‚îÇ Code    ‚îÇ            ‚îÇQuery     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=out-of-memory-errors><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.2</span> Out of Memory Errors</h3> <p>Executor OOM:</p> <div class=highlight><pre><span></span><code><span class=c1># Symptoms:</span>
<span class=c1># - java.lang.OutOfMemoryError: Java heap space</span>
<span class=c1># - Container killed by YARN</span>

<span class=c1># Solutions:</span>

<span class=c1># 1. Increase executor memory</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"8g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memoryOverhead"</span><span class=p>,</span> <span class=s2>"2g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># 2. Increase partitions (reduce data per partition)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"400"</span><span class=p>)</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=mi>400</span><span class=p>)</span>

<span class=c1># 3. Avoid collect() on large datasets</span>
<span class=c1># Bad: data = df.collect()</span>
<span class=c1># Good: df.write.parquet("output")</span>

<span class=c1># 4. Use persist() strategically</span>
<span class=n>df</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>  <span class=c1># Only if reused multiple times</span>

<span class=c1># 5. Process data in batches</span>
<span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>randomSplit</span><span class=p>([</span><span class=mf>0.1</span><span class=p>]</span> <span class=o>*</span> <span class=mi>10</span><span class=p>):</span>
    <span class=n>batch</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>"append"</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"output"</span><span class=p>)</span>
</code></pre></div> <p>Driver OOM:</p> <div class=highlight><pre><span></span><code><span class=c1># Symptoms:</span>
<span class=c1># - OutOfMemoryError in driver</span>
<span class=c1># - collect() or take(large_n) fails</span>

<span class=c1># Solutions:</span>

<span class=c1># 1. Increase driver memory</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># 2. Avoid collecting large data to driver</span>
<span class=c1># Bad: results = df.collect()</span>
<span class=c1># Good: df.write.parquet("output")</span>

<span class=c1># 3. Use limit() before collect()</span>
<span class=n>sample</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=mi>1000</span><span class=p>)</span><span class=o>.</span><span class=n>collect</span><span class=p>()</span>

<span class=c1># 4. Don't broadcast large variables</span>
<span class=c1># Bad: bc = spark.sparkContext.broadcast(large_data)  # &gt; 2GB</span>
<span class=c1># Good: Use join instead</span>

<span class=c1># 5. Check for data leaks in UDFs</span>
<span class=c1># Bad: global_list = []  # Grows indefinitely</span>
</code></pre></div> <h3 id=slow-performance><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.3</span> Slow Performance</h3> <p>Identify bottleneck:</p> <div class=highlight><pre><span></span><code><span class=c1># 1. Check Spark UI (http://localhost:4040)</span>
<span class=c1># - Jobs tab: See failed/slow jobs</span>
<span class=c1># - Stages tab: Identify slow stages</span>
<span class=c1># - Storage tab: Check cache usage</span>
<span class=c1># - Executors tab: Check resource usage</span>
<span class=c1># - SQL tab: View query plans</span>

<span class=c1># 2. Enable query execution time logging</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.debug.maxToStringFields"</span><span class=p>,</span> <span class=s2>"100"</span><span class=p>)</span>

<span class=c1># 3. Explain query plan</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"extended"</span><span class=p>)</span>

<span class=c1># 4. Check for skew</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span><span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"count"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># 5. Monitor GC time</span>
<span class=c1># Check Executor tab in Spark UI for GC time</span>
<span class=c1># If GC time &gt; 10% execution time, tune memory</span>
</code></pre></div> <p>Common performance fixes:</p> <div class=highlight><pre><span></span><code><span class=c1># 1. Use DataFrame API over RDD</span>
<span class=c1># Bad: rdd.map().filter().collect()</span>
<span class=c1># Good: df.select().filter().collect()</span>

<span class=c1># 2. Filter early</span>
<span class=c1># Good: df.filter(col("year") == 2023).select("name")</span>
<span class=c1># Bad: df.select("name").filter(col("year") == 2023)</span>

<span class=c1># 3. Avoid UDFs when possible</span>
<span class=c1># Bad: df.withColumn("upper", my_udf(col("name")))</span>
<span class=c1># Good: df.withColumn("upper", upper(col("name")))</span>

<span class=c1># 4. Use broadcast joins</span>
<span class=c1># Good: large_df.join(broadcast(small_df), "key")</span>

<span class=c1># 5. Adjust shuffle partitions</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"200"</span><span class=p>)</span>

<span class=c1># 6. Enable adaptive query execution</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
</code></pre></div> <h3 id=serialization-errors><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.4</span> Serialization Errors</h3> <div class=highlight><pre><span></span><code><span class=c1># Symptoms:</span>
<span class=c1># - Task not serializable</span>
<span class=c1># - NotSerializableException</span>

<span class=c1># Solutions:</span>

<span class=c1># 1. Don't reference class members in RDD/DataFrame operations</span>
<span class=c1># Bad:</span>
<span class=k>class</span><span class=w> </span><span class=nc>MyClass</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data</span> <span class=o>=</span> <span class=n>some_data</span>

    <span class=k>def</span><span class=w> </span><span class=nf>process</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rdd</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>  <span class=c1># Serializes entire class</span>

<span class=c1># Good:</span>
<span class=k>class</span><span class=w> </span><span class=nc>MyClass</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data</span> <span class=o>=</span> <span class=n>some_data</span>

    <span class=k>def</span><span class=w> </span><span class=nf>process</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rdd</span><span class=p>):</span>
        <span class=n>local_data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>data</span>  <span class=c1># Extract to local variable</span>
        <span class=k>return</span> <span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>+</span> <span class=n>local_data</span><span class=p>)</span>

<span class=c1># 2. Use broadcast for non-serializable objects</span>
<span class=n>non_serializable_obj</span> <span class=o>=</span> <span class=n>MyComplexObject</span><span class=p>()</span>
<span class=n>bc_obj</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>broadcast</span><span class=p>(</span><span class=n>non_serializable_obj</span><span class=p>)</span>
<span class=n>rdd</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>bc_obj</span><span class=o>.</span><span class=n>value</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>

<span class=c1># 3. Make class serializable</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pickle</span>

<span class=k>class</span><span class=w> </span><span class=nc>MyClass</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=nf>__reduce__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=p>,</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=p>,))</span>
</code></pre></div> <h3 id=data-skew><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.5</span> Data Skew</h3> <div class=highlight><pre><span></span><code><span class=c1># Symptoms:</span>
<span class=c1># - Few tasks take much longer than others</span>
<span class=c1># - Some executors idle while others work</span>
<span class=c1># - Uneven data distribution</span>

<span class=c1># Detect skew:</span>
<span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"count"</span><span class=p>)</span><span class=o>.</span><span class=n>desc</span><span class=p>())</span> \
    <span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span>

<span class=c1># Solution 1: Salting</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>rand</span><span class=p>,</span> <span class=n>floor</span><span class=p>,</span> <span class=n>concat</span><span class=p>,</span> <span class=n>lit</span>

<span class=c1># Add random salt to skewed key</span>
<span class=n>df_salted</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"salt"</span><span class=p>,</span> <span class=p>(</span><span class=n>rand</span><span class=p>()</span> <span class=o>*</span> <span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=s2>"int"</span><span class=p>))</span> \
    <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"salted_key"</span><span class=p>,</span> <span class=n>concat</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"key"</span><span class=p>),</span> <span class=n>lit</span><span class=p>(</span><span class=s2>"_"</span><span class=p>),</span> <span class=n>col</span><span class=p>(</span><span class=s2>"salt"</span><span class=p>)))</span>

<span class=c1># Solution 2: Separate hot keys</span>
<span class=n>hot_keys</span> <span class=o>=</span> <span class=p>[</span><span class=s2>"key1"</span><span class=p>,</span> <span class=s2>"key2"</span><span class=p>,</span> <span class=s2>"key3"</span><span class=p>]</span>
<span class=n>df_hot</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>isin</span><span class=p>(</span><span class=n>hot_keys</span><span class=p>))</span>
<span class=n>df_normal</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=o>~</span><span class=n>col</span><span class=p>(</span><span class=s2>"key"</span><span class=p>)</span><span class=o>.</span><span class=n>isin</span><span class=p>(</span><span class=n>hot_keys</span><span class=p>))</span>

<span class=c1># Process separately</span>
<span class=n>result_hot</span> <span class=o>=</span> <span class=n>df_hot</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_df</span><span class=p>),</span> <span class=s2>"key"</span><span class=p>)</span>
<span class=n>result_normal</span> <span class=o>=</span> <span class=n>df_normal</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>small_df</span><span class=p>,</span> <span class=s2>"key"</span><span class=p>)</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>result_hot</span><span class=o>.</span><span class=n>union</span><span class=p>(</span><span class=n>result_normal</span><span class=p>)</span>

<span class=c1># Solution 3: Adaptive skew join (Spark 3.0+)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.skewedPartitionFactor"</span><span class=p>,</span> <span class=s2>"5"</span><span class=p>)</span>
</code></pre></div> <h3 id=connection-issues><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.6</span> Connection Issues</h3> <div class=highlight><pre><span></span><code><span class=c1># JDBC connection issues:</span>
<span class=c1># 1. Check driver is available</span>
<span class=c1># 2. Increase timeout</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://localhost:5432/db"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"connectTimeout"</span><span class=p>,</span> <span class=s2>"60"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"socketTimeout"</span><span class=p>,</span> <span class=s2>"60"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># 3. Use connection pooling</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"numPartitions"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"batchsize"</span><span class=p>,</span> <span class=s2>"10000"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>save</span><span class=p>()</span>

<span class=c1># Kafka connection issues:</span>
<span class=c1># 1. Check broker reachability</span>
<span class=c1># 2. Verify topic exists</span>
<span class=c1># 3. Check security settings</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>readStream</span> \
    <span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"kafka"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"kafka.bootstrap.servers"</span><span class=p>,</span> <span class=s2>"localhost:9092"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"failOnDataLoss"</span><span class=p>,</span> <span class=s2>"false"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>
</code></pre></div> <h3 id=common-error-messages><span class="enumerate-headings-plugin enumerate-heading-plugin">1.10.7</span> Common Error Messages</h3> <div class=highlight><pre><span></span><code><span class=c1># "py4j.protocol.Py4JJavaError"</span>
<span class=c1># - Java exception in Spark</span>
<span class=c1># - Read the caused by section for root cause</span>

<span class=c1># "Analysis exception"</span>
<span class=c1># - Column not found</span>
<span class=c1># - Check column names and case sensitivity</span>

<span class=c1># "Stage cancelled"</span>
<span class=c1># - Job failed or manually stopped</span>
<span class=c1># - Check previous errors</span>

<span class=c1># "Executor lost"</span>
<span class=c1># - Executor crashed (OOM, network issue)</span>
<span class=c1># - Check executor logs</span>

<span class=c1># "No space left on device"</span>
<span class=c1># - Disk full on executor/driver</span>
<span class=c1># - Increase disk space or clean temp files</span>

<span class=c1># "Failed to bind to port"</span>
<span class=c1># - Port already in use</span>
<span class=c1># - Change port or stop conflicting process</span>
</code></pre></div> <h2 id=spark-configuration><span class="enumerate-headings-plugin enumerate-heading-plugin">1.11</span> Spark Configuration</h2> <h3 id=configuration-hierarchy><span class="enumerate-headings-plugin enumerate-heading-plugin">1.11.1</span> Configuration Hierarchy</h3> <div class=highlight><pre><span></span><code>    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  spark-defaults.conf‚îÇ  (Lowest priority)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì Overridden by
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ SparkConf in code   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì Overridden by
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ spark-submit flags  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì Overridden by
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Runtime config      ‚îÇ  (Highest priority)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <h3 id=essential-configurations><span class="enumerate-headings-plugin enumerate-heading-plugin">1.11.2</span> Essential Configurations</h3> <p>Application settings:</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>pyspark</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkConf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkSession</span>

<span class=c1># Using SparkConf</span>
<span class=n>conf</span> <span class=o>=</span> <span class=n>SparkConf</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>setAppName</span><span class=p>(</span><span class=s2>"MyApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>setMaster</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span>  <span class=c1># local[*], yarn, mesos, k8s</span>

<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=n>conf</span><span class=o>=</span><span class=n>conf</span><span class=p>)</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Using builder pattern</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"MyApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"2g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Runtime configuration</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"200"</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>))</span>
</code></pre></div> <p>Memory configurations:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.maxResultSize"</span><span class=p>,</span> <span class=s2>"2g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"8g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memoryOverhead"</span><span class=p>,</span> <span class=s2>"2g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.memory.fraction"</span><span class=p>,</span> <span class=s2>"0.8"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.memory.storageFraction"</span><span class=p>,</span> <span class=s2>"0.5"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Memory breakdown:</span>
<span class=c1># Total Executor Memory = executor.memory + memoryOverhead</span>
<span class=c1># Spark Memory = executor.memory √ó memory.fraction</span>
<span class=c1># Storage Memory = Spark Memory √ó storageFraction</span>
<span class=c1># Execution Memory = Spark Memory √ó (1 - storageFraction)</span>
</code></pre></div> <p>Executor configurations:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.cores"</span><span class=p>,</span> <span class=s2>"4"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.instances"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.minExecutors"</span><span class=p>,</span> <span class=s2>"2"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.maxExecutors"</span><span class=p>,</span> <span class=s2>"50"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.initialExecutors"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Optimal executor sizing:</span>
<span class=c1># - 4-5 cores per executor</span>
<span class=c1># - 4-8 GB memory per executor</span>
<span class=c1># - Leave 1 core and 1GB for OS on each node</span>
</code></pre></div> <p>Shuffle and parallelism:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"200"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.default.parallelism"</span><span class=p>,</span> <span class=s2>"100"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.files.maxPartitionBytes"</span><span class=p>,</span> <span class=s2>"134217728"</span><span class=p>)</span>  <span class=c1># 128MB \</span>
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.files.openCostInBytes"</span><span class=p>,</span> <span class=s2>"4194304"</span><span class=p>)</span>  <span class=c1># 4MB \</span>
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Tuning guidelines:</span>
<span class=c1># shuffle.partitions: 2-3x number of cores (small data)</span>
<span class=c1>#                     aim for 128MB-1GB per partition (large data)</span>
<span class=c1># default.parallelism: 2-3x number of cores</span>
</code></pre></div> <p>Serialization:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.serializer"</span><span class=p>,</span> <span class=s2>"org.apache.spark.serializer.KryoSerializer"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.kryo.registrationRequired"</span><span class=p>,</span> <span class=s2>"false"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.kryoserializer.buffer.max"</span><span class=p>,</span> <span class=s2>"512m"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Adaptive Query Execution (AQE):</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.coalescePartitions.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.coalescePartitions.minPartitionNum"</span><span class=p>,</span> <span class=s2>"1"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.advisoryPartitionSizeInBytes"</span><span class=p>,</span> <span class=s2>"134217728"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.skewedPartitionFactor"</span><span class=p>,</span> <span class=s2>"5"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes"</span><span class=p>,</span> <span class=s2>"256MB"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Broadcast settings:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.autoBroadcastJoinThreshold"</span><span class=p>,</span> <span class=s2>"10485760"</span><span class=p>)</span>  <span class=c1># 10MB \</span>
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.broadcastTimeout"</span><span class=p>,</span> <span class=s2>"300"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Set to -1 to disable auto broadcast</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.autoBroadcastJoinThreshold"</span><span class=p>,</span> <span class=s2>"-1"</span><span class=p>)</span>
</code></pre></div> <p>Storage and checkpointing:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.local.dir"</span><span class=p>,</span> <span class=s2>"/tmp/spark"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.dir"</span><span class=p>,</span> <span class=s2>"file:///tmp/spark-events"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.history.fs.logDirectory"</span><span class=p>,</span> <span class=s2>"file:///tmp/spark-events"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># Set checkpoint directory</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>setCheckpointDir</span><span class=p>(</span><span class=s2>"hdfs://path/to/checkpoint"</span><span class=p>)</span>
</code></pre></div> <p>Network and timeout:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.network.timeout"</span><span class=p>,</span> <span class=s2>"800s"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.heartbeatInterval"</span><span class=p>,</span> <span class=s2>"60s"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.rpc.askTimeout"</span><span class=p>,</span> <span class=s2>"600s"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.broadcastTimeout"</span><span class=p>,</span> <span class=s2>"600"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Compression:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.io.compression.codec"</span><span class=p>,</span> <span class=s2>"snappy"</span><span class=p>)</span>  <span class=c1># snappy, lz4, gzip \</span>
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.rdd.compress"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.inMemoryColumnarStorage.compressed"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.parquet.compression.codec"</span><span class=p>,</span> <span class=s2>"snappy"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Dynamic allocation:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.minExecutors"</span><span class=p>,</span> <span class=s2>"1"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.maxExecutors"</span><span class=p>,</span> <span class=s2>"100"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.initialExecutors"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.executorIdleTimeout"</span><span class=p>,</span> <span class=s2>"60s"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.dynamicAllocation.schedulerBacklogTimeout"</span><span class=p>,</span> <span class=s2>"1s"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <h3 id=common-configuration-patterns><span class="enumerate-headings-plugin enumerate-heading-plugin">1.11.3</span> Common Configuration Patterns</h3> <p>Local development:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"LocalDev"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"local[*]"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"10"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Production (YARN):</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"ProductionApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"yarn"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.submit.deployMode"</span><span class=p>,</span> <span class=s2>"cluster"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>,</span> <span class=s2>"8g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.cores"</span><span class=p>,</span> <span class=s2>"4"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.executor.instances"</span><span class=p>,</span> <span class=s2>"20"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.driver.memory"</span><span class=p>,</span> <span class=s2>"4g"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"400"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.serializer"</span><span class=p>,</span> <span class=s2>"org.apache.spark.serializer.KryoSerializer"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <p>Streaming application:</p> <div class=highlight><pre><span></span><code><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"StreamingApp"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.streaming.backpressure.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.streaming.kafka.maxRatePerPartition"</span><span class=p>,</span> <span class=s2>"1000"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.sql.streaming.checkpointLocation"</span><span class=p>,</span> <span class=s2>"/path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</code></pre></div> <h3 id=view-and-update-configuration><span class="enumerate-headings-plugin enumerate-heading-plugin">1.11.4</span> View and Update Configuration</h3> <div class=highlight><pre><span></span><code><span class=c1># Get all configurations</span>
<span class=n>all_conf</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>getConf</span><span class=p>()</span><span class=o>.</span><span class=n>getAll</span><span class=p>()</span>
<span class=k>for</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=n>all_conf</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>value</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>

<span class=c1># Get specific configuration</span>
<span class=n>value</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>)</span>

<span class=c1># Set configuration at runtime (only for SQL configurations)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>,</span> <span class=s2>"100"</span><span class=p>)</span>

<span class=c1># Check if configuration is modifiable</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>isModifiable</span><span class=p>(</span><span class=s2>"spark.sql.shuffle.partitions"</span><span class=p>)</span>  <span class=c1># True</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>isModifiable</span><span class=p>(</span><span class=s2>"spark.executor.memory"</span><span class=p>)</span>  <span class=c1># False (static)</span>
</code></pre></div> <h2 id=tips-and-best-practices><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12</span> Tips and Best Practices</h2> <h3 id=development-best-practices><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.1</span> Development Best Practices</h3> <div class=highlight><pre><span></span><code>    Development Workflow:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Local Dev    ‚îÇ
    ‚îÇ(Small Sample)‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Test
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Unit Tests  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Deploy
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Staging      ‚îÇ
    ‚îÇ(Medium Data) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì Validate
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Production   ‚îÇ
    ‚îÇ (Full Data)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div> <p>Code organization:</p> <div class=highlight><pre><span></span><code><span class=c1># Use virtual environments</span>
<span class=n>python</span> <span class=o>-</span><span class=n>m</span> <span class=n>venv</span> <span class=n>venv</span>
<span class=n>source</span> <span class=n>venv</span><span class=o>/</span><span class=nb>bin</span><span class=o>/</span><span class=n>activate</span>
<span class=n>pip</span> <span class=n>install</span> <span class=n>pyspark</span>

<span class=c1># Project structure</span>
<span class=n>my_pyspark_project</span><span class=o>/</span>
<span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>src</span><span class=o>/</span>
<span class=err>‚îÇ</span>   <span class=err>‚îú‚îÄ‚îÄ</span> <span class=fm>__init__</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îÇ</span>   <span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>transformations</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îÇ</span>   <span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>utils</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îÇ</span>   <span class=err>‚îî‚îÄ‚îÄ</span> <span class=n>config</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>tests</span><span class=o>/</span>
<span class=err>‚îÇ</span>   <span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>test_transformations</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îÇ</span>   <span class=err>‚îî‚îÄ‚îÄ</span> <span class=n>test_utils</span><span class=o>.</span><span class=n>py</span>
<span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>configs</span><span class=o>/</span>
<span class=err>‚îÇ</span>   <span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>dev</span><span class=o>.</span><span class=n>conf</span>
<span class=err>‚îÇ</span>   <span class=err>‚îî‚îÄ‚îÄ</span> <span class=n>prod</span><span class=o>.</span><span class=n>conf</span>
<span class=err>‚îú‚îÄ‚îÄ</span> <span class=n>requirements</span><span class=o>.</span><span class=n>txt</span>
<span class=err>‚îî‚îÄ‚îÄ</span> <span class=n>main</span><span class=o>.</span><span class=n>py</span>

<span class=c1># Use meaningful names</span>
<span class=c1># Good</span>
<span class=n>user_purchases_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"user_purchases"</span><span class=p>)</span>
<span class=n>active_users</span> <span class=o>=</span> <span class=n>user_purchases_df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"status"</span><span class=p>)</span> <span class=o>==</span> <span class=s2>"active"</span><span class=p>)</span>

<span class=c1># Bad</span>
<span class=n>df1</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"data"</span><span class=p>)</span>
<span class=n>df2</span> <span class=o>=</span> <span class=n>df1</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"x"</span><span class=p>)</span> <span class=o>==</span> <span class=s2>"y"</span><span class=p>)</span>
</code></pre></div> <p>Testing:</p> <div class=highlight><pre><span></span><code><span class=c1># Unit tests with pytest</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pytest</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql</span><span class=w> </span><span class=kn>import</span> <span class=n>SparkSession</span>
<span class=kn>from</span><span class=w> </span><span class=nn>src.transformations</span><span class=w> </span><span class=kn>import</span> <span class=n>filter_active_users</span>

<span class=nd>@pytest</span><span class=o>.</span><span class=n>fixture</span><span class=p>(</span><span class=n>scope</span><span class=o>=</span><span class=s2>"session"</span><span class=p>)</span>
<span class=k>def</span><span class=w> </span><span class=nf>spark</span><span class=p>():</span>
    <span class=k>return</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
        <span class=o>.</span><span class=n>master</span><span class=p>(</span><span class=s2>"local[2]"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>"test"</span><span class=p>)</span> \
        <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=k>def</span><span class=w> </span><span class=nf>test_filter_active_users</span><span class=p>(</span><span class=n>spark</span><span class=p>):</span>
    <span class=c1># Create test data</span>
    <span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
        <span class=p>(</span><span class=s2>"user1"</span><span class=p>,</span> <span class=s2>"active"</span><span class=p>),</span>
        <span class=p>(</span><span class=s2>"user2"</span><span class=p>,</span> <span class=s2>"inactive"</span><span class=p>),</span>
        <span class=p>(</span><span class=s2>"user3"</span><span class=p>,</span> <span class=s2>"active"</span><span class=p>)</span>
    <span class=p>]</span>
    <span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=p>[</span><span class=s2>"user_id"</span><span class=p>,</span> <span class=s2>"status"</span><span class=p>])</span>

    <span class=c1># Apply transformation</span>
    <span class=n>result</span> <span class=o>=</span> <span class=n>filter_active_users</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>

    <span class=c1># Assert</span>
    <span class=k>assert</span> <span class=n>result</span><span class=o>.</span><span class=n>count</span><span class=p>()</span> <span class=o>==</span> <span class=mi>2</span>
    <span class=k>assert</span> <span class=n>result</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"status"</span><span class=p>)</span> <span class=o>==</span> <span class=s2>"inactive"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span> <span class=o>==</span> <span class=mi>0</span>
</code></pre></div> <h3 id=performance-best-practices><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.2</span> Performance Best Practices</h3> <p>DO's:</p> <div class=highlight><pre><span></span><code><span class=c1># ‚úì Use DataFrame API over RDD</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span>  <span class=c1># Good</span>

<span class=c1># ‚úì Filter early and select only needed columns</span>
<span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"year"</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2023</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=s2>"age"</span><span class=p>)</span>

<span class=c1># ‚úì Use built-in functions</span>
<span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>"upper"</span><span class=p>,</span> <span class=n>upper</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"name"</span><span class=p>)))</span>

<span class=c1># ‚úì Broadcast small tables</span>
<span class=n>large_df</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>broadcast</span><span class=p>(</span><span class=n>small_df</span><span class=p>),</span> <span class=s2>"key"</span><span class=p>)</span>

<span class=c1># ‚úì Cache when reusing DataFrames</span>
<span class=n>df_cached</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>25</span><span class=p>)</span><span class=o>.</span><span class=n>cache</span><span class=p>()</span>
<span class=n>df_cached</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>  <span class=c1># Materialize</span>
<span class=n>result1</span> <span class=o>=</span> <span class=n>df_cached</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"city"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>
<span class=n>result2</span> <span class=o>=</span> <span class=n>df_cached</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"country"</span><span class=p>)</span><span class=o>.</span><span class=n>avg</span><span class=p>(</span><span class=s2>"salary"</span><span class=p>)</span>

<span class=c1># ‚úì Use vectorized Pandas UDFs</span>
<span class=nd>@pandas_udf</span><span class=p>(</span><span class=n>StringType</span><span class=p>())</span>
<span class=k>def</span><span class=w> </span><span class=nf>to_upper</span><span class=p>(</span><span class=n>s</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>:</span>
    <span class=k>return</span> <span class=n>s</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>

<span class=c1># ‚úì Partition appropriately</span>
<span class=n>df</span><span class=o>.</span><span class=n>repartition</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=s2>"user_id"</span><span class=p>)</span>

<span class=c1># ‚úì Enable Adaptive Query Execution</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.adaptive.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span>

<span class=c1># ‚úì Use Parquet format</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"output"</span><span class=p>)</span>

<span class=c1># ‚úì Specify schema when reading</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>schema</span><span class=p>(</span><span class=n>schema</span><span class=p>)</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"input"</span><span class=p>)</span>
</code></pre></div> <p>DON'Ts:</p> <div class=highlight><pre><span></span><code><span class=c1># ‚úó Avoid collect() on large datasets</span>
<span class=c1># data = df.collect()  # Bad</span>

<span class=c1># ‚úó Avoid Python UDFs when built-in exists</span>
<span class=c1># df.withColumn("upper", my_udf(col("name")))  # Bad</span>

<span class=c1># ‚úó Avoid groupByKey (use reduceByKey)</span>
<span class=c1># rdd.groupByKey()  # Bad</span>

<span class=c1># ‚úó Don't select all columns when not needed</span>
<span class=c1># df.select("*").filter(...)  # Bad</span>

<span class=c1># ‚úó Avoid unnecessary shuffles</span>
<span class=c1># df.repartition(10).repartition(20)  # Bad</span>

<span class=c1># ‚úó Don't cache everything</span>
<span class=c1># df1.cache()  # Only if reused multiple times</span>
<span class=c1># df2.cache()</span>
<span class=c1># df3.cache()</span>

<span class=c1># ‚úó Avoid wide transformations when possible</span>
<span class=c1># Multiple joins/groupBy in sequence without optimization</span>
</code></pre></div> <h3 id=data-quality-practices><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.3</span> Data Quality Practices</h3> <div class=highlight><pre><span></span><code><span class=c1># Validate schema</span>
<span class=n>expected_schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"id"</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
    <span class=n>StructField</span><span class=p>(</span><span class=s2>"name"</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>)</span>
<span class=p>])</span>

<span class=k>if</span> <span class=n>df</span><span class=o>.</span><span class=n>schema</span> <span class=o>!=</span> <span class=n>expected_schema</span><span class=p>:</span>
    <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>"Schema mismatch"</span><span class=p>)</span>

<span class=c1># Check for nulls</span>
<span class=n>null_counts</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>([</span>
    <span class=n>count</span><span class=p>(</span><span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=n>c</span><span class=p>)</span><span class=o>.</span><span class=n>isNull</span><span class=p>(),</span> <span class=n>c</span><span class=p>))</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=n>c</span><span class=p>)</span> 
    <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>columns</span>
<span class=p>])</span>
<span class=n>null_counts</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Data validation</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.functions</span><span class=w> </span><span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>when</span><span class=p>,</span> <span class=n>count</span>

<span class=c1># Check data ranges</span>
<span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span>
    <span class=nb>min</span><span class=p>(</span><span class=s2>"age"</span><span class=p>),</span>
    <span class=nb>max</span><span class=p>(</span><span class=s2>"age"</span><span class=p>),</span>
    <span class=n>avg</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span>
<span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Identify duplicates</span>
<span class=n>duplicate_count</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>"id"</span><span class=p>)</span><span class=o>.</span><span class=n>count</span><span class=p>()</span> \
    <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"count"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>count</span><span class=p>()</span>

<span class=c1># Add data quality flags</span>
<span class=n>df_with_quality</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span>
    <span class=s2>"quality_flag"</span><span class=p>,</span>
    <span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span><span class=o>.</span><span class=n>isNull</span><span class=p>(),</span> <span class=s2>"missing_age"</span><span class=p>)</span>
    <span class=o>.</span><span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>"invalid_age"</span><span class=p>)</span>
    <span class=o>.</span><span class=n>when</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>"age"</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>150</span><span class=p>,</span> <span class=s2>"outlier_age"</span><span class=p>)</span>
    <span class=o>.</span><span class=n>otherwise</span><span class=p>(</span><span class=s2>"valid"</span><span class=p>)</span>
<span class=p>)</span>
</code></pre></div> <h3 id=production-checklist><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.4</span> Production Checklist</h3> <div class=highlight><pre><span></span><code><span class=c1># 1. Enable event logging</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.eventLog.dir"</span><span class=p>,</span> <span class=s2>"hdfs://path/to/logs"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># 2. Set appropriate log level</span>
<span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span><span class=o>.</span><span class=n>setLogLevel</span><span class=p>(</span><span class=s2>"WARN"</span><span class=p>)</span>

<span class=c1># 3. Use checkpointing for streaming</span>
<span class=n>query</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>writeStream</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"checkpointLocation"</span><span class=p>,</span> <span class=s2>"hdfs://path/to/checkpoint"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>start</span><span class=p>()</span>

<span class=c1># 4. Handle failures gracefully</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pyspark.sql.utils</span><span class=w> </span><span class=kn>import</span> <span class=n>AnalysisException</span>

<span class=k>try</span><span class=p>:</span>
    <span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"input"</span><span class=p>)</span>
<span class=k>except</span> <span class=n>AnalysisException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
    <span class=n>logger</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>"Failed to read data: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
    <span class=k>raise</span>

<span class=c1># 5. Monitor query execution</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>()</span>
<span class=n>df</span><span class=o>.</span><span class=n>explain</span><span class=p>(</span><span class=n>mode</span><span class=o>=</span><span class=s2>"cost"</span><span class=p>)</span>

<span class=c1># 6. Use appropriate file formats</span>
<span class=c1># - Parquet: Best for analytics (columnar)</span>
<span class=c1># - ORC: Good for Hive integration</span>
<span class=c1># - Avro: Good for row-based operations</span>
<span class=c1># - Delta: Best for ACID operations</span>

<span class=c1># 7. Partition large datasets</span>
<span class=n>df</span><span class=o>.</span><span class=n>write</span> \
    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=s2>"year"</span><span class=p>,</span> <span class=s2>"month"</span><span class=p>,</span> <span class=s2>"day"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>"output"</span><span class=p>)</span>

<span class=c1># 8. Set reasonable timeouts</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.network.timeout"</span><span class=p>,</span> <span class=s2>"600s"</span><span class=p>)</span>
<span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>"spark.sql.broadcastTimeout"</span><span class=p>,</span> <span class=s2>"600"</span><span class=p>)</span>

<span class=c1># 9. Clean up resources</span>
<span class=n>df</span><span class=o>.</span><span class=n>unpersist</span><span class=p>()</span>
<span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>clearCache</span><span class=p>()</span>
<span class=n>spark</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</code></pre></div> <h3 id=security-best-practices><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.5</span> Security Best Practices</h3> <div class=highlight><pre><span></span><code><span class=c1># 1. Don't hardcode credentials</span>
<span class=c1># Bad</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>jdbc</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>table</span><span class=p>,</span> <span class=n>properties</span><span class=o>=</span><span class=p>{</span><span class=s2>"user"</span><span class=p>:</span> <span class=s2>"admin"</span><span class=p>,</span> <span class=s2>"password"</span><span class=p>:</span> <span class=s2>"pass123"</span><span class=p>})</span>

<span class=c1># Good - use environment variables</span>
<span class=kn>import</span><span class=w> </span><span class=nn>os</span>
<span class=n>properties</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>"user"</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>"DB_USER"</span><span class=p>),</span>
    <span class=s2>"password"</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>"DB_PASSWORD"</span><span class=p>)</span>
<span class=p>}</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>jdbc</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>table</span><span class=p>,</span> <span class=n>properties</span><span class=o>=</span><span class=n>properties</span><span class=p>)</span>

<span class=c1># 2. Enable encryption</span>
<span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.authenticate"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.network.crypto.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>"spark.io.encryption.enabled"</span><span class=p>,</span> <span class=s2>"true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>

<span class=c1># 3. Use secure connections</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>"jdbc"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>"url"</span><span class=p>,</span> <span class=s2>"jdbc:postgresql://host:5432/db?ssl=true"</span><span class=p>)</span> \
    <span class=o>.</span><span class=n>load</span><span class=p>()</span>

<span class=c1># 4. Audit data access</span>
<span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>"User </span><span class=si>{</span><span class=n>username</span><span class=si>}</span><span class=s2> accessed table </span><span class=si>{</span><span class=n>table_name</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</code></pre></div> <h3 id=key-recommendations><span class="enumerate-headings-plugin enumerate-heading-plugin">1.12.6</span> Key Recommendations</h3> <ol> <li><strong>Always prefer DataFrames over RDDs</strong> for automatic optimization</li> <li><strong>Filter early, select only needed columns</strong> to reduce data movement</li> <li><strong>Use built-in functions</strong> instead of UDFs when possible</li> <li><strong>Broadcast small tables</strong> (&lt;2GB) for efficient joins</li> <li><strong>Cache strategically</strong> only when DataFrames are reused multiple times</li> <li><strong>Monitor Spark UI</strong> to identify and fix bottlenecks</li> <li><strong>Use Parquet</strong> as default storage format for better performance</li> <li><strong>Enable Adaptive Query Execution</strong> for automatic optimization</li> <li><strong>Partition wisely</strong> based on query patterns (typically by date)</li> <li><strong>Test with sample data</strong> before running on full datasets</li> </ol> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright ¬© 2020 - <script>document.write(/\d{4}/.exec(Date())[0])</script> ‚Ä¢ <strong>Kuldeep Singh Sidhu</strong> ‚Ä¢ <u><a href=https://choosealicense.com/licenses/agpl-3.0/ target=‚Äù_blank‚Äù>License</a></u> ‚Ä¢ <u><a href=/privacy>Privacy Policy</a></u> ‚Ä¢ <u><a href=/contact>Contact</a></u> ‚Ä¢ </div> </div> <div class=md-social> <a class=md-social__link href=https://github.com/singhsidhukuldeep/ rel=noopener target=_blank title=github.com> <svg viewbox="0 0 16 16" xmlns=http://www.w3.org/2000/svg><path d="M8 0c4.42 0 8 3.58 8 8a8.01 8.01 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27s-1.36.09-2 .27c-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8"></path></svg> </a> <a class=md-social__link href=https://linkedin.com/in/singhsidhukuldeep rel=noopener target=_blank title=linkedin.com> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> <a class=md-social__link href=https://twitter.com/kuldeep_s_s rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"></path></svg> </a> <a class=md-social__link href=https://stackoverflow.com/u/7182350/ rel=noopener target=_blank title=stackoverflow.com> <svg viewbox="0 0 384 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 300zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-200v39.7h200zm39.7 80H42.7V320h-40v160h359.5V320h-40z"></path></svg> </a> <a class=md-social__link href=https://huggingface.co/singhsidhukuldeep rel=noopener target=_blank title=huggingface.co> <svg fill=none height=463 viewbox="0 0 500 463" width=500 xmlns=http://www.w3.org/2000/svg> <path d="M496.592 369.699C500.563 381.093 499.61 393.227 494.315 403.778C490.503 411.48 485.05 417.441 478.379 422.769C470.331 429.099 460.324 434.48 448.253 439.65C433.852 445.77 416.274 451.52 408.226 453.63C387.63 458.958 367.829 462.334 347.762 462.493C319.066 462.756 294.34 456.004 276.762 438.753C267.656 439.861 258.443 440.494 249.178 440.494C240.389 440.494 231.706 439.967 223.076 438.912C205.445 456.057 180.825 462.756 152.234 462.493C132.168 462.334 112.366 458.958 91.7177 453.63C83.7229 451.52 66.145 445.77 51.7439 439.65C39.6723 434.48 29.6656 429.099 21.6708 422.769C14.9467 417.441 9.49334 411.48 5.68127 403.778C0.439661 393.227 -0.566304 381.093 3.45755 369.699C-0.248631 360.994 -1.20165 351.024 1.71035 339.998C3.03399 334.987 5.20476 330.344 7.95792 326.229C7.37552 324.067 6.89901 321.851 6.58134 319.424C4.56941 304.97 9.59923 291.781 19.0765 281.547C23.7357 276.43 28.7655 272.895 34.0071 270.627C30.1421 254.273 28.1302 237.445 28.1302 220.247C28.1302 98.5969 127.085 0 249.178 0C291.111 0 330.343 11.6058 363.805 31.8633C369.84 35.5561 375.77 39.5126 381.436 43.7329C384.242 45.8431 387.048 48.006 389.748 50.2744C392.501 52.49 395.201 54.8112 397.796 57.1851C405.632 64.3069 412.991 71.9562 419.715 80.133C421.992 82.8235 424.163 85.6194 426.28 88.4681C430.569 94.1128 434.54 99.9685 438.193 106.035C443.752 115.109 448.623 124.604 452.859 134.469C455.665 141.064 458.101 147.816 460.271 154.727C463.501 165.067 465.99 175.723 467.684 186.696C468.213 190.336 468.69 194.028 469.06 197.721C469.802 205.107 470.225 212.598 470.225 220.247C470.225 237.234 468.213 253.904 464.454 269.994C470.278 272.262 475.784 275.955 480.92 281.547C490.397 291.781 495.427 305.022 493.415 319.477C493.098 321.851 492.621 324.067 492.039 326.229C494.792 330.344 496.963 334.987 498.286 339.998C501.198 351.024 500.245 360.994 496.592 369.699Z" fill=white></path> <path d="M433.839 221.75C433.839 120.838 351.531 39.0323 250 39.0323C148.469 39.0323 66.1613 120.838 66.1613 221.75C66.1613 322.662 148.469 404.468 250 404.468C351.531 404.468 433.839 322.662 433.839 221.75ZM45 221.75C45 109.222 136.782 18 250 18C363.218 18 455 109.222 455 221.75C455 334.278 363.218 425.5 250 425.5C136.782 425.5 45 334.278 45 221.75Z" fill=black></path> <path d="M250 405.5C352.173 405.5 435 323.232 435 221.75C435 120.268 352.173 38 250 38C147.827 38 65 120.268 65 221.75C65 323.232 147.827 405.5 250 405.5Z" fill=white></path> <path d="M202.198 404.174C216.789 383.118 215.755 367.316 195.735 347.627C175.715 327.943 164.062 299.145 164.062 299.145C164.062 299.145 159.709 282.419 149.794 283.958C139.88 285.497 132.6 310.492 153.368 325.783C174.135 341.069 149.232 351.456 141.242 337.099C133.252 322.741 111.435 285.831 100.121 278.772C88.8117 271.713 80.8483 275.668 83.5151 290.218C86.182 304.769 133.48 340.036 128.878 347.668C124.276 355.296 108.058 338.7 108.058 338.7C108.058 338.7 57.3079 293.255 46.2587 305.097C35.2096 316.94 54.641 326.863 82.3328 343.359C110.03 359.85 112.177 364.206 108.248 370.446C104.314 376.685 43.1836 325.971 37.4417 347.47C31.705 368.969 99.8291 375.209 95.6247 390.051C91.4203 404.899 47.6372 361.958 38.6823 378.689C29.7221 395.425 100.465 415.088 101.038 415.234C123.889 421.067 181.924 433.426 202.198 404.174Z" fill=white></path> <path d="M90.9935 255C82.4744 255 74.8603 258.477 69.551 264.784C66.2675 268.69 62.8367 274.986 62.5578 284.414C58.985 283.394 55.5489 282.824 52.3391 282.824C44.183 282.824 36.8163 285.93 31.6069 291.573C24.9137 298.815 21.9407 307.715 23.2351 316.62C23.8508 320.861 25.2768 324.663 27.4079 328.182C22.9142 331.795 19.6044 336.826 18.0047 342.876C16.7524 347.619 15.4685 357.497 22.1722 367.673C21.746 368.337 21.3461 369.027 20.9725 369.733C16.9418 377.336 16.684 385.927 20.2411 393.928C25.6346 406.054 39.0368 415.608 65.0625 425.863C81.2536 432.242 96.0661 436.321 96.1976 436.357C117.603 441.874 136.962 444.677 153.721 444.677C184.525 444.677 206.578 435.301 219.27 416.811C239.697 387.036 236.776 359.803 210.346 333.552C195.717 319.026 185.993 297.607 183.967 292.906C179.884 278.986 169.086 263.513 151.138 263.513H151.133C149.622 263.513 148.096 263.633 146.592 263.869C138.73 265.097 131.858 269.595 126.949 276.361C121.65 269.814 116.504 264.606 111.847 261.667C104.827 257.243 97.813 255 90.9935 255ZM90.9935 275.917C93.6771 275.917 96.9553 277.051 100.57 279.331C111.794 286.406 133.452 323.403 141.382 337.793C144.039 342.614 148.581 344.654 152.669 344.654C160.783 344.654 167.118 336.638 153.411 326.451C132.8 311.124 140.03 286.072 149.87 284.529C150.301 284.461 150.727 284.43 151.138 284.43C160.083 284.43 164.03 299.751 164.03 299.751C164.03 299.751 175.595 328.616 195.465 348.346C215.334 368.08 216.36 383.919 201.879 405.024C192.002 419.415 173.096 421.292 153.721 421.292C133.626 421.292 112.99 417.772 101.445 414.796C100.877 414.65 30.7019 396.255 39.5946 379.48C41.089 376.661 43.5516 375.532 46.6509 375.532C59.1744 375.532 81.9535 394.054 91.746 394.054C93.935 394.054 95.5662 392.371 96.1976 390.112C100.555 374.522 32.6646 369.738 38.3633 348.189C39.3683 344.377 42.094 342.829 45.9248 342.834C62.4737 342.834 99.6021 371.756 107.385 371.756C107.979 371.756 108.405 371.584 108.637 371.218C112.536 364.964 110.74 359.872 83.257 343.343C55.7738 326.808 36.1428 317.588 47.114 305.718C48.3768 304.347 50.1659 303.741 52.3391 303.741C69.0248 303.746 108.447 339.398 108.447 339.398C108.447 339.398 119.087 350.395 125.523 350.395C127.001 350.395 128.259 349.815 129.111 348.382C133.673 340.737 86.7366 305.388 84.0898 290.804C82.2955 280.921 85.3474 275.917 90.9935 275.917Z" fill=black></path> <path d="M296.9 404.174C282.31 383.118 283.343 367.316 303.363 347.627C323.383 327.943 335.037 299.145 335.037 299.145C335.037 299.145 339.39 282.419 349.304 283.958C359.219 285.497 366.498 310.492 345.731 325.783C324.963 341.069 349.866 351.456 357.856 337.099C365.846 322.741 387.663 285.831 398.978 278.772C410.287 271.713 418.25 275.668 415.583 290.218C412.916 304.769 365.618 340.036 370.22 347.668C374.822 355.296 391.041 338.7 391.041 338.7C391.041 338.7 441.791 293.255 452.84 305.097C463.889 316.94 444.457 326.863 416.766 343.359C389.068 359.85 386.921 364.206 390.85 370.446C394.784 376.685 455.915 325.971 461.657 347.47C467.393 368.969 399.269 375.209 403.474 390.051C407.678 404.899 451.461 361.958 460.416 378.689C469.376 395.425 398.633 415.088 398.06 415.234C375.209 421.067 317.175 433.426 296.9 404.174Z" fill=white></path> <path d="M408.105 255C416.624 255 424.238 258.477 429.547 264.784C432.831 268.69 436.262 274.986 436.541 284.414C440.113 283.394 443.549 282.824 446.759 282.824C454.915 282.824 462.282 285.93 467.491 291.573C474.185 298.815 477.158 307.715 475.863 316.62C475.248 320.861 473.822 324.663 471.69 328.182C476.184 331.795 479.494 336.826 481.094 342.876C482.346 347.619 483.63 357.497 476.926 367.673C477.352 368.337 477.752 369.027 478.126 369.733C482.157 377.336 482.414 385.927 478.857 393.928C473.464 406.054 460.062 415.608 434.036 425.863C417.845 432.242 403.032 436.321 402.901 436.357C381.495 441.874 362.136 444.677 345.377 444.677C314.573 444.677 292.52 435.301 279.829 416.811C259.402 387.036 262.322 359.803 288.753 333.552C303.381 319.026 313.105 297.607 315.131 292.906C319.214 278.986 330.012 263.513 347.961 263.513H347.966C349.476 263.513 351.002 263.633 352.507 263.869C360.368 265.097 367.24 269.595 372.15 276.361C377.449 269.814 382.595 264.606 387.252 261.667C394.271 257.243 401.285 255 408.105 255ZM408.105 275.917C405.421 275.917 402.143 277.051 398.528 279.331C387.304 286.406 365.646 323.403 357.716 337.793C355.059 342.614 350.518 344.654 346.429 344.654C338.315 344.654 331.98 336.638 345.687 326.451C366.299 311.124 359.069 286.072 349.229 284.529C348.797 284.461 348.371 284.43 347.961 284.43C339.015 284.43 335.069 299.751 335.069 299.751C335.069 299.751 323.503 328.616 303.634 348.346C283.764 368.08 282.738 383.919 297.219 405.024C307.096 419.415 326.002 421.292 345.377 421.292C365.472 421.292 386.108 417.772 397.653 414.796C398.221 414.65 468.397 396.255 459.504 379.48C458.009 376.661 455.547 375.532 452.447 375.532C439.924 375.532 417.145 394.054 407.352 394.054C405.163 394.054 403.532 392.371 402.901 390.112C398.543 374.522 466.434 369.738 460.735 348.189C459.73 344.377 457.004 342.829 453.174 342.834C436.625 342.834 399.496 371.756 391.714 371.756C391.119 371.756 390.693 371.584 390.461 371.218C386.562 364.964 388.358 359.872 415.841 343.343C443.325 326.808 462.956 317.588 451.984 305.718C450.722 304.347 448.932 303.741 446.759 303.741C430.074 303.746 390.651 339.398 390.651 339.398C390.651 339.398 380.011 350.395 373.576 350.395C372.097 350.395 370.84 349.815 369.987 348.382C365.425 340.737 412.362 305.388 415.009 290.804C416.803 280.921 413.751 275.917 408.105 275.917Z" fill=black></path> <path d="M319.277 228.901C319.277 205.236 288.585 241.304 250.637 241.465C212.692 241.306 182 205.238 182 228.901C182 244.591 189.507 270.109 209.669 285.591C213.681 271.787 235.726 260.729 238.877 262.317C243.364 264.578 243.112 270.844 250.637 276.365C258.163 270.844 257.911 264.58 262.398 262.317C265.551 260.729 287.594 271.787 291.605 285.591C311.767 270.109 319.275 244.591 319.275 228.903L319.277 228.901Z" fill=#0E1116></path> <path d="M262.4 262.315C257.913 264.576 258.165 270.842 250.639 276.363C243.114 270.842 243.366 264.578 238.879 262.315C235.726 260.727 213.683 271.785 209.672 285.589C219.866 293.417 233.297 298.678 250.627 298.806C250.631 298.806 250.635 298.806 250.641 298.806C250.646 298.806 250.65 298.806 250.656 298.806C267.986 298.68 281.417 293.417 291.611 285.589C287.6 271.785 265.555 260.727 262.404 262.315H262.4Z" fill=#FF323D></path> <path d="M373 196C382.389 196 390 188.389 390 179C390 169.611 382.389 162 373 162C363.611 162 356 169.611 356 179C356 188.389 363.611 196 373 196Z" fill=black></path> <path d="M128 196C137.389 196 145 188.389 145 179C145 169.611 137.389 162 128 162C118.611 162 111 169.611 111 179C111 188.389 118.611 196 128 196Z" fill=black></path> <path d="M313.06 171.596C319.796 173.968 322.476 187.779 329.281 184.171C342.167 177.337 347.06 161.377 340.208 148.524C333.356 135.671 317.354 130.792 304.467 137.626C291.58 144.46 286.688 160.419 293.54 173.272C296.774 179.339 307.039 169.475 313.06 171.596Z" fill=#0E1116></path> <path d="M188.554 171.596C181.818 173.968 179.138 187.779 172.334 184.171C159.447 177.337 154.555 161.377 161.407 148.524C168.259 135.671 184.26 130.792 197.147 137.626C210.034 144.46 214.926 160.419 208.074 173.272C204.84 179.339 194.575 169.475 188.554 171.596Z" fill=#0E1116></path> </svg> </a> <a class=md-social__link href=http://kuldeepsinghsidhu.com rel=noopener target=_blank title=kuldeepsinghsidhu.com> <svg viewbox="0 0 16 16" xmlns=http://www.w3.org/2000/svg><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0M5.78 8.75a9.64 9.64 0 0 0 1.363 4.177q.383.64.857 1.215c.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a10 10 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.51 6.51 0 0 0 4.666 5.5q-.184-.271-.352-.552c-.715-1.192-1.437-2.874-1.581-4.948m-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948q.18-.295.353-.552a6.51 6.51 0 0 0-4.666 5.5m10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948q-.18.296-.353.552a6.51 6.51 0 0 0 4.666-5.5Zm2.733-1.5a6.51 6.51 0 0 0-4.666-5.5q.184.272.353.552c.714 1.192 1.436 2.874 1.58 4.948Z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "search.highlight", "search.share", "search.suggest", "content.tooltips", "navigation.instant.progress", "navigation.path", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../javascripts/xfile.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Science","text":""},{"location":"#crack-data-science-interviews","title":"Crack Data Science Interviews","text":"<ul> <li>  Interview Questions <p>These are currently most commonly asked interview questions.</p> <p>Questions can be removed if they are no longer popular in interview circles and added as new question banks are released.</p> <ul> <li>\ud83d\udcda Interview Question Resources - Community-curated sources for all topics</li> <li>DSA (Data Structures &amp; Algorithms)</li> <li>System Design</li> <li>Natural Language Processing (NLP)</li> <li>Probability</li> <li>A/B Testing</li> <li>SQL</li> <li>ML-Algorithms</li> <li>Python</li> <li>Pandas</li> <li>NumPy</li> <li>Scikit-Learn</li> <li>LangChain</li> <li>LangGraph</li> </ul> </li> <li> Cheat Sheets <p>Distilled down important concepts for your quick reference</p> <ul> <li>Cheat-Sheets/Django</li> <li>Cheat-Sheets/Flask</li> <li>Cheat-Sheets/Hypothesis-Tests</li> <li>Cheat-Sheets/Keras</li> <li>Cheat-Sheets/NumPy</li> <li>Cheat-Sheets/Pandas</li> <li>Cheat-Sheets/PySpark</li> <li>Cheat-Sheets/PyTorch</li> <li>Cheat-Sheets/Python</li> <li>Cheat-Sheets/RegEx</li> <li>Cheat-Sheets/Sk-learn</li> <li>Cheat-Sheets/SQL</li> <li>Cheat-Sheets/tensorflow</li> </ul> </li> <li> ML Algorithms <p>From scratch implementation and documentation of all ML algorithms</p> <ul> <li>ARIMA</li> <li>Activation functions</li> <li>Collaborative Filtering</li> <li>Confusion Matrix</li> <li>DBSCAN</li> <li>Decision Trees</li> <li>Gradient Boosting</li> <li>K-means clustering</li> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Loss Function MAE, RMSE</li> <li>Neural Networks</li> <li>Normal Distribution</li> <li>Normalization Regularisation</li> <li>Overfitting, Underfitting</li> <li>PCA</li> <li>Random Forest</li> <li>Support Vector Machines</li> <li>Unbalanced, Skewed data</li> <li>kNN</li> </ul> </li> <li> Online Resources <p>Most popular and commonly reffered online resources</p> <ul> <li>Online Study Material</li> <li>Popular Blogs</li> </ul> </li> </ul> <p>This is a completely open-source platform for maintaining curated list of interview questions and answers for people looking and preparing for data science opportunities.</p> <p>Not only this, the platform will also serve as one point destination for all your needs like tutorials, online materials, etc.</p> <p>This platform is maintained by you! \ud83e\udd17 You can help us by answering/ improving existing questions as well as by sharing any new questions that you faced during your interviews. You can also improve topics and articles.</p> Current Platform Status  Done Under Development To Do About <ul> <li>Cheat-Sheets/Django</li> <li>Cheat-Sheets/Flask</li> <li>Cheat-Sheets/Hypothesis-Tests</li> <li>Cheat-Sheets/Keras</li> <li>Cheat-Sheets/NumPy</li> <li>Cheat-Sheets/Pandas</li> <li>Cheat-Sheets/PySpark</li> <li>Cheat-Sheets/PyTorch</li> <li>Cheat-Sheets/Python</li> <li>Cheat-Sheets/RegEx</li> <li>Cheat-Sheets/Sk-learn</li> <li>Cheat-Sheets/SQL</li> <li>Cheat-Sheets/tensorflow</li> <li>Interview-Questions/DSA</li> <li>Interview-Questions/System-Design</li> <li>Interview-Questions/Natural-Language-Processing</li> <li>Interview-Questions/Probability</li> <li>Interview-Questions/AB-Testing</li> <li>Interview-Questions/SQL</li> <li>Interview-Questions/ML-Algorithms</li> <li>Interview-Questions/Python</li> <li>Interview-Questions/Pandas</li> <li>Interview-Questions/NumPy</li> <li>Interview-Questions/Scikit-Learn</li> <li>Interview-Questions/LangChain</li> <li>Interview-Questions/LangGraph</li> <li>Machine-Learning/ARIMA</li> <li>Machine-Learning/Activation-Functions</li> <li>Machine-Learning/Collaborative-Filtering</li> <li>Machine-Learning/Confusion-Matrix</li> <li>Machine-Learning/DBSCAN</li> <li>Machine-Learning/Decision-Trees</li> <li>Machine-Learning/Gradient-Boosting</li> <li>Machine-Learning/K-means-Clustering</li> <li>Machine-Learning/Linear-Regression</li> <li>Machine-Learning/Logistic-Regression</li> <li>Machine-Learning/Loss-Function-MAE-RMSE</li> <li>Machine-Learning/Neural-Networks</li> <li>Machine-Learning/Normal-Distribution</li> <li>Machine-Learning/Normalization-Regularisation</li> <li>Machine-Learning/Overfitting-Underfitting</li> <li>Machine-Learning/PCA</li> <li>Machine-Learning/Random-Forest</li> <li>Machine-Learning/Support-Vector-Machines</li> <li>Machine-Learning/Unbalanced-Skewed-Data</li> <li>Machine-Learning/kNN</li> </ul> <p>Learn about How to contribute?  You can pick anyone, write in <code>.py</code>, <code>.md</code>, <code>.txt</code> or <code>.ipynb</code>; I will format it!</p> <p>Currently no pending items - all major sections are complete!</p> <p> Useful Commands </p> <ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>mkdocs gh-deploy</code> - Use\u00a0<code>mkdocs gh-deploy --help</code>\u00a0to get a full list of options available for the\u00a0<code>gh-deploy</code>\u00a0command.     Be aware that you will not be able to review the built site before it is pushed to GitHub. Therefore, you may want to verify any changes you make to the docs beforehand by using the\u00a0<code>build</code>\u00a0or\u00a0<code>serve</code>\u00a0commands and reviewing the built files locally.</li> <li><code>mkdocs new [dir-name]</code> - Create a new project. No need to create a new project</li> </ul> <p> Useful Documents </p> <ul> <li> <p>\ud83d\udcd1 MkDocs: </p> <ul> <li>GitHub: https://github.com/mkdocs/mkdocs</li> <li>Documentation: https://www.mkdocs.org/</li> </ul> </li> <li> <p>\ud83c\udfa8 Theme: </p> <ul> <li>GitHub: https://github.com/squidfunk/mkdocs-material</li> <li>Documentation: https://squidfunk.github.io/mkdocs-material/getting-started/</li> </ul> </li> </ul> <ul> <li>:: Project Maintainer</li> <li> All Contributors list</li> <li> AGPL-3.0 license</li> <li> Reach Out</li> </ul> <ul> <li>:: Project Maintainer</li> <li> All Contributors list</li> </ul>"},{"location":"Contribute/","title":"Contributions to singhsidhukuldeep.github.io","text":"<p>For any correspondence please check contact</p> <p>Detailed step by step contributions guide coming soon!</p> <p>For now, plese open a discussion here for anything!</p> <p>List of things to contribute!</p> <p>Thank you, Kuldeep</p>"},{"location":"Introduction/","title":"Home","text":""},{"location":"Introduction/#introduction","title":"Introduction","text":"<p>This is a completely open-source platform for maintaining curated list of interview questions and answers for people looking and preparing for data science opportunities.</p> <p>Not only this, the platform will also serve as one point destination for all your needs like tutorials, online materials, etc.</p> <p>This platform is maintained by you! \ud83e\udd17 You can help us by answering/ improving existing questions as well as by sharing any new questions that you faced during your interviews.</p>"},{"location":"Introduction/#contribute-to-the-platform","title":"Contribute to the platform","text":"<p>Contribution in any form will be deeply appreciated. \ud83d\ude4f</p>"},{"location":"Introduction/#add-questions","title":"Add questions","text":"<p>\u2753 Add your questions here. Please ensure to provide a detailed description to allow your fellow contributors to understand your questions and answer them to your satisfaction.</p> <p></p> <p>\ud83e\udd1d Please note that as of now, you cannot directly add a question via a pull request. This will help us to maintain the quality of the content for you.</p>"},{"location":"Introduction/#add-answerstopics","title":"Add answers/topics","text":"<p>\ud83d\udcdd These are the answers/topics that need your help at the moment</p> <ul> <li> Add documentation for the project</li> <li> Online Material for Learning</li> <li> Suggested Learning Paths</li> <li> Cheat Sheets<ul> <li> Django</li> <li> Flask</li> <li> Numpy</li> <li> Pandas</li> <li> PySpark</li> <li> Python</li> <li> RegEx</li> <li> SQL</li> </ul> </li> <li> NLP Interview Questions</li> <li> Add python common DSA interview questions</li> <li> Add Major ML topics<ul> <li> Linear Regression </li> <li> Logistic Regression </li> <li> SVM </li> <li> Random Forest </li> <li> Gradient boosting </li> <li> PCA </li> <li> Collaborative Filtering </li> <li> K-means clustering </li> <li> kNN </li> <li> ARIMA </li> <li> Neural Networks </li> <li> Decision Trees </li> <li> Overfitting, Underfitting</li> <li> Unbalanced, Skewed data</li> <li> Activation functions relu/ leaky relu</li> <li> Normalization</li> <li> DBSCAN </li> <li> Normal Distribution </li> <li> Precision, Recall </li> <li> Loss Function MAE, RMSE </li> </ul> </li> <li> Add Pandas questions</li> <li> Add NumPy questions</li> <li> Add TensorFlow questions</li> <li> Add PyTorch questions</li> <li> Add list of learning resources</li> </ul>"},{"location":"Introduction/#reportsolve-issues","title":"Report/Solve Issues","text":"<p>\ud83d\udd27 To report any issues find me on LinkedIn or raise an issue on GitHub.</p> <p>\ud83d\udee0 You can also solve existing issues on GitHub and create a pull request.</p>"},{"location":"Introduction/#say-thanks","title":"Say Thanks","text":"<p>\ud83d\ude0a If this platform helped you in any way, it would be great if you could share it with others.</p> <p> </p> <pre><code>Check out this \ud83d\udc47 platform \ud83d\udc47 for data science content:\n\ud83d\udc49 https://singhsidhukuldeep.github.io/data-science-interview-prep/ \ud83d\udc48\n</code></pre> <p>You can also star the repository on GitHub    and watch-out for any updates </p>"},{"location":"Introduction/#features","title":"Features","text":"<ul> <li> <p>\ud83c\udfa8 Beautiful: The design is built on top of most popular libraries like MkDocs and material which allows the platform to be responsive and to work on all sorts of devices \u2013 from mobile phones to wide-screens. The underlying fluid layout will always adapt perfectly to the available screen space.</p> </li> <li> <p>\ud83e\uddd0 Searchable: almost magically, all the content on the website is searchable without any further ado. The built-in search \u2013 server-less \u2013 is fast and accurate in responses to any of the queries.</p> </li> <li> <p>\ud83d\ude4c Accessible:</p> <ul> <li>Easy to use: \ud83d\udc4c The website is hosted on github-pages and is free and open to use to over 40 million users of GitHub in 100+ countries.</li> <li>Easy to contribute: \ud83e\udd1d The website embodies the concept of collaboration to the latter. Allowing anyone to add/improve the content. To make contributing easy, everything is written in MarkDown and then compiled to beautiful html.</li> </ul> </li> </ul>"},{"location":"Introduction/#setup","title":"Setup","text":"<p>No setup is required for usage of the platform</p> <p>Important: It is strongly advised to use virtual environment and not change anything in <code>gh-pages</code></p>"},{"location":"Introduction/#linux-systems","title":"<code>Linux</code> Systems","text":"<pre><code>python3 -m venv ./venv\n\nsource venv/bin/activate\n\npip3 install -r requirements.txt\n</code></pre> <pre><code>deactivate\n</code></pre>"},{"location":"Introduction/#windows-systems","title":"<code>Windows</code> Systems","text":"<pre><code>python3 -m venv ./venv\n\nvenv\\Scripts\\activate\n\npip3 install -r requirements.txt\n</code></pre> <pre><code>venv\\Scripts\\deactivate\n</code></pre>"},{"location":"Introduction/#to-install-the-latest","title":"To install the latest","text":"<pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-minify-plugin\npip3 install mkdocs-git-revision-date-localized-plugin\n</code></pre>"},{"location":"Introduction/#useful-commands","title":"Useful Commands","text":"<ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>mkdocs gh-deploy</code> - Use\u00a0<code>mkdocs gh-deploy --help</code>\u00a0to get a full list of options available for the\u00a0<code>gh-deploy</code>\u00a0command.     Be aware that you will not be able to review the built site before it is pushed to GitHub. Therefore, you may want to verify any changes you make to the docs beforehand by using the\u00a0<code>build</code>\u00a0or\u00a0<code>serve</code>\u00a0commands and reviewing the built files locally.</li> <li><code>mkdocs new [dir-name]</code> - Create a new project. No need to create a new project</li> </ul>"},{"location":"Introduction/#useful-documents","title":"Useful Documents","text":"<ul> <li> <p>\ud83d\udcd1 MkDocs: </p> <ul> <li>GitHub: https://github.com/mkdocs/mkdocs</li> <li>Documentation: https://www.mkdocs.org/</li> </ul> </li> <li> <p>\ud83c\udfa8 Theme: </p> <ul> <li>GitHub: https://github.com/squidfunk/mkdocs-material</li> <li>Documentation: https://squidfunk.github.io/mkdocs-material/getting-started/</li> </ul> </li> </ul>"},{"location":"Introduction/#faq","title":"FAQ","text":"<ul> <li> <p>Can I filter questions based on companies? \ud83e\udd2a</p> <p>As much as this platform aims to help you with your interview preparation, it is not a short-cut to crack one. Think of this platform as a practicing field to help you sharpen your skills for your interview processes. However, for your convenience we have sorted all the questions by topics for you. \ud83e\udd13</p> <p>This doesn't mean that such feature won't be added in the future.  \"Never say Never\"</p> <p>But as of now there is neither plan nor data to do so. \ud83d\ude22</p> </li> <li> <p>Why is this platform free? \ud83e\udd17</p> <p>Currently there is no major cost involved in maintaining this platform other than time and effort that is put in by every contributor.  If you want to help you can contribute here. </p> <p>If you still want to pay for something that is free, we would request you to donate it to a charity of your choice instead. \ud83d\ude07</p> </li> </ul>"},{"location":"Introduction/#credits","title":"Credits","text":""},{"location":"Introduction/#maintained-by","title":"Maintained by","text":"<p>\ud83d\udc68\u200d\ud83c\udf93 Kuldeep Singh Sidhu </p> <p>Github: github/singhsidhukuldeep <code>https://github.com/singhsidhukuldeep</code></p> <p>Website: Kuldeep Singh Sidhu (Website) <code>http://kuldeepsinghsidhu.com</code></p> <p>LinkedIn: Kuldeep Singh Sidhu (LinkedIn) <code>https://www.linkedin.com/in/singhsidhukuldeep/</code></p>"},{"location":"Introduction/#contributors","title":"Contributors","text":"<p>\ud83d\ude0e The full list of all the contributors is available here</p>"},{"location":"Introduction/#current-status","title":"Current Status","text":""},{"location":"contact/","title":"Contact for https://singhsidhukuldeep.github.io","text":"<p>Welcome to https://singhsidhukuldeep.github.io/ </p> <p>For any information, request or official correspondence please email to: singhsidhukuldeep@gmail.com</p> <p>Mailing Address:</p> <p>Kuldeep Singh Sidhu</p> <p>Street No 4, Malviya Nagar Bathinda, Punjab, 151001 India</p>"},{"location":"contact/#follow-on-social-media","title":"Follow on Social Media","text":"Platform Link GitHub https://github.com/singhsidhukuldeep LinkedIn https://www.linkedin.com/in/singhsidhukuldeep/ Twitter (X) https://twitter.com/kuldeep_s_s HuggingFace https://huggingface.co/singhsidhukuldeep StackOverflow https://stackoverflow.com/users/7182350 Website http://kuldeepsinghsidhu.com/"},{"location":"privacy/","title":"Privacy Policy for https://singhsidhukuldeep.github.io","text":""},{"location":"privacy/#introduction","title":"Introduction","text":"<p>Welcome to https://singhsidhukuldeep.github.io/ (the \"Website\"). Your privacy is important to us, and we are committed to protecting the personal information you share with us. This Privacy Policy explains how we collect, use, and disclose your information, and our commitment to ensuring that your personal data is handled with care and security.</p> <p>This policy complies with the General Data Protection Regulation (GDPR), ePrivacy Directive (EPD), California Privacy Rights Act (CPRA), Colorado Privacy Act (CPA), Virginia Consumer Data Protection Act (VCDPA), and Brazil's Lei Geral de Prote\u00e7\u00e3o de Dados (LGPD).</p>"},{"location":"privacy/#information-we-collect","title":"Information We Collect","text":""},{"location":"privacy/#personal-information","title":"Personal Information","text":"<p>We may collect personally identifiable information about you, such as:</p> <ul> <li>Name</li> <li>Email address</li> <li>IP address</li> <li>Other information you voluntarily provide through contact forms or interactions with the Website</li> </ul>"},{"location":"privacy/#non-personal-information","title":"Non-Personal Information","text":"<p>We may also collect non-personal information such as:</p> <ul> <li>Browser type</li> <li>Language preference</li> <li>Referring site</li> <li>Date and time of each visitor request</li> <li>Aggregated data on how visitors use the Website</li> </ul>"},{"location":"privacy/#cookies-and-web-beacons","title":"Cookies and Web Beacons","text":"<p>Our Website uses cookies to enhance your experience. A cookie is a small file that is placed on your device when you visit our Website. Cookies help us to:</p> <ul> <li>Remember your preferences and settings</li> <li>Understand how you interact with our Website</li> <li>Track and analyze usage patterns</li> </ul> <p>You can disable cookies through your browser settings; however, doing so may affect your ability to access certain features of the Website.</p>"},{"location":"privacy/#google-adsense","title":"Google AdSense","text":"<p>We use Google AdSense to display advertisements on our Website. Google AdSense may use cookies and web beacons to collect information about your interaction with the ads displayed on our Website. This information may include:</p> <ul> <li>Your IP address</li> <li>The type of browser you use</li> <li>The pages you visit on our Website</li> </ul> <p>Google may use this information to show you personalized ads based on your interests and browsing history. For more information on how Google uses your data, please visit the Google Privacy &amp; Terms page.</p>"},{"location":"privacy/#legal-bases-for-processing-your-data-gdpr-compliance","title":"Legal Bases for Processing Your Data (GDPR Compliance)","text":"<p>We process your personal data under the following legal bases:</p> <ul> <li>Consent: When you have given explicit consent for us to process your data for specific purposes.</li> <li>Contract: When processing your data is necessary to fulfill a contract with you or to take steps at your request before entering into a contract.</li> <li>Legitimate Interests: When the processing is necessary for our legitimate interests, such as improving our services, provided these are not overridden by your rights.</li> <li>Compliance with Legal Obligations: When we need to process your data to comply with a legal obligation.</li> </ul>"},{"location":"privacy/#how-your-data-will-be-used-to-show-ads","title":"How Your Data Will Be Used to Show Ads","text":"<p>We work with third-party vendors, including Google, to serve ads on our Website. These vendors use cookies and similar technologies to collect and use data about your visits to this and other websites to show you ads that are more relevant to your interests.</p>"},{"location":"privacy/#types-of-data-used","title":"Types of Data Used","text":"<p>The data used to show you ads may include:</p> <ul> <li>Demographic Information: Age, gender, and other demographic details</li> <li>Location Data: Approximate geographical location based on your IP address</li> <li>Behavioral Data: Your browsing behavior, such as pages visited, links clicked, and time spent on our Website</li> <li>Interests and Preferences: Based on your browsing history, the types of ads you interact with, and your preferences across websites</li> </ul>"},{"location":"privacy/#purpose-of-data-usage","title":"Purpose of Data Usage","text":"<p>The primary purpose of collecting and using this data is to:</p> <ul> <li>Serve ads that are relevant and tailored to your interests</li> <li>Improve ad targeting and effectiveness</li> <li>Analyze and optimize the performance of ads on our Website</li> </ul>"},{"location":"privacy/#opting-out-of-personalized-ads","title":"Opting Out of Personalized Ads","text":"<p>You can opt out of personalized ads by adjusting your ad settings with Google and other third-party vendors. For more information on how to opt out of personalized ads, please visit the Google Ads Settings page and review the options available to manage your preferences.</p>"},{"location":"privacy/#data-subject-rights-gdpr-cpra-cpa-vcdpa-lgpd-compliance","title":"Data Subject Rights (GDPR, CPRA, CPA, VCDPA, LGPD Compliance)","text":"<p>Depending on your jurisdiction, you have the following rights regarding your personal data:</p>"},{"location":"privacy/#right-to-access","title":"Right to Access","text":"<p>You have the right to request access to the personal data we hold about you and to receive a copy of this data.</p>"},{"location":"privacy/#right-to-rectification","title":"Right to Rectification","text":"<p>You have the right to request that we correct any inaccuracies in the personal data we hold about you.</p>"},{"location":"privacy/#right-to-erasure-right-to-be-forgotten","title":"Right to Erasure (Right to Be Forgotten)","text":"<p>You have the right to request that we delete your personal data, subject to certain conditions and legal obligations.</p>"},{"location":"privacy/#right-to-restriction-of-processing","title":"Right to Restriction of Processing","text":"<p>You have the right to request that we restrict the processing of your personal data in certain circumstances, such as when you contest the accuracy of the data.</p>"},{"location":"privacy/#right-to-data-portability","title":"Right to Data Portability","text":"<p>You have the right to receive your personal data in a structured, commonly used, and machine-readable format and to transmit this data to another controller.</p>"},{"location":"privacy/#right-to-object","title":"Right to Object","text":"<p>You have the right to object to the processing of your personal data based on legitimate interests or for direct marketing purposes.</p>"},{"location":"privacy/#right-to-withdraw-consent","title":"Right to Withdraw Consent","text":"<p>Where we rely on your consent to process your personal data, you have the right to withdraw your consent at any time.</p>"},{"location":"privacy/#right-to-non-discrimination-cpra-compliance","title":"Right to Non-Discrimination (CPRA Compliance)","text":"<p>We will not discriminate against you for exercising any of your privacy rights under CPRA or any other applicable laws.</p>"},{"location":"privacy/#exercising-your-rights","title":"Exercising Your Rights","text":"<p>To exercise any of these rights, please contact us at:</p> <p>Email: singhsidhukuldeep@gmail.com</p> <p>We will respond to your request within the timeframes required by applicable law.</p>"},{"location":"privacy/#how-we-use-your-information","title":"How We Use Your Information","text":"<p>We use the information collected from you to:</p> <ul> <li>Improve the content and functionality of our Website</li> <li>Display relevant advertisements through Google AdSense and other ad networks</li> <li>Respond to your inquiries and provide customer support</li> <li>Analyze usage patterns and improve our services</li> </ul>"},{"location":"privacy/#data-sharing-and-disclosure","title":"Data Sharing and Disclosure","text":""},{"location":"privacy/#third-party-service-providers","title":"Third-Party Service Providers","text":"<p>We may share your personal data with third-party service providers who assist us in operating our Website, conducting our business, or servicing you, as long as these parties agree to keep this information confidential.</p>"},{"location":"privacy/#legal-obligations","title":"Legal Obligations","text":"<p>We may disclose your personal data when required by law or to comply with legal processes, such as a court order or subpoena.</p>"},{"location":"privacy/#business-transfers","title":"Business Transfers","text":"<p>In the event of a merger, acquisition, or sale of all or a portion of our assets, your personal data may be transferred to the acquiring entity.</p>"},{"location":"privacy/#data-retention","title":"Data Retention","text":"<p>We will retain your personal data only for as long as necessary to fulfill the purposes outlined in this Privacy Policy unless a longer retention period is required or permitted by law.</p>"},{"location":"privacy/#data-security","title":"Data Security","text":"<p>We take reasonable measures to protect your information from unauthorized access, alteration, disclosure, or destruction. However, no method of transmission over the internet or electronic storage is 100% secure, and we cannot guarantee absolute security.</p>"},{"location":"privacy/#cross-border-data-transfers","title":"Cross-Border Data Transfers","text":"<p>Your personal data may be transferred to, and processed in, countries other than the country in which you are resident. These countries may have data protection laws that are different from the laws of your country.</p> <p>Where we transfer your personal data to other countries, we will take appropriate measures to ensure that your personal data remains protected in accordance with this Privacy Policy and applicable data protection laws.</p>"},{"location":"privacy/#your-consent","title":"Your Consent","text":"<p>By using our Website, you consent to our Privacy Policy and agree to its terms.</p>"},{"location":"privacy/#changes-to-this-privacy-policy","title":"Changes to This Privacy Policy","text":"<p>We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page. You are advised to review this Privacy Policy periodically for any changes.</p>"},{"location":"privacy/#contact-us","title":"Contact Us","text":"<p>If you have any questions about this Privacy Policy, or if you would like to exercise your rights under GDPR, CPRA, CPA, VCDPA, or LGPD, please contact us at:</p> <p>Email: singhsidhukuldeep@gmail.com</p> <p>Mailing Address:</p> <p>Kuldeep Singh Sidhu</p> <p>Street No 4, Malviya Nagar Bathinda, Punjab, 151001 India</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#introduction","title":"Introduction","text":"<p>These are the projects that you can take inspiration from and try to improve on them. \u270d\ufe0f</p> <p></p>"},{"location":"projects/#popular-sources","title":"Popular Sources","text":""},{"location":"projects/#list-of-projects","title":"List of projects","text":""},{"location":"projects/#natural-language-processing-nlp","title":"Natural Language processing (NLP)","text":"Title Description Source Author Text Classification with Facebook fasttext Building the User Review Model with fastText (Text Classification) with response time of less than one second Kuldeep Singh Sidhu Chat-bot using ChatterBot ChatterBot is a Python library that makes it easy to generate automated responses to a user\u2019s input. Kuldeep Singh Sidhu Text Summarizer Comparing state of the art models for text summary generation Kuldeep Singh Sidhu NLP with Spacy Building NLP pipeline using Spacy Kuldeep Singh Sidhu"},{"location":"projects/#recommendation-engine","title":"Recommendation Engine","text":"Title Description Source Author Recommendation Engine with Surprise Comparing different recommendation systems algorithms like SVD, SVDpp (Matrix Factorization), KNN Baseline, KNN Basic, KNN Means, KNN ZScore), Baseline, Co Clustering Kuldeep Singh Sidhu"},{"location":"projects/#image-processing","title":"Image Processing","text":"Title Description Source Author Facial Landmarks Using Dlib, a library capable of giving you 68 points (land marks) of the face. Kuldeep Singh Sidhu"},{"location":"projects/#reinforcement-learning","title":"Reinforcement Learning","text":"Title Description Source Author Google Dopamine Dopamine is a research framework for fast prototyping of reinforcement learning algorithms. Kuldeep Singh Sidhu Tic Tac Toe Training a computer to play Tic Tac Toe using reinforcement learning algorithms. Kuldeep Singh Sidhu"},{"location":"projects/#others","title":"Others","text":"Title Description Source Author TensorFlow Eager Execution Eager Execution (EE) enables you to run operations immediately. Kuldeep Singh Sidhu"},{"location":"Cheat-Sheets/Django/","title":"Django Cheat Sheet","text":"<ul> <li>Django Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Create a Project</li> <li>Create an App</li> <li>Run the Development Server</li> </ul> </li> <li>Models<ul> <li>Define a Model (models.py)</li> <li>Model Fields</li> <li>Model Meta Options</li> <li>Querying the Database</li> <li>Raw SQL Queries</li> </ul> </li> <li>Views<ul> <li>Create a View (views.py)</li> <li>Class-Based Views</li> <li>Function-Based View Decorators</li> </ul> </li> <li>URLs<ul> <li>Define URL Patterns (urls.py)</li> <li>URL Reversing</li> </ul> </li> <li>Templates<ul> <li>Create a Template (mytemplate.html)</li> <li>Template Inheritance</li> <li>Template Tags and Filters</li> <li>Common Template Filters</li> </ul> </li> <li>Forms<ul> <li>Define a Form (forms.py)</li> <li>Form Fields</li> <li>Form Widgets</li> <li>Render a Form in a Template</li> <li>Process Form Data in a View</li> </ul> </li> <li>Admin Interface<ul> <li>Register a Model (admin.py)</li> <li>Customize Admin Interface</li> <li>Inline Admin</li> </ul> </li> <li>Settings (settings.py)<ul> <li>Key Settings</li> <li>Database Configuration</li> <li>Static Files Configuration</li> <li>Middleware Configuration</li> <li>Caching Configuration</li> <li>Email Configuration</li> </ul> </li> <li>Common Commands</li> <li>Django REST Framework (DRF)<ul> <li>Installation</li> <li>Serializers (serializers.py)</li> <li>Views (views.py)</li> <li>URLs (urls.py)</li> <li>Authentication and Permissions</li> <li>APIView</li> </ul> </li> <li>Security<ul> <li>CSRF Protection</li> <li>SQL Injection</li> <li>XSS (Cross-Site Scripting)</li> <li>Clickjacking</li> <li>Security Headers</li> <li>HTTPS</li> <li>Authentication</li> <li>Test Client</li> </ul> </li> <li>Deployment<ul> <li>Production Settings</li> <li>Web Server (Gunicorn)</li> <li>Process Manager (systemd)</li> <li>Static Files</li> <li>Media Files</li> <li>Database</li> <li>Environment Variables</li> </ul> </li> <li>Caching<ul> <li>Per-Site Cache</li> <li>Per-View Cache</li> <li>Template Fragment Caching</li> <li>Low-Level Cache API</li> </ul> </li> <li>Signals<ul> <li>Define a Signal (signals.py)</li> <li>Connect Signals (apps.py)</li> <li>Common Signals</li> </ul> </li> <li>Internationalization (i18n) and Localization (l10n)<ul> <li>Enable i18n and l10n</li> <li>Set the Language Code</li> <li>Translate Strings</li> <li>Mark Strings for Translation</li> <li>Translate Strings with Context</li> <li>Pluralization</li> <li>Switch Language</li> </ul> </li> <li>Custom Management Commands<ul> <li>Create a Command (management/commands/mycommand.py)</li> <li>Run the Command</li> </ul> </li> <li>Middleware<ul> <li>Create a Middleware (middleware.py)</li> <li>Activate Middleware</li> </ul> </li> <li>File Handling<ul> <li>Uploading Files</li> <li>Serving Files</li> </ul> </li> <li>Logging<ul> <li>Configure Logging (settings.py)</li> <li>Use Logging</li> </ul> </li> <li>Django Channels (Asynchronous)<ul> <li>Installation</li> <li>Configure Channels (settings.py)</li> <li>Create a Consumer (consumers.py)</li> <li>Configure Routing (routing.py)</li> <li>Update ASGI Application (asgi.py)</li> </ul> </li> <li>Django Allauth (Authentication)<ul> <li>Installation</li> <li>Configuration (settings.py)</li> <li>URLs (urls.py)</li> <li>Templates</li> </ul> </li> <li>Django Debug Toolbar<ul> <li>Installation</li> <li>Configuration (settings.py)</li> <li>URLs (urls.py)</li> </ul> </li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the Django web framework, covering essential commands, concepts, and code snippets for efficient Django development. It aims to be a one-stop reference for common tasks and best practices.</p>"},{"location":"Cheat-Sheets/Django/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/Django/#installation","title":"Installation","text":"<pre><code>pip install django\n</code></pre> <p>Consider using a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Linux/macOS\nvenv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"Cheat-Sheets/Django/#create-a-project","title":"Create a Project","text":"<pre><code>django-admin startproject myproject\ncd myproject\n</code></pre>"},{"location":"Cheat-Sheets/Django/#create-an-app","title":"Create an App","text":"<pre><code>python manage.py startapp myapp\n</code></pre>"},{"location":"Cheat-Sheets/Django/#run-the-development-server","title":"Run the Development Server","text":"<pre><code>python manage.py runserver\n</code></pre>"},{"location":"Cheat-Sheets/Django/#models","title":"Models","text":""},{"location":"Cheat-Sheets/Django/#define-a-model-modelspy","title":"Define a Model (models.py)","text":"<pre><code>from django.db import models\n\nclass MyModel(models.Model):\n    name = models.CharField(max_length=100, help_text=\"Enter the name\")\n    description = models.TextField(blank=True, null=True)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n    is_active = models.BooleanField(default=True)\n    order = models.PositiveIntegerField(default=0)\n\n    def __str__(self):\n        return self.name\n\n    class Meta:\n        ordering = ['order', '-created_at']  # Default ordering\n        verbose_name = \"My Model Entry\"\n        verbose_name_plural = \"My Model Entries\"\n</code></pre>"},{"location":"Cheat-Sheets/Django/#model-fields","title":"Model Fields","text":"<ul> <li><code>AutoField</code>: An auto-incrementing integer field (primary key by default).</li> <li><code>BigAutoField</code>: A 64-bit integer, similar to AutoField.</li> <li><code>CharField</code>: For short to medium-length strings. <code>max_length</code> is required.</li> <li><code>TextField</code>: For long strings of unlimited length. <code>blank=True</code> allows the field to be empty in forms, <code>null=True</code> allows the field to store NULL values in the database.</li> <li><code>IntegerField</code>: For integer values.</li> <li><code>PositiveIntegerField</code>: An integer field that must be positive.</li> <li><code>SmallIntegerField</code>, <code>BigIntegerField</code>: Smaller and larger integer fields.</li> <li><code>FloatField</code>: For floating-point numbers.</li> <li><code>DecimalField</code>: For fixed-precision decimal numbers. Requires <code>max_digits</code> and <code>decimal_places</code>.</li> <li><code>BooleanField</code>: For boolean values (True/False).</li> <li><code>NullBooleanField</code>: A BooleanField that also accepts NULL.</li> <li><code>DateField</code>: For dates (YYYY-MM-DD).</li> <li><code>DateTimeField</code>: For dates and times (YYYY-MM-DD HH:MM:SS). <code>auto_now_add=True</code> sets the field to the current date/time when the object is created. <code>auto_now=True</code> updates the field every time the object is saved.</li> <li><code>TimeField</code>: For times (HH:MM:SS).</li> <li><code>DurationField</code>: Stores periods of time \u2013 modeled in Python by timedelta.</li> <li><code>EmailField</code>: A CharField that validates if the input is an email address.</li> <li><code>URLField</code>: A CharField that validates URLs.</li> <li><code>FileField</code>: For file uploads. Requires <code>upload_to</code> to specify the storage directory.</li> <li><code>ImageField</code>: For image uploads. Requires Pillow library and <code>upload_to</code>.</li> <li><code>FilePathField</code>: A CharField whose choices are limited to the filenames in a certain directory on the filesystem.</li> <li><code>SlugField</code>: A CharField intended to store a \"slug\" \u2013 a short label containing only letters, numbers, underscores or hyphens.</li> <li><code>BinaryField</code>: For storing raw binary data.</li> <li><code>ForeignKey</code>: For creating one-to-many relationships with other models. Requires <code>on_delete</code> to specify what happens when the related object is deleted (e.g., <code>models.CASCADE</code>, <code>models.SET_NULL</code>).</li> <li><code>ManyToManyField</code>: For creating many-to-many relationships.</li> <li><code>OneToOneField</code>: For creating one-to-one relationships.</li> <li><code>GenericIPAddressField</code>: For storing IPv4 or IPv6 addresses.</li> <li><code>UUIDField</code>: For storing universally unique identifiers.</li> </ul>"},{"location":"Cheat-Sheets/Django/#model-meta-options","title":"Model Meta Options","text":"<ul> <li><code>ordering</code>: Defines the default ordering of objects.</li> <li><code>verbose_name</code>: A human-readable name for the model.</li> <li><code>verbose_name_plural</code>: The plural form of the verbose name.</li> <li><code>abstract = True</code>: Makes the model an abstract base class.</li> <li><code>db_table</code>: Specifies the name of the database table.</li> <li><code>unique_together</code>: Defines a set of fields that, taken together, must be unique.</li> <li><code>index_together</code>: Defines a set of fields that should be indexed together.</li> <li><code>get_latest_by</code>: Specifies a field to use for retrieving the \"latest\" object.</li> </ul>"},{"location":"Cheat-Sheets/Django/#querying-the-database","title":"Querying the Database","text":"<pre><code>from .models import MyModel\n\n# Get all objects\nall_objects = MyModel.objects.all()\n\n# Filter objects\nfiltered_objects = MyModel.objects.filter(name__contains='keyword', is_active=True)\n\n# Get a single object by primary key\nsingle_object = MyModel.objects.get(pk=1)\n\n# Get a single object, handling DoesNotExist exception\nfrom django.shortcuts import get_object_or_404\nsingle_object = get_object_or_404(MyModel, pk=1)\n\n# Create a new object\nnew_object = MyModel.objects.create(name='New Object', description='...')\n\n# Update an existing object\nobj = MyModel.objects.get(pk=1)\nobj.name = 'Updated Name'\nobj.save()\n\n# Delete an object\nobj = MyModel.objects.get(pk=1)\nobj.delete()\n\n# Complex lookups with Q objects\nfrom django.db.models import Q\nobjects = MyModel.objects.filter(Q(name__startswith='A') | Q(description__icontains='data'))\n\n# Ordering\nordered_objects = MyModel.objects.order_by('name', '-created_at')\n\n# Limiting results\nlimited_objects = MyModel.objects.all()[:10]\n\n# Chaining queries\nchained_objects = MyModel.objects.filter(is_active=True).order_by('name')\n</code></pre>"},{"location":"Cheat-Sheets/Django/#raw-sql-queries","title":"Raw SQL Queries","text":"<pre><code>from django.db import connection\n\ndef my_raw_query():\n    with connection.cursor() as cursor:\n        cursor.execute(\"SELECT * FROM myapp_mymodel WHERE name = %s\", ['My Name'])\n        row = cursor.fetchone()\n        return row\n</code></pre>"},{"location":"Cheat-Sheets/Django/#views","title":"Views","text":""},{"location":"Cheat-Sheets/Django/#create-a-view-viewspy","title":"Create a View (views.py)","text":"<pre><code>from django.shortcuts import render, get_object_or_404\nfrom django.http import HttpResponse, JsonResponse\nfrom .models import MyModel\n\ndef my_view(request):\n    data = MyModel.objects.all()  # Get all objects from MyModel\n    context = {'data': data}\n    return render(request, 'myapp/mytemplate.html', context)\n\ndef detail_view(request, pk):\n    item = get_object_or_404(MyModel, pk=pk)\n    return render(request, 'myapp/detail.html', {'item': item})\n\ndef json_response(request):\n    data = {'message': 'Hello, world!'}\n    return JsonResponse(data)\n\ndef http_response(request):\n    return HttpResponse(\"&lt;h1&gt;Hello, world!&lt;/h1&gt;\", content_type=\"text/html\")\n</code></pre>"},{"location":"Cheat-Sheets/Django/#class-based-views","title":"Class-Based Views","text":"<pre><code>from django.views.generic import ListView, DetailView, CreateView, UpdateView, DeleteView\nfrom django.urls import reverse_lazy\nfrom .models import MyModel\n\nclass MyListView(ListView):\n    model = MyModel\n    template_name = 'myapp/mymodel_list.html'\n    context_object_name = 'data'  # Renames the object_list in the template\n    paginate_by = 10  # Enable pagination\n\nclass MyDetailView(DetailView):\n    model = MyModel\n    template_name = 'myapp/mymodel_detail.html'\n    context_object_name = 'item'\n\nclass MyCreateView(CreateView):\n    model = MyModel\n    fields = ['name', 'description', 'is_active']  # Fields to include in the form\n    template_name = 'myapp/mymodel_form.html'\n    success_url = reverse_lazy('myapp:my_list_view')  # Redirect after successful creation\n\nclass MyUpdateView(UpdateView):\n    model = MyModel\n    fields = ['name', 'description', 'is_active']\n    template_name = 'myapp/mymodel_form.html'\n    success_url = reverse_lazy('myapp:my_list_view')\n\nclass MyDeleteView(DeleteView):\n    model = MyModel\n    template_name = 'myapp/mymodel_confirm_delete.html'\n    success_url = reverse_lazy('myapp:my_list_view')\n</code></pre>"},{"location":"Cheat-Sheets/Django/#function-based-view-decorators","title":"Function-Based View Decorators","text":"<ul> <li><code>@require_http_methods([\"GET\", \"POST\"])</code>: Only allows specified HTTP methods.</li> <li><code>@require_GET</code>, <code>@require_POST</code>: Shorthand for requiring GET or POST.</li> <li><code>@login_required</code>: Requires the user to be logged in.</li> <li><code>@permission_required('myapp.change_mymodel')</code>: Requires the user to have a specific permission.</li> <li><code>@staff_member_required</code>: Requires the user to be a staff member.</li> <li><code>@cache_page(60 * 15)</code>: Caches the view output for 15 minutes (requires cache configuration).</li> </ul>"},{"location":"Cheat-Sheets/Django/#urls","title":"URLs","text":""},{"location":"Cheat-Sheets/Django/#define-url-patterns-urlspy","title":"Define URL Patterns (urls.py)","text":"<p>Project <code>urls.py</code>:</p> <pre><code>from django.contrib import admin\nfrom django.urls import path, include\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('myapp/', include('myapp.urls', namespace='myapp')),  # Include app's URLs with namespace\n]\n</code></pre> <p>App <code>urls.py</code>:</p> <pre><code>from django.urls import path\nfrom . import views\n\napp_name = 'myapp'  # App namespace\n\nurlpatterns = [\n    path('', views.my_view, name='my_view'),\n    path('list/', views.MyListView.as_view(), name='my_list_view'),\n    path('detail/&lt;int:pk&gt;/', views.MyDetailView.as_view(), name='my_detail_view'),\n    path('create/', views.MyCreateView.as_view(), name='my_create_view'),\n    path('update/&lt;int:pk&gt;/', views.MyUpdateView.as_view(), name='my_update_view'),\n    path('delete/&lt;int:pk&gt;/', views.MyDeleteView.as_view(), name='my_delete_view'),\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#url-reversing","title":"URL Reversing","text":"<p>In templates:</p> <pre><code>&lt;a href=\"{% url 'myapp:my_detail_view' item.pk %}\"&gt;{{ item.name }}&lt;/a&gt;\n</code></pre> <p>In Python code:</p> <pre><code>from django.urls import reverse\n\nurl = reverse('myapp:my_detail_view', kwargs={'pk': 1})\n</code></pre>"},{"location":"Cheat-Sheets/Django/#templates","title":"Templates","text":""},{"location":"Cheat-Sheets/Django/#create-a-template-mytemplatehtml","title":"Create a Template (mytemplate.html)","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{% block title %}My Template{% endblock %}&lt;/title&gt;\n    {% load static %}\n    &lt;link rel=\"stylesheet\" href=\"{% static 'myapp/css/style.css' %}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;header&gt;\n        &lt;h1&gt;{% block header %}My Website{% endblock %}&lt;/h1&gt;\n    &lt;/header&gt;\n\n    &lt;main&gt;\n        {% block content %}\n            &lt;h1&gt;Data from MyModel:&lt;/h1&gt;\n            &lt;ul&gt;\n                {% for item in data %}\n                    &lt;li&gt;&lt;a href=\"{% url 'myapp:my_detail_view' item.pk %}\"&gt;{{ item.name }}&lt;/a&gt; - {{ item.description }}&lt;/li&gt;\n                {% empty %}\n                    &lt;li&gt;No data available.&lt;/li&gt;\n                {% endfor %}\n            &lt;/ul&gt;\n        {% endblock %}\n    &lt;/main&gt;\n\n    &lt;footer&gt;\n        &lt;p&gt;&amp;copy; 2025 My Website&lt;/p&gt;\n    &lt;/footer&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Django/#template-inheritance","title":"Template Inheritance","text":"<p>Create a base template (<code>base.html</code>):</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{% block title %}My Website{% endblock %}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;header&gt;\n        &lt;h1&gt;{% block header %}My Website{% endblock %}&lt;/h1&gt;\n    &lt;/header&gt;\n\n    &lt;main&gt;\n        {% block content %}{% endblock %}\n    &lt;/main&gt;\n\n    &lt;footer&gt;\n        &lt;p&gt;&amp;copy; 2025 My Website&lt;/p&gt;\n    &lt;/footer&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Extend the base template (<code>mytemplate.html</code>):</p> <pre><code>{% extends 'base.html' %}\n\n{% block title %}My Custom Title{% endblock %}\n\n{% block content %}\n    &lt;h1&gt;Data from MyModel:&lt;/h1&gt;\n    &lt;ul&gt;\n        {% for item in data %}\n            &lt;li&gt;{{ item.name }} - {{ item.description }}&lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n{% endblock %}\n</code></pre>"},{"location":"Cheat-Sheets/Django/#template-tags-and-filters","title":"Template Tags and Filters","text":"<ul> <li><code>{{ variable }}</code>: Outputs a variable.</li> <li><code>{% tag %}</code>: Template logic tag (e.g., <code>for</code>, <code>if</code>).</li> <li><code>{{ variable|filter }}</code>: Applies a filter to a variable.</li> <li><code>{% load static %}</code>: Loads the <code>static</code> template tag library for serving static files.</li> <li><code>{% url 'view_name' arg1 arg2 %}</code>: Reverses a URL pattern by its name.</li> <li><code>{% csrf_token %}</code>: Adds a CSRF token to a form.</li> <li><code>{% now \"Y-m-d H:i\" %}</code>: Displays the current date and time.</li> <li><code>{% include \"template_name.html\" %}</code>: Includes another template.</li> <li><code>{% extends \"base.html\" %}</code>: Extends a base template.</li> <li><code>{% block block_name %}{% endblock %}</code>: Defines a block for template inheritance.</li> <li><code>{% if condition %}{% endif %}</code>: Conditional logic.</li> <li><code>{% for item in items %}{% endfor %}</code>: Loop through a list.</li> <li><code>{% with total=items|length %}</code>: Assign a value to a variable within the template.</li> </ul>"},{"location":"Cheat-Sheets/Django/#common-template-filters","title":"Common Template Filters","text":"<ul> <li><code>safe</code>: Marks a string as safe for HTML output.</li> <li><code>date:\"FORMAT_STRING\"</code>: Formats a date. See Django's documentation for format string options.</li> <li><code>time:\"FORMAT_STRING\"</code>: Formats a time.</li> <li><code>timesince</code>: Displays the time elapsed since a date.</li> <li><code>truncatechars:LENGTH</code>: Truncates a string to a certain length.</li> <li><code>truncatewords:NUM</code>: Truncates a string to a certain number of words.</li> <li><code>lower</code>, <code>upper</code>: Converts a string to lowercase or uppercase.</li> <li><code>title</code>: Converts a string to title case.</li> <li><code>capfirst</code>: Capitalizes the first character of a string.</li> <li><code>length</code>: Returns the length of a value.</li> <li><code>default:VALUE</code>: Provides a default value if a variable is False.</li> <li><code>filesizeformat</code>: Formats a number as a human-readable file size.</li> <li><code>stringformat:\"E\"</code>: Formats a number according to a string format specifier.</li> <li><code>linebreaks</code>: Replaces line breaks in plain text with appropriate HTML; a single newline becomes an HTML line break (<code>&lt;br&gt;</code>) and a new line surrounded by empty lines becomes a paragraph break (<code>&lt;p&gt;</code>).</li> <li><code>urlencode</code>: Encodes a string for use in a URL.</li> <li><code>json_script</code>: Safely outputs data as JSON for use in JavaScript.</li> </ul>"},{"location":"Cheat-Sheets/Django/#forms","title":"Forms","text":""},{"location":"Cheat-Sheets/Django/#define-a-form-formspy","title":"Define a Form (forms.py)","text":"<pre><code>from django import forms\nfrom .models import MyModel\n\nclass MyForm(forms.Form):\n    name = forms.CharField(label=\"Your Name\", max_length=100,\n                           widget=forms.TextInput(attrs={'class': 'form-control'}))\n    email = forms.EmailField(label=\"Your Email\",\n                            widget=forms.EmailInput(attrs={'class': 'form-control'}))\n    message = forms.CharField(widget=forms.Textarea(attrs={'class': 'form-control'}),\n                              label=\"Your Message\")\n    agree = forms.BooleanField(label=\"I agree to the terms\", required=True)\n\n    # Custom validation\n    def clean_name(self):\n        name = self.cleaned_data['name']\n        if len(name) &lt; 3:\n            raise forms.ValidationError(\"Name must be at least 3 characters long.\")\n        return name\n\nclass MyModelForm(forms.ModelForm):\n    class Meta:\n        model = MyModel\n        fields = ['name', 'description', 'is_active']\n        widgets = {\n            'description': forms.Textarea(attrs={'rows': 4, 'cols': 40}),\n        }\n        labels = {\n            'name': 'Model Name',\n            'description': 'Model Description',\n        }\n        help_texts = {\n            'name': 'Enter a descriptive name for the model.',\n        }\n        error_messages = {\n            'name': {\n                'required': 'Please enter a name.',\n            },\n        }\n</code></pre>"},{"location":"Cheat-Sheets/Django/#form-fields","title":"Form Fields","text":"<ul> <li><code>CharField</code>: For text input.</li> <li><code>IntegerField</code>: For integer input.</li> <li><code>FloatField</code>: For floating-point input.</li> <li><code>BooleanField</code>: For checkbox input.</li> <li><code>DateField</code>, <code>DateTimeField</code>: For date and time input.</li> <li><code>EmailField</code>: For email input.</li> <li><code>URLField</code>: For URL input.</li> <li><code>ChoiceField</code>: For select input. Requires <code>choices</code> argument.</li> <li><code>MultipleChoiceField</code>: For multiple select input.</li> <li><code>FileField</code>: For file upload.</li> <li><code>ImageField</code>: For image upload.</li> <li><code>ModelChoiceField</code>: For selecting a model instance from a queryset.</li> <li><code>ModelMultipleChoiceField</code>: For selecting multiple model instances.</li> <li><code>TypedChoiceField</code>: Like <code>ChoiceField</code>, but coerces values to a specific type.</li> <li><code>TypedMultipleChoiceField</code>: Like <code>MultipleChoiceField</code>, but coerces values to a specific type.</li> <li><code>RegexField</code>: A CharField that validates against a regular expression.</li> </ul>"},{"location":"Cheat-Sheets/Django/#form-widgets","title":"Form Widgets","text":"<ul> <li><code>TextInput</code>: Default text input.</li> <li><code>Textarea</code>: Multi-line text input.</li> <li><code>NumberInput</code>: For number input.</li> <li><code>EmailInput</code>: For email input.</li> <li><code>URLInput</code>: For URL input.</li> <li><code>PasswordInput</code>: For password input.</li> <li><code>HiddenInput</code>: A hidden input field.</li> <li><code>Select</code>: For single select dropdown.</li> <li><code>SelectMultiple</code>: For multiple select dropdown.</li> <li><code>RadioSelect</code>: For radio button selection.</li> <li><code>CheckboxInput</code>: For a single checkbox.</li> <li><code>CheckboxSelectMultiple</code>: For multiple checkboxes.</li> <li><code>FileInput</code>: For file uploads.</li> <li><code>ClearableFileInput</code>: A FileInput with a checkbox to clear the current file.</li> <li><code>DateInput</code>, <code>DateTimeInput</code>, <code>TimeInput</code>: For date, datetime, and time input, respectively.</li> </ul>"},{"location":"Cheat-Sheets/Django/#render-a-form-in-a-template","title":"Render a Form in a Template","text":"<pre><code>&lt;form method=\"post\"&gt;\n    {% csrf_token %}\n    {% if form.errors %}\n        &lt;div class=\"alert alert-danger\"&gt;\n            Please correct the errors below.\n        &lt;/div&gt;\n    {% endif %}\n    {{ form.as_p }}  {# Renders the form as a series of &lt;p&gt; tags #}\n    {# Or render fields individually: #}\n    {# &lt;div class=\"form-group\"&gt;\n        {{ form.name.label_tag }}\n        {{ form.name }}\n        {{ form.name.errors }}\n    &lt;/div&gt; #}\n    &lt;button type=\"submit\"&gt;Submit&lt;/button&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Django/#process-form-data-in-a-view","title":"Process Form Data in a View","text":"<pre><code>from django.shortcuts import render, redirect\nfrom .forms import MyForm, MyModelForm\n\ndef my_form_view(request):\n    if request.method == 'POST':\n        form = MyForm(request.POST)\n        if form.is_valid():\n            name = form.cleaned_data['name']\n            email = form.cleaned_data['email']\n            message = form.cleaned_data['message']\n            # Process the data (e.g., save to database, send email)\n            return redirect('success_url')  # Redirect to a success page\n        else:\n            # Form is invalid, display errors\n            return render(request, 'myapp/myform.html', {'form': form})\n    else:\n        form = MyForm()\n    return render(request, 'myapp/myform.html', {'form': form})\n\ndef my_model_form_view(request):\n    if request.method == 'POST':\n        form = MyModelForm(request.POST, request.FILES) # Include request.FILES for file uploads\n        if form.is_valid():\n            instance = form.save()  # Save the model instance\n            # Or, to process data before saving:\n            # new_instance = form.save(commit=False)\n            # new_instance.some_field = 'some_value'\n            # new_instance.save()\n            return redirect('my_list_view')\n        else:\n            return render(request, 'myapp/mymodel_form.html', {'form': form})\n    else:\n        form = MyModelForm()\n    return render(request, 'myapp/mymodel_form.html', {'form': form})\n</code></pre>"},{"location":"Cheat-Sheets/Django/#admin-interface","title":"Admin Interface","text":""},{"location":"Cheat-Sheets/Django/#register-a-model-adminpy","title":"Register a Model (admin.py)","text":"<pre><code>from django.contrib import admin\nfrom .models import MyModel\n\nadmin.site.register(MyModel)\n</code></pre>"},{"location":"Cheat-Sheets/Django/#customize-admin-interface","title":"Customize Admin Interface","text":"<pre><code>from django.contrib import admin\nfrom .models import MyModel\n\n@admin.register(MyModel)\nclass MyModelAdmin(admin.ModelAdmin):\n    list_display = ('name', 'created_at', 'is_active')  # Columns to display in list view\n    search_fields = ('name', 'description') # Enable search\n    list_filter = ('is_active', 'created_at')          # Enable filtering\n    ordering = ('name',)\n    readonly_fields = ('created_at', 'updated_at')\n    date_hierarchy = 'created_at' # Drill-down by date\n    prepopulated_fields = {'slug': ('name',)} # Automatically populate slug field\n    raw_id_fields = ('related_model',) # Use raw ID lookup for ForeignKey/ManyToManyField\n    filter_horizontal = ('many_to_many_field',) # Use horizontal filter for ManyToManyField\n    filter_vertical = ('another_many_to_many_field',) # Use vertical filter for ManyToManyField\n    fieldsets = (\n        (None, {\n            'fields': ('name', 'description')\n        }),\n        ('Advanced options', {\n            'classes': ('collapse',),\n            'fields': ('is_active', 'order'),\n        }),\n    )\n    actions = ['make_active', 'make_inactive']\n\n    def make_active(self, request, queryset):\n        queryset.update(is_active=True)\n    make_active.short_description = \"Mark selected entries as active\"\n\n    def make_inactive(self, request, queryset):\n        queryset.update(is_active=False)\n    make_inactive.short_description = \"Mark selected entries as inactive\"\n</code></pre>"},{"location":"Cheat-Sheets/Django/#inline-admin","title":"Inline Admin","text":"<pre><code>from django.contrib import admin\nfrom .models import MyModel, RelatedModel\n\nclass RelatedModelAdminInline(admin.TabularInline):  # Or admin.StackedInline\n    model = RelatedModel\n    extra = 1  # Number of empty forms to display\n    fk_name = 'mymodel' # Specify the ForeignKey field name in RelatedModel\n\n@admin.register(MyModel)\nclass MyModelAdmin(admin.ModelAdmin):\n    inlines = [RelatedModelAdminInline]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#settings-settingspy","title":"Settings (settings.py)","text":""},{"location":"Cheat-Sheets/Django/#key-settings","title":"Key Settings","text":"<ul> <li><code>DEBUG = True</code>: Enables debug mode (for development only!).</li> <li><code>SECRET_KEY</code>: A secret key for security. Never share this! Use environment variables.</li> <li><code>ALLOWED_HOSTS = ['*']</code>: List of allowed hostnames for the Django project. In production, set this to your domain name.</li> <li><code>INSTALLED_APPS</code>: List of installed Django apps.</li> <li><code>MIDDLEWARE</code>: List of enabled middleware.</li> <li><code>ROOT_URLCONF</code>: Specifies the root URL configuration module.</li> <li><code>DATABASES</code>: Database connection settings.</li> <li><code>STATIC_URL</code>: URL to serve static files.</li> <li><code>STATIC_ROOT</code>: The absolute path to the directory where <code>collectstatic</code> will collect static files for production.</li> <li><code>STATICFILES_DIRS</code>: List of directories where Django will look for static files.</li> <li><code>MEDIA_URL</code>: URL to serve media files (user-uploaded files).</li> <li><code>MEDIA_ROOT</code>: The absolute path to the directory where user-uploaded media files will be stored.</li> <li><code>TEMPLATES</code>: Template engine settings.</li> <li><code>LANGUAGE_CODE</code>: The default language code for the project.</li> <li><code>TIME_ZONE</code>: The timezone for the project.</li> <li><code>USE_I18N = True</code>: Enables internationalization.</li> <li><code>USE_L10N = True</code>: Enables localization.</li> <li><code>USE_TZ = True</code>: Enables timezone support.</li> <li><code>DEFAULT_AUTO_FIELD</code>: Default auto-field type for primary keys (Django 3.2+).</li> <li><code>SESSION_ENGINE</code>: Defines the session storage engine.</li> <li><code>CSRF_COOKIE_SECURE = True</code>: Ensures the CSRF cookie is only sent over HTTPS (production).</li> <li><code>SESSION_COOKIE_SECURE = True</code>: Ensures the session cookie is only sent over HTTPS (production).</li> <li><code>SECURE_SSL_REDIRECT = True</code>: Redirects all HTTP traffic to HTTPS (production).</li> <li><code>SECURE_HSTS_SECONDS = 31536000</code>: Enables HTTP Strict Transport Security (HSTS) (production).</li> <li><code>SECURE_HSTS_INCLUDE_SUBDOMAINS = True</code>: Includes subdomains in HSTS policy (production).</li> <li><code>SECURE_HSTS_PRELOAD = True</code>: Enables HSTS preloading (production).</li> </ul>"},{"location":"Cheat-Sheets/Django/#database-configuration","title":"Database Configuration","text":"<pre><code>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',  # Or 'django.db.backends.mysql', 'django.db.backends.sqlite3'\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'HOST': 'localhost',\n        'PORT': '5432',\n    }\n}\n</code></pre>"},{"location":"Cheat-Sheets/Django/#static-files-configuration","title":"Static Files Configuration","text":"<pre><code>STATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, 'myapp/static'),\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#middleware-configuration","title":"Middleware Configuration","text":"<pre><code>MIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.cache.UpdateCacheMiddleware', # Add for caching\n    'django.middleware.cache.FetchFromCacheMiddleware', # Add for caching\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#caching-configuration","title":"Caching Configuration","text":"<pre><code>CACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n        'LOCATION': 'redis://127.0.0.1:6379',\n    }\n}\n\nCACHE_MIDDLEWARE_ALIAS = 'default'\nCACHE_MIDDLEWARE_SECONDS = 600  # Cache for 10 minutes\nCACHE_MIDDLEWARE_KEY_PREFIX = ''\n</code></pre>"},{"location":"Cheat-Sheets/Django/#email-configuration","title":"Email Configuration","text":"<pre><code>EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST = 'smtp.gmail.com'\nEMAIL_PORT = 587\nEMAIL_USE_TLS = True\nEMAIL_HOST_USER = 'your_email@gmail.com'\nEMAIL_HOST_PASSWORD = 'your_password'\nDEFAULT_FROM_EMAIL = 'your_email@gmail.com'\n</code></pre>"},{"location":"Cheat-Sheets/Django/#common-commands","title":"Common Commands","text":"<ul> <li><code>python manage.py runserver</code>: Starts the development server.</li> <li><code>python manage.py shell</code>: Opens a Python shell with Django environment loaded.</li> <li><code>python manage.py createsuperuser</code>: Creates an admin user.</li> <li><code>python manage.py makemigrations</code>: Creates new migrations based on model changes.</li> <li><code>python manage.py migrate</code>: Applies migrations to the database.</li> <li><code>python manage.py collectstatic</code>: Collects static files into <code>STATIC_ROOT</code>.</li> <li><code>python manage.py test</code>: Runs the project's tests.</li> <li><code>python manage.py dbshell</code>: Opens a shell for the database.</li> <li><code>python manage.py dumpdata</code>: Exports data from the database as JSON or XML.</li> <li><code>python manage.py loaddata</code>: Loads data from a JSON or XML fixture into the database.</li> <li><code>python manage.py check</code>: Performs system checks to identify potential problems.</li> <li><code>python manage.py showmigrations</code>: Shows the status of migrations.</li> <li><code>python manage.py inspectdb</code>: Generates models from an existing database.</li> <li><code>python manage.py flush</code>: Empties the database.</li> <li><code>python manage.py changepassword &lt;username&gt;</code>: Changes a user's password.</li> </ul>"},{"location":"Cheat-Sheets/Django/#django-rest-framework-drf","title":"Django REST Framework (DRF)","text":""},{"location":"Cheat-Sheets/Django/#installation_1","title":"Installation","text":"<pre><code>pip install djangorestframework\n</code></pre> <p>Add <code>'rest_framework'</code> to <code>INSTALLED_APPS</code> in <code>settings.py</code>.</p>"},{"location":"Cheat-Sheets/Django/#serializers-serializerspy","title":"Serializers (serializers.py)","text":"<pre><code>from rest_framework import serializers\nfrom .models import MyModel\n\nclass MyModelSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = MyModel\n        fields = '__all__'  # Or specify a tuple of field names\n        # or exclude = ('field1', 'field2')\n        read_only_fields = ('created_at', 'updated_at') # Make fields read-only\n\n    # Custom field validation\n    def validate_name(self, value):\n        if len(value) &lt; 5:\n            raise serializers.ValidationError(\"Name must be at least 5 characters long.\")\n        return value\n\n    # Object-level validation\n    def validate(self, data):\n        if data['name'] == data['description']:\n            raise serializers.ValidationError(\"Name and description cannot be the same.\")\n        return data\n</code></pre>"},{"location":"Cheat-Sheets/Django/#views-viewspy","title":"Views (views.py)","text":"<pre><code>from rest_framework import generics, permissions, status\nfrom rest_framework.response import Response\nfrom .models import MyModel\nfrom .serializers import MyModelSerializer\n\nclass MyModelList(generics.ListCreateAPIView):\n    queryset = MyModel.objects.all()\n    serializer_class = MyModelSerializer\n    permission_classes = [permissions.IsAuthenticatedOrReadOnly] # Require authentication for write operations\n\n    def perform_create(self, serializer):\n        serializer.save() # Save the object\n\nclass MyModelDetail(generics.RetrieveUpdateDestroyAPIView):\n    queryset = MyModel.objects.all()\n    serializer_class = MyModelSerializer\n    permission_classes = [permissions.IsAuthenticatedOrReadOnly]\n\n    def delete(self, request, *args, **kwargs):\n        instance = self.get_object()\n        self.perform_destroy(instance)\n        return Response(status=status.HTTP_204_NO_CONTENT) # Return 204 No Content on successful deletion\n</code></pre>"},{"location":"Cheat-Sheets/Django/#urls-urlspy","title":"URLs (urls.py)","text":"<pre><code>from django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('mymodel/', views.MyModelList.as_view(), name='mymodel-list'),\n    path('mymodel/&lt;int:pk&gt;/', views.MyModelDetail.as_view(), name='mymodel-detail'),\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#authentication-and-permissions","title":"Authentication and Permissions","text":"<ul> <li><code>permissions.AllowAny</code>: Allows access to anyone, authenticated or not.</li> <li><code>permissions.IsAuthenticated</code>: Only allows access to authenticated users.</li> <li><code>permissions.IsAdminUser</code>: Only allows access to admin users.</li> <li><code>permissions.IsAuthenticatedOrReadOnly</code>: Allows read access to anyone, but write access only to authenticated users.</li> <li>Custom permissions: You can create custom permission classes to define more specific access control rules.</li> </ul>"},{"location":"Cheat-Sheets/Django/#apiview","title":"APIView","text":"<p>For more control, use <code>APIView</code>:</p> <pre><code>from rest_framework.views import APIView\nfrom rest_framework.response import Response\nfrom rest_framework import status\nfrom .models import MyModel\nfrom .serializers import MyModelSerializer\n\nclass MyCustomAPIView(APIView):\n    def get(self, request):\n        data = MyModel.objects.all()\n        serializer = MyModelSerializer(data, many=True)\n        return Response(serializer.data)\n\n    def post(self, request):\n        serializer = MyModelSerializer(data=request.data)\n        if serializer.is_valid():\n            serializer.save()\n            return Response(serializer.data, status=status.HTTP_201_CREATED)\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n</code></pre>"},{"location":"Cheat-Sheets/Django/#security","title":"Security","text":""},{"location":"Cheat-Sheets/Django/#csrf-protection","title":"CSRF Protection","text":"<ul> <li>Use the <code>{% csrf_token %}</code> template tag in forms.</li> <li>Ensure <code>django.middleware.csrf.CsrfViewMiddleware</code> is in <code>MIDDLEWARE</code>.</li> </ul>"},{"location":"Cheat-Sheets/Django/#sql-injection","title":"SQL Injection","text":"<ul> <li>Use Django's ORM to avoid raw SQL queries whenever possible.</li> <li>If you must use raw SQL, use parameterized queries to escape user input.</li> </ul>"},{"location":"Cheat-Sheets/Django/#xss-cross-site-scripting","title":"XSS (Cross-Site Scripting)","text":"<ul> <li>Use the <code>safe</code> template filter with caution. Only use it on data you trust.</li> <li>Sanitize user input before displaying it.</li> </ul>"},{"location":"Cheat-Sheets/Django/#clickjacking","title":"Clickjacking","text":"<ul> <li>Ensure <code>django.middleware.clickjacking.XFrameOptionsMiddleware</code> is in <code>MIDDLEWARE</code>.</li> <li>Set <code>X_FRAME_OPTIONS = 'DENY'</code> or <code>X_FRAME_OPTIONS = 'SAMEORIGIN'</code> in <code>settings.py</code>.</li> </ul>"},{"location":"Cheat-Sheets/Django/#security-headers","title":"Security Headers","text":"<p>Use <code>django-security</code> or similar package to set security headers.</p>"},{"location":"Cheat-Sheets/Django/#https","title":"HTTPS","text":"<ul> <li>Configure your web server to use HTTPS.</li> <li>Set <code>SECURE_SSL_REDIRECT = True</code> in <code>settings.py</code> to redirect HTTP requests to HTTPS.</li> <li>Set <code>SECURE_HSTS_SECONDS</code> and <code>SECURE_HSTS_INCLUDE_SUBDOMAINS</code> for HTTP Strict Transport Security.</li> <li>Set <code>SECURE_HSTS_PRELOAD = True</code> to enable HSTS preloading.</li> </ul>"},{"location":"Cheat-Sheets/Django/#authentication","title":"Authentication","text":"<p>Use Django's built-in authentication <pre><code>import TestCase\nfrom .models import MyModel\n\nclass MyModelTest(TestCase):\n    def setUp(self):\n        MyModel.objects.create(name='Test Object', description='Test Description')\n\n    def test_model_content(self):\n        obj = MyModel.objects.get(name='Test Object')\n        self.assertEqual(obj.description, 'Test Description')\n</code></pre></p>"},{"location":"Cheat-Sheets/Django/#test-client","title":"Test Client","text":"<pre><code>from django.test import Client\n\nclass MyViewTest(TestCase):\n    def setUp(self):\n        self.client = Client()\n\n    def test_my_view(self):\n        response = self.client.get('/myapp/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'My Template')\n</code></pre>"},{"location":"Cheat-Sheets/Django/#deployment","title":"Deployment","text":""},{"location":"Cheat-Sheets/Django/#production-settings","title":"Production Settings","text":"<p>Create a <code>production.py</code> settings file:</p> <pre><code>from .settings import *\n\nDEBUG = False\nALLOWED_HOSTS = ['yourdomain.com', 'www.yourdomain.com']\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n</code></pre>"},{"location":"Cheat-Sheets/Django/#web-server-gunicorn","title":"Web Server (Gunicorn)","text":"<p>Install Gunicorn:</p> <pre><code>pip install gunicorn\n</code></pre> <p>Run Gunicorn:</p> <pre><code>gunicorn myproject.wsgi:application --bind 0.0.0.0:8000\n</code></pre>"},{"location":"Cheat-Sheets/Django/#process-manager-systemd","title":"Process Manager (systemd)","text":"<p>Create a systemd service file (<code>/etc/systemd/system/myproject.service</code>):</p> <pre><code>[Unit]\nDescription=My Django Project\nAfter=network.target\n\n[Service]\nUser=myuser\nGroup=mygroup\nWorkingDirectory=/path/to/myproject\nExecStart=/path/to/venv/bin/gunicorn myproject.wsgi:application --bind 0.0.0.0:8000\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start the service:</p> <pre><code>sudo systemctl enable myproject\nsudo systemctl start myproject\n</code></pre>"},{"location":"Cheat-Sheets/Django/#static-files","title":"Static Files","text":"<ul> <li>Run <code>python manage.py collectstatic</code> to collect static files.</li> <li>Configure your web server (e.g., Nginx, Apache) to serve static files from <code>STATIC_ROOT</code>.</li> </ul>"},{"location":"Cheat-Sheets/Django/#media-files","title":"Media Files","text":"<ul> <li>Configure your web server to serve media files from <code>MEDIA_ROOT</code>.</li> <li>Consider using a cloud storage service (e.g., AWS S3) for media files.</li> </ul>"},{"location":"Cheat-Sheets/Django/#database","title":"Database","text":"<ul> <li>Use a production-ready database (e.g., PostgreSQL, MySQL).</li> <li>Configure the database connection settings in <code>settings.py</code>.</li> <li>Back up your database regularly.</li> </ul>"},{"location":"Cheat-Sheets/Django/#environment-variables","title":"Environment Variables","text":"<ul> <li>Use environment variables for sensitive settings (e.g., <code>SECRET_KEY</code>, database credentials).</li> <li>Use a package like <code>python-dotenv</code> to manage environment variables.</li> </ul>"},{"location":"Cheat-Sheets/Django/#caching","title":"Caching","text":""},{"location":"Cheat-Sheets/Django/#per-site-cache","title":"Per-Site Cache","text":"<p>Add <code>django.middleware.cache.UpdateCacheMiddleware</code> and <code>django.middleware.cache.FetchFromCacheMiddleware</code> to <code>MIDDLEWARE</code> in <code>settings.py</code>.</p> <pre><code># settings.py\nMIDDLEWARE = [\n    'django.middleware.cache.UpdateCacheMiddleware',\n    # ... other middleware ...\n    'django.middleware.cache.FetchFromCacheMiddleware',\n]\n</code></pre> <p>Configure the cache backend:</p> <pre><code># settings.py\nCACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n        'LOCATION': 'redis://127.0.0.1:6379',  # Example using Redis\n    }\n}\n\nCACHE_MIDDLEWARE_ALIAS = 'default'\nCACHE_MIDDLEWARE_SECONDS = 600  # Cache for 10 minutes\nCACHE_MIDDLEWARE_KEY_PREFIX = ''\n</code></pre>"},{"location":"Cheat-Sheets/Django/#per-view-cache","title":"Per-View Cache","text":"<p>Use the <code>@cache_page</code> decorator:</p> <pre><code>from django.views.decorators.cache import cache_page\n\n@cache_page(60 * 15)  # Cache for 15 minutes\ndef my_view(request):\n    # ...\n    return render(request, 'myapp/mytemplate.html', context)\n</code></pre>"},{"location":"Cheat-Sheets/Django/#template-fragment-caching","title":"Template Fragment Caching","text":"<pre><code>{% load cache %}\n\n{% cache 600 \"my_template_fragment\" %}\n    {# Expensive template code here #}\n{% endcache %}\n</code></pre>"},{"location":"Cheat-Sheets/Django/#low-level-cache-api","title":"Low-Level Cache API","text":"<pre><code>from django.core.cache import cache\n\n# Set a value\ncache.set('my_key', 'my_value', 600)  # Cache for 10 minutes\n\n# Get a value\nvalue = cache.get('my_key')\n\n# Delete a value\ncache.delete('my_key')\n</code></pre>"},{"location":"Cheat-Sheets/Django/#signals","title":"Signals","text":""},{"location":"Cheat-Sheets/Django/#define-a-signal-signalspy","title":"Define a Signal (signals.py)","text":"<pre><code>from django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom .models import MyModel\n\n@receiver(post_save, sender=MyModel)\ndef my_model_post_save(sender, instance, created, **kwargs):\n    if created:\n        # Perform actions when a new MyModel instance is created\n        print(f\"New MyModel instance created: {instance.name}\")\n    else:\n        # Perform actions when a MyModel instance is updated\n        print(f\"MyModel instance updated: {instance.name}\")\n</code></pre>"},{"location":"Cheat-Sheets/Django/#connect-signals-appspy","title":"Connect Signals (apps.py)","text":"<pre><code>from django.apps import AppConfig\n\nclass MyappConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'myapp'\n\n    def ready(self):\n        import myapp.signals  # Import the signals module\n</code></pre>"},{"location":"Cheat-Sheets/Django/#common-signals","title":"Common Signals","text":"<ul> <li><code>pre_save</code>, <code>post_save</code>: Sent before and after a model's <code>save()</code> method is called.</li> <li><code>pre_delete</code>, <code>post_delete</code>: Sent before and after a model instance is deleted.</li> <li><code>m2m_changed</code>: Sent when a ManyToManyField is changed.</li> <li><code>pre_migrate</code>, <code>post_migrate</code>: Sent before and after migrations are applied.</li> </ul>"},{"location":"Cheat-Sheets/Django/#internationalization-i18n-and-localization-l10n","title":"Internationalization (i18n) and Localization (l10n)","text":""},{"location":"Cheat-Sheets/Django/#enable-i18n-and-l10n","title":"Enable i18n and l10n","text":"<p>Set <code>USE_I18N = True</code> and <code>USE_L10N = True</code> in <code>settings.py</code>.</p>"},{"location":"Cheat-Sheets/Django/#set-the-language-code","title":"Set the Language Code","text":"<p>Set <code>LANGUAGE_CODE = 'en-us'</code> in <code>settings.py</code>.</p>"},{"location":"Cheat-Sheets/Django/#translate-strings","title":"Translate Strings","text":"<p>In Python code:</p> <pre><code>from django.utils.translation import gettext as _\n\ndef my_view(request):\n    message = _(\"Hello, world!\")\n    return render(request, 'myapp/mytemplate.html', {'message': message})\n</code></pre> <p>In templates:</p> <pre><code>{% load i18n %}\n&lt;h1&gt;{% trans \"Hello, world!\" %}&lt;/h1&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Django/#mark-strings-for-translation","title":"Mark Strings for Translation","text":"<p>Use <code>makemessages</code> command:</p> <pre><code>python manage.py makemessages -l de  # Create a translation file for German\n</code></pre>"},{"location":"Cheat-Sheets/Django/#translate-strings-with-context","title":"Translate Strings with Context","text":"<pre><code>from django.utils.translation import pgettext as _\n\nmessage = _(\"context\", \"Hello, world!\")\n</code></pre>"},{"location":"Cheat-Sheets/Django/#pluralization","title":"Pluralization","text":"<pre><code>from django.utils.translation import ngettext\n\ndef my_view(request, count):\n    message = ngettext(\n        'There is %(count)d object',\n        'There are %(count)d objects',\n        count\n    ) % {'count': count}\n    return render(request, 'myapp/mytemplate.html', {'message': message})\n</code></pre>"},{"location":"Cheat-Sheets/Django/#switch-language","title":"Switch Language","text":"<pre><code>&lt;form action=\"{% url 'set_language' %}\" method=\"post\"&gt;\n    {% csrf_token %}\n    &lt;input name=\"language\" type=\"hidden\" value=\"de\"&gt;\n    &lt;button type=\"submit\"&gt;Switch to German&lt;/button&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Django/#custom-management-commands","title":"Custom Management Commands","text":""},{"location":"Cheat-Sheets/Django/#create-a-command-managementcommandsmycommandpy","title":"Create a Command (management/commands/mycommand.py)","text":"<pre><code>from django.core.management.base import BaseCommand\n\nclass Command(BaseCommand):\n    help = 'My custom command'\n\n    def add_arguments(self, parser):\n        parser.add_argument('argument', nargs='?', type=str, help='An argument for the command')\n\n    def handle(self, *args, **options):\n        argument = options['argument']\n        self.stdout.write(self.style.SUCCESS(f'Successfully executed mycommand with argument: {argument}'))\n</code></pre>"},{"location":"Cheat-Sheets/Django/#run-the-command","title":"Run the Command","text":"<pre><code>python manage.py mycommand \"My Argument\"\n</code></pre>"},{"location":"Cheat-Sheets/Django/#middleware","title":"Middleware","text":""},{"location":"Cheat-Sheets/Django/#create-a-middleware-middlewarepy","title":"Create a Middleware (middleware.py)","text":"<pre><code>class MyMiddleware:\n    def __init__(self, get_response):\n        self.get_response = get_response\n\n    def __call__(self, request):\n        # Code to be executed for each request before the view (pre-processing)\n        print(\"Before view\")\n\n        response = self.get_response(request)\n\n        # Code to be executed for each request after the view (post-processing)\n        print(\"After view\")\n\n        return response\n</code></pre>"},{"location":"Cheat-Sheets/Django/#activate-middleware","title":"Activate Middleware","text":"<p>Add the middleware to <code>MIDDLEWARE</code> in <code>settings.py</code>:</p> <pre><code>MIDDLEWARE = [\n    # ...\n    'myapp.middleware.MyMiddleware',\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#file-handling","title":"File Handling","text":""},{"location":"Cheat-Sheets/Django/#uploading-files","title":"Uploading Files","text":"<p>In <code>forms.py</code>:</p> <pre><code>class UploadFileForm(forms.Form):\n    file = forms.FileField()\n</code></pre> <p>In <code>views.py</code>:</p> <pre><code>def upload_file(request):\n    if request.method == 'POST':\n        form = UploadFileForm(request.POST, request.FILES)\n        if form.is_valid():\n            uploaded_file = request.FILES['file']\n            # Process the uploaded file (e.g., save to MEDIA_ROOT)\n            with open(os.path.join(settings.MEDIA_ROOT, uploaded_file.name), 'wb+') as destination:\n                for chunk in uploaded_file.chunks():\n                    destination.write(chunk)\n            return HttpResponse(\"File uploaded successfully\")\n    else:\n        form = UploadFileForm()\n    return render(request, 'myapp/upload.html', {'form': form})\n</code></pre> <p>In <code>templates/myapp/upload.html</code>:</p> <pre><code>&lt;form method=\"post\" enctype=\"multipart/form-data\"&gt;\n    {% csrf_token %}\n    {{ form.as_p }}\n    &lt;button type=\"submit\"&gt;Upload&lt;/button&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Django/#serving-files","title":"Serving Files","text":"<p>Configure <code>MEDIA_URL</code> and <code>MEDIA_ROOT</code> in <code>settings.py</code>.</p> <p>In <code>urls.py</code>:</p> <pre><code>from django.conf import settings\nfrom django.conf.urls.static import static\n\nurlpatterns = [\n    # ... your other URL patterns ...\n] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n</code></pre>"},{"location":"Cheat-Sheets/Django/#logging","title":"Logging","text":""},{"location":"Cheat-Sheets/Django/#configure-logging-settingspy","title":"Configure Logging (settings.py)","text":"<pre><code>LOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n        },\n        'file': {\n            'level': 'DEBUG',\n            'class': 'logging.FileHandler',\n            'filename': os.path.join(BASE_DIR, 'debug.log'),\n        },\n    },\n    'loggers': {\n        'django': {\n            'handlers': ['console', 'file'],\n            'level': 'INFO',\n        },\n    },\n}\n</code></pre>"},{"location":"Cheat-Sheets/Django/#use-logging","title":"Use Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef my_view(request):\n    logger.info(\"My view was accessed\")\n    try:\n        # ... some code that might raise an exception ...\n    except Exception as e:\n        logger.exception(\"An error occurred\")\n</code></pre>"},{"location":"Cheat-Sheets/Django/#django-channels-asynchronous","title":"Django Channels (Asynchronous)","text":""},{"location":"Cheat-Sheets/Django/#installation_2","title":"Installation","text":"<pre><code>pip install channels\n</code></pre>"},{"location":"Cheat-Sheets/Django/#configure-channels-settingspy","title":"Configure Channels (settings.py)","text":"<pre><code>ASGI_APPLICATION = 'myproject.asgi.application'\n\nCHANNEL_LAYERS = {\n    'default': {\n        'BACKEND': 'channels_redis.core.RedisChannelLayer',\n        'CONFIG': {\n            \"hosts\": [('127.0.0.1', 6379)],\n        },\n    },\n}\n</code></pre>"},{"location":"Cheat-Sheets/Django/#create-a-consumer-consumerspy","title":"Create a Consumer (consumers.py)","text":"<pre><code>from channels.generic.websocket import WebsocketConsumer\nimport json\n\nclass MyConsumer(WebsocketConsumer):\n    def connect(self):\n        self.accept()\n\n    def disconnect(self, close_code):\n        pass\n\n    def receive(self, text_data):\n        text_data_json = json.loads(text_data)\n        message = text_data_json['message']\n\n        self.send(text_data=json.dumps({\n            'message': message\n        }))\n</code></pre>"},{"location":"Cheat-Sheets/Django/#configure-routing-routingpy","title":"Configure Routing (routing.py)","text":"<pre><code>from django.urls import re_path\n\nfrom . import consumers\n\nwebsocket_urlpatterns = [\n    re_path(r'ws/myapp/$', consumers.MyConsumer.as_asgi()),\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#update-asgi-application-asgipy","title":"Update ASGI Application (asgi.py)","text":"<pre><code>import os\n\nfrom django.core.asgi import get_asgi_application\nfrom channels.routing import ProtocolTypeRouter, URLRouter\nfrom channels.auth import AuthMiddlewareStack\nimport myapp.routing\n\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'myproject.settings')\n\napplication = ProtocolTypeRouter({\n    \"http\": get_asgi_application(),\n    \"websocket\": AuthMiddlewareStack(\n        URLRouter(\n            myapp.routing.websocket_urlpatterns\n        )\n    ),\n})\n</code></pre>"},{"location":"Cheat-Sheets/Django/#django-allauth-authentication","title":"Django Allauth (Authentication)","text":""},{"location":"Cheat-Sheets/Django/#installation_3","title":"Installation","text":"<pre><code>pip install django-allauth\n</code></pre>"},{"location":"Cheat-Sheets/Django/#configuration-settingspy","title":"Configuration (settings.py)","text":"<pre><code>INSTALLED_APPS = [\n    # ...\n    'django.contrib.sites',\n    'allauth',\n    'allauth.account',\n    'allauth.socialaccount',\n    # ... include providers you want to use ...\n    # 'allauth.socialaccount.providers.google',\n]\n\nAUTHENTICATION_BACKENDS = [\n    'django.contrib.auth.backends.ModelBackend',\n    'allauth.account.auth_backends.AuthenticationBackend',\n]\n\nSITE_ID = 1\n\nLOGIN_REDIRECT_URL = '/'\nACCOUNT_EMAIL_REQUIRED = True\nACCOUNT_USERNAME_REQUIRED = False\nACCOUNT_AUTHENTICATION_METHOD = 'email'\nACCOUNT_EMAIL_VERIFICATION = 'mandatory'\n</code></pre>"},{"location":"Cheat-Sheets/Django/#urls-urlspy_1","title":"URLs (urls.py)","text":"<pre><code>from django.urls import include, path\n\nurlpatterns = [\n    path('accounts/', include('allauth.urls')),\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#templates_1","title":"Templates","text":"<p>Use Allauth's template tags and forms for registration, login, etc.</p>"},{"location":"Cheat-Sheets/Django/#django-debug-toolbar","title":"Django Debug Toolbar","text":""},{"location":"Cheat-Sheets/Django/#installation_4","title":"Installation","text":"<pre><code>pip install django-debug-toolbar\n</code></pre>"},{"location":"Cheat-Sheets/Django/#configuration-settingspy_1","title":"Configuration (settings.py)","text":"<pre><code>INSTALLED_APPS = [\n    # ...\n    'debug_toolbar',\n]\n\nMIDDLEWARE = [\n    # ...\n    'debug_toolbar.middleware.DebugToolbarMiddleware',\n]\n\nINTERNAL_IPS = [\n    '127.0.0.1',\n]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#urls-urlspy_2","title":"URLs (urls.py)","text":"<pre><code>from django.urls import include, path\n\nurlpatterns = [\n    # ...\n]\n\nif settings.DEBUG:\n    import debug_toolbar\n    urlpatterns += [\n        path('__debug__/', include(debug_toolbar.urls)),\n    ]\n</code></pre>"},{"location":"Cheat-Sheets/Django/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Keep <code>SECRET_KEY</code> secure and out of your codebase. Use environment variables.</li> <li>Use meaningful names for models, views, and URLs.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use Django's built-in security features (e.g., CSRF protection).</li> <li>Configure static file serving correctly in production.</li> <li>Use a production-ready web server (e.g., Gunicorn, uWSGI) and a process manager (e.g., Supervisor, systemd) for deployment.</li> <li>Use a linter (like <code>flake8</code>) and formatter (like <code>black</code>) to ensure consistent code style.</li> <li>Use a well-defined project structure.</li> <li>Keep your code modular and reusable.</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Follow Django's coding style guidelines.</li> <li>Use Django's built-in caching mechanisms to improve performance.</li> <li>Monitor your application for errors and performance issues.</li> <li>Use a CDN (Content Delivery Network) for static files.</li> <li>Optimize database queries.</li> <li>Use asynchronous tasks for long-running operations (e.g., sending emails).</li> <li>Implement proper logging and error handling.</li> <li>Regularly update Django and its dependencies.</li> <li>Use a security scanner to identify potential vulnerabilities.</li> <li>Follow security best practices.</li> </ul>"},{"location":"Cheat-Sheets/Flask/","title":"Flask Cheat Sheet","text":"<ul> <li>Flask Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Basic App Structure</li> <li>Running the App</li> </ul> </li> <li>Routing<ul> <li>Basic Route</li> <li>Dynamic Routes</li> <li>HTTP Methods</li> <li>URL Building</li> </ul> </li> <li>Templates<ul> <li>Basic Template Rendering</li> <li>Template (templates/hello.html)</li> <li>Template Inheritance</li> <li>Jinja2 Template Engine</li> <li>Common Template Filters</li> </ul> </li> <li>Forms<ul> <li>Basic Form</li> <li>Using Flask-WTF</li> <li>Define a Form (forms.py)</li> <li>Render a Form in a Template</li> <li>Process Form Data in a View</li> </ul> </li> <li>Databases<ul> <li>Using Flask-SQLAlchemy</li> <li>Define a Model</li> <li>Create and Manage Tables</li> <li>Querying the Database</li> </ul> </li> <li>Static Files<ul> <li>Configure Static Files</li> </ul> </li> <li>Blueprints<ul> <li>Create a Blueprint</li> <li>Register a Blueprint</li> </ul> </li> <li>Flask Extensions<ul> <li>Flask-Mail</li> <li>Flask-Migrate</li> <li>Flask-Login</li> </ul> </li> <li>Testing<ul> <li>Using pytest</li> <li>Using unittest</li> </ul> </li> <li>Deployment<ul> <li>Production Settings</li> <li>WSGI Servers</li> <li>Environment Variables</li> <li>Example Deployment with Gunicorn and Nginx</li> </ul> </li> <li>Security</li> <li>Logging<ul> <li>Configure Logging</li> <li>Logging to a File</li> </ul> </li> <li>Flask CLI</li> <li>Context Processors</li> <li>Error Handling<ul> <li>Custom Error Pages</li> <li>Logging Exceptions</li> </ul> </li> <li>Flask-RESTful<ul> <li>Installation</li> <li>Define Resources</li> <li>Request Parsing</li> </ul> </li> <li>Session Management</li> <li>Flask-CORS<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Testing<ul> <li>Using pytest</li> <li>Using unittest</li> </ul> </li> <li>Deployment<ul> <li>Production Settings</li> <li>WSGI Servers</li> <li>Environment Variables</li> <li>Example Deployment with Gunicorn and Nginx</li> </ul> </li> <li>Security</li> <li>Logging<ul> <li>Configure Logging</li> <li>Logging to a File</li> </ul> </li> <li>Flask CLI</li> <li>Context Processors</li> <li>Testing<ul> <li>Using pytest</li> <li>Using unittest</li> </ul> </li> <li>Deployment<ul> <li>Production Settings</li> <li>WSGI Servers</li> <li>Environment Variables</li> <li>Example Deployment with Gunicorn and Nginx</li> </ul> </li> <li>Security</li> <li>Logging<ul> <li>Configure Logging</li> <li>Logging to a File</li> </ul> </li> <li>Flask CLI</li> <li>Context Processors</li> <li>Error Handling<ul> <li>Custom Error Pages</li> <li>Logging Exceptions</li> </ul> </li> <li>Flask-RESTful<ul> <li>Installation</li> <li>Define Resources</li> <li>Request Parsing</li> </ul> </li> <li>Session Management</li> <li>Flask-CORS<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Signals<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Limiter<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-APScheduler<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Sitemap<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-WTF CSRF Protection<ul> <li>Configuration</li> <li>Usage in Templates</li> </ul> </li> <li>Flask-FlatPages<ul> <li>Installation</li> <li>Configuration</li> <li>Usage</li> </ul> </li> <li>Flask-Assets<ul> <li>Installation</li> <li>Configuration</li> <li>Usage in Templates</li> </ul> </li> <li>Flask-Babel<ul> <li>Installation</li> <li>Configuration</li> <li>Usage</li> </ul> </li> <li>Flask-SocketIO<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Principal<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-JWT-Extended<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Uploads<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Mail<ul> <li>Installation</li> <li>Configuration</li> <li>Sending Emails</li> </ul> </li> <li>Flask-APScheduler<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-Sitemap<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>Flask-WTF CSRF Protection<ul> <li>Configuration</li> <li>Usage in Templates</li> </ul> </li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the Flask micro web framework, covering essential commands, concepts, and code snippets for efficient Flask development. It aims to be a one-stop reference for common tasks and best practices.</p>"},{"location":"Cheat-Sheets/Flask/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/Flask/#installation","title":"Installation","text":"<pre><code>pip install flask\n</code></pre> <p>Consider using a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Linux/macOS\nvenv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#basic-app-structure","title":"Basic App Structure","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#running-the-app","title":"Running the App","text":"<pre><code>python your_app_name.py\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#routing","title":"Routing","text":""},{"location":"Cheat-Sheets/Flask/#basic-route","title":"Basic Route","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return 'Index Page'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#dynamic-routes","title":"Dynamic Routes","text":"<pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/user/&lt;username&gt;')\ndef show_user_profile(username):\n    # show the user profile for that user\n    return f'User {username}'\n\n@app.route('/post/&lt;int:post_id&gt;')\ndef show_post(post_id):\n    # show the post with the given id, the id is an integer\n    return f'Post {post_id}'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#http-methods","title":"HTTP Methods","text":"<pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        return \"Do the login\"\n    else:\n        return \"Show the login form\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#url-building","title":"URL Building","text":"<pre><code>from flask import Flask, url_for\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return 'Index'\n\n@app.route('/login')\ndef login():\n    return 'Login'\n\n@app.route('/user/&lt;username&gt;')\ndef profile(username):\n    return f'{username}\\'s profile'\n\nwith app.test_request_context():\n    print(url_for('index'))\n    print(url_for('login'))\n    print(url_for('login', next='/'))\n    print(url_for('profile', username='John Doe'))\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#templates","title":"Templates","text":""},{"location":"Cheat-Sheets/Flask/#basic-template-rendering","title":"Basic Template Rendering","text":"<pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/hello/')\n@app.route('/hello/&lt;name&gt;')\ndef hello(name=None):\n    return render_template('hello.html', name=name)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#template-templateshellohtml","title":"Template (templates/hello.html)","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Hello&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    {% if name %}\n        &lt;h1&gt;Hello {{ name }}!&lt;/h1&gt;\n    {% else %}\n        &lt;h1&gt;Hello, World!&lt;/h1&gt;\n    {% endif %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#template-inheritance","title":"Template Inheritance","text":"<p>Base template (<code>templates/base.html</code>):</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{% block title %}My Website{% endblock %}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;header&gt;\n        &lt;h1&gt;{% block header %}My Website{% endblock %}&lt;/h1&gt;\n    &lt;/header&gt;\n\n    &lt;main&gt;\n        {% block content %}{% endblock %}\n    &lt;/main&gt;\n\n    &lt;footer&gt;\n        &lt;p&gt;&amp;copy; 2025 My Website&lt;/p&gt;\n    &lt;/footer&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Child template (<code>templates/hello.html</code>):</p> <pre><code>{% extends \"base.html\" %}\n\n{% block title %}Hello{% endblock %}\n\n{% block content %}\n    {% if name %}\n        &lt;h1&gt;Hello {{ name }}!&lt;/h1&gt;\n    {% else %}\n        &lt;h1&gt;Hello, World!&lt;/h1&gt;\n    {% endif %}\n{% endblock %}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#jinja2-template-engine","title":"Jinja2 Template Engine","text":"<ul> <li><code>{{ variable }}</code>: Outputs a variable.</li> <li><code>{% tag %}</code>: Template logic tag (e.g., <code>for</code>, <code>if</code>).</li> <li><code>{{ variable|filter }}</code>: Applies a filter to a variable.</li> <li><code>{% extends \"base.html\" %}</code>: Extends a base template.</li> <li><code>{% block block_name %}{% endblock %}</code>: Defines a block for template inheritance.</li> <li><code>{% include \"template_name.html\" %}</code>: Includes another template.</li> <li><code>{% url_for 'view_name' arg1=value1 %}</code>: Generates a URL for a view.</li> </ul>"},{"location":"Cheat-Sheets/Flask/#common-template-filters","title":"Common Template Filters","text":"<ul> <li><code>safe</code>: Marks a string as safe for HTML output.</li> <li><code>capitalize</code>: Capitalizes the first character of a string.</li> <li><code>lower</code>, <code>upper</code>: Converts a string to lowercase or uppercase.</li> <li><code>title</code>: Converts a string to title case.</li> <li><code>trim</code>: Removes leading and trailing whitespace.</li> <li><code>striptags</code>: Strips SGML/XML tags.</li> <li><code>length</code>: Returns the length of a value.</li> <li><code>default(value, default_value='')</code>: Provides a default value if a variable is undefined.</li> <li><code>replace(old, new, count=None)</code>: Replaces occurrences of a substring.</li> <li><code>format(value, *args, **kwargs)</code>: Formats a string using Python's string formatting.</li> </ul>"},{"location":"Cheat-Sheets/Flask/#forms","title":"Forms","text":""},{"location":"Cheat-Sheets/Flask/#basic-form","title":"Basic Form","text":"<pre><code>&lt;form method=\"post\"&gt;\n    &lt;label for=\"name\"&gt;Name:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"text\" id=\"name\" name=\"name\"&gt;&lt;br&gt;\n    &lt;label for=\"email\"&gt;Email:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"email\" id=\"email\" name=\"email\"&gt;&lt;br&gt;\n    &lt;input type=\"submit\" value=\"Submit\"&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#using-flask-wtf","title":"Using Flask-WTF","text":"<p>Installation:</p> <pre><code>pip install flask-wtf\n</code></pre> <p>Configuration (in your app):</p> <pre><code>import os\nfrom flask import Flask\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField\nfrom wtforms.validators import DataRequired\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY') or 'your_secret_key'\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#define-a-form-formspy","title":"Define a Form (forms.py)","text":"<pre><code>from flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField, TextAreaField, EmailField, BooleanField\nfrom wtforms.validators import DataRequired, Length, Email\n\nclass MyForm(FlaskForm):\n    name = StringField('Name', validators=[DataRequired(), Length(min=2, max=20)])\n    email = EmailField('Email', validators=[DataRequired(), Email()])\n    message = TextAreaField('Message', validators=[DataRequired()])\n    agree = BooleanField('I agree to the terms', validators=[DataRequired()])\n    submit = SubmitField('Submit')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#render-a-form-in-a-template","title":"Render a Form in a Template","text":"<pre><code>&lt;form method=\"post\"&gt;\n    {{ form.csrf_token }}\n    &lt;div class=\"form-group\"&gt;\n        {{ form.name.label }}&lt;br&gt;\n        {{ form.name(class=\"form-control\") }}\n        {% if form.name.errors %}\n            &lt;ul class=\"errors\"&gt;\n                {% for error in form.name.errors %}\n                    &lt;li&gt;{{ error }}&lt;/li&gt;\n                {% endfor %}\n            &lt;/ul&gt;\n        {% endif %}\n    &lt;/div&gt;\n    &lt;div class=\"form-group\"&gt;\n        {{ form.email.label }}&lt;br&gt;\n        {{ form.email(class=\"form-control\") }}\n        {% if form.email.errors %}\n            &lt;ul class=\"errors\"&gt;\n                {% for error in form.email.errors %}\n                    &lt;li&gt;{{ error }}&lt;/li&gt;\n                {% endfor %}\n            &lt;/ul&gt;\n        {% endif %}\n    &lt;/div&gt;\n    &lt;div class=\"form-group\"&gt;\n        {{ form.message.label }}&lt;br&gt;\n        {{ form.message(class=\"form-control\") }}\n        {% if form.message.errors %}\n            &lt;ul class=\"errors\"&gt;\n                {% for error in form.message.errors %}\n                    &lt;li&gt;{{ error }}&lt;/li&gt;\n                {% endfor %}\n            &lt;/ul&gt;\n        {% endif %}\n    &lt;/div&gt;\n    &lt;div class=\"form-group\"&gt;\n        {{ form.agree.label }}\n        {{ form.agree }}\n        {% if form.agree.errors %}\n            &lt;ul class=\"errors\"&gt;\n                {% for error in form.agree.errors %}\n                    &lt;li&gt;{{ error }}&lt;/li&gt;\n                {% endfor %}\n            &lt;/ul&gt;\n        {% endif %}\n    &lt;/div&gt;\n    {{ form.submit(class=\"btn btn-primary\") }}\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#process-form-data-in-a-view","title":"Process Form Data in a View","text":"<pre><code>from flask import Flask, render_template, request, redirect, url_for\nfrom forms import MyForm\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\n\n@app.route('/form', methods=['GET', 'POST'])\ndef my_form_view():\n    form = MyForm()\n    if form.validate_on_submit():\n        name = form.name.data\n        email = form.email.data\n        message = form.message.data\n        # Process the data (e.g., save to database, send email)\n        return redirect(url_for('success'))\n    return render_template('myform.html', form=form)\n\n@app.route('/success')\ndef success():\n    return \"Form submitted successfully!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#databases","title":"Databases","text":""},{"location":"Cheat-Sheets/Flask/#using-flask-sqlalchemy","title":"Using Flask-SQLAlchemy","text":"<p>Installation:</p> <pre><code>pip install flask-sqlalchemy\n</code></pre> <p>Configuration:</p> <pre><code>from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nimport os\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL') or 'sqlite:///site.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\ndb = SQLAlchemy(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#define-a-model","title":"Define a Model","text":"<pre><code>from flask_sqlalchemy import SQLAlchemy\nfrom datetime import datetime\n\ndb = SQLAlchemy()\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(20), unique=True, nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    image_file = db.Column(db.String(20), nullable=False, default='default.jpg')\n    password = db.Column(db.String(60), nullable=False)\n    posts = db.relationship('Post', backref='author', lazy=True)\n\n    def __repr__(self):\n        return f\"User('{self.username}', '{self.email}', '{self.image_file}')\"\n\nclass Post(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(100), nullable=False)\n    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)\n    content = db.Column(db.Text, nullable=False)\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n\n    def __repr__(self):\n        return f\"Post('{self.title}', '{self.date_posted}')\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#create-and-manage-tables","title":"Create and Manage Tables","text":"<pre><code>from yourapp import app, db\n\nwith app.app_context():\n    db.create_all()  # Create tables\n\n# In the Python shell:\n# from yourapp import db, User, Post\n# user_1 = User(username='Corey', email='corey@example.com')\n# db.session.add(user_1)\n# db.session.commit()\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#querying-the-database","title":"Querying the Database","text":"<pre><code>from yourapp import db, User, Post\n\n# Get all users\nall_users = User.query.all()\n\n# Filter users\nfiltered_users = User.query.filter_by(username='Corey')\n\n# Get a single user by ID\nuser = User.query.get(1)\n\n# Get a single user, handling 404 error\nuser = User.query.get_or_404(1)\n\n# Create a new user\nnew_user = User(username='NewUser', email='new@example.com', password='password')\ndb.session.add(new_user)\ndb.session.commit()\n\n# Update an existing user\nuser = User.query.get(1)\nuser.email = 'updated@example.com'\ndb.session.commit()\n\n# Delete a user\nuser = User.query.get(1)\ndb.session.delete(user)\ndb.session.commit()\n\n# Relationships\nuser = User.query.get(1)\nposts = user.posts  # Access posts related to the user\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#static-files","title":"Static Files","text":""},{"location":"Cheat-Sheets/Flask/#configure-static-files","title":"Configure Static Files","text":"<p>Create a <code>static</code> folder in your app directory.</p> <p>In your template:</p> <pre><code>{% load static %}\n&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{% static 'css/style.css' %}\"&gt;\n&lt;img src=\"{% static 'images/logo.png' %}\"&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#blueprints","title":"Blueprints","text":""},{"location":"Cheat-Sheets/Flask/#create-a-blueprint","title":"Create a Blueprint","text":"<pre><code>from flask import Blueprint\n\nmain = Blueprint('main', __name__)\n\n@main.route('/')\ndef index():\n    return \"Main Blueprint Index\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#register-a-blueprint","title":"Register a Blueprint","text":"<pre><code>from flask import Flask\nfrom yourapp.main import main\n\napp = Flask(__name__)\napp.register_blueprint(main)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-extensions","title":"Flask Extensions","text":""},{"location":"Cheat-Sheets/Flask/#flask-mail","title":"Flask-Mail","text":"<p>Installation:</p> <pre><code>pip install flask-mail\n</code></pre> <p>Configuration:</p> <pre><code>from flask import Flask\nfrom flask_mail import Mail, Message\n\napp = Flask(__name__)\napp.config['MAIL_SERVER'] = 'smtp.gmail.com'\napp.config['MAIL_PORT'] = 587\napp.config['MAIL_USE_TLS'] = True\napp.config['MAIL_USE_SSL'] = False\napp.config['MAIL_USERNAME'] = 'your_email@gmail.com'\napp.config['MAIL_PASSWORD'] = 'your_password'\nmail = Mail(app)\n</code></pre> <p>Sending Emails:</p> <pre><code>from flask import Flask, render_template\nfrom flask_mail import Mail, Message\n\napp = Flask(__name__)\napp.config['MAIL_SERVER'] = 'smtp.gmail.com'\napp.config['MAIL_PORT'] = 587\napp.config['MAIL_USE_TLS'] = True\napp.config['MAIL_USE_SSL'] = False\napp.config['MAIL_USERNAME'] = 'your_email@gmail.com'\napp.config['MAIL_PASSWORD'] = 'your_password'\nmail = Mail(app)\n\n@app.route('/send')\ndef send_email():\n    msg = Message(\"Hello\",\n                  sender=\"your_email@gmail.com\",\n                  recipients=[\"recipient@example.com\"])\n    msg.body = \"Hello Flask message sent from Flask-Mail\"\n    mail.send(msg)\n    return \"Sent\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-migrate","title":"Flask-Migrate","text":"<p>Installation:</p> <pre><code>pip install flask-migrate\n</code></pre> <p>Configuration:</p> <pre><code>from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.db'\ndb = SQLAlchemy(app)\nmigrate = Migrate(app, db)\n</code></pre> <p>Migration Commands:</p> <pre><code>flask db init  # Initialize the migration repository\nflask db migrate -m \"Initial migration\"  # Create a new migration\nflask db upgrade  # Apply the latest migration\nflask db downgrade  # Revert to a previous migration\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-login","title":"Flask-Login","text":"<p>Installation:</p> <pre><code>pip install flask-login\n</code></pre> <p>Configuration:</p> <pre><code>from flask import Flask\nfrom flask_login import LoginManager\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\nlogin_manager = LoginManager(app)\nlogin_manager.login_view = 'login'  # Specify the login view\n</code></pre> <p>User Model:</p> <pre><code>from flask_login import UserMixin\n\nclass User(db.Model, UserMixin):\n    id = db.Column(db.Integer, primary_key=True)\n    # ... other fields ...\n</code></pre> <p>User Loader Callback:</p> <pre><code>from yourapp import login_manager, User\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n</code></pre> <p>Protecting Views:</p> <pre><code>from flask_login import login_required\n\n@app.route('/protected')\n@login_required\ndef protected():\n    return \"Protected View\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#testing","title":"Testing","text":""},{"location":"Cheat-Sheets/Flask/#using-pytest","title":"Using pytest","text":"<p>Installation:</p> <pre><code>pip install pytest pytest-flask\n</code></pre> <p>Test Example:</p> <pre><code>import pytest\nfrom yourapp import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n\ndef test_index_route(client):\n    response = client.get('/')\n    assert response.status_code == 200\n    assert b'Hello, World!' in response.data\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#using-unittest","title":"Using unittest","text":"<pre><code>import unittest\nfrom yourapp import app\n\nclass MyTestCase(unittest.TestCase):\n    def setUp(self):\n        app.config['TESTING'] = True\n        self.app = app.test_client()\n\n    def test_index_route(self):\n        response = self.app.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Hello, World!', response.data)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#deployment","title":"Deployment","text":""},{"location":"Cheat-Sheets/Flask/#production-settings","title":"Production Settings","text":"<ul> <li>Set <code>FLASK_ENV=production</code> to disable debug mode.</li> <li>Use a production WSGI server (e.g., Gunicorn, uWSGI).</li> <li>Configure your web server (e.g., Nginx, Apache) to proxy requests to the WSGI server.</li> <li>Use a process manager (e.g., Supervisor, systemd) to manage the WSGI server.</li> <li>Configure logging.</li> <li>Use HTTPS.</li> </ul>"},{"location":"Cheat-Sheets/Flask/#wsgi-servers","title":"WSGI Servers","text":"<p>Gunicorn:</p> <pre><code>pip install gunicorn\ngunicorn yourapp:app --bind 0.0.0.0:8000\n</code></pre> <p>uWSGI:</p> <pre><code>pip install uwsgi\nuwsgi --http 0.0.0.0:8000 --module yourapp\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#environment-variables","title":"Environment Variables","text":"<p>Use environment variables for sensitive settings (e.g., <code>SECRET_KEY</code>, database credentials).</p>"},{"location":"Cheat-Sheets/Flask/#example-deployment-with-gunicorn-and-nginx","title":"Example Deployment with Gunicorn and Nginx","text":"<ol> <li>Install Gunicorn: <code>pip install gunicorn</code></li> <li>Create a WSGI entry point: <code>yourapp.py</code> (already covered)</li> <li>Create a systemd service file: <code>/etc/systemd/system/yourapp.service</code></li> </ol> <pre><code>[Unit]\nDescription=Gunicorn instance to serve yourapp\nAfter=network.target\n\n[Service]\nUser=youruser\nGroup=www-data\nWorkingDirectory=/path/to/your/app\nExecStart=/path/to/your/venv/bin/gunicorn --workers 3 --max-requests 500 --bind unix:/run/yourapp.sock yourapp:app\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Create an Nginx configuration file: <code>/etc/nginx/sites-available/yourapp</code></li> </ol> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com www.yourdomain.com;\n\n    location / {\n        include proxy_params;\n        proxy_pass http://unix:/run/yourapp.sock;\n    }\n\n    location /static {\n        alias /path/to/your/app/static;\n    }\n}\n</code></pre> <ol> <li>Create a symbolic link:</li> </ol> <pre><code>sudo ln -s /etc/nginx/sites-available/yourapp /etc/nginx/sites-enabled\n</code></pre> <ol> <li>Restart Nginx:</li> </ol> <pre><code>sudo systemctl restart nginx\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#security","title":"Security","text":"<ul> <li>Use a strong <code>SECRET_KEY</code> and keep it secret.</li> <li>Use HTTPS.</li> <li>Sanitize user input to prevent XSS attacks.</li> <li>Use parameterized queries to prevent SQL injection.</li> <li>Use a Content Security Policy (CSP) to prevent various attacks.</li> <li>Protect against CSRF attacks using Flask-WTF.</li> <li>Limit file upload sizes.</li> <li>Validate file uploads.</li> <li>Use a security linter (e.g., Bandit).</li> </ul>"},{"location":"Cheat-Sheets/Flask/#logging","title":"Logging","text":""},{"location":"Cheat-Sheets/Flask/#configure-logging","title":"Configure Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# In your code:\nlogger.info('This is an info message')\nlogger.warning('This is a warning message')\nlogger.error('This is an error message')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#logging-to-a-file","title":"Logging to a File","text":"<pre><code>import logging\nimport logging.handlers\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# Create a file handler\nlog_handler = logging.handlers.RotatingFileHandler('yourapp.log', maxBytes=10240, backupCount=5)\nlog_handler.setLevel(logging.DEBUG)\n\n# Create a logging format\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(formatter)\n\n# Add the handlers to the logger\nlogger.addHandler(log_handler)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-cli","title":"Flask CLI","text":"<p>Flask provides a command-line interface for managing your application.</p> <ul> <li><code>flask run</code>: Runs the development server.</li> <li><code>flask shell</code>: Opens a Python shell with the Flask application context.</li> <li><code>flask routes</code>: Shows the registered routes.</li> <li><code>flask db</code>: Manages database migrations (requires Flask-Migrate).</li> </ul>"},{"location":"Cheat-Sheets/Flask/#context-processors","title":"Context Processors","text":"<p>Context processors inject variables automatically into all templates.</p> <pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.context_processor\ndef inject_variables():\n    return dict(site_name=\"My Awesome Website\")\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n</code></pre> <p>In <code>templates/index.html</code>:</p> <pre><code>&lt;h1&gt;Welcome to {{ site_name }}!&lt;/h1&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#error-handling","title":"Error Handling","text":""},{"location":"Cheat-Sheets/Flask/#custom-error-pages","title":"Custom Error Pages","text":"<pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    return render_template('404.html'), 404\n\n@app.errorhandler(500)\ndef internal_server_error(e):\n    return render_template('500.html'), 500\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#logging-exceptions","title":"Logging Exceptions","text":"<pre><code>import logging\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\nlogger = logging.getLogger(__name__)\n\n@app.route('/')\ndef index():\n    try:\n        # Some code that might raise an exception\n        raise ValueError(\"Something went wrong\")\n    except Exception as e:\n        logger.exception(\"An error occurred\")\n        return render_template('error.html', error=str(e)), 500\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-restful","title":"Flask-RESTful","text":""},{"location":"Cheat-Sheets/Flask/#installation_1","title":"Installation","text":"<pre><code>pip install flask-restful\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#define-resources","title":"Define Resources","text":"<pre><code>from flask import Flask\nfrom flask_restful import Api, Resource\n\napp = Flask(__name__)\napi = Api(app)\n\nclass HelloWorld(Resource):\n    def get(self):\n        return {'hello': 'world'}\n\napi.add_resource(HelloWorld, '/')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#request-parsing","title":"Request Parsing","text":"<pre><code>from flask_restful import reqparse\n\nparser = reqparse.RequestParser()\nparser.add_argument('name', required=True, help=\"Name is required\")\n\nclass MyResource(Resource):\n    def post(self):\n        args = parser.parse_args()\n        name = args['name']\n        return {'message': f'Hello, {name}!'}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#session-management","title":"Session Management","text":"<pre><code>from flask import Flask, session, redirect, url_for, escape, request\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\n\n@app.route('/')\ndef index():\n    if 'username' in session:\n        return f'Logged in as {escape(session[\"username\"])}\nClick here to &lt;a href=\"{url_for(\"logout\")}\"&gt;logout&lt;/a&gt;'\n    return 'You are not logged in\nClick here to &lt;a href=\"{url_for(\"login\")}\"&gt;login&lt;/a&gt;'\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        session['username'] = request.form['username']\n        return redirect(url_for('index'))\n    return '''\n        &lt;form method=\"post\"&gt;\n            &lt;p&gt;&lt;input type=text name=username&gt;\n            &lt;p&gt;&lt;input type=submit value=Login&gt;\n        &lt;/form&gt;\n    '''\n\n@app.route('/logout')\ndef logout():\n    session.pop('username', None)\n    return redirect(url_for('index'))\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-cors","title":"Flask-CORS","text":""},{"location":"Cheat-Sheets/Flask/#installation_2","title":"Installation","text":"<pre><code>pip install flask-cors\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS for all routes\n\n@app.route(\"/api/data\")\ndef get_data():\n    return {\"message\": \"This is CORS enabled!\"}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#testing_1","title":"Testing","text":""},{"location":"Cheat-Sheets/Flask/#using-pytest_1","title":"Using pytest","text":"<p>Installation:</p> <pre><code>pip install pytest pytest-flask\n</code></pre> <p>Test Example:</p> <pre><code>import pytest\nfrom yourapp import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n\ndef test_index_route(client):\n    response = client.get('/')\n    assert response.status_code == 200\n    assert b'Hello, World!' in response.data\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#using-unittest_1","title":"Using unittest","text":"<pre><code>import unittest\nfrom yourapp import app\n\nclass MyTestCase(unittest.TestCase):\n    def setUp(self):\n        app.config['TESTING'] = True\n        self.app = app.test_client()\n\n    def test_index_route(self):\n        response = self.app.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Hello, World!', response.data)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#deployment_1","title":"Deployment","text":""},{"location":"Cheat-Sheets/Flask/#production-settings_1","title":"Production Settings","text":"<ul> <li>Set <code>FLASK_ENV=production</code> to disable debug mode.</li> <li>Use a production WSGI server (e.g., Gunicorn, uWSGI).</li> <li>Configure your web server (e.g., Nginx, Apache) to proxy requests to the WSGI server.</li> <li>Use a process manager (e.g., Supervisor, systemd) to manage the WSGI server.</li> <li>Configure logging.</li> <li>Use HTTPS.</li> </ul>"},{"location":"Cheat-Sheets/Flask/#wsgi-servers_1","title":"WSGI Servers","text":"<p>Gunicorn:</p> <pre><code>pip install gunicorn\ngunicorn yourapp:app --bind 0.0.0.0:8000\n</code></pre> <p>uWSGI:</p> <pre><code>pip install uwsgi\nuwsgi --http 0.0.0.0:8000 --module yourapp\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#environment-variables_1","title":"Environment Variables","text":"<p>Use environment variables for sensitive settings (e.g., <code>SECRET_KEY</code>, database credentials).</p>"},{"location":"Cheat-Sheets/Flask/#example-deployment-with-gunicorn-and-nginx_1","title":"Example Deployment with Gunicorn and Nginx","text":"<ol> <li>Install Gunicorn: <code>pip install gunicorn</code></li> <li>Create a WSGI entry point: <code>yourapp.py</code> (already covered)</li> <li>Create a systemd service file: <code>/etc/systemd/system/yourapp.service</code></li> </ol> <pre><code>[Unit]\nDescription=Gunicorn instance to serve yourapp\nAfter=network.target\n\n[Service]\nUser=youruser\nGroup=www-data\nWorkingDirectory=/path/to/your/app\nExecStart=/path/to/your/venv/bin/gunicorn --workers 3 --max-requests 500 --bind unix:/run/yourapp.sock yourapp:app\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Create an Nginx configuration file: <code>/etc/nginx/sites-available/yourapp</code></li> </ol> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com www.yourdomain.com;\n\n    location / {\n        include proxy_params;\n        proxy_pass http://unix:/run/yourapp.sock;\n    }\n\n    location /static {\n        alias /path/to/your/app/static;\n    }\n}\n</code></pre> <ol> <li>Create a symbolic link:</li> </ol> <pre><code>sudo ln -s /etc/nginx/sites-available/yourapp /etc/nginx/sites-enabled\n</code></pre> <ol> <li>Restart Nginx:</li> </ol> <pre><code>sudo systemctl restart nginx\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#security_1","title":"Security","text":"<ul> <li>Use a strong <code>SECRET_KEY</code> and keep it secret.</li> <li>Use HTTPS.</li> <li>Sanitize user input to prevent XSS attacks.</li> <li>Use parameterized queries to prevent SQL injection.</li> <li>Use a Content Security Policy (CSP) to prevent various attacks.</li> <li>Protect against CSRF attacks using Flask-WTF.</li> <li>Limit file upload sizes.</li> <li>Validate file uploads.</li> <li>Use a security linter (e.g., Bandit).</li> </ul>"},{"location":"Cheat-Sheets/Flask/#logging_1","title":"Logging","text":""},{"location":"Cheat-Sheets/Flask/#configure-logging_1","title":"Configure Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# In your code:\nlogger.info('This is an info message')\nlogger.warning('This is a warning message')\nlogger.error('This is an error message')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#logging-to-a-file_1","title":"Logging to a File","text":"<pre><code>import logging\nimport logging.handlers\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# Create a file handler\nlog_handler = logging.handlers.RotatingFileHandler('yourapp.log', maxBytes=10240, backupCount=5)\nlog_handler.setLevel(logging.DEBUG)\n\n# Create a logging format\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(formatter)\n\n# Add the handlers to the logger\nlogger.addHandler(log_handler)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-cli_1","title":"Flask CLI","text":"<p>Flask provides a command-line interface for managing your application.</p> <ul> <li><code>flask run</code>: Runs the development server.</li> <li><code>flask shell</code>: Opens a Python shell with the Flask application context.</li> <li><code>flask routes</code>: Shows the registered routes.</li> <li><code>flask db</code>: Manages database migrations (requires Flask-Migrate).</li> </ul> <p>To use the Flask CLI, you need to set the <code>FLASK_APP</code> environment variable:</p> <pre><code>export FLASK_APP=yourapp.py\n</code></pre> <p>Then, you can use the <code>flask</code> command:</p> <pre><code>flask run\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#context-processors_1","title":"Context Processors","text":"<p>Context processors inject variables automatically <pre><code>from flask import Flask\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS for all routes\n\n@app.route(\"/api/data\")\ndef get_data():\n    return {\"message\": \"This is CORS enabled!\"}\n</code></pre></p>"},{"location":"Cheat-Sheets/Flask/#testing_2","title":"Testing","text":""},{"location":"Cheat-Sheets/Flask/#using-pytest_2","title":"Using pytest","text":"<p>Installation:</p> <pre><code>pip install pytest pytest-flask\n</code></pre> <p>Test Example:</p> <pre><code>import pytest\nfrom yourapp import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n\ndef test_index_route(client):\n    response = client.get('/')\n    assert response.status_code == 200\n    assert b'Hello, World!' in response.data\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#using-unittest_2","title":"Using unittest","text":"<pre><code>import unittest\nfrom yourapp import app\n\nclass MyTestCase(unittest.TestCase):\n    def setUp(self):\n        app.config['TESTING'] = True\n        self.app = app.test_client()\n\n    def test_index_route(self):\n        response = self.app.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Hello, World!', response.data)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#deployment_2","title":"Deployment","text":""},{"location":"Cheat-Sheets/Flask/#production-settings_2","title":"Production Settings","text":"<ul> <li>Set <code>FLASK_ENV=production</code> to disable debug mode.</li> <li>Use a production WSGI server (e.g., Gunicorn, uWSGI).</li> <li>Configure your web server (e.g., Nginx, Apache) to proxy requests to the WSGI server.</li> <li>Use a process manager (e.g., Supervisor, systemd) to manage the WSGI server.</li> <li>Configure logging.</li> <li>Use HTTPS.</li> </ul>"},{"location":"Cheat-Sheets/Flask/#wsgi-servers_2","title":"WSGI Servers","text":"<p>Gunicorn:</p> <pre><code>pip install gunicorn\ngunicorn yourapp:app --bind 0.0.0.0:8000\n</code></pre> <p>uWSGI:</p> <pre><code>pip install uwsgi\nuwsgi --http 0.0.0.0:8000 --module yourapp\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#environment-variables_2","title":"Environment Variables","text":"<p>Use environment variables for sensitive settings (e.g., <code>SECRET_KEY</code>, database credentials).</p>"},{"location":"Cheat-Sheets/Flask/#example-deployment-with-gunicorn-and-nginx_2","title":"Example Deployment with Gunicorn and Nginx","text":"<ol> <li>Install Gunicorn: <code>pip install gunicorn</code></li> <li>Create a WSGI entry point: <code>yourapp.py</code> (already covered)</li> <li>Create a systemd service file: <code>/etc/systemd/system/yourapp.service</code></li> </ol> <pre><code>[Unit]\nDescription=Gunicorn instance to serve yourapp\nAfter=network.target\n\n[Service]\nUser=youruser\nGroup=www-data\nWorkingDirectory=/path/to/your/app\nExecStart=/path/to/your/venv/bin/gunicorn --workers 3 --max-requests 500 --bind unix:/run/yourapp.sock yourapp:app\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Create an Nginx configuration file: <code>/etc/nginx/sites-available/yourapp</code></li> </ol> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com www.yourdomain.com;\n\n    location / {\n        include proxy_params;\n        proxy_pass http://unix:/run/yourapp.sock;\n    }\n\n    location /static {\n        alias /path/to/your/app/static;\n    }\n}\n</code></pre> <ol> <li>Create a symbolic link:</li> </ol> <pre><code>sudo ln -s /etc/nginx/sites-available/yourapp /etc/nginx/sites-enabled\n</code></pre> <ol> <li>Restart Nginx:</li> </ol> <pre><code>sudo systemctl restart nginx\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#security_2","title":"Security","text":"<ul> <li>Use a strong <code>SECRET_KEY</code> and keep it secret.</li> <li>Use HTTPS.</li> <li>Sanitize user input to prevent XSS attacks.</li> <li>Use parameterized queries to prevent SQL injection.</li> <li>Use a Content Security Policy (CSP) to prevent various attacks.</li> <li>Protect against CSRF attacks using Flask-WTF.</li> <li>Limit file upload sizes.</li> <li>Validate file uploads.</li> <li>Use a security linter (e.g., Bandit).</li> </ul>"},{"location":"Cheat-Sheets/Flask/#logging_2","title":"Logging","text":""},{"location":"Cheat-Sheets/Flask/#configure-logging_2","title":"Configure Logging","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# In your code:\nlogger.info('This is an info message')\nlogger.warning('This is a warning message')\nlogger.error('This is an error message')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#logging-to-a-file_2","title":"Logging to a File","text":"<pre><code>import logging\nimport logging.handlers\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# Create a file handler\nlog_handler = logging.handlers.RotatingFileHandler('yourapp.log', maxBytes=10240, backupCount=5)\nlog_handler.setLevel(logging.DEBUG)\n\n# Create a logging format\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlog_handler.setFormatter(formatter)\n\n# Add the handlers to the logger\nlogger.addHandler(log_handler)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-cli_2","title":"Flask CLI","text":"<p>Flask provides a command-line interface for managing your application.</p> <ul> <li><code>flask run</code>: Runs the development server.</li> <li><code>flask shell</code>: Opens a Python shell with the Flask application context.</li> <li><code>flask routes</code>: Shows the registered routes.</li> <li><code>flask db</code>: Manages database migrations (requires Flask-Migrate).</li> </ul> <p>To use the Flask CLI, you need to set the <code>FLASK_APP</code> environment variable:</p> <pre><code>export FLASK_APP=yourapp.py\n</code></pre> <p>Then, you can use the <code>flask</code> command:</p> <pre><code>flask run\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#context-processors_2","title":"Context Processors","text":"<p>Context processors inject variables automatically into all templates.</p> <pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.context_processor\ndef inject_variables():\n    return dict(site_name=\"My Awesome Website\")\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n</code></pre> <p>In <code>templates/index.html</code>:</p> <pre><code>&lt;h1&gt;Welcome to {{ site_name }}!&lt;/h1&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#error-handling_1","title":"Error Handling","text":""},{"location":"Cheat-Sheets/Flask/#custom-error-pages_1","title":"Custom Error Pages","text":"<pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    return render_template('404.html'), 404\n\n@app.errorhandler(500)\ndef internal_server_error(e):\n    return render_template('500.html'), 500\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#logging-exceptions_1","title":"Logging Exceptions","text":"<pre><code>import logging\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\nlogger = logging.getLogger(__name__)\n\n@app.route('/')\ndef index():\n    try:\n        # Some code that might raise an exception\n        raise ValueError(\"Something went wrong\")\n    except Exception as e:\n        logger.exception(\"An error occurred\")\n        return render_template('error.html', error=str(e)), 500\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-restful_1","title":"Flask-RESTful","text":""},{"location":"Cheat-Sheets/Flask/#installation_3","title":"Installation","text":"<pre><code>pip install flask-restful\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#define-resources_1","title":"Define Resources","text":"<pre><code>from flask import Flask\nfrom flask_restful import Api, Resource\n\napp = Flask(__name__)\napi = Api(app)\n\nclass HelloWorld(Resource):\n    def get(self):\n        return {'hello': 'world'}\n\napi.add_resource(HelloWorld, '/')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#request-parsing_1","title":"Request Parsing","text":"<pre><code>from flask_restful import reqparse\n\nparser = reqparse.RequestParser()\nparser.add_argument('name', required=True, help=\"Name is required\")\n\nclass MyResource(Resource):\n    def post(self):\n        args = parser.parse_args()\n        name = args['name']\n        return {'message': f'Hello, {name}!'}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#session-management_1","title":"Session Management","text":"<pre><code>from flask import Flask, session, redirect, url_for, escape, request\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\n\n@app.route('/')\ndef index():\n    if 'username' in session:\n        return f'Logged in as {escape(session[\"username\"])}\nClick here to &lt;a href=\"{url_for(\"logout\")}\"&gt;logout&lt;/a&gt;'\n    return 'You are not logged in\nClick here to &lt;a href=\"{url_for(\"login\")}\"&gt;login&lt;/a&gt;'\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        session['username'] = request.form['username']\n        return redirect(url_for('index'))\n    return '''\n        &lt;form method=\"post\"&gt;\n            &lt;p&gt;&lt;input type=text name=username&gt;\n            &lt;p&gt;&lt;input type=submit value=Login&gt;\n        &lt;/form&gt;\n    '''\n\n@app.route('/logout')\ndef logout():\n    session.pop('username', None)\n    return redirect(url_for('index'))\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-cors_1","title":"Flask-CORS","text":""},{"location":"Cheat-Sheets/Flask/#installation_4","title":"Installation","text":"<pre><code>pip install flask-cors\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_1","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app)  # Enable CORS for all routes\n\n@app.route(\"/api/data\")\ndef get_data():\n    return {\"message\": \"This is CORS enabled!\"}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#signals","title":"Signals","text":"<p>Flask doesn't have built-in signals like Django, but you can use a third-party library like <code>blinker</code> to implement signals.</p>"},{"location":"Cheat-Sheets/Flask/#installation_5","title":"Installation","text":"<pre><code>pip install blinker\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_2","title":"Usage","text":"<pre><code>from flask import Flask\nfrom blinker import signal\n\napp = Flask(__name__)\n\nbefore_request = signal('before_request')\n\n@app.before_request\ndef before_request_handler():\n    before_request.send(app)\n\n@before_request.connect\ndef my_listener(sender):\n    print(\"Before request signal received\")\n\n@app.route('/')\ndef index():\n    return \"Hello, World!\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-limiter","title":"Flask-Limiter","text":""},{"location":"Cheat-Sheets/Flask/#installation_6","title":"Installation","text":"<pre><code>pip install Flask-Limiter\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_3","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_limiter import Limiter\nfrom flask_limiter.util import get_remote_address\n\napp = Flask(__name__)\nlimiter = Limiter(\n    app,\n    key_func=get_remote_address,\n    default_limits=[\"200 per day\", \"50 per hour\"]\n)\n\n@app.route(\"/slow\")\n@limiter.limit(\"10 per minute\")\ndef slow():\n    return \"Slow route\"\n\n@app.route(\"/fast\")\ndef fast():\n    return \"Fast route\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-apscheduler","title":"Flask-APScheduler","text":""},{"location":"Cheat-Sheets/Flask/#installation_7","title":"Installation","text":"<pre><code>pip install flask-apscheduler\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_4","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_apscheduler import APScheduler\nimport time\n\nclass Config(object):\n    JOBS = [\n        {\n            'id': 'job1',\n            'func': 'yourapp:job1',\n            'trigger': 'interval',\n            'seconds': 10\n        }\n    ]\n    SCHEDULER_API_ENABLED = True\n\napp = Flask(__name__)\napp.config.from_object(Config())\n\nscheduler = APScheduler()\n# it is also possible to enable the API directly\n# scheduler.api_enabled = True\nscheduler.init_app(app)\nscheduler.start()\n\ndef job1():\n    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-sitemap","title":"Flask-Sitemap","text":""},{"location":"Cheat-Sheets/Flask/#installation_8","title":"Installation","text":"<pre><code>pip install Flask-Sitemap\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_5","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_sitemap import Sitemap\n\napp = Flask(__name__)\next = Sitemap(app=app)\n\n@app.route(\"/sitemap.xml\")\ndef sitemap():\n    return ext.generate(base_url='http://example.com')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-wtf-csrf-protection","title":"Flask-WTF CSRF Protection","text":""},{"location":"Cheat-Sheets/Flask/#configuration","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_wtf.csrf import CSRFProtect\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\ncsrf = CSRFProtect(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage-in-templates","title":"Usage in Templates","text":"<pre><code>&lt;form method=\"post\"&gt;\n    {{ form.csrf_token }}\n    &lt;!-- Your form fields --&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-flatpages","title":"Flask-FlatPages","text":""},{"location":"Cheat-Sheets/Flask/#installation_9","title":"Installation","text":"<pre><code>pip install Flask-FlatPages\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#configuration_1","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_flatpages import FlatPages\n\napp = Flask(__name__)\napp.config['FLATPAGES_EXTENSION'] = '.md'\napp.config['FLATPAGES_ROOT'] = 'pages'\npages = FlatPages(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_6","title":"Usage","text":"<p>Create a directory named <code>pages</code> in your project root. Add your flat pages as <code>.md</code> files.</p> <pre><code>from flask import Flask, render_template\nfrom flask_flatpages import FlatPages, pygments_style_defs\n\napp = Flask(__name__)\napp.config['FLATPAGES_EXTENSION'] = '.md'\napp.config['FLATPAGES_ROOT'] = 'pages'\napp.config['FLATPAGES_MARKDOWN_EXTENSIONS'] = ['codehilite', 'fenced_code']\napp.config['PYGMENTS_STYLE'] = 'default'\npages = FlatPages(app)\n\n@app.route('/page/&lt;path:path&gt;')\ndef page(path):\n    page = pages.get_or_404(path)\n    return render_template('page.html', page=page, pygments_style=pygments_style_defs())\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-assets","title":"Flask-Assets","text":""},{"location":"Cheat-Sheets/Flask/#installation_10","title":"Installation","text":"<pre><code>pip install Flask-Assets\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#configuration_2","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_assets import Environment, Bundle\n\napp = Flask(__name__)\nassets = Environment(app)\n\njs = Bundle('js/jquery.js', 'js/base.js', filters='jsmin', output='gen/packed.js')\ncss = Bundle('css/base.css', 'css/common.css', filters='cssmin', output='gen/all.css')\n\nassets.register('all_js', js)\nassets.register('all_css', css)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage-in-templates_1","title":"Usage in Templates","text":"<pre><code>{% assets \"all_js\" %}\n    &lt;script type=\"text/javascript\" src=\"{{ ASSET_URL }}\"&gt;&lt;/script&gt;\n{% endassets %}\n\n{% assets \"all_css\" %}\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{{ ASSET_URL }}\"&gt;\n{% endassets %}\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-babel","title":"Flask-Babel","text":""},{"location":"Cheat-Sheets/Flask/#installation_11","title":"Installation","text":"<pre><code>pip install Flask-Babel\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#configuration_3","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_babel import Babel\n\napp = Flask(__name__)\napp.config['BABEL_DEFAULT_LOCALE'] = 'en'\nbabel = Babel(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_7","title":"Usage","text":"<pre><code>from flask import Flask, render_template\nfrom flask_babel import Babel, gettext\n\napp = Flask(__name__)\napp.config['BABEL_DEFAULT_LOCALE'] = 'en'\nbabel = Babel(app)\n\n@app.route('/')\ndef index():\n    title = gettext('Welcome')\n    return render_template('index.html', title=title)\n</code></pre> <p>In <code>templates/index.html</code>:</p> <pre><code>&lt;h1&gt;{{ title }}&lt;/h1&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-socketio","title":"Flask-SocketIO","text":""},{"location":"Cheat-Sheets/Flask/#installation_12","title":"Installation","text":"<pre><code>pip install flask-socketio\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_8","title":"Usage","text":"<pre><code>from flask import Flask, render_template\nfrom flask_socketio import SocketIO, emit\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'secret!'\nsocketio = SocketIO(app)\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@socketio.on('connect')\ndef test_connect():\n    emit('my response', {'data': 'Connected!'})\n\n@socketio.on('my event')\ndef handle_my_custom_event(json):\n    print('received json: ' + str(json))\n    socketio.emit('my response', json)\n\nif __name__ == '__main__':\n    socketio.run(app, debug=True)\n</code></pre> <p>In <code>templates/index.html</code>:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Flask-SocketIO Test&lt;/title&gt;\n    &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js\" integrity=\"sha512-q/dWj3kcmNeAqFvv3EY9JJ/KEvVcjtgJBmWsGGHa+YwdlOfjoOvozUvCpJlPzl5lwCDsLQIY9Mq1v8XtZiuCQ==\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/javascript\" charset=\"utf-8\"&gt;\n        $(document).ready(function() {\n            var socket = io();\n            socket.on('connect', function() {\n                socket.emit('my event', {data: 'I\\'m connected!'});\n            });\n            socket.on('my response', function(msg) {\n                $('#log').append('&lt;p&gt;Received: ' + msg.data + '&lt;/p&gt;');\n            });\n            $('form#emit').submit(function(event) {\n                socket.emit('my event', {data: $('#emit_data').val()});\n                return false;\n            });\n        });\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Flask-SocketIO Test&lt;/h1&gt;\n    &lt;div id=\"log\"&gt;&lt;/div&gt;\n    &lt;form id=\"emit\" method=\"POST\" action=\"#\"&gt;\n        &lt;input type=\"text\" id=\"emit_data\" name=\"emit_data\" placeholder=\"Message\"&gt;\n        &lt;input type=\"submit\" value=\"Echo\"&gt;\n    &lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-principal","title":"Flask-Principal","text":""},{"location":"Cheat-Sheets/Flask/#installation_13","title":"Installation","text":"<pre><code>pip install Flask-Principal\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_9","title":"Usage","text":"<pre><code>from flask import Flask, g\nfrom flask_principal import Principal, Permission, RoleNeed, UserNeed, identity_loaded, UserContext, Identity, AnonymousIdentity\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\n\nprincipals = Principal(app)\n\n# Define Needs\nadmin_permission = Permission(RoleNeed('admin'))\nposter_permission = Permission(RoleNeed('poster'))\n\n# Define Roles\nadmin_role = RoleNeed('admin')\nposter_role = RoleNeed('poster')\nuser_need = UserNeed(1)\n\n@identity_loaded.connect_via(app)\ndef on_identity_loaded(sender, identity):\n    # Set the identity user object\n    identity.user = get_user()\n\n    # Add the UserNeed to the identity\n    identity.provides.add(UserNeed(identity.user.id))\n\n    # Assuming the user has a method that returns a list of roles\n    for role in identity.user.roles:\n        identity.provides.add(RoleNeed(role.name))\n\ndef get_user():\n    # Replace with your user loading logic (e.g., from database)\n    class User(object):\n        def __init__(self, id, roles):\n            self.id = id\n            self.roles = roles\n\n    class Role(object):\n        def __init__(self, name):\n            self.name = name\n\n    admin_role = Role('admin')\n    poster_role = Role('poster')\n\n    user = User(1, [admin_role, poster_role])\n    return user\n\n@app.route('/')\ndef index():\n    with UserContext(Identity(1)):\n        if admin_permission.can():\n            return \"Admin access granted\"\n        elif poster_permission.can():\n            return \"Poster access granted\"\n        else:\n            return \"Access denied\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-jwt-extended","title":"Flask-JWT-Extended","text":""},{"location":"Cheat-Sheets/Flask/#installation_14","title":"Installation","text":"<pre><code>pip install Flask-JWT-Extended\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_10","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_jwt_extended import JWTManager, create_access_token, jwt_required, get_jwt_identity\n\napp = Flask(__name__)\napp.config[\"JWT_SECRET_KEY\"] = \"super-secret\"  # Change this!\njwt = JWTManager(app)\n\n@app.route(\"/login\", methods=[\"POST\"])\ndef login():\n    username = request.json.get(\"username\", None)\n    password = request.json.get(\"password\", None)\n    if username != \"test\" or password != \"test\":\n        return jsonify({\"msg\": \"Bad username or password\"}), 401\n\n    access_token = create_access_token(identity=username)\n    return jsonify(access_token=access_token)\n\n@app.route(\"/protected\", methods=[\"GET\"])\n@jwt_required()\ndef protected():\n    current_user = get_jwt_identity()\n    return jsonify(logged_in_as=current_user), 200\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-uploads","title":"Flask-Uploads","text":""},{"location":"Cheat-Sheets/Flask/#installation_15","title":"Installation","text":"<pre><code>pip install Flask-Uploads\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_11","title":"Usage","text":"<pre><code>from flask import Flask, request, render_template\nfrom flask_uploads import UploadSet, configure_uploads, IMAGES, patch_request_class\n\napp = Flask(__name__)\napp.config['UPLOADED_PHOTOS_DEST'] = 'uploads'\napp.config['SECRET_KEY'] = 'super secret key'\nphotos = UploadSet('photos', IMAGES)\n\nconfigure_uploads(app, photos)\npatch_request_class(app)  # set maximum file size, default is 16MB\n\n@app.route('/upload', methods=['GET', 'POST'])\ndef upload():\n    if request.method == 'POST' and 'photo' in request.files:\n        filename = photos.save(request.files['photo'])\n        url = photos.url(filename)\n        return render_template('upload.html', filename=filename, url=url)\n    return render_template('upload.html')\n</code></pre> <p>In <code>templates/upload.html</code>:</p> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Upload&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    {% if filename %}\n        &lt;img src=\"{{ url }}\" alt=\"Uploaded Image\"&gt;\n    {% else %}\n        &lt;form method=\"post\" enctype=\"multipart/form-data\"&gt;\n            &lt;input type=\"file\" name=\"photo\"&gt;\n            &lt;button type=\"submit\"&gt;Upload&lt;/button&gt;\n        &lt;/form&gt;\n    {% endif %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-mail_1","title":"Flask-Mail","text":""},{"location":"Cheat-Sheets/Flask/#installation_16","title":"Installation","text":"<pre><code>pip install flask-mail\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#configuration_4","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_mail import Mail, Message\n\napp = Flask(__name__)\napp.config['MAIL_SERVER'] = 'smtp.gmail.com'\napp.config['MAIL_PORT'] = 587\napp.config['MAIL_USE_TLS'] = True\napp.config['MAIL_USE_SSL'] = False\napp.config['MAIL_USERNAME'] = 'your_email@gmail.com'\napp.config['MAIL_PASSWORD'] = 'your_password'\nmail = Mail(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#sending-emails","title":"Sending Emails","text":"<pre><code>from flask import Flask, render_template\nfrom flask_mail import Mail, Message\n\napp = Flask(__name__)\napp.config['MAIL_SERVER'] = 'smtp.gmail.com'\napp.config['MAIL_PORT'] = 587\napp.config['MAIL_USE_TLS'] = True\napp.config['MAIL_USE_SSL'] = False\napp.config['MAIL_USERNAME'] = 'your_email@gmail.com'\napp.config['MAIL_PASSWORD'] = 'your_password'\nmail = Mail(app)\n\n@app.route('/send')\ndef send_email():\n    msg = Message(\"Hello\",\n                  sender=\"your_email@gmail.com\",\n                  recipients=[\"recipient@example.com\"])\n    msg.body = \"Hello Flask message sent from Flask-Mail\"\n    mail.send(msg)\n    return \"Sent\"\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-apscheduler_1","title":"Flask-APScheduler","text":""},{"location":"Cheat-Sheets/Flask/#installation_17","title":"Installation","text":"<pre><code>pip install flask-apscheduler\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_12","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_apscheduler import APScheduler\nimport time\n\nclass Config(object):\n    JOBS = [\n        {\n            'id': 'job1',\n            'func': 'yourapp:job1',\n            'trigger': 'interval',\n            'seconds': 10\n        }\n    ]\n    SCHEDULER_API_ENABLED = True\n\napp = Flask(__name__)\napp.config.from_object(Config())\n\nscheduler = APScheduler()\n# it is also possible to enable the API directly\n# scheduler.api_enabled = True\nscheduler.init_app(app)\nscheduler.start()\n\ndef job1():\n    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-sitemap_1","title":"Flask-Sitemap","text":""},{"location":"Cheat-Sheets/Flask/#installation_18","title":"Installation","text":"<pre><code>pip install Flask-Sitemap\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage_13","title":"Usage","text":"<pre><code>from flask import Flask\nfrom flask_sitemap import Sitemap\n\napp = Flask(__name__)\next = Sitemap(app=app)\n\n@app.route(\"/sitemap.xml\")\ndef sitemap():\n    return ext.generate(base_url='http://example.com')\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#flask-wtf-csrf-protection_1","title":"Flask-WTF CSRF Protection","text":""},{"location":"Cheat-Sheets/Flask/#configuration_5","title":"Configuration","text":"<pre><code>from flask import Flask\nfrom flask_wtf.csrf import CSRFProtect\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\ncsrf = CSRFProtect(app)\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#usage-in-templates_2","title":"Usage in Templates","text":"<pre><code>&lt;form method=\"post\"&gt;\n    {{ form.csrf_token }}\n    &lt;!-- Your form fields --&gt;\n&lt;/form&gt;\n</code></pre>"},{"location":"Cheat-Sheets/Flask/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Keep <code>SECRET_KEY</code> secure and out of your codebase. Use environment variables.</li> <li>Use meaningful names for routes, variables, and functions.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use a production-ready web server (e.g., Gunicorn, uWSGI) and a process manager (e.g., Supervisor, systemd) for deployment.</li> <li>Use a linter (like <code>flake8</code>) and formatter (like <code>black</code>) to ensure consistent code style.</li> <li>Keep your code modular and reusable.</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Follow Flask's coding style guidelines.</li> <li>Use Flask's built-in session management or a more robust solution like Flask-Session.</li> <li>Monitor your application for errors and performance issues.</li> <li>Use a CDN (Content Delivery Network) for static files.</li> <li>Optimize database queries.</li> <li>Use asynchronous tasks for long-running operations (e.g., sending emails) using Celery or similar.</li> <li>Implement proper logging and error handling.</li> <li>Regularly update Flask and its dependencies.</li> <li>Use a security scanner to identify potential vulnerabilities.</li> <li>Follow security best practices.</li> <li>Use a reverse proxy like Nginx or Apache in front of your WSGI server.</li> <li>Use a load balancer for high availability.</li> <li>Automate deployments using tools like Fabric or Ansible.</li> <li>Use a monitoring tool like Sentry or New Relic.</li> <li>Implement health checks for your application.</li> <li>Use a CDN for static assets.</li> <li>Cache frequently accessed data.</li> <li>Use a database connection pool.</li> <li>Optimize your database queries.</li> <li>Use a task queue for long-running tasks.</li> <li>Use a background worker for asynchronous tasks.</li> <li>Use a message queue for inter-process communication.</li> <li>Use a service discovery tool for microservices.</li> <li>Use a containerization tool like Docker.</li> <li>Use an orchestration tool like Kubernetes.</li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/","title":"Hypothesis Tests in Python","text":"<ul> <li>Hypothesis Tests in Python<ul> <li>Normality Tests<ul> <li>Shapiro-Wilk Test</li> <li>D\u2019Agostino\u2019s K^2 Test</li> <li>Anderson-Darling Test</li> </ul> </li> <li>Correlation Tests<ul> <li>Pearson\u2019s Correlation Coefficient</li> <li>Spearman\u2019s Rank Correlation</li> <li>Kendall\u2019s Rank Correlation</li> <li>Chi-Squared Test</li> </ul> </li> <li>Stationary Tests<ul> <li>Augmented Dickey-Fuller Unit Root Test</li> <li>Kwiatkowski-Phillips-Schmidt-Shin</li> </ul> </li> <li>Parametric Statistical Hypothesis Tests<ul> <li>Student\u2019s t-test</li> <li>Paired Student\u2019s t-test</li> <li>Analysis of Variance Test (ANOVA)</li> <li>Repeated Measures ANOVA Test</li> </ul> </li> <li>Nonparametric Statistical Hypothesis Tests<ul> <li>Mann-Whitney U Test</li> <li>Wilcoxon Signed-Rank Test</li> <li>Kruskal-Wallis H Test</li> <li>Friedman Test</li> </ul> </li> <li>Equality of variance test<ul> <li>Levene's test</li> </ul> </li> </ul> </li> </ul> <p>A\u00a0statistical hypothesis test\u00a0is a method of\u00a0statistical inference\u00a0used to decide whether the data at hand sufficiently support a particular hypothesis. Hypothesis testing allows us to make probabilistic statements about population parameters.</p> <p>Few Notes:</p> <ul> <li>When it comes to assumptions such as the expected distribution of data or sample size, the results of a given test are likely to degrade gracefully rather than become immediately unusable if an assumption is violated.</li> <li>Generally, data samples need to be representative of the domain and large enough to expose their distribution to analysis.</li> <li>In some cases, the data can be corrected to meet the assumptions, such as correcting a nearly normal distribution to be normal by removing outliers, or using a correction to the degrees of freedom in a statistical test when samples have differing variance, to name two examples.</li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#normality-tests","title":"Normality Tests","text":"<p>This section lists statistical tests that you can use to check if your data has a Gaussian distribution.</p> <p>Gaussian distribution (also known as normal distribution) is a bell-shaped curve.</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#shapiro-wilk-test","title":"Shapiro-Wilk Test","text":"<p>Tests whether a data sample has a Gaussian distribution/Normal distribution.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the sample has a Gaussian distribution.</li> <li>H1: the sample does not have a Gaussian distribution.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Shapiro-Wilk Normality Test\nfrom scipy.stats import shapiro\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = shapiro(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.shapiro</li> <li>Shapiro-Wilk test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#dagostinos-k2-test","title":"D\u2019Agostino\u2019s K^2 Test","text":"<p>Tests whether a data sample has a Gaussian distribution/Normal distribution.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the sample has a Gaussian distribution.</li> <li>H1: the sample does not have a Gaussian distribution.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the D'Agostino's K^2 Normality Test\nfrom scipy.stats import normaltest\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nstat, p = normaltest(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.normaltest</li> <li>D'Agostino's K-squared test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#anderson-darling-test","title":"Anderson-Darling Test","text":"<p>Tests whether a data sample has a Gaussian distribution/Normal distribution.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the sample has a Gaussian distribution.</li> <li>H1: the sample does not have a Gaussian distribution.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Anderson-Darling Normality Test\nfrom scipy.stats import anderson\ndata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\nresult = anderson(data)\nprint('stat=%.3f' % (result.statistic))\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.critical_values[i]\n    if result.statistic &lt; cv:\n        print('Probably Gaussian at the %.1f%% level' % (sl))\n    else:\n        print('Probably not Gaussian at the %.1f%% level' % (sl))\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.anderson</li> <li>Anderson-Darling test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#correlation-tests","title":"Correlation Tests","text":"<p>This section lists statistical tests that you can use to check if two samples are related.</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#pearsons-correlation-coefficient","title":"Pearson\u2019s Correlation Coefficient","text":"<p>Tests whether two samples have a linear relationship.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample are normally distributed.</li> <li>Observations in each sample have the same variance.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the two samples are independent.</li> <li>H1: there is a dependency between the samples.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Pearson's Correlation test\nfrom scipy.stats import pearsonr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = pearsonr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.pearsonr</li> <li>Pearson's correlation coefficient on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#spearmans-rank-correlation","title":"Spearman\u2019s Rank Correlation","text":"<p>Tests whether two samples have a monotonic relationship.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the two samples are independent.</li> <li>H1: there is a dependency between the samples.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Spearman's Rank Correlation Test\nfrom scipy.stats import spearmanr\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = spearmanr(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.spearmanr</li> <li>Spearman's rank correlation coefficient on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#kendalls-rank-correlation","title":"Kendall\u2019s Rank Correlation","text":"<p>Tests whether two samples have a monotonic relationship.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the two samples are independent.</li> <li>H1: there is a dependency between the samples.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Kendall's Rank Correlation Test\nfrom scipy.stats import kendalltau\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\nstat, p = kendalltau(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.kendalltau</li> <li>Kendall rank correlation coefficient on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#chi-squared-test","title":"Chi-Squared Test","text":"<p>Tests whether two categorical variables are related or independent.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations used in the calculation of the contingency table are independent.</li> <li>25 or more examples in each cell of the contingency table.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the two samples are independent.</li> <li>H1: there is a dependency between the samples.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Chi-Squared Test\nfrom scipy.stats import chi2_contingency\ntable = [[10, 20, 30],[6,  9,  17]]\nstat, p, dof, expected = chi2_contingency(table)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.chi2_contingency</li> <li>Chi-Squared test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#stationary-tests","title":"Stationary Tests","text":"<p>This section lists statistical tests that you can use to check if a time series is stationary or not.</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#augmented-dickey-fuller-unit-root-test","title":"Augmented Dickey-Fuller Unit Root Test","text":"<p>Tests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in are temporally ordered.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: a unit root is present (series is non-stationary).</li> <li>H1: a unit root is not present (series is stationary).</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Augmented Dickey-Fuller unit root test\nfrom statsmodels.tsa.stattools import adfuller\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, obs, crit, t = adfuller(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably not Stationary')\nelse:\n    print('Probably Stationary')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>statsmodels.tsa.stattools.adfuller API.</li> <li>Augmented Dickey--Fuller test, Wikipedia.</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#kwiatkowski-phillips-schmidt-shin","title":"Kwiatkowski-Phillips-Schmidt-Shin","text":"<p>Tests whether a time series is trend stationary or not.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in are temporally ordered.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the time series is trend-stationary.</li> <li>H1: the time series is not trend-stationary.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Kwiatkowski-Phillips-Schmidt-Shin test\nfrom statsmodels.tsa.stattools import kpss\ndata = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nstat, p, lags, crit = kpss(data)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably Stationary')\nelse:\n    print('Probably not Stationary')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>statsmodels.tsa.stattools.kpss API.</li> <li>KPSS test, Wikipedia.</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#parametric-statistical-hypothesis-tests","title":"Parametric Statistical Hypothesis Tests","text":"<p>This section lists statistical tests that you can use to compare data samples.</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#students-t-test","title":"Student\u2019s t-test","text":"<p>Tests whether the means of two independent samples are significantly different.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample are normally distributed.</li> <li>Observations in each sample have the same variance.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the means of the samples are equal.</li> <li>H1: the means of the samples are unequal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Student's t-test\nfrom scipy.stats import ttest_ind\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_ind(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.ttest_ind</li> <li>Student's t-test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#paired-students-t-test","title":"Paired Student\u2019s t-test","text":"<p>Tests whether the means of two independent samples are significantly different.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample are normally distributed.</li> <li>Observations in each sample have the same variance.</li> <li>Observations across each sample are paired.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the means of the samples are equal.</li> <li>H1: the means of the samples are unequal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Paired Student's t-test\nfrom scipy.stats import ttest_rel\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = ttest_rel(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.ttest_rel</li> <li>Student's t-test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#analysis-of-variance-test-anova","title":"Analysis of Variance Test (ANOVA)","text":"<p>Tests whether the means of two or more independent samples are significantly different.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample are normally distributed.</li> <li>Observations in each sample have the same variance.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the means of the samples are equal.</li> <li>H1: the means of the samples are unequal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Analysis of Variance Test\nfrom scipy.stats import f_oneway\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = f_oneway(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.f_oneway</li> <li>Analysis of variance on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#repeated-measures-anova-test","title":"Repeated Measures ANOVA Test","text":"<p>Tests whether the means of two or more paired samples are significantly different.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample are normally distributed.</li> <li>Observations in each sample have the same variance.</li> <li>Observations across each sample are paired.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the means of the samples are equal.</li> <li>H1: one or more of the means of the samples are unequal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Currently not supported in Python. :(\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>Analysis of variance on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#nonparametric-statistical-hypothesis-tests","title":"Nonparametric Statistical Hypothesis Tests","text":"<p>In Non-Parametric tests, we don't make any assumption about the parameters for the given population or the population we are studying. In fact, these tests don't depend on the population. Hence, there is no fixed set of parameters is available, and also there is no distribution (normal distribution, etc.)</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#mann-whitney-u-test","title":"Mann-Whitney U Test","text":"<p>Tests whether the distributions of two independent samples are equal or not.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the distributions of both samples are equal.</li> <li>H1: the distributions of both samples are not equal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Mann-Whitney U Test\nfrom scipy.stats import mannwhitneyu\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = mannwhitneyu(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.mannwhitneyu</li> <li>Mann-Whitney U test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#wilcoxon-signed-rank-test","title":"Wilcoxon Signed-Rank Test","text":"<p>Tests whether the distributions of two paired samples are equal or not.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> <li>Observations across each sample are paired.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the distributions of both samples are equal.</li> <li>H1: the distributions of both samples are not equal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Wilcoxon Signed-Rank Test\nfrom scipy.stats import wilcoxon\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = wilcoxon(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.wilcoxon</li> <li>Wilcoxon signed-rank test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#kruskal-wallis-h-test","title":"Kruskal-Wallis H Test","text":"<p>Tests whether the distributions of two or more independent samples are equal or not.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the distributions of all samples are equal.</li> <li>H1: the distributions of one or more samples are not equal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Kruskal-Wallis H Test\nfrom scipy.stats import kruskal\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\nstat, p = kruskal(data1, data2)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.kruskal</li> <li>Kruskal-Wallis one-way analysis of variance on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#friedman-test","title":"Friedman Test","text":"<p>Tests whether the distributions of two or more paired samples are equal or not.</p> <ul> <li> <p>Assumptions</p> <ul> <li>Observations in each sample are independent and identically distributed (iid).</li> <li>Observations in each sample can be ranked.</li> <li>Observations across each sample are paired.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: the distributions of all samples are equal.</li> <li>H1: the distributions of one or more samples are not equal.</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Friedman Test\nfrom scipy.stats import friedmanchisquare\ndata1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\ndata2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\ndata3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\nstat, p = friedmanchisquare(data1, data2, data3)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same distribution')\nelse:\n    print('Probably different distributions')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.friedmanchisquare</li> <li>Friedman test on Wikipedia</li> </ul> </li> </ul>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#equality-of-variance-test","title":"Equality of variance test","text":"<p>Test is used to assess the equality of variance between two different samples.</p>"},{"location":"Cheat-Sheets/Hypothesis-Tests/#levenes-test","title":"Levene's test","text":"<p>Levene\u2019s test is used to assess the equality of variance between two or more different samples.</p> <ul> <li> <p>Assumptions</p> <ul> <li>The samples from the populations under consideration are independent.</li> <li>The populations under consideration are approximately normally distributed.</li> </ul> </li> <li> <p>Interpretation</p> <ul> <li>H0: All the samples variances are equal</li> <li>H1: At least one variance is different from the rest</li> </ul> </li> <li> <p>Python Code</p> <pre><code># Example of the Levene's test\nfrom scipy.stats import levene\na = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\nb = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\nc = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\nstat, p = levene(a, b, c)\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p &gt; 0.05:\n    print('Probably the same variances')\nelse:\n    print('Probably at least one variance is different from the rest')\n</code></pre> </li> <li> <p>Sources</p> <ul> <li>scipy.stats.levene</li> <li>Levene's test on Wikipedia</li> </ul> </li> </ul> <p>Source: https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/</p>"},{"location":"Cheat-Sheets/Keras/","title":"Keras Cheat Sheet","text":"<ul> <li>Keras Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Importing Keras</li> </ul> </li> <li>Model Building<ul> <li>Sequential Model</li> <li>Functional API</li> <li>Model Subclassing</li> </ul> </li> <li>Layers<ul> <li>Core Layers</li> <li>Convolutional Layers</li> <li>Pooling Layers</li> <li>Recurrent Layers</li> <li>Normalization Layers</li> <li>Advanced Activation Layers</li> <li>Embedding Layers</li> <li>Merge Layers</li> <li>Writing Custom Layers</li> </ul> </li> <li>Activation Functions</li> <li>Loss Functions<ul> <li>Regression Losses</li> <li>Classification Losses</li> <li>Custom Loss Functions</li> </ul> </li> <li>Optimizers<ul> <li>Optimizer Configuration</li> </ul> </li> <li>Metrics<ul> <li>Custom Metrics</li> </ul> </li> <li>Model Compilation</li> <li>Training<ul> <li>Training with NumPy Arrays</li> <li>Training with tf.data.Dataset</li> <li>Validation</li> <li>Callbacks</li> </ul> </li> <li>Evaluation</li> <li>Prediction</li> <li>Saving and Loading Models<ul> <li>Save the Entire Model</li> <li>Load the Entire Model</li> <li>Save Model Architecture as JSON</li> <li>Load Model Architecture from JSON</li> <li>Save Model Weights</li> <li>Load Model Weights</li> </ul> </li> <li>Regularization<ul> <li>L1 and L2 Regularization</li> <li>Dropout</li> <li>Batch Normalization</li> </ul> </li> <li>Transfer Learning<ul> <li>Feature Extraction</li> <li>Fine-Tuning</li> </ul> </li> <li>Callbacks<ul> <li>ModelCheckpoint</li> <li>EarlyStopping</li> <li>ReduceLROnPlateau</li> <li>TensorBoard</li> </ul> </li> <li>Custom Training Loops</li> <li>Distributed Training<ul> <li>MirroredStrategy</li> </ul> </li> <li>Hyperparameter Tuning<ul> <li>Using Keras Tuner</li> </ul> </li> <li>TensorFlow Datasets<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>TensorFlow Hub<ul> <li>Installation</li> <li>Usage</li> </ul> </li> <li>TensorFlow Lite<ul> <li>Convert to TensorFlow Lite</li> </ul> </li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the Keras deep learning library, covering essential concepts, code snippets, and best practices for efficient model building, training, and evaluation. It aims to be a one-stop reference for common tasks.</p>"},{"location":"Cheat-Sheets/Keras/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/Keras/#installation","title":"Installation","text":"<pre><code>pip install tensorflow  # Installs TensorFlow with Keras\n# or\npip install keras # Installs Keras with a backend (TensorFlow, Theano, or CNTK)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#importing-keras","title":"Importing Keras","text":"<pre><code>import tensorflow as tf  # If using TensorFlow backend\nfrom tensorflow import keras\n# or\nimport keras  # If using standalone Keras\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#model-building","title":"Model Building","text":""},{"location":"Cheat-Sheets/Keras/#sequential-model","title":"Sequential Model","text":"<pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,)),\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#functional-api","title":"Functional API","text":"<pre><code>from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\ninputs = Input(shape=(784,))\nx = Dense(128, activation='relu')(inputs)\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#model-subclassing","title":"Model Subclassing","text":"<pre><code>import tensorflow as tf\n\nclass MyModel(tf.keras.Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#layers","title":"Layers","text":""},{"location":"Cheat-Sheets/Keras/#core-layers","title":"Core Layers","text":"<ul> <li><code>Dense</code>: Fully connected layer.</li> <li><code>Activation</code>: Applies an activation function.</li> <li><code>Dropout</code>: Applies dropout regularization.</li> <li><code>Flatten</code>: Flattens the input.</li> <li><code>Input</code>: Creates an input tensor.</li> <li><code>Reshape</code>: Reshapes the input.</li> <li><code>Embedding</code>: Turns positive integers (indexes) into dense vectors of fixed size.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#convolutional-layers","title":"Convolutional Layers","text":"<ul> <li><code>Conv1D</code>: 1D convolution layer.</li> <li><code>Conv2D</code>: 2D convolution layer.</li> <li><code>Conv3D</code>: 3D convolution layer.</li> <li><code>SeparableConv2D</code>: Depthwise separable 2D convolution layer.</li> <li><code>DepthwiseConv2D</code>: Depthwise 2D convolution layer.</li> <li><code>Conv2DTranspose</code>: Transposed convolution layer (deconvolution).</li> </ul>"},{"location":"Cheat-Sheets/Keras/#pooling-layers","title":"Pooling Layers","text":"<ul> <li><code>MaxPooling1D</code>, <code>MaxPooling2D</code>, <code>MaxPooling3D</code>: Max pooling layers.</li> <li><code>AveragePooling1D</code>, <code>AveragePooling2D</code>, <code>AveragePooling3D</code>: Average pooling layers.</li> <li><code>GlobalMaxPooling1D</code>, <code>GlobalMaxPooling2D</code>, <code>GlobalMaxPooling3D</code>: Global max pooling layers.</li> <li><code>GlobalAveragePooling1D</code>, <code>GlobalAveragePooling2D</code>, <code>GlobalAveragePooling3D</code>: Global average pooling layers.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#recurrent-layers","title":"Recurrent Layers","text":"<ul> <li><code>LSTM</code>: Long Short-Term Memory layer.</li> <li><code>GRU</code>: Gated Recurrent Unit layer.</li> <li><code>SimpleRNN</code>: Fully-connected RNN where the output is to be fed back to input.</li> <li><code>Bidirectional</code>: Wraps another recurrent layer to run it in both directions.</li> <li><code>ConvLSTM2D</code>: ConvLSTM2D layer.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#normalization-layers","title":"Normalization Layers","text":"<ul> <li><code>BatchNormalization</code>: Applies batch normalization.</li> <li><code>LayerNormalization</code>: Applies layer normalization.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#advanced-activation-layers","title":"Advanced Activation Layers","text":"<ul> <li><code>LeakyReLU</code>: Leaky version of a Rectified Linear Unit.</li> <li><code>PReLU</code>: Parametric Rectified Linear Unit.</li> <li><code>ELU</code>: Exponential Linear Unit.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#embedding-layers","title":"Embedding Layers","text":"<ul> <li><code>Embedding</code>: Turns positive integers (indexes) into dense vectors of fixed size.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#merge-layers","title":"Merge Layers","text":"<ul> <li><code>Add</code>: Adds inputs.</li> <li><code>Multiply</code>: Multiplies inputs.</li> <li><code>Average</code>: Averages inputs.</li> <li><code>Maximum</code>: Takes the maximum of inputs.</li> <li><code>Concatenate</code>: Concatenates inputs.</li> <li><code>Dot</code>: Performs a dot product between inputs.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#writing-custom-layers","title":"Writing Custom Layers","text":"<pre><code>import tensorflow as tf\n\nclass MyCustomLayer(tf.keras.layers.Layer):\n    def __init__(self, units=32):\n        super(MyCustomLayer, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n        self.b = self.add_weight(shape=(self.units,),\n                                 initializer='zeros',\n                                 trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#activation-functions","title":"Activation Functions","text":"<ul> <li><code>relu</code>: Rectified Linear Unit.</li> <li><code>sigmoid</code>: Sigmoid function.</li> <li><code>tanh</code>: Hyperbolic tangent function.</li> <li><code>softmax</code>: Softmax function (for multi-class classification).</li> <li><code>elu</code>: Exponential Linear Unit.</li> <li><code>selu</code>: Scaled Exponential Linear Unit.</li> <li><code>linear</code>: Linear (identity) activation.</li> <li><code>LeakyReLU</code>: Leaky Rectified Linear Unit.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#loss-functions","title":"Loss Functions","text":""},{"location":"Cheat-Sheets/Keras/#regression-losses","title":"Regression Losses","text":"<ul> <li><code>MeanSquaredError</code>: Mean squared error.</li> <li><code>MeanAbsoluteError</code>: Mean absolute error.</li> <li><code>MeanAbsolutePercentageError</code>: Mean absolute percentage error.</li> <li><code>MeanSquaredLogarithmicError</code>: Mean squared logarithmic error.</li> <li><code>Huber</code>: Huber loss.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#classification-losses","title":"Classification Losses","text":"<ul> <li><code>BinaryCrossentropy</code>: Binary cross-entropy (for binary classification).</li> <li><code>CategoricalCrossentropy</code>: Categorical cross-entropy (for multi-class classification with one-hot encoded labels).</li> <li><code>SparseCategoricalCrossentropy</code>: Sparse categorical cross-entropy (for multi-class classification with integer labels).</li> <li><code>Hinge</code>: Hinge loss (for \"maximum-margin\" classification).</li> <li><code>KLDivergence</code>: Kullback-Leibler Divergence loss.</li> <li><code>Poisson</code>: Poisson loss.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>import tensorflow as tf\n\ndef my_custom_loss(y_true, y_pred):\n    squared_difference = tf.square(y_true - y_pred)\n    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#optimizers","title":"Optimizers","text":"<ul> <li><code>SGD</code>: Stochastic Gradient Descent.</li> <li><code>Adam</code>: Adaptive Moment Estimation.</li> <li><code>RMSprop</code>: Root Mean Square Propagation.</li> <li><code>Adagrad</code>: Adaptive Gradient Algorithm.</li> <li><code>Adadelta</code>: Adaptive Delta.</li> <li><code>Adamax</code>: Adamax optimizer from Adam and max operators.</li> <li><code>Nadam</code>: Nesterov Adam optimizer.</li> <li><code>Ftrl</code>: Follow The Regularized Leader optimizer.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#optimizer-configuration","title":"Optimizer Configuration","text":"<pre><code>from tensorflow.keras.optimizers import Adam\n\noptimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#metrics","title":"Metrics","text":"<ul> <li><code>Accuracy</code>: Accuracy.</li> <li><code>BinaryAccuracy</code>: Binary accuracy.</li> <li><code>CategoricalAccuracy</code>: Categorical accuracy.</li> <li><code>SparseCategoricalAccuracy</code>: Sparse categorical accuracy.</li> <li><code>TopKCategoricalAccuracy</code>: Computes how often targets are in the top K predictions.</li> <li><code>MeanAbsoluteError</code>: Mean absolute error.</li> <li><code>MeanSquaredError</code>: Mean squared error.</li> <li><code>Precision</code>: Precision.</li> <li><code>Recall</code>: Recall.</li> <li><code>AUC</code>: Area Under the Curve.</li> <li><code>F1Score</code>: F1 Score.</li> </ul>"},{"location":"Cheat-Sheets/Keras/#custom-metrics","title":"Custom Metrics","text":"<pre><code>import tensorflow as tf\n\nclass MyCustomMetric(tf.keras.metrics.Metric):\n    def __init__(self, name='my_custom_metric', **kwargs):\n        super(MyCustomMetric, self).__init__(name=name, **kwargs)\n        self.sum = self.add_weight(name='sum', initializer='zeros')\n        self.count = self.add_weight(name='count', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        values = tf.abs(y_true - y_pred)\n        if sample_weight is not None:\n            sample_weight = tf.cast(sample_weight, self.dtype)\n            values = tf.multiply(values, sample_weight)\n        self.sum.assign_add(tf.reduce_sum(values))\n        self.count.assign_add(tf.cast(tf.size(y_true), self.dtype))\n\n    def result(self):\n        return self.sum / self.count\n\n    def reset_state(self):\n        self.sum.assign(0.0)\n        self.count.assign(0.0)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#model-compilation","title":"Model Compilation","text":"<pre><code>model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#training","title":"Training","text":""},{"location":"Cheat-Sheets/Keras/#training-with-numpy-arrays","title":"Training with NumPy Arrays","text":"<pre><code>import numpy as np\n\ndata = np.random.random((1000, 784))\nlabels = np.random.randint(10, size=(1000,))\none_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=10)\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#training-with-tfdatadataset","title":"Training with tf.data.Dataset","text":"<pre><code>import tensorflow as tf\n\ndataset = tf.data.Dataset.from_tensor_slices((data, one_hot_labels))\ndataset = dataset.batch(32)\n\nmodel.fit(dataset, epochs=10)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#validation","title":"Validation","text":"<pre><code>val_data = np.random.random((100, 784))\nval_labels = np.random.randint(10, size=(100,))\none_hot_val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=10)\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32,\n          validation_data=(val_data, one_hot_val_labels))\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#callbacks","title":"Callbacks","text":"<ul> <li><code>ModelCheckpoint</code>: Saves the model at certain intervals.</li> <li><code>EarlyStopping</code>: Stops training when a monitored metric has stopped improving.</li> <li><code>TensorBoard</code>: Enables visualization of metrics and more.</li> <li><code>ReduceLROnPlateau</code>: Reduces the learning rate when a metric has stopped improving.</li> <li><code>CSVLogger</code>: Streams epoch results to a CSV file.</li> </ul> <pre><code>from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n\ncheckpoint_callback = ModelCheckpoint(filepath='./checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5',\n                                     save_best_only=True,\n                                     monitor='val_loss',\n                                     verbose=1)\n\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n\ntensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32,\n          validation_data=(val_data, one_hot_val_labels),\n          callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#evaluation","title":"Evaluation","text":"<pre><code>loss, accuracy = model.evaluate(val_data, one_hot_val_labels)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#prediction","title":"Prediction","text":"<pre><code>predictions = model.predict(val_data)\npredicted_classes = np.argmax(predictions, axis=1)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#saving-and-loading-models","title":"Saving and Loading Models","text":""},{"location":"Cheat-Sheets/Keras/#save-the-entire-model","title":"Save the Entire Model","text":"<pre><code>model.save('my_model.h5')  # Saves the model architecture, weights, and optimizer state\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#load-the-entire-model","title":"Load the Entire Model","text":"<pre><code>from tensorflow.keras.models import load_model\n\nloaded_model = load_model('my_model.h5')\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#save-model-architecture-as-json","title":"Save Model Architecture as JSON","text":"<pre><code>json_string = model.to_json()\n# Save the JSON string to a file\nwith open('model_architecture.json', 'w') as f:\n    f.write(json_string)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#load-model-architecture-from-json","title":"Load Model Architecture from JSON","text":"<pre><code>from tensorflow.keras.models import model_from_json\n\n# Load the JSON string from a file\nwith open('model_architecture.json', 'r') as f:\n    json_string = f.read()\n\nmodel = model_from_json(json_string)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#save-model-weights","title":"Save Model Weights","text":"<pre><code>model.save_weights('model_weights.h5')\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#load-model-weights","title":"Load Model Weights","text":"<pre><code>model.load_weights('model_weights.h5')\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#regularization","title":"Regularization","text":""},{"location":"Cheat-Sheets/Keras/#l1-and-l2-regularization","title":"L1 and L2 Regularization","text":"<pre><code>from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,),\n          kernel_regularizer=regularizers.l1(0.01),  # L1 regularization\n          bias_regularizer=regularizers.l2(0.01)),    # L2 regularization\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#dropout","title":"Dropout","text":"<pre><code>from tensorflow.keras.layers import Dropout\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,)),\n    Dropout(0.5),  # Dropout layer with 50% dropout rate\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#batch-normalization","title":"Batch Normalization","text":"<pre><code>from tensorflow.keras.layers import BatchNormalization\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,)),\n    BatchNormalization(),  # Batch normalization layer\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#transfer-learning","title":"Transfer Learning","text":""},{"location":"Cheat-Sheets/Keras/#feature-extraction","title":"Feature Extraction","text":"<pre><code>from tensorflow.keras.applications import VGG16\n\n# Load pre-trained VGG16 model without the top (classification) layer\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the weights of the base model\nbase_model.trainable = False\n\n# Add custom classification layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\nmodel = Sequential([\n    base_model,\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#fine-tuning","title":"Fine-Tuning","text":"<pre><code># Unfreeze some of the layers in the base model\nbase_model.trainable = True\nfor layer in base_model.layers[:-4]:  # Unfreeze the last 4 layers\n    layer.trainable = False\n\n# Recompile the model\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Continue training\nmodel.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels))\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#callbacks_1","title":"Callbacks","text":""},{"location":"Cheat-Sheets/Keras/#modelcheckpoint","title":"ModelCheckpoint","text":"<pre><code>from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\n    filepath='best_model.h5',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#earlystopping","title":"EarlyStopping","text":"<pre><code>from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#reducelronplateau","title":"ReduceLROnPlateau","text":"<pre><code>from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr_callback = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.1,\n    patience=3,\n    verbose=1\n)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#tensorboard","title":"TensorBoard","text":"<pre><code>from tensorflow.keras.callbacks import TensorBoard\n\ntensorboard_callback = TensorBoard(\n    log_dir='./logs',\n    histogram_freq=1,\n    write_graph=True,\n    write_images=True\n)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#custom-training-loops","title":"Custom Training Loops","text":"<pre><code>import tensorflow as tf\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\nmetric_fn = tf.keras.metrics.CategoricalAccuracy()\n\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_fn(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    metric_fn.update_state(labels, predictions)\n    return loss\n\nepochs = 10\nfor epoch in range(epochs):\n    for images, labels in dataset:\n        loss = train_step(images, labels)\n    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}, Accuracy: {metric_fn.result().numpy():.4f}\")\n    metric_fn.reset_state()\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#distributed-training","title":"Distributed Training","text":""},{"location":"Cheat-Sheets/Keras/#mirroredstrategy","title":"MirroredStrategy","text":"<pre><code>import tensorflow as tf\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(784,)),\n        Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"Cheat-Sheets/Keras/#using-keras-tuner","title":"Using Keras Tuner","text":"<p>Installation:</p> <pre><code>pip install keras-tuner\n</code></pre> <p>Define a Hypermodel:</p> <pre><code>from tensorflow import keras\nfrom kerastuner.tuners import RandomSearch\n\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    model.add(keras.layers.Dense(\n        hp.Choice('units', [32, 64, 128]),\n        activation='relu'))\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n</code></pre> <p>Run the Tuner:</p> <pre><code>tuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='my_dir',\n    project_name='my_project')\n\ntuner.search_space_summary()\n\ntuner.search(x_train, y_train,\n             epochs=10,\n             validation_data=(x_val, y_val))\n\nbest_model = tuner.get_best_models(num_models=1)[0]\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#tensorflow-datasets","title":"TensorFlow Datasets","text":""},{"location":"Cheat-Sheets/Keras/#installation_1","title":"Installation","text":"<pre><code>pip install tensorflow-datasets\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#usage","title":"Usage","text":"<pre><code>import tensorflow_datasets as tfds\n\n(ds_train, ds_test), ds_info = tfds.load(\n    'mnist',\n    split=['train', 'test'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\n\ndef normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -&gt; `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label\n\nds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\nds_train = ds_train.batch(128)\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n\nds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_test = ds_test.batch(128)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n\nmodel.fit(ds_train, epochs=12, validation_data=ds_test)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#tensorflow-hub","title":"TensorFlow Hub","text":""},{"location":"Cheat-Sheets/Keras/#installation_2","title":"Installation","text":"<pre><code>pip install tensorflow-hub\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#usage_1","title":"Usage","text":"<pre><code>import tensorflow_hub as hub\n\nembedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#tensorflow-lite","title":"TensorFlow Lite","text":""},{"location":"Cheat-Sheets/Keras/#convert-to-tensorflow-lite","title":"Convert to TensorFlow Lite","text":"<pre><code>converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)\n</code></pre>"},{"location":"Cheat-Sheets/Keras/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Use meaningful names for layers, models, and variables.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use a consistent coding style.</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Use a GPU for training if possible.</li> <li>Monitor your training progress with TensorBoard.</li> <li>Use callbacks to save the best model and stop training early.</li> <li>Use regularization techniques to prevent overfitting.</li> <li>Experiment with different optimizers and learning rates.</li> <li>Use data augmentation to improve model performance.</li> <li>Use transfer learning to leverage pre-trained models.</li> <li>Use a TPU for faster training.</li> <li>Use a distributed training strategy for large datasets.</li> <li>Use a profiler to identify performance bottlenecks.</li> <li>Use a model quantization technique to reduce model size.</li> <li>Use a model pruning technique to reduce model complexity.</li> <li>Use a model distillation technique to create a smaller model.</li> <li>Use a model compression technique to reduce model size.</li> <li>Use a model deployment tool to deploy your model to production.</li> <li>Use a model monitoring tool to monitor your model's performance in production.</li> </ul>"},{"location":"Cheat-Sheets/NumPy/","title":"NumPy","text":"\ud83d\udcca Click to view NumPy Mindmaps \ud83d\udccb Click to view NumPy Cheat Sheets In\u00a0[1]: Copied! <pre>import numpy as np  # Importing NumPy\nnp.__version__  # Check version of NumPy\n</pre> import numpy as np  # Importing NumPy np.__version__  # Check version of NumPy Out[1]: <pre>'1.26.4'</pre> In\u00a0[2]: Copied! <pre>arr1 = np.array([1, 2, 3])\narr1\n</pre> arr1 = np.array([1, 2, 3]) arr1 Out[2]: <pre>array([1, 2, 3])</pre> In\u00a0[3]: Copied! <pre>arr2 = np.array((1, 2, 3))\narr2\n</pre> arr2 = np.array((1, 2, 3)) arr2 Out[3]: <pre>array([1, 2, 3])</pre> In\u00a0[4]: Copied! <pre>arr3 = np.frombuffer(b'Hello World', dtype='S1')\narr3\n</pre> arr3 = np.frombuffer(b'Hello World', dtype='S1') arr3 Out[4]: <pre>array([b'H', b'e', b'l', b'l', b'o', b' ', b'W', b'o', b'r', b'l', b'd'],\n      dtype='|S1')</pre> In\u00a0[5]: Copied! <pre>arr_zeros = np.zeros((2, 3))  # 2x3 array of zeros\narr_zeros\n</pre> arr_zeros = np.zeros((2, 3))  # 2x3 array of zeros arr_zeros Out[5]: <pre>array([[0., 0., 0.],\n       [0., 0., 0.]])</pre> In\u00a0[6]: Copied! <pre>arr_ones = np.ones((2, 3))  # 2x3 array of ones\narr_ones\n</pre> arr_ones = np.ones((2, 3))  # 2x3 array of ones arr_ones Out[6]: <pre>array([[1., 1., 1.],\n       [1., 1., 1.]])</pre> In\u00a0[7]: Copied! <pre>arr_empty = np.empty((2, 3))  # 2x3 empty array\narr_empty\n</pre> arr_empty = np.empty((2, 3))  # 2x3 empty array arr_empty Out[7]: <pre>array([[1., 1., 1.],\n       [1., 1., 1.]])</pre> In\u00a0[8]: Copied! <pre>arr_range = np.arange(0, 10, 2)  # Array from 0 to 9 with step 2\narr_range\n</pre> arr_range = np.arange(0, 10, 2)  # Array from 0 to 9 with step 2 arr_range Out[8]: <pre>array([0, 2, 4, 6, 8])</pre> In\u00a0[9]: Copied! <pre>arr_linspace = np.linspace(0, 1, 5)  # 5 equally spaced numbers from 0 to 1\narr_linspace\n</pre> arr_linspace = np.linspace(0, 1, 5)  # 5 equally spaced numbers from 0 to 1 arr_linspace Out[9]: <pre>array([0.  , 0.25, 0.5 , 0.75, 1.  ])</pre> In\u00a0[10]: Copied! <pre>arr_random = np.random.rand(2, 3)  # 2x3 array with random numbers between 0 and 1\narr_random\n</pre> arr_random = np.random.rand(2, 3)  # 2x3 array with random numbers between 0 and 1 arr_random Out[10]: <pre>array([[0.78160058, 0.52687888, 0.29604995],\n       [0.63947724, 0.99231115, 0.3488577 ]])</pre> In\u00a0[11]: Copied! <pre>arr_identity = np.eye(3)  # 3x3 identity matrix\narr_identity\n</pre> arr_identity = np.eye(3)  # 3x3 identity matrix arr_identity Out[11]: <pre>array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])</pre> In\u00a0[12]: Copied! <pre>arr_diag = np.diag([1, 2, 3])  # Diagonal matrix from a list\narr_diag\n</pre> arr_diag = np.diag([1, 2, 3])  # Diagonal matrix from a list arr_diag Out[12]: <pre>array([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])</pre> In\u00a0[13]: Copied! <pre>dt = np.dtype([('age', np.int32), ('name', np.str_, 10)])\narr_structured = np.array([(21, 'Alice'), (25, 'Bob')], dtype=dt)\narr_structured\n</pre> dt = np.dtype([('age', np.int32), ('name', np.str_, 10)]) arr_structured = np.array([(21, 'Alice'), (25, 'Bob')], dtype=dt) arr_structured Out[13]: <pre>array([(21, 'Alice'), (25, 'Bob')],\n      dtype=[('age', '&lt;i4'), ('name', '&lt;U10')])</pre> In\u00a0[14]: Copied! <pre>arr_full = np.full((2, 3), 7)  # Create a 2x3 array filled with the value 7\narr_full\n</pre> arr_full = np.full((2, 3), 7)  # Create a 2x3 array filled with the value 7 arr_full Out[14]: <pre>array([[7, 7, 7],\n       [7, 7, 7]])</pre> In\u00a0[15]: Copied! <pre>arr_tile = np.tile([1, 2], (2, 3))  # Repeat [1, 2] in a 2x3 grid\narr_tile\n</pre> arr_tile = np.tile([1, 2], (2, 3))  # Repeat [1, 2] in a 2x3 grid arr_tile Out[15]: <pre>array([[1, 2, 1, 2, 1, 2],\n       [1, 2, 1, 2, 1, 2]])</pre> In\u00a0[16]: Copied! <pre>arr1.shape  # Dimensions of the array\n</pre> arr1.shape  # Dimensions of the array Out[16]: <pre>(3,)</pre> In\u00a0[17]: Copied! <pre>arr1.size  # Total number of elements\n</pre> arr1.size  # Total number of elements Out[17]: <pre>3</pre> In\u00a0[18]: Copied! <pre>arr1.ndim  # Number of dimensions\n</pre> arr1.ndim  # Number of dimensions Out[18]: <pre>1</pre> In\u00a0[19]: Copied! <pre>arr1.dtype  # Data type of elements\n</pre> arr1.dtype  # Data type of elements Out[19]: <pre>dtype('int64')</pre> In\u00a0[20]: Copied! <pre>arr1_float = arr1.astype(float)  # Convert to another type\narr1_float\n</pre> arr1_float = arr1.astype(float)  # Convert to another type arr1_float Out[20]: <pre>array([1., 2., 3.])</pre> In\u00a0[21]: Copied! <pre>arr1.itemsize  # Size of one element in bytes\n</pre> arr1.itemsize  # Size of one element in bytes Out[21]: <pre>8</pre> In\u00a0[22]: Copied! <pre>arr1.nbytes  # Total memory used by array\n</pre> arr1.nbytes  # Total memory used by array Out[22]: <pre>24</pre> In\u00a0[23]: Copied! <pre>arr1.flags  # Memory layout information\n</pre> arr1.flags  # Memory layout information Out[23]: <pre>  C_CONTIGUOUS : True\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False</pre> In\u00a0[24]: Copied! <pre>arr_nan_inf = np.array([1, 2, np.nan, np.inf])\nnp.isnan(arr_nan_inf), np.isinf(arr_nan_inf), np.isfinite(arr_nan_inf)\n</pre> arr_nan_inf = np.array([1, 2, np.nan, np.inf]) np.isnan(arr_nan_inf), np.isinf(arr_nan_inf), np.isfinite(arr_nan_inf) Out[24]: <pre>(array([False, False,  True, False]),\n array([False, False, False,  True]),\n array([ True,  True, False, False]))</pre> In\u00a0[25]: Copied! <pre>arr_add = arr1 + 1  # Add 1 to each element\narr_add\n</pre> arr_add = arr1 + 1  # Add 1 to each element arr_add Out[25]: <pre>array([2, 3, 4])</pre> In\u00a0[26]: Copied! <pre>arr_mul = arr1 * 2  # Multiply each element by 2\narr_mul\n</pre> arr_mul = arr1 * 2  # Multiply each element by 2 arr_mul Out[26]: <pre>array([2, 4, 6])</pre> In\u00a0[27]: Copied! <pre>arr_sum = np.add(arr1, arr2)  # Add arrays element-wise\narr_sum\n</pre> arr_sum = np.add(arr1, arr2)  # Add arrays element-wise arr_sum Out[27]: <pre>array([2, 4, 6])</pre> In\u00a0[28]: Copied! <pre>arr_diff = np.subtract(arr1, arr2)  # Subtract arrays element-wise\narr_diff\n</pre> arr_diff = np.subtract(arr1, arr2)  # Subtract arrays element-wise arr_diff Out[28]: <pre>array([0, 0, 0])</pre> In\u00a0[29]: Copied! <pre>arr_sum_total = np.sum(arr1)  # Sum of all elements\narr_sum_total\n</pre> arr_sum_total = np.sum(arr1)  # Sum of all elements arr_sum_total Out[29]: <pre>6</pre> In\u00a0[30]: Copied! <pre>arr_mean = np.mean(arr1)  # Mean of elements\narr_mean\n</pre> arr_mean = np.mean(arr1)  # Mean of elements arr_mean Out[30]: <pre>2.0</pre> In\u00a0[31]: Copied! <pre>arr_max = np.max(arr1)  # Maximum value\narr_max\n</pre> arr_max = np.max(arr1)  # Maximum value arr_max Out[31]: <pre>3</pre> In\u00a0[32]: Copied! <pre>arr_min = np.min(arr1)  # Minimum value\narr_min\n</pre> arr_min = np.min(arr1)  # Minimum value arr_min Out[32]: <pre>1</pre> In\u00a0[33]: Copied! <pre>arr_prod = np.prod(arr1)  # Product of elements\narr_prod\n</pre> arr_prod = np.prod(arr1)  # Product of elements arr_prod Out[33]: <pre>6</pre> In\u00a0[34]: Copied! <pre>arr_cumsum = np.cumsum(arr1)  # Cumulative sum of elements\narr_cumsum\n</pre> arr_cumsum = np.cumsum(arr1)  # Cumulative sum of elements arr_cumsum Out[34]: <pre>array([1, 3, 6])</pre> In\u00a0[35]: Copied! <pre>arr_cumprod = np.cumprod(arr1)  # Cumulative product of elements\narr_cumprod\n</pre> arr_cumprod = np.cumprod(arr1)  # Cumulative product of elements arr_cumprod Out[35]: <pre>array([1, 2, 6])</pre> In\u00a0[36]: Copied! <pre>arr_exp = np.exp(arr1)  # Exponential of each element\narr_exp\n</pre> arr_exp = np.exp(arr1)  # Exponential of each element arr_exp Out[36]: <pre>array([ 2.71828183,  7.3890561 , 20.08553692])</pre> In\u00a0[37]: Copied! <pre>arr_log = np.log(arr1)  # Natural logarithm\narr_log\n</pre> arr_log = np.log(arr1)  # Natural logarithm arr_log Out[37]: <pre>array([0.        , 0.69314718, 1.09861229])</pre> In\u00a0[38]: Copied! <pre>arr_log10 = np.log10(arr1)  # Base-10 logarithm\narr_log10\n</pre> arr_log10 = np.log10(arr1)  # Base-10 logarithm arr_log10 Out[38]: <pre>array([0.        , 0.30103   , 0.47712125])</pre> In\u00a0[39]: Copied! <pre>arr_expm1 = np.expm1(arr1)  # Compute exp(x) - 1\narr_expm1\n</pre> arr_expm1 = np.expm1(arr1)  # Compute exp(x) - 1 arr_expm1 Out[39]: <pre>array([ 1.71828183,  6.3890561 , 19.08553692])</pre> In\u00a0[40]: Copied! <pre>arr_sin = np.sin(arr1)  # Sine of each element\narr_sin\n</pre> arr_sin = np.sin(arr1)  # Sine of each element arr_sin Out[40]: <pre>array([0.84147098, 0.90929743, 0.14112001])</pre> In\u00a0[41]: Copied! <pre>arr_cos = np.cos(arr1)  # Cosine of each element\narr_cos\n</pre> arr_cos = np.cos(arr1)  # Cosine of each element arr_cos Out[41]: <pre>array([ 0.54030231, -0.41614684, -0.9899925 ])</pre> In\u00a0[42]: Copied! <pre>arr_tan = np.tan(arr1)  # Tangent of each element\narr_tan\n</pre> arr_tan = np.tan(arr1)  # Tangent of each element arr_tan Out[42]: <pre>array([ 1.55740772, -2.18503986, -0.14254654])</pre> In\u00a0[43]: Copied! <pre>arr_arcsin = np.arcsin(arr1 / 10)  # Inverse sine\narr_arcsin\n</pre> arr_arcsin = np.arcsin(arr1 / 10)  # Inverse sine arr_arcsin Out[43]: <pre>array([0.10016742, 0.20135792, 0.30469265])</pre> In\u00a0[44]: Copied! <pre>arr_arccos = np.arccos(arr1 / 10)  # Inverse cosine\narr_arccos\n</pre> arr_arccos = np.arccos(arr1 / 10)  # Inverse cosine arr_arccos Out[44]: <pre>array([1.47062891, 1.36943841, 1.26610367])</pre> In\u00a0[45]: Copied! <pre>arr_arctan = np.arctan(arr1 / 10)  # Inverse tangent\narr_arctan\n</pre> arr_arctan = np.arctan(arr1 / 10)  # Inverse tangent arr_arctan Out[45]: <pre>array([0.09966865, 0.19739556, 0.29145679])</pre> In\u00a0[46]: Copied! <pre>arr_round = np.round(arr1_float, decimals=2)  # Round to 2 decimal places\narr_round\n</pre> arr_round = np.round(arr1_float, decimals=2)  # Round to 2 decimal places arr_round Out[46]: <pre>array([1., 2., 3.])</pre> In\u00a0[47]: Copied! <pre>arr_floor = np.floor(arr1_float)  # Floor operation\narr_floor\n</pre> arr_floor = np.floor(arr1_float)  # Floor operation arr_floor Out[47]: <pre>array([1., 2., 3.])</pre> In\u00a0[48]: Copied! <pre>arr_ceil = np.ceil(arr1_float)  # Ceiling operation\narr_ceil\n</pre> arr_ceil = np.ceil(arr1_float)  # Ceiling operation arr_ceil Out[48]: <pre>array([1., 2., 3.])</pre> In\u00a0[49]: Copied! <pre>arr_trunc = np.trunc(arr1_float)  # Truncate elements to integers\narr_trunc\n</pre> arr_trunc = np.trunc(arr1_float)  # Truncate elements to integers arr_trunc Out[49]: <pre>array([1., 2., 3.])</pre> In\u00a0[50]: Copied! <pre>arr_reshaped = arr1.reshape((3, 1))  # Reshape to 3x1 array\narr_reshaped\n</pre> arr_reshaped = arr1.reshape((3, 1))  # Reshape to 3x1 array arr_reshaped Out[50]: <pre>array([[1],\n       [2],\n       [3]])</pre> In\u00a0[51]: Copied! <pre>arr_flattened = arr1.flatten()  # Flatten the array to 1D\narr_flattened\n</pre> arr_flattened = arr1.flatten()  # Flatten the array to 1D arr_flattened Out[51]: <pre>array([1, 2, 3])</pre> In\u00a0[52]: Copied! <pre>arr_raveled = np.ravel(arr1)  # Return a flattened array\narr_raveled\n</pre> arr_raveled = np.ravel(arr1)  # Return a flattened array arr_raveled Out[52]: <pre>array([1, 2, 3])</pre> In\u00a0[53]: Copied! <pre>arr_T = arr1.reshape((1, 3)).T  # Transpose of the array\narr_T\n</pre> arr_T = arr1.reshape((1, 3)).T  # Transpose of the array arr_T Out[53]: <pre>array([[1],\n       [2],\n       [3]])</pre> In\u00a0[54]: Copied! <pre>arr_custom_T = np.transpose(arr1.reshape((3, 1)), (1, 0))  # Custom transpose\narr_custom_T\n</pre> arr_custom_T = np.transpose(arr1.reshape((3, 1)), (1, 0))  # Custom transpose arr_custom_T Out[54]: <pre>array([[1, 2, 3]])</pre> In\u00a0[55]: Copied! <pre>arr_concat = np.concatenate((arr1, arr2))  # Join arrays\narr_concat\n</pre> arr_concat = np.concatenate((arr1, arr2))  # Join arrays arr_concat Out[55]: <pre>array([1, 2, 3, 1, 2, 3])</pre> In\u00a0[56]: Copied! <pre>arr_hstack = np.hstack((arr1.reshape((3, 1)), arr2.reshape((3, 1))))  # Horizontal stack\narr_hstack\n</pre> arr_hstack = np.hstack((arr1.reshape((3, 1)), arr2.reshape((3, 1))))  # Horizontal stack arr_hstack Out[56]: <pre>array([[1, 1],\n       [2, 2],\n       [3, 3]])</pre> In\u00a0[57]: Copied! <pre>arr_vstack = np.vstack((arr1, arr2))  # Vertical stack\narr_vstack\n</pre> arr_vstack = np.vstack((arr1, arr2))  # Vertical stack arr_vstack Out[57]: <pre>array([[1, 2, 3],\n       [1, 2, 3]])</pre> In\u00a0[58]: Copied! <pre>arr_split = np.split(arr_concat, 3)  # Split into 3 equal parts\narr_split\n</pre> arr_split = np.split(arr_concat, 3)  # Split into 3 equal parts arr_split Out[58]: <pre>[array([1, 2]), array([3, 1]), array([2, 3])]</pre> In\u00a0[59]: Copied! <pre>arr_hsplit = np.hsplit(arr_hstack, 2)  # Split horizontally\narr_hsplit\n</pre> arr_hsplit = np.hsplit(arr_hstack, 2)  # Split horizontally arr_hsplit Out[59]: <pre>[array([[1],\n        [2],\n        [3]]),\n array([[1],\n        [2],\n        [3]])]</pre> In\u00a0[60]: Copied! <pre>arr_vsplit = np.vsplit(arr_vstack, 2)  # Split vertically\narr_vsplit\n</pre> arr_vsplit = np.vsplit(arr_vstack, 2)  # Split vertically arr_vsplit Out[60]: <pre>[array([[1, 2, 3]]), array([[1, 2, 3]])]</pre> In\u00a0[61]: Copied! <pre>arr_expanded = np.expand_dims(arr1, axis=0)  # Expand dimensions\narr_expanded\n</pre> arr_expanded = np.expand_dims(arr1, axis=0)  # Expand dimensions arr_expanded Out[61]: <pre>array([[1, 2, 3]])</pre> In\u00a0[62]: Copied! <pre>arr_squeezed = np.squeeze(arr_expanded)  # Remove single-dimensional entries\narr_squeezed\n</pre> arr_squeezed = np.squeeze(arr_expanded)  # Remove single-dimensional entries arr_squeezed Out[62]: <pre>array([1, 2, 3])</pre> In\u00a0[63]: Copied! <pre>arr_tiled = np.tile(arr1, (2, 3))  # Repeat array\narr_tiled\n</pre> arr_tiled = np.tile(arr1, (2, 3))  # Repeat array arr_tiled Out[63]: <pre>array([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n       [1, 2, 3, 1, 2, 3, 1, 2, 3]])</pre> In\u00a0[64]: Copied! <pre>arr_repeated = np.repeat(arr1, 3)  # Repeat elements of an array\narr_repeated\n</pre> arr_repeated = np.repeat(arr1, 3)  # Repeat elements of an array arr_repeated Out[64]: <pre>array([1, 1, 1, 2, 2, 2, 3, 3, 3])</pre> In\u00a0[65]: Copied! <pre>arr_rot90 = np.rot90(arr1.reshape((3, 1)))  # Rotate array by 90 degrees\narr_rot90\n</pre> arr_rot90 = np.rot90(arr1.reshape((3, 1)))  # Rotate array by 90 degrees arr_rot90 Out[65]: <pre>array([[1, 2, 3]])</pre> In\u00a0[66]: Copied! <pre>arr_fliplr = np.fliplr(arr1.reshape((3, 1)))  # Flip array left to right\narr_fliplr\n</pre> arr_fliplr = np.fliplr(arr1.reshape((3, 1)))  # Flip array left to right arr_fliplr Out[66]: <pre>array([[1],\n       [2],\n       [3]])</pre> In\u00a0[67]: Copied! <pre>arr_flipud = np.flipud(arr1.reshape((3, 1)))  # Flip array upside down\narr_flipud\n</pre> arr_flipud = np.flipud(arr1.reshape((3, 1)))  # Flip array upside down arr_flipud Out[67]: <pre>array([[3],\n       [2],\n       [1]])</pre> In\u00a0[68]: Copied! <pre>arr_dot = np.dot(arr1, arr2)  # Dot product\narr_dot\n</pre> arr_dot = np.dot(arr1, arr2)  # Dot product arr_dot Out[68]: <pre>14</pre> In\u00a0[69]: Copied! <pre>arr_matmul = np.matmul(arr1.reshape((3, 1)), arr2.reshape((1, 3)))  # Matrix multiplication\narr_matmul\n</pre> arr_matmul = np.matmul(arr1.reshape((3, 1)), arr2.reshape((1, 3)))  # Matrix multiplication arr_matmul Out[69]: <pre>array([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])</pre> In\u00a0[70]: Copied! <pre>arr_matmul_op = arr1.reshape((3, 1)) @ arr2.reshape((1, 3))  # Matrix multiplication using @\narr_matmul_op\n</pre> arr_matmul_op = arr1.reshape((3, 1)) @ arr2.reshape((1, 3))  # Matrix multiplication using @ arr_matmul_op Out[70]: <pre>array([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])</pre> In\u00a0[71]: Copied! <pre>A = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])\nx = np.linalg.solve(A, b)  # Solve linear equations Ax = b\nx\n</pre> A = np.array([[3, 1], [1, 2]]) b = np.array([9, 8]) x = np.linalg.solve(A, b)  # Solve linear equations Ax = b x Out[71]: <pre>array([2., 3.])</pre> In\u00a0[72]: Copied! <pre>arr_eigvals, arr_eigvecs = np.linalg.eig(A)  # Eigenvalues and eigenvectors\narr_eigvals, arr_eigvecs\n</pre> arr_eigvals, arr_eigvecs = np.linalg.eig(A)  # Eigenvalues and eigenvectors arr_eigvals, arr_eigvecs Out[72]: <pre>(array([3.61803399, 1.38196601]),\n array([[ 0.85065081, -0.52573111],\n        [ 0.52573111,  0.85065081]]))</pre> In\u00a0[73]: Copied! <pre>arr_inv = np.linalg.inv(A)  # Inverse of a matrix\narr_inv\n</pre> arr_inv = np.linalg.inv(A)  # Inverse of a matrix arr_inv Out[73]: <pre>array([[ 0.4, -0.2],\n       [-0.2,  0.6]])</pre> In\u00a0[74]: Copied! <pre>arr_det = np.linalg.det(A)  # Determinant of a matrix\narr_det\n</pre> arr_det = np.linalg.det(A)  # Determinant of a matrix arr_det Out[74]: <pre>5.000000000000001</pre> In\u00a0[75]: Copied! <pre>U, S, V = np.linalg.svd(A)  # Singular Value Decomposition\nU, S, V\n</pre> U, S, V = np.linalg.svd(A)  # Singular Value Decomposition U, S, V Out[75]: <pre>(array([[-0.85065081, -0.52573111],\n        [-0.52573111,  0.85065081]]),\n array([3.61803399, 1.38196601]),\n array([[-0.85065081, -0.52573111],\n        [-0.52573111,  0.85065081]]))</pre> In\u00a0[76]: Copied! <pre>arr_norm = np.linalg.norm(arr1)  # Compute matrix or vector norm\narr_norm\n</pre> arr_norm = np.linalg.norm(arr1)  # Compute matrix or vector norm arr_norm Out[76]: <pre>3.7416573867739413</pre> In\u00a0[77]: Copied! <pre>arr_cond = np.linalg.cond(A)  # Compute the condition number of a matrix\narr_cond\n</pre> arr_cond = np.linalg.cond(A)  # Compute the condition number of a matrix arr_cond Out[77]: <pre>2.618033988749896</pre> In\u00a0[78]: Copied! <pre>arr_mean = np.mean(arr1)  # Mean\narr_mean\n</pre> arr_mean = np.mean(arr1)  # Mean arr_mean Out[78]: <pre>2.0</pre> In\u00a0[79]: Copied! <pre>arr_median = np.median(arr1)  # Median\narr_median\n</pre> arr_median = np.median(arr1)  # Median arr_median Out[79]: <pre>2.0</pre> In\u00a0[80]: Copied! <pre>arr_var = np.var(arr1)  # Variance\narr_var\n</pre> arr_var = np.var(arr1)  # Variance arr_var Out[80]: <pre>0.6666666666666666</pre> In\u00a0[81]: Copied! <pre>arr_std = np.std(arr1)  # Standard deviation\narr_std\n</pre> arr_std = np.std(arr1)  # Standard deviation arr_std Out[81]: <pre>0.816496580927726</pre> In\u00a0[82]: Copied! <pre>arr_percentile = np.percentile(arr1, 50)  # 50th percentile (median)\narr_percentile\n</pre> arr_percentile = np.percentile(arr1, 50)  # 50th percentile (median) arr_percentile Out[82]: <pre>2.0</pre> In\u00a0[83]: Copied! <pre>arr_corr = np.corrcoef(arr1, arr2)  # Correlation coefficient\narr_corr\n</pre> arr_corr = np.corrcoef(arr1, arr2)  # Correlation coefficient arr_corr Out[83]: <pre>array([[1., 1.],\n       [1., 1.]])</pre> In\u00a0[84]: Copied! <pre>arr_cov = np.cov(arr1, arr2)  # Covariance\narr_cov\n</pre> arr_cov = np.cov(arr1, arr2)  # Covariance arr_cov Out[84]: <pre>array([[1., 1.],\n       [1., 1.]])</pre> In\u00a0[85]: Copied! <pre>arr_hist, arr_bins = np.histogram(arr1, bins=3)  # Histogram of an array\narr_hist, arr_bins\n</pre> arr_hist, arr_bins = np.histogram(arr1, bins=3)  # Histogram of an array arr_hist, arr_bins Out[85]: <pre>(array([1, 1, 1]), array([1.        , 1.66666667, 2.33333333, 3.        ]))</pre> In\u00a0[86]: Copied! <pre>from scipy import stats\narr_binned_statistic = stats.binned_statistic(arr1, arr1, statistic='mean', bins=3)  # Compute binned statistics\narr_binned_statistic.statistic\n</pre> from scipy import stats arr_binned_statistic = stats.binned_statistic(arr1, arr1, statistic='mean', bins=3)  # Compute binned statistics arr_binned_statistic.statistic Out[86]: <pre>array([1., 2., 3.])</pre> In\u00a0[87]: Copied! <pre>arr_broadcast_add = arr1 + 5  # Add 5 to all elements\narr_broadcast_add\n</pre> arr_broadcast_add = arr1 + 5  # Add 5 to all elements arr_broadcast_add Out[87]: <pre>array([6, 7, 8])</pre> In\u00a0[88]: Copied! <pre>arr_broadcast_array = arr1 + np.array([1, 2, 3])  # Add array [1, 2, 3] to each row\narr_broadcast_array\n</pre> arr_broadcast_array = arr1 + np.array([1, 2, 3])  # Add array [1, 2, 3] to each row arr_broadcast_array Out[88]: <pre>array([2, 4, 6])</pre> In\u00a0[89]: Copied! <pre>arr_broadcast_mult = arr1 * np.array([1, 2, 3])  # Element-wise multiplication with broadcasting\narr_broadcast_mult\n</pre> arr_broadcast_mult = arr1 * np.array([1, 2, 3])  # Element-wise multiplication with broadcasting arr_broadcast_mult Out[89]: <pre>array([1, 4, 9])</pre> In\u00a0[90]: Copied! <pre>arr_broadcast_expand = np.expand_dims(arr1, axis=0) + arr1  # Broadcasting with dimension expansion\narr_broadcast_expand\n</pre> arr_broadcast_expand = np.expand_dims(arr1, axis=0) + arr1  # Broadcasting with dimension expansion arr_broadcast_expand Out[90]: <pre>array([[2, 4, 6]])</pre> In\u00a0[91]: Copied! <pre>first_element = arr1[0]  # First element\nfirst_element\n</pre> first_element = arr1[0]  # First element first_element Out[91]: <pre>1</pre> In\u00a0[92]: Copied! <pre>last_element = arr1[-1]  # Last element\nlast_element\n</pre> last_element = arr1[-1]  # Last element last_element Out[92]: <pre>3</pre> In\u00a0[93]: Copied! <pre>element_0_2 = arr1[0]  # First element\nthird_element = arr1[2]  # Third element\nfirst_element, third_element\n</pre> element_0_2 = arr1[0]  # First element third_element = arr1[2]  # Third element first_element, third_element Out[93]: <pre>(1, 3)</pre> In\u00a0[94]: Copied! <pre>arr_slice_1_3 = arr1[1:3]  # Elements from index 1 to 2\narr_slice_1_3\n</pre> arr_slice_1_3 = arr1[1:3]  # Elements from index 1 to 2 arr_slice_1_3 Out[94]: <pre>array([2, 3])</pre> In\u00a0[95]: Copied! <pre>arr_slice_all = arr1[:]  # All elements\narr_slice_all\n</pre> arr_slice_all = arr1[:]  # All elements arr_slice_all Out[95]: <pre>array([1, 2, 3])</pre> In\u00a0[96]: Copied! <pre>arr_slice_skip = arr1[::2]  # Every other element\narr_slice_skip\n</pre> arr_slice_skip = arr1[::2]  # Every other element arr_slice_skip Out[96]: <pre>array([1, 3])</pre> In\u00a0[97]: Copied! <pre>arr_fancy_index = arr1[[0, 2]]  # Elements 0 and 2\narr_fancy_index\n</pre> arr_fancy_index = arr1[[0, 2]]  # Elements 0 and 2 arr_fancy_index Out[97]: <pre>array([1, 3])</pre> In\u00a0[98]: Copied! <pre>arr_bool_mask = arr1[arr1 &gt; 2]  # Elements greater than 2\narr_bool_mask\n</pre> arr_bool_mask = arr1[arr1 &gt; 2]  # Elements greater than 2 arr_bool_mask Out[98]: <pre>array([3])</pre> In\u00a0[99]: Copied! <pre>arr_where = np.where(arr1 &gt; 2, arr1, -arr1)  # Replace negative values with their absolute value\narr_where\n</pre> arr_where = np.where(arr1 &gt; 2, arr1, -arr1)  # Replace negative values with their absolute value arr_where Out[99]: <pre>array([-1, -2,  3])</pre> In\u00a0[100]: Copied! <pre>arr_set_values = arr1.copy()\narr_set_values[arr_set_values &gt; 2] = 0  # Set all positive elements to 0\narr_set_values\n</pre> arr_set_values = arr1.copy() arr_set_values[arr_set_values &gt; 2] = 0  # Set all positive elements to 0 arr_set_values Out[100]: <pre>array([1, 2, 0])</pre> In\u00a0[101]: Copied! <pre>arr_ix = np.ix_([0, 1], [2, 3])  # Create a mesh grid from indexing arrays\narr_ix\n</pre> arr_ix = np.ix_([0, 1], [2, 3])  # Create a mesh grid from indexing arrays arr_ix Out[101]: <pre>(array([[0],\n        [1]]),\n array([[2, 3]]))</pre> In\u00a0[102]: Copied! <pre>arr_rand = np.random.rand(2, 3)  # Uniform distribution (0, 1)\narr_rand\n</pre> arr_rand = np.random.rand(2, 3)  # Uniform distribution (0, 1) arr_rand Out[102]: <pre>array([[0.67485015, 0.42229856, 0.98348739],\n       [0.01204425, 0.90966669, 0.70587384]])</pre> In\u00a0[103]: Copied! <pre>arr_randn = np.random.randn(2, 3)  # Standard normal distribution\narr_randn\n</pre> arr_randn = np.random.randn(2, 3)  # Standard normal distribution arr_randn Out[103]: <pre>array([[ 0.74664931, -0.14473226,  0.11518257],\n       [-1.03882137,  1.94984805,  1.95339008]])</pre> In\u00a0[104]: Copied! <pre>arr_randint = np.random.randint(0, 10, size=(2, 3))  # Random integers between 0 and 9\narr_randint\n</pre> arr_randint = np.random.randint(0, 10, size=(2, 3))  # Random integers between 0 and 9 arr_randint Out[104]: <pre>array([[4, 7, 8],\n       [9, 8, 8]])</pre> In\u00a0[105]: Copied! <pre>arr_perm = np.random.permutation(arr1)  # Randomly permute an array\narr_perm\n</pre> arr_perm = np.random.permutation(arr1)  # Randomly permute an array arr_perm Out[105]: <pre>array([2, 3, 1])</pre> In\u00a0[106]: Copied! <pre>arr_choice = np.random.choice(arr1, size=3, replace=False)  # Random sample without replacement\narr_choice\n</pre> arr_choice = np.random.choice(arr1, size=3, replace=False)  # Random sample without replacement arr_choice Out[106]: <pre>array([2, 3, 1])</pre> In\u00a0[107]: Copied! <pre>arr_binomial = np.random.binomial(n=10, p=0.5, size=10)  # Binomial distribution\narr_binomial\n</pre> arr_binomial = np.random.binomial(n=10, p=0.5, size=10)  # Binomial distribution arr_binomial Out[107]: <pre>array([4, 4, 1, 5, 6, 7, 3, 7, 6, 7])</pre> In\u00a0[108]: Copied! <pre>arr_poisson = np.random.poisson(lam=3, size=10)  # Poisson distribution\narr_poisson\n</pre> arr_poisson = np.random.poisson(lam=3, size=10)  # Poisson distribution arr_poisson Out[108]: <pre>array([2, 6, 2, 3, 1, 1, 2, 3, 2, 5])</pre> In\u00a0[109]: Copied! <pre>np.random.seed(42)  # Set random seed for reproducibility\narr_rand_seed = np.random.rand(2, 3)\narr_rand_seed\n</pre> np.random.seed(42)  # Set random seed for reproducibility arr_rand_seed = np.random.rand(2, 3) arr_rand_seed Out[109]: <pre>array([[0.37454012, 0.95071431, 0.73199394],\n       [0.59865848, 0.15601864, 0.15599452]])</pre> In\u00a0[110]: Copied! <pre>np.save('array.npy', arr1)  # Save array to binary file\narr_loaded = np.load('array.npy')  # Load array from binary file\narr_loaded\n</pre> np.save('array.npy', arr1)  # Save array to binary file arr_loaded = np.load('array.npy')  # Load array from binary file arr_loaded Out[110]: <pre>array([1, 2, 3])</pre> In\u00a0[111]: Copied! <pre>np.savetxt('array.txt', arr1)  # Save array to text file\narr_loaded_txt = np.loadtxt('array.txt')  # Load array from text file\narr_loaded_txt\n</pre> np.savetxt('array.txt', arr1)  # Save array to text file arr_loaded_txt = np.loadtxt('array.txt')  # Load array from text file arr_loaded_txt Out[111]: <pre>array([1., 2., 3.])</pre> In\u00a0[112]: Copied! <pre>np.savez('arrays.npz', arr1=arr1, arr2=arr2)  # Save multiple arrays to a compressed file\nnpzfile = np.load('arrays.npz')\nnpzfile['arr1'], npzfile['arr2']\n</pre> np.savez('arrays.npz', arr1=arr1, arr2=arr2)  # Save multiple arrays to a compressed file npzfile = np.load('arrays.npz') npzfile['arr1'], npzfile['arr2'] Out[112]: <pre>(array([1, 2, 3]), array([1, 2, 3]))</pre> In\u00a0[113]: Copied! <pre>arr1\n</pre> arr1 Out[113]: <pre>array([1, 2, 3])</pre> In\u00a0[114]: Copied! <pre>np.savetxt('data.csv', arr1, delimiter=',')  # Save data to CSV file\n</pre> np.savetxt('data.csv', arr1, delimiter=',')  # Save data to CSV file In\u00a0[115]: Copied! <pre>arr_csv = np.genfromtxt('data.csv', delimiter=',')  # Load data from CSV file\narr_csv\n</pre> arr_csv = np.genfromtxt('data.csv', delimiter=',')  # Load data from CSV file arr_csv Out[115]: <pre>array([1., 2., 3.])</pre> In\u00a0[116]: Copied! <pre>p = np.poly1d([1, 2, 3])  # Define a polynomial p(x) = 1x^2 + 2x + 3\np(2)  # Evaluate polynomial at x = 2\n</pre> p = np.poly1d([1, 2, 3])  # Define a polynomial p(x) = 1x^2 + 2x + 3 p(2)  # Evaluate polynomial at x = 2 Out[116]: <pre>11</pre> In\u00a0[117]: Copied! <pre>p.roots  # Find roots of the polynomial\n</pre> p.roots  # Find roots of the polynomial Out[117]: <pre>array([-1.+1.41421356j, -1.-1.41421356j])</pre> In\u00a0[118]: Copied! <pre>x = np.array([1, 2, 3, 4])\ny = np.array([1, 4, 9, 16])\np_fit = np.polyfit(x, y, deg=2)  # Fit a polynomial of degree 2 to data points (x, y)\np_fit\n</pre> x = np.array([1, 2, 3, 4]) y = np.array([1, 4, 9, 16]) p_fit = np.polyfit(x, y, deg=2)  # Fit a polynomial of degree 2 to data points (x, y) p_fit Out[118]: <pre>array([ 1.00000000e+00, -6.00566855e-15,  9.41435428e-15])</pre> In\u00a0[119]: Copied! <pre>p_deriv = p.deriv()  # Derivative of the polynomial\np_deriv\n</pre> p_deriv = p.deriv()  # Derivative of the polynomial p_deriv Out[119]: <pre>poly1d([2, 2])</pre> In\u00a0[120]: Copied! <pre>p_integ = p.integ()  # Integral of the polynomial\np_integ\n</pre> p_integ = p.integ()  # Integral of the polynomial p_integ Out[120]: <pre>poly1d([0.33333333, 1.        , 3.        , 0.        ])</pre> In\u00a0[121]: Copied! <pre>def add_five(x):\n    return x + 5\n\nvectorized_func = np.vectorize(add_five)  # Apply a function element-wise to an array\nvectorized_func(arr1)\n</pre> def add_five(x):     return x + 5  vectorized_func = np.vectorize(add_five)  # Apply a function element-wise to an array vectorized_func(arr1) Out[121]: <pre>array([6, 7, 8])</pre> In\u00a0[122]: Copied! <pre>x = np.array([1, 2, 3])\ny = np.array([4, 5, 6])\nX, Y = np.meshgrid(x, y)  # Create a coordinate grid from 1D arrays x and y\nX, Y\n</pre> x = np.array([1, 2, 3]) y = np.array([4, 5, 6]) X, Y = np.meshgrid(x, y)  # Create a coordinate grid from 1D arrays x and y X, Y Out[122]: <pre>(array([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]]),\n array([[4, 4, 4],\n        [5, 5, 5],\n        [6, 6, 6]]))</pre> In\u00a0[123]: Copied! <pre>arr_add_at = np.array([1, 2, 3])\nnp.add.at(arr_add_at, [0, 1], 5)  # Increment elements at indices `idx` by 5\narr_add_at\n</pre> arr_add_at = np.array([1, 2, 3]) np.add.at(arr_add_at, [0, 1], 5)  # Increment elements at indices `idx` by 5 arr_add_at Out[123]: <pre>array([6, 7, 3])</pre> In\u00a0[124]: Copied! <pre>arr_sorted = np.sort(arr1)  # Sort array\narr_sorted\n</pre> arr_sorted = np.sort(arr1)  # Sort array arr_sorted Out[124]: <pre>array([1, 2, 3])</pre> In\u00a0[125]: Copied! <pre>arr_argsort = np.argsort(arr1)  # Indices of the sorted array\narr_argsort\n</pre> arr_argsort = np.argsort(arr1)  # Indices of the sorted array arr_argsort Out[125]: <pre>array([0, 1, 2])</pre> In\u00a0[126]: Copied! <pre>arr_where_condition = np.where(arr1 &gt; 2)  # Indices where the condition is met\narr_where_condition\n</pre> arr_where_condition = np.where(arr1 &gt; 2)  # Indices where the condition is met arr_where_condition Out[126]: <pre>(array([2]),)</pre> In\u00a0[127]: Copied! <pre>arr_count_nonzero = np.count_nonzero(arr1)  # Count non-zero elements\narr_count_nonzero\n</pre> arr_count_nonzero = np.count_nonzero(arr1)  # Count non-zero elements arr_count_nonzero Out[127]: <pre>3</pre> In\u00a0[128]: Copied! <pre>arr_flags = arr1.flags  # Check memory layout (C_CONTIGUOUS, F_CONTIGUOUS)\narr_flags\n</pre> arr_flags = arr1.flags  # Check memory layout (C_CONTIGUOUS, F_CONTIGUOUS) arr_flags Out[128]: <pre>  C_CONTIGUOUS : True\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False</pre> In\u00a0[129]: Copied! <pre>arr_contig = np.ascontiguousarray(arr1)  # Convert to C-contiguous array\narr_contig.flags\n</pre> arr_contig = np.ascontiguousarray(arr1)  # Convert to C-contiguous array arr_contig.flags Out[129]: <pre>  C_CONTIGUOUS : True\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False</pre> In\u00a0[130]: Copied! <pre>memmap_arr = np.memmap('data.dat', dtype='float32', mode='w+', shape=(3, 3))  # Memory-mapped file\nmemmap_arr\n</pre> memmap_arr = np.memmap('data.dat', dtype='float32', mode='w+', shape=(3, 3))  # Memory-mapped file memmap_arr Out[130]: <pre>memmap([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], dtype=float32)</pre> In\u00a0[131]: Copied! <pre>arr_copy = arr1.copy()  # Create a deep copy of the array\narr_copy\n</pre> arr_copy = arr1.copy()  # Create a deep copy of the array arr_copy Out[131]: <pre>array([1, 2, 3])</pre> In\u00a0[132]: Copied! <pre>arr_view = arr1.view()  # Create a view of the array (shallow copy)\narr_view\n</pre> arr_view = arr1.view()  # Create a view of the array (shallow copy) arr_view Out[132]: <pre>array([1, 2, 3])</pre> In\u00a0[133]: Copied! <pre>arr_take = np.take(arr1, [0, 2])  # Take elements at indices 0 and 2\narr_take\n</pre> arr_take = np.take(arr1, [0, 2])  # Take elements at indices 0 and 2 arr_take Out[133]: <pre>array([1, 3])</pre> In\u00a0[134]: Copied! <pre>arr_put = arr1.copy()\nnp.put(arr_put, [0, 2], [-1, -2])  # Set elements at indices 0 and 2\narr_put\n</pre> arr_put = arr1.copy() np.put(arr_put, [0, 2], [-1, -2])  # Set elements at indices 0 and 2 arr_put Out[134]: <pre>array([-1,  2, -2])</pre> In\u00a0[135]: Copied! <pre>arr_choose = np.choose([0, 1], arr1)  # Construct an array from elements chosen from `arr1`\narr_choose\n</pre> arr_choose = np.choose([0, 1], arr1)  # Construct an array from elements chosen from `arr1` arr_choose Out[135]: <pre>array([1, 2])</pre> In\u00a0[136]: Copied! <pre>arr_lexsort = np.lexsort((arr2, arr1))  # Sort by `arr1`, then by `arr2`\narr_lexsort\n</pre> arr_lexsort = np.lexsort((arr2, arr1))  # Sort by `arr1`, then by `arr2` arr_lexsort Out[136]: <pre>array([0, 1, 2])</pre> In\u00a0[137]: Copied! <pre>arr_determinant = np.linalg.det(A)  # Determinant of a matrix\narr_determinant\n</pre> arr_determinant = np.linalg.det(A)  # Determinant of a matrix arr_determinant Out[137]: <pre>5.000000000000001</pre> In\u00a0[138]: Copied! <pre>arr_rank = np.linalg.matrix_rank(A)  # Rank of a matrix\narr_rank\n</pre> arr_rank = np.linalg.matrix_rank(A)  # Rank of a matrix arr_rank Out[138]: <pre>2</pre> In\u00a0[139]: Copied! <pre>arr_trace = np.trace(A)  # Sum of diagonal elements (trace)\narr_trace\n</pre> arr_trace = np.trace(A)  # Sum of diagonal elements (trace) arr_trace Out[139]: <pre>5</pre> In\u00a0[140]: Copied! <pre>arr_kron = np.kron(arr1, arr2)  # Kronecker product of two arrays\narr_kron\n</pre> arr_kron = np.kron(arr1, arr2)  # Kronecker product of two arrays arr_kron Out[140]: <pre>array([1, 2, 3, 2, 4, 6, 3, 6, 9])</pre> In\u00a0[141]: Copied! <pre>arr_outer = np.outer(arr1, arr2)  # Outer product of two arrays\narr_outer\n</pre> arr_outer = np.outer(arr1, arr2)  # Outer product of two arrays arr_outer Out[141]: <pre>array([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])</pre> In\u00a0[142]: Copied! <pre>arr_solve = np.linalg.solve(A, b)  # Solve Ax = b for x\narr_solve\n</pre> arr_solve = np.linalg.solve(A, b)  # Solve Ax = b for x arr_solve Out[142]: <pre>array([2., 3.])</pre> In\u00a0[143]: Copied! <pre>arr_lstsq = np.linalg.lstsq(A, b, rcond=None)  # Solve Ax = b using least squares\narr_lstsq[0]\n</pre> arr_lstsq = np.linalg.lstsq(A, b, rcond=None)  # Solve Ax = b using least squares arr_lstsq[0] Out[143]: <pre>array([2., 3.])</pre> In\u00a0[144]: Copied! <pre>arr_dtype = np.array([1, 2, 3], dtype=np.float32)  # Specify data type\narr_dtype\n</pre> arr_dtype = np.array([1, 2, 3], dtype=np.float32)  # Specify data type arr_dtype Out[144]: <pre>array([1., 2., 3.], dtype=float32)</pre> In\u00a0[145]: Copied! <pre>arr_converted_dtype = arr1.astype(np.int32)  # Convert array to specified data type\narr_converted_dtype\n</pre> arr_converted_dtype = arr1.astype(np.int32)  # Convert array to specified data type arr_converted_dtype Out[145]: <pre>array([1, 2, 3], dtype=int32)</pre> In\u00a0[146]: Copied! <pre>arr_complex = np.array([1+2j, 3+4j], dtype=np.complex64)  # Complex data type\narr_complex\n</pre> arr_complex = np.array([1+2j, 3+4j], dtype=np.complex64)  # Complex data type arr_complex Out[146]: <pre>array([1.+2.j, 3.+4.j], dtype=complex64)</pre> In\u00a0[147]: Copied! <pre>arr_dtype_check = arr_complex.dtype  # Check data type\narr_dtype_check\n</pre> arr_dtype_check = arr_complex.dtype  # Check data type arr_dtype_check Out[147]: <pre>dtype('complex64')</pre> In\u00a0[148]: Copied! <pre>np.issubdtype(arr_complex.dtype, np.number)  # Check if the data type is a subtype of `np.number`\n</pre> np.issubdtype(arr_complex.dtype, np.number)  # Check if the data type is a subtype of `np.number` Out[148]: <pre>True</pre>"},{"location":"Cheat-Sheets/NumPy/#numpy","title":"NumPy\u00b6","text":"<p>NumPy, which stands for Numerical Python, is a free, open-source Python library for working with arrays. It's one of the most popular packages for scientific computing in Python, and is used for data manipulation and analysis, including data cleaning, transformation, and aggregation.</p> <ul> <li>Official Website: https://numpy.org/</li> <li>Installation:  (https://numpy.org/install/)<pre>pip install numpy\n</pre> </li> <li>Documentation: https://numpy.org/doc</li> <li>GitHub: https://github.com/numpy/numpy</li> </ul>"},{"location":"Cheat-Sheets/NumPy/#basics","title":"Basics\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#array-creation","title":"Array Creation\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#from-lists-tuples-and-buffers","title":"From Lists, Tuples, and Buffers\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#zeros-ones-and-empty-arrays","title":"Zeros, Ones, and Empty Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#ranges-and-random-numbers","title":"Ranges and Random Numbers\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#identity-and-diagonal-matrices","title":"Identity and Diagonal Matrices\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#structured-arrays","title":"Structured Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#using-npfull-and-nptile","title":"Using <code>np.full</code> and <code>np.tile</code>\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#array-inspection","title":"Array Inspection\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#shape-and-size","title":"Shape and Size\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#data-type","title":"Data Type\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#memory-layout","title":"Memory Layout\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#checking-for-nan-and-inf-values","title":"Checking for <code>NaN</code> and <code>Inf</code> Values\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#array-mathematics","title":"Array Mathematics\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#basic-operations","title":"Basic Operations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#aggregate-functions","title":"Aggregate Functions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#exponentials-and-logarithms","title":"Exponentials and Logarithms\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#trigonometric-functions","title":"Trigonometric Functions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#rounding-and-precision-control","title":"Rounding and Precision Control\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#array-manipulation","title":"Array Manipulation\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#reshaping","title":"Reshaping\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#transposing","title":"Transposing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#joining-and-splitting-arrays","title":"Joining and Splitting Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#changing-dimensions","title":"Changing Dimensions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#array-repetition","title":"Array Repetition\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#rotating-and-flipping-arrays","title":"Rotating and Flipping Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#linear-algebra","title":"Linear Algebra\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#dot-product-and-matrix-multiplication","title":"Dot Product and Matrix Multiplication\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#solving-linear-equations","title":"Solving Linear Equations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#inverse-and-determinant","title":"Inverse and Determinant\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#norms-and-condition-numbers","title":"Norms and Condition Numbers\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#statistics","title":"Statistics\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#descriptive-statistics","title":"Descriptive Statistics\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#percentiles","title":"Percentiles\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#correlation-and-covariance","title":"Correlation and Covariance\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#histogram","title":"Histogram\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#binned-statistics","title":"Binned Statistics\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#basic-broadcasting","title":"Basic Broadcasting\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#advanced-broadcasting","title":"Advanced Broadcasting\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#indexing-and-slicing","title":"Indexing and Slicing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#basic-indexing","title":"Basic Indexing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#slicing","title":"Slicing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#fancy-indexing","title":"Fancy Indexing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#boolean-masking-and-advanced-indexing","title":"Boolean Masking and Advanced Indexing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#boolean-masking","title":"Boolean Masking\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#advanced-indexing-with-conditions","title":"Advanced Indexing with Conditions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#setting-values","title":"Setting Values\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#advanced-indexing-with-npix_","title":"Advanced Indexing with np.ix_\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#random","title":"Random\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#random-numbers","title":"Random Numbers\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#random-permutations","title":"Random Permutations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#sampling-and-distributions","title":"Sampling and Distributions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#setting-seed","title":"Setting Seed\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#io-with-numpy","title":"I/O with NumPy\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#reading-and-writing-files","title":"Reading and Writing Files\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#saving-and-loading-multiple-arrays","title":"Saving and Loading Multiple Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#reading-and-writing-csv-files","title":"Reading and Writing CSV Files\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#polynomials","title":"Polynomials\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#polynomial-operations","title":"Polynomial Operations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#polynomial-fitting","title":"Polynomial Fitting\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#polynomial-derivatives-and-integrals","title":"Polynomial Derivatives and Integrals\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#advanced-array-operations","title":"Advanced Array Operations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#vectorize-functions","title":"Vectorize Functions\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#meshgrid","title":"Meshgrid\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#broadcasting-with-advanced-indexing","title":"Broadcasting with Advanced Indexing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#sorting-arrays","title":"Sorting Arrays\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#searching-and-counting-elements","title":"Searching and Counting Elements\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#memory-management","title":"Memory Management\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#memory-layout-and-optimization","title":"Memory Layout and Optimization\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#memory-mapping-files","title":"Memory Mapping Files\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#copying-and-views","title":"Copying and Views\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#advanced-indexing","title":"Advanced Indexing\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#using-nptake-and-npput","title":"Using <code>np.take</code> and <code>np.put</code>\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#using-npchoose","title":"Using <code>np.choose</code>\u00b6","text":"<p><code>np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)])</code></p>"},{"location":"Cheat-Sheets/NumPy/#using-nplexsort","title":"Using <code>np.lexsort</code>\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#matrix-operations","title":"Matrix Operations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#determinant-rank-and-trace","title":"Determinant, Rank, and Trace\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#kronecker-product-and-outer-product","title":"Kronecker Product and Outer Product\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#solving-systems-of-linear-equations","title":"Solving Systems of Linear Equations\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#data-types","title":"Data Types\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#specifying-data-types","title":"Specifying Data Types\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#converting-data-types","title":"Converting Data Types\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#complex-data-types","title":"Complex Data Types\u00b6","text":""},{"location":"Cheat-Sheets/NumPy/#checking-data-types","title":"Checking Data Types\u00b6","text":""},{"location":"Cheat-Sheets/Pandas/","title":"Pandas","text":"\ud83d\udcca Click to view Pandas Mindmaps \ud83d\udccb Click to view Pandas Cheat Sheets In\u00a0[1]: Copied! <pre># Import Pandas\nimport pandas as pd\npd.__version__ \n\n# only used in this cheat sheet to display the DataFrames\n# from IPython.display import display \n</pre> # Import Pandas import pandas as pd pd.__version__   # only used in this cheat sheet to display the DataFrames # from IPython.display import display  Out[1]: <pre>'2.2.3'</pre> In\u00a0[2]: Copied! <pre># Create a Series from a list\ns = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\nprint(\"Series s:\")\ndisplay(s)\n\n# Create a Series from a dictionary\ndata_dict = {'x': 100, 'y': 200, 'z': 300}\ns_dict = pd.Series(data_dict)\nprint(\"\\nSeries s_dict:\")\ndisplay(s_dict)\n\n# Accessing elements\nprint(\"\\nAccess element by label s['b']:\", s['b'])\nprint(\"Access element by position s.iloc[2]:\", s.iloc[2])\n</pre> # Create a Series from a list s = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd']) print(\"Series s:\") display(s)  # Create a Series from a dictionary data_dict = {'x': 100, 'y': 200, 'z': 300} s_dict = pd.Series(data_dict) print(\"\\nSeries s_dict:\") display(s_dict)  # Accessing elements print(\"\\nAccess element by label s['b']:\", s['b']) print(\"Access element by position s.iloc[2]:\", s.iloc[2]) <pre>Series s:\n</pre> <pre>a    10\nb    20\nc    30\nd    40\ndtype: int64</pre> <pre>\nSeries s_dict:\n</pre> <pre>x    100\ny    200\nz    300\ndtype: int64</pre> <pre>\nAccess element by label s['b']: 20\nAccess element by position s.iloc[2]: 30\n</pre> In\u00a0[3]: Copied! <pre># Create DataFrame from a dictionary of lists\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Paris', 'London']\n})\nprint(\"DataFrame df:\")\ndisplay(df)\n\n# Create DataFrame from a list of lists\ndf2 = pd.DataFrame(\n    [[1, 2, 3],\n     [4, 5, 6],\n     [7, 8, 9]],\n    columns=['ColA', 'ColB', 'ColC']\n)\nprint(\"\\nDataFrame df2:\")\ndisplay(df2)\n\n# Check basic attributes\nprint(\"\\nShape of df2:\", df2.shape)\nprint(\"Columns of df2:\", df2.columns)\nprint(\"Index of df2:\", df2.index)\n</pre> # Create DataFrame from a dictionary of lists df = pd.DataFrame({     'Name': ['Alice', 'Bob', 'Charlie'],     'Age': [25, 30, 35],     'City': ['New York', 'Paris', 'London'] }) print(\"DataFrame df:\") display(df)  # Create DataFrame from a list of lists df2 = pd.DataFrame(     [[1, 2, 3],      [4, 5, 6],      [7, 8, 9]],     columns=['ColA', 'ColB', 'ColC'] ) print(\"\\nDataFrame df2:\") display(df2)  # Check basic attributes print(\"\\nShape of df2:\", df2.shape) print(\"Columns of df2:\", df2.columns) print(\"Index of df2:\", df2.index)  <pre>DataFrame df:\n</pre> Name Age City 0 Alice 25 New York 1 Bob 30 Paris 2 Charlie 35 London <pre>\nDataFrame df2:\n</pre> ColA ColB ColC 0 1 2 3 1 4 5 6 2 7 8 9 <pre>\nShape of df2: (3, 3)\nColumns of df2: Index(['ColA', 'ColB', 'ColC'], dtype='object')\nIndex of df2: RangeIndex(start=0, stop=3, step=1)\n</pre> In\u00a0[4]: Copied! <pre>import numpy as np\n\n# Create a MultiIndex from tuples\ntuples = [('Group1', 'Row1'), ('Group1', 'Row2'),\n          ('Group2', 'Row1'), ('Group2', 'Row2')]\nmulti_index = pd.MultiIndex.from_tuples(tuples, names=['Group', 'Row'])\n\n# Create a DataFrame using the MultiIndex\ndf_multi = pd.DataFrame(\n    np.random.randn(4, 2),\n    index=multi_index,\n    columns=['ColX', 'ColY']\n)\nprint(\"MultiIndex DataFrame:\\n\")\ndisplay(df_multi)\nprint(\"\\nIndex levels:\", df_multi.index.levels)\n</pre> import numpy as np  # Create a MultiIndex from tuples tuples = [('Group1', 'Row1'), ('Group1', 'Row2'),           ('Group2', 'Row1'), ('Group2', 'Row2')] multi_index = pd.MultiIndex.from_tuples(tuples, names=['Group', 'Row'])  # Create a DataFrame using the MultiIndex df_multi = pd.DataFrame(     np.random.randn(4, 2),     index=multi_index,     columns=['ColX', 'ColY'] ) print(\"MultiIndex DataFrame:\\n\") display(df_multi) print(\"\\nIndex levels:\", df_multi.index.levels)  <pre>MultiIndex DataFrame:\n\n</pre> ColX ColY Group Row Group1 Row1 -0.587480 -0.817371 Row2 -1.394396 -1.291968 Group2 Row1 1.178137 0.866652 Row2 0.920178 0.593349 <pre>\nIndex levels: [['Group1', 'Group2'], ['Row1', 'Row2']]\n</pre> In\u00a0[5]: Copied! <pre># Using the df we created above\nprint(\"Original df:\")\ndisplay(df)\n\n# Select a single column by label\nprint(\"\\nSingle column df['Name']:\")\ndisplay(df['Name'])\n\n# Using .loc (label-based)\nprint(\"\\nUsing df.loc[0, 'Age']:\")\ndisplay(df.loc[0, 'Age'])\nprint(\"\\nUsing df.loc[0:1, ['Name', 'City']]:\")\ndisplay(df.loc[0:1, ['Name', 'City']])\n\n# Using .iloc (integer position-based)\nprint(\"\\nUsing df.iloc[1, 2]:\")  # second row, third column\ndisplay(df.iloc[1, 2])\nprint(\"\\nUsing df.iloc[0:2, 0:2]:\")\ndisplay(df.iloc[0:2, 0:2])\n\n# Boolean indexing\nprint(\"\\nRows where Age &gt; 25:\")\ndisplay(df[df['Age'] &gt; 25])\n</pre> # Using the df we created above print(\"Original df:\") display(df)  # Select a single column by label print(\"\\nSingle column df['Name']:\") display(df['Name'])  # Using .loc (label-based) print(\"\\nUsing df.loc[0, 'Age']:\") display(df.loc[0, 'Age']) print(\"\\nUsing df.loc[0:1, ['Name', 'City']]:\") display(df.loc[0:1, ['Name', 'City']])  # Using .iloc (integer position-based) print(\"\\nUsing df.iloc[1, 2]:\")  # second row, third column display(df.iloc[1, 2]) print(\"\\nUsing df.iloc[0:2, 0:2]:\") display(df.iloc[0:2, 0:2])  # Boolean indexing print(\"\\nRows where Age &gt; 25:\") display(df[df['Age'] &gt; 25]) <pre>Original df:\n</pre> Name Age City 0 Alice 25 New York 1 Bob 30 Paris 2 Charlie 35 London <pre>\nSingle column df['Name']:\n</pre> <pre>0      Alice\n1        Bob\n2    Charlie\nName: Name, dtype: object</pre> <pre>\nUsing df.loc[0, 'Age']:\n</pre> <pre>np.int64(25)</pre> <pre>\nUsing df.loc[0:1, ['Name', 'City']]:\n</pre> Name City 0 Alice New York 1 Bob Paris <pre>\nUsing df.iloc[1, 2]:\n</pre> <pre>'Paris'</pre> <pre>\nUsing df.iloc[0:2, 0:2]:\n</pre> Name Age 0 Alice 25 1 Bob 30 <pre>\nRows where Age &gt; 25:\n</pre> Name Age City 1 Bob 30 Paris 2 Charlie 35 London In\u00a0[6]: Copied! <pre># Example DataFrame\ndf_wide = pd.DataFrame({\n    'id': [1, 2, 3],\n    'varA': [10, 20, 30],\n    'varB': [40, 50, 60]\n})\nprint(\"Wide DataFrame:\")\ndisplay(df_wide)\n\n# Melt (wide -&gt; long)\ndf_long = pd.melt(df_wide, id_vars='id', var_name='variable', value_name='value')\nprint(\"\\nMelted (long) DataFrame:\")\ndisplay(df_long)\n\n# Pivot (long -&gt; wide)\ndf_pivoted = df_long.pivot(index='id', columns='variable', values='value')\nprint(\"\\nPivoted (wide) DataFrame:\")\ndisplay(df_pivoted)\n\n# Concat examples\ndf_part1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\ndf_part2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\ndf_concat = pd.concat([df_part1, df_part2], ignore_index=True)\nprint(\"\\nConcatenated DataFrame:\")\ndisplay(df_concat)\n</pre> # Example DataFrame df_wide = pd.DataFrame({     'id': [1, 2, 3],     'varA': [10, 20, 30],     'varB': [40, 50, 60] }) print(\"Wide DataFrame:\") display(df_wide)  # Melt (wide -&gt; long) df_long = pd.melt(df_wide, id_vars='id', var_name='variable', value_name='value') print(\"\\nMelted (long) DataFrame:\") display(df_long)  # Pivot (long -&gt; wide) df_pivoted = df_long.pivot(index='id', columns='variable', values='value') print(\"\\nPivoted (wide) DataFrame:\") display(df_pivoted)  # Concat examples df_part1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]}) df_part2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]}) df_concat = pd.concat([df_part1, df_part2], ignore_index=True) print(\"\\nConcatenated DataFrame:\") display(df_concat) <pre>Wide DataFrame:\n</pre> id varA varB 0 1 10 40 1 2 20 50 2 3 30 60 <pre>\nMelted (long) DataFrame:\n</pre> id variable value 0 1 varA 10 1 2 varA 20 2 3 varA 30 3 1 varB 40 4 2 varB 50 5 3 varB 60 <pre>\nPivoted (wide) DataFrame:\n</pre> variable varA varB id 1 10 40 2 20 50 3 30 60 <pre>\nConcatenated DataFrame:\n</pre> A B 0 1 3 1 2 4 2 5 7 3 6 8 In\u00a0[7]: Copied! <pre># Using df_wide again\nprint(\"df_wide:\")\ndisplay(df_wide)\n\n# Subset columns\nsubset_columns = df_wide[['id', 'varA']]\nprint(\"\\nSubset of columns [id, varA]:\")\ndisplay(subset_columns)\n\n# Subset rows by index slicing\nsubset_rows = df_wide.iloc[0:2]  # first two rows\nprint(\"\\nSubset of rows (first two rows):\")\ndisplay(subset_rows)\n\n# Subset using a condition\ncondition_subset = df_wide[df_wide['varA'] &gt; 10]\nprint(\"\\nSubset where varA &gt; 10:\")\ndisplay(condition_subset)\n</pre> # Using df_wide again print(\"df_wide:\") display(df_wide)  # Subset columns subset_columns = df_wide[['id', 'varA']] print(\"\\nSubset of columns [id, varA]:\") display(subset_columns)  # Subset rows by index slicing subset_rows = df_wide.iloc[0:2]  # first two rows print(\"\\nSubset of rows (first two rows):\") display(subset_rows)  # Subset using a condition condition_subset = df_wide[df_wide['varA'] &gt; 10] print(\"\\nSubset where varA &gt; 10:\") display(condition_subset) <pre>df_wide:\n</pre> id varA varB 0 1 10 40 1 2 20 50 2 3 30 60 <pre>\nSubset of columns [id, varA]:\n</pre> id varA 0 1 10 1 2 20 2 3 30 <pre>\nSubset of rows (first two rows):\n</pre> id varA varB 0 1 10 40 1 2 20 50 <pre>\nSubset where varA &gt; 10:\n</pre> id varA varB 1 2 20 50 2 3 30 60 In\u00a0[8]: Copied! <pre>import numpy as np\n\ndf_stats = pd.DataFrame({\n    'Col1': np.random.randint(0, 100, 5),\n    'Col2': np.random.randint(0, 100, 5),\n    'Col3': np.random.randint(0, 100, 5),\n    'Category': ['A', 'B', 'A', 'C', 'B'],\n    'Status': ['Active', 'Inactive', 'Active', 'Active', 'Inactive']\n})\nprint(\"DataFrame with mixed types:\")\ndisplay(df_stats)\n\n# Basic stats (numeric columns only)\nprint(\"\\nNumerical Statistics (describe):\")\ndisplay(df_stats.describe())\n\n# Basic stats for object columns\nprint(\"\\nCategorical Statistics (describe):\")\ndisplay(df_stats.describe(include=['object']))\n\n# Value counts for categorical columns\nprint(\"\\nCategory value counts:\")\ndisplay(df_stats['Category'].value_counts())\nprint(\"\\nStatus value counts:\")\ndisplay(df_stats['Status'].value_counts())\n\n# Mean of numeric columns\nprint(\"\\nMean of numeric columns:\")\ndisplay(df_stats.mean(numeric_only=True))\n\n# Count of all columns (works for both numeric and object types)\nprint(\"\\nCount of all columns:\")\ndisplay(df_stats.count())\n</pre> import numpy as np  df_stats = pd.DataFrame({     'Col1': np.random.randint(0, 100, 5),     'Col2': np.random.randint(0, 100, 5),     'Col3': np.random.randint(0, 100, 5),     'Category': ['A', 'B', 'A', 'C', 'B'],     'Status': ['Active', 'Inactive', 'Active', 'Active', 'Inactive'] }) print(\"DataFrame with mixed types:\") display(df_stats)  # Basic stats (numeric columns only) print(\"\\nNumerical Statistics (describe):\") display(df_stats.describe())  # Basic stats for object columns print(\"\\nCategorical Statistics (describe):\") display(df_stats.describe(include=['object']))  # Value counts for categorical columns print(\"\\nCategory value counts:\") display(df_stats['Category'].value_counts()) print(\"\\nStatus value counts:\") display(df_stats['Status'].value_counts())  # Mean of numeric columns print(\"\\nMean of numeric columns:\") display(df_stats.mean(numeric_only=True))  # Count of all columns (works for both numeric and object types) print(\"\\nCount of all columns:\") display(df_stats.count()) <pre>DataFrame with mixed types:\n</pre> Col1 Col2 Col3 Category Status 0 74 65 92 A Active 1 96 70 76 B Inactive 2 63 68 83 A Active 3 35 85 75 C Active 4 56 11 42 B Inactive <pre>\nNumerical Statistics (describe):\n</pre> Col1 Col2 Col3 count 5.000000 5.000000 5.000000 mean 64.800000 59.800000 73.600000 std 22.509998 28.349603 18.928814 min 35.000000 11.000000 42.000000 25% 56.000000 65.000000 75.000000 50% 63.000000 68.000000 76.000000 75% 74.000000 70.000000 83.000000 max 96.000000 85.000000 92.000000 <pre>\nCategorical Statistics (describe):\n</pre> Category Status count 5 5 unique 3 2 top A Active freq 2 3 <pre>\nCategory value counts:\n</pre> <pre>Category\nA    2\nB    2\nC    1\nName: count, dtype: int64</pre> <pre>\nStatus value counts:\n</pre> <pre>Status\nActive      3\nInactive    2\nName: count, dtype: int64</pre> <pre>\nMean of numeric columns:\n</pre> <pre>Col1    64.8\nCol2    59.8\nCol3    73.6\ndtype: float64</pre> <pre>\nCount of all columns:\n</pre> <pre>Col1        5\nCol2        5\nCol3        5\nCategory    5\nStatus      5\ndtype: int64</pre> In\u00a0[9]: Copied! <pre># Create a DataFrame with NaN values\ndf_missing = pd.DataFrame({\n    'A': [1, np.nan, 3, np.nan],\n    'B': [5, 6, np.nan, 8]\n})\nprint(\"Original df_missing:\")\ndisplay(df_missing)\n\n# Detect missing values\nprint(\"\\nMissing value check:\")\ndisplay(df_missing.isnull())\n\n# Drop rows with any missing values\nprint(\"\\nDrop rows with any NaN:\")\ndisplay(df_missing.dropna())\n\n# Fill missing values with a constant\nfilled_df = df_missing.fillna(0)\nprint(\"\\nFill NaN with 0:\")\ndisplay(filled_df)\n</pre> # Create a DataFrame with NaN values df_missing = pd.DataFrame({     'A': [1, np.nan, 3, np.nan],     'B': [5, 6, np.nan, 8] }) print(\"Original df_missing:\") display(df_missing)  # Detect missing values print(\"\\nMissing value check:\") display(df_missing.isnull())  # Drop rows with any missing values print(\"\\nDrop rows with any NaN:\") display(df_missing.dropna())  # Fill missing values with a constant filled_df = df_missing.fillna(0) print(\"\\nFill NaN with 0:\") display(filled_df) <pre>Original df_missing:\n</pre> A B 0 1.0 5.0 1 NaN 6.0 2 3.0 NaN 3 NaN 8.0 <pre>\nMissing value check:\n</pre> A B 0 False False 1 True False 2 False True 3 True False <pre>\nDrop rows with any NaN:\n</pre> A B 0 1.0 5.0 <pre>\nFill NaN with 0:\n</pre> A B 0 1.0 5.0 1 0.0 6.0 2 3.0 0.0 3 0.0 8.0 In\u00a0[10]: Copied! <pre>df_new_cols = pd.DataFrame({\n    'Length': [2, 3, 4],\n    'Width': [5, 6, 7]\n})\nprint(\"Original df_new_cols:\")\ndisplay(df_new_cols)\n\n# Direct assignment\ndf_new_cols['Area'] = df_new_cols['Length'] * df_new_cols['Width']\nprint(\"\\nAdded 'Area' column:\")\ndisplay(df_new_cols)\n\n# Using assign (returns a copy)\ndf_assigned = df_new_cols.assign(Perimeter=lambda x: 2 * (x['Length'] + x['Width']))\nprint(\"\\nAssigned 'Perimeter' column:\")\ndisplay(df_assigned)\n</pre> df_new_cols = pd.DataFrame({     'Length': [2, 3, 4],     'Width': [5, 6, 7] }) print(\"Original df_new_cols:\") display(df_new_cols)  # Direct assignment df_new_cols['Area'] = df_new_cols['Length'] * df_new_cols['Width'] print(\"\\nAdded 'Area' column:\") display(df_new_cols)  # Using assign (returns a copy) df_assigned = df_new_cols.assign(Perimeter=lambda x: 2 * (x['Length'] + x['Width'])) print(\"\\nAssigned 'Perimeter' column:\") display(df_assigned) <pre>Original df_new_cols:\n</pre> Length Width 0 2 5 1 3 6 2 4 7 <pre>\nAdded 'Area' column:\n</pre> Length Width Area 0 2 5 10 1 3 6 18 2 4 7 28 <pre>\nAssigned 'Perimeter' column:\n</pre> Length Width Area Perimeter 0 2 5 10 14 1 3 6 18 18 2 4 7 28 22 In\u00a0[11]: Copied! <pre>df_group = pd.DataFrame({\n    'Category': ['A', 'A', 'B', 'B', 'C'],\n    'Values': [10, 15, 10, 25, 5]\n})\n\nprint(\"df_group:\")\ndisplay(df_group)\n\n# Group by 'Category'\ngrouped = df_group.groupby('Category')\nprint(\"\\nSum by Category:\")\ndisplay(grouped['Values'].sum())\nprint(\"\\nMean by Category:\")\ndisplay(grouped['Values'].mean())\n\n# Multiple aggregation methods at once\nagg_results = grouped['Values'].agg(['min', 'max', 'mean', 'count'])\nprint(\"\\nMultiple aggregations:\")\ndisplay(agg_results)\n</pre> df_group = pd.DataFrame({     'Category': ['A', 'A', 'B', 'B', 'C'],     'Values': [10, 15, 10, 25, 5] })  print(\"df_group:\") display(df_group)  # Group by 'Category' grouped = df_group.groupby('Category') print(\"\\nSum by Category:\") display(grouped['Values'].sum()) print(\"\\nMean by Category:\") display(grouped['Values'].mean())  # Multiple aggregation methods at once agg_results = grouped['Values'].agg(['min', 'max', 'mean', 'count']) print(\"\\nMultiple aggregations:\") display(agg_results)  <pre>df_group:\n</pre> Category Values 0 A 10 1 A 15 2 B 10 3 B 25 4 C 5 <pre>\nSum by Category:\n</pre> <pre>Category\nA    25\nB    35\nC     5\nName: Values, dtype: int64</pre> <pre>\nMean by Category:\n</pre> <pre>Category\nA    12.5\nB    17.5\nC     5.0\nName: Values, dtype: float64</pre> <pre>\nMultiple aggregations:\n</pre> min max mean count Category A 10 15 12.5 2 B 10 25 17.5 2 C 5 5 5.0 1 In\u00a0[12]: Copied! <pre># Create a Series for rolling example\ns_rolling = pd.Series([1, 2, 3, 4, 5, 6, 7])\nprint(\"Original Series:\")\ndisplay(s_rolling)\n\n# Rolling window of size 3, compute mean\nrolling_mean = s_rolling.rolling(3).mean()\nprint(\"\\nRolling mean with window=3:\")\ndisplay(rolling_mean)\n\n# Expanding: cumulative computations\nexpanding_sum = s_rolling.expanding().sum()\nprint(\"\\nExpanding sum:\")\ndisplay(expanding_sum)\n</pre> # Create a Series for rolling example s_rolling = pd.Series([1, 2, 3, 4, 5, 6, 7]) print(\"Original Series:\") display(s_rolling)  # Rolling window of size 3, compute mean rolling_mean = s_rolling.rolling(3).mean() print(\"\\nRolling mean with window=3:\") display(rolling_mean)  # Expanding: cumulative computations expanding_sum = s_rolling.expanding().sum() print(\"\\nExpanding sum:\") display(expanding_sum) <pre>Original Series:\n</pre> <pre>0    1\n1    2\n2    3\n3    4\n4    5\n5    6\n6    7\ndtype: int64</pre> <pre>\nRolling mean with window=3:\n</pre> <pre>0    NaN\n1    NaN\n2    2.0\n3    3.0\n4    4.0\n5    5.0\n6    6.0\ndtype: float64</pre> <pre>\nExpanding sum:\n</pre> <pre>0     1.0\n1     3.0\n2     6.0\n3    10.0\n4    15.0\n5    21.0\n6    28.0\ndtype: float64</pre> In\u00a0[13]: Copied! <pre>left = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'A': ['A0', 'A1', 'A2']})\nright = pd.DataFrame({'key': ['K0', 'K2', 'K3'], 'B': ['B0', 'B2', 'B3']})\n\nprint(\"left:\")\ndisplay(left)\nprint(\"\\nright:\")\ndisplay(right)\n\n# Merge using an inner join\nmerged_inner = pd.merge(left, right, on='key', how='inner')\nprint(\"\\nMerged (inner join):\")\ndisplay(merged_inner)\n\n# Merge using an outer join\nmerged_outer = pd.merge(left, right, on='key', how='outer')\nprint(\"\\nMerged (outer join):\")\ndisplay(merged_outer)\n\n# Concat example (stacking rows)\nconcat_example = pd.concat([left, right], axis=0, ignore_index=True)\nprint(\"\\nConcatenated (stack rows):\")\ndisplay(concat_example)\n</pre> left = pd.DataFrame({'key': ['K0', 'K1', 'K2'], 'A': ['A0', 'A1', 'A2']}) right = pd.DataFrame({'key': ['K0', 'K2', 'K3'], 'B': ['B0', 'B2', 'B3']})  print(\"left:\") display(left) print(\"\\nright:\") display(right)  # Merge using an inner join merged_inner = pd.merge(left, right, on='key', how='inner') print(\"\\nMerged (inner join):\") display(merged_inner)  # Merge using an outer join merged_outer = pd.merge(left, right, on='key', how='outer') print(\"\\nMerged (outer join):\") display(merged_outer)  # Concat example (stacking rows) concat_example = pd.concat([left, right], axis=0, ignore_index=True) print(\"\\nConcatenated (stack rows):\") display(concat_example) <pre>left:\n</pre> key A 0 K0 A0 1 K1 A1 2 K2 A2 <pre>\nright:\n</pre> key B 0 K0 B0 1 K2 B2 2 K3 B3 <pre>\nMerged (inner join):\n</pre> key A B 0 K0 A0 B0 1 K2 A2 B2 <pre>\nMerged (outer join):\n</pre> key A B 0 K0 A0 B0 1 K1 A1 NaN 2 K2 A2 B2 3 K3 NaN B3 <pre>\nConcatenated (stack rows):\n</pre> key A B 0 K0 A0 NaN 1 K1 A1 NaN 2 K2 A2 NaN 3 K0 NaN B0 4 K2 NaN B2 5 K3 NaN B3 In\u00a0[29]: Copied! <pre># Create sample DataFrames with multiple columns\nleft = pd.DataFrame({\n    'id_left': ['K0', 'K1', 'K2'],\n    'name': ['A0', 'A1', 'A2'],\n    'score': [10, 20, 30]\n})\nright = pd.DataFrame({\n    'id_right': ['K0', 'K2', 'K3'],\n    'name': ['B0', 'B2', 'B3'],\n    'score': [15, 35, 45]\n})\n\n# Merge with different key columns and suffixes\nmerged = pd.merge(\n    left, right,\n    left_on=['id_left', 'name'],\n    right_on=['id_right', 'name'],\n    how='outer',\n    suffixes=('_L', '_R')\n)\nprint(\"Merged with multiple keys and suffixes:\")\ndisplay(merged)\n</pre> # Create sample DataFrames with multiple columns left = pd.DataFrame({     'id_left': ['K0', 'K1', 'K2'],     'name': ['A0', 'A1', 'A2'],     'score': [10, 20, 30] }) right = pd.DataFrame({     'id_right': ['K0', 'K2', 'K3'],     'name': ['B0', 'B2', 'B3'],     'score': [15, 35, 45] })  # Merge with different key columns and suffixes merged = pd.merge(     left, right,     left_on=['id_left', 'name'],     right_on=['id_right', 'name'],     how='outer',     suffixes=('_L', '_R') ) print(\"Merged with multiple keys and suffixes:\") display(merged) <pre>Merged with multiple keys and suffixes:\n</pre> id_left name score_L id_right score_R 0 K0 A0 10.0 NaN NaN 1 NaN B0 NaN K0 15.0 2 K1 A1 20.0 NaN NaN 3 K2 A2 30.0 NaN NaN 4 NaN B2 NaN K2 35.0 5 NaN B3 NaN K3 45.0 In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\n\ndf_plot = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [3, 5, 2, 8, 7]\n})\n\n# Line plot\ndf_plot.plot(x='x', y='y', kind='line', title='Line Plot')\nplt.show()\n\n# Scatter plot\ndf_plot.plot.scatter(x='x', y='y', title='Scatter Plot')\nplt.show()\n</pre> import matplotlib.pyplot as plt  df_plot = pd.DataFrame({     'x': [1, 2, 3, 4, 5],     'y': [3, 5, 2, 8, 7] })  # Line plot df_plot.plot(x='x', y='y', kind='line', title='Line Plot') plt.show()  # Scatter plot df_plot.plot.scatter(x='x', y='y', title='Scatter Plot') plt.show()  In\u00a0[15]: Copied! <pre>print(\"df_plot info:\")\ndisplay(df_plot.info())\n\nprint(\"\\nDataFrame attributes:\")\nprint(\"Shape:\", df_plot.shape)\nprint(\"Columns:\", df_plot.columns)\nprint(\"Index:\", df_plot.index)\nprint(\"\\nDataFrame dtypes:\")\ndisplay(df_plot.dtypes)\n\nprint(\"\\nDescriptive statistics:\")\ndisplay(df_plot.describe())\n</pre> print(\"df_plot info:\") display(df_plot.info())  print(\"\\nDataFrame attributes:\") print(\"Shape:\", df_plot.shape) print(\"Columns:\", df_plot.columns) print(\"Index:\", df_plot.index) print(\"\\nDataFrame dtypes:\") display(df_plot.dtypes)  print(\"\\nDescriptive statistics:\") display(df_plot.describe()) <pre>df_plot info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   x       5 non-null      int64\n 1   y       5 non-null      int64\ndtypes: int64(2)\nmemory usage: 212.0 bytes\n</pre> <pre>None</pre> <pre>\nDataFrame attributes:\nShape: (5, 2)\nColumns: Index(['x', 'y'], dtype='object')\nIndex: RangeIndex(start=0, stop=5, step=1)\n\nDataFrame dtypes:\n</pre> <pre>x    int64\ny    int64\ndtype: object</pre> <pre>\nDescriptive statistics:\n</pre> x y count 5.000000 5.00000 mean 3.000000 5.00000 std 1.581139 2.54951 min 1.000000 2.00000 25% 2.000000 3.00000 50% 3.000000 5.00000 75% 4.000000 7.00000 max 5.000000 8.00000 In\u00a0[16]: Copied! <pre>print(\"df_stats\")\ndisplay(df_stats)\n\n# Apply a function to each column\nprint(\"Applying sum to each column:\")\ndisplay(df_stats[['Col1', 'Col2']].apply(sum))\n\n# Apply a function to each element\ndf_applied = df_stats.applymap(lambda x: x * 2)\nprint(\"\\nApplying lambda x: x * 2 to each element:\")\ndisplay(df_applied)\n\n# For a single column (Series)\ndf_stats_col1_applied = df_stats['Col1'].apply(lambda x: x + 10)\nprint(\"\\nAdding 10 to each element in Col1:\")\ndisplay(df_stats_col1_applied)\n\n# Apply function to two columns row-wise\ntemp_df = df_stats.copy() # copyinf DataFrame\ntemp_df['row_sums'] = temp_df.apply(lambda x: x['Col1'] + x['Col2'], axis=1)\nprint(\"\\nSum of Col1 and Col2:\")\ndisplay(temp_df)\n</pre> print(\"df_stats\") display(df_stats)  # Apply a function to each column print(\"Applying sum to each column:\") display(df_stats[['Col1', 'Col2']].apply(sum))  # Apply a function to each element df_applied = df_stats.applymap(lambda x: x * 2) print(\"\\nApplying lambda x: x * 2 to each element:\") display(df_applied)  # For a single column (Series) df_stats_col1_applied = df_stats['Col1'].apply(lambda x: x + 10) print(\"\\nAdding 10 to each element in Col1:\") display(df_stats_col1_applied)  # Apply function to two columns row-wise temp_df = df_stats.copy() # copyinf DataFrame temp_df['row_sums'] = temp_df.apply(lambda x: x['Col1'] + x['Col2'], axis=1) print(\"\\nSum of Col1 and Col2:\") display(temp_df) <pre>df_stats\n</pre> Col1 Col2 Col3 Category Status 0 74 65 92 A Active 1 96 70 76 B Inactive 2 63 68 83 A Active 3 35 85 75 C Active 4 56 11 42 B Inactive <pre>Applying sum to each column:\n</pre> <pre>Col1    324\nCol2    299\ndtype: int64</pre> <pre>\nApplying lambda x: x * 2 to each element:\n</pre> <pre>/var/folders/wj/pbgm2qxx6vbfvq55xm8k42jh0000gn/T/ipykernel_43554/389234560.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_applied = df_stats.applymap(lambda x: x * 2)\n</pre> Col1 Col2 Col3 Category Status 0 148 130 184 AA ActiveActive 1 192 140 152 BB InactiveInactive 2 126 136 166 AA ActiveActive 3 70 170 150 CC ActiveActive 4 112 22 84 BB InactiveInactive <pre>\nAdding 10 to each element in Col1:\n</pre> <pre>0     84\n1    106\n2     73\n3     45\n4     66\nName: Col1, dtype: int64</pre> <pre>\nSum of Col1 and Col2:\n</pre> Col1 Col2 Col3 Category Status row_sums 0 74 65 92 A Active 139 1 96 70 76 B Inactive 166 2 63 68 83 A Active 131 3 35 85 75 C Active 120 4 56 11 42 B Inactive 67 In\u00a0[17]: Copied! <pre>s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\ns2 = pd.Series([4, 5, 6], index=['b', 'c', 'd'])\n\nprint(\"s1:\")\ndisplay(s1)\nprint(\"\\ns2:\")\ndisplay(s2)\n\n# Addition auto-aligns on index\ns_sum = s1 + s2\nprint(\"\\nAuto-aligned sum:\")\ndisplay(s_sum)\n\n# Fill missing with 0 while adding\ns_sum_fill = s1.add(s2, fill_value=0)\nprint(\"\\nSum with fill_value=0:\")\ndisplay(s_sum_fill)\n</pre> s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c']) s2 = pd.Series([4, 5, 6], index=['b', 'c', 'd'])  print(\"s1:\") display(s1) print(\"\\ns2:\") display(s2)  # Addition auto-aligns on index s_sum = s1 + s2 print(\"\\nAuto-aligned sum:\") display(s_sum)  # Fill missing with 0 while adding s_sum_fill = s1.add(s2, fill_value=0) print(\"\\nSum with fill_value=0:\") display(s_sum_fill) <pre>s1:\n</pre> <pre>a    1\nb    2\nc    3\ndtype: int64</pre> <pre>\ns2:\n</pre> <pre>b    4\nc    5\nd    6\ndtype: int64</pre> <pre>\nAuto-aligned sum:\n</pre> <pre>a    NaN\nb    6.0\nc    8.0\nd    NaN\ndtype: float64</pre> <pre>\nSum with fill_value=0:\n</pre> <pre>a    1.0\nb    6.0\nc    8.0\nd    6.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre># NOTE: Below lines are examples. They require actual files or database connections to run successfully.\n\n# Reading a CSV\n# df_read_csv = pd.read_csv(\"my_data.csv\")\n\n# Writing to a CSV\n# df_read_csv.to_csv(\"my_output.csv\", index=False)\n\n# Reading an Excel file\n# df_excel = pd.read_excel(\"my_data.xlsx\", sheet_name=\"Sheet1\")\n\n# Writing to Excel\n# df_excel.to_excel(\"my_new_excel.xlsx\", index=False)\n\n# SQL example (requires a real engine and table)\n# from sqlalchemy import create_engine\n# engine = create_engine(\"sqlite:///my_database.db\")\n# df_from_sql = pd.read_sql(\"SELECT * FROM my_table\", engine)\n# df_from_sql.to_sql(\"my_new_table\", engine, if_exists=\"replace\", index=False)\n</pre> # NOTE: Below lines are examples. They require actual files or database connections to run successfully.  # Reading a CSV # df_read_csv = pd.read_csv(\"my_data.csv\")  # Writing to a CSV # df_read_csv.to_csv(\"my_output.csv\", index=False)  # Reading an Excel file # df_excel = pd.read_excel(\"my_data.xlsx\", sheet_name=\"Sheet1\")  # Writing to Excel # df_excel.to_excel(\"my_new_excel.xlsx\", index=False)  # SQL example (requires a real engine and table) # from sqlalchemy import create_engine # engine = create_engine(\"sqlite:///my_database.db\") # df_from_sql = pd.read_sql(\"SELECT * FROM my_table\", engine) # df_from_sql.to_sql(\"my_new_table\", engine, if_exists=\"replace\", index=False)  In\u00a0[18]: Copied! <pre>df_chain = pd.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': [5, 6, 7, 8]\n})\n\n# Example of method chaining: melt, rename, query\ndf_chain_melted = (\n    df_chain\n    .melt(var_name='Variable', value_name='Value')\n    .rename(columns={'Variable': 'var', 'Value': 'val'})\n    .query('val &gt; 3')\n)\ndisplay(df_chain_melted)\n</pre> df_chain = pd.DataFrame({     'A': [1, 2, 3, 4],     'B': [5, 6, 7, 8] })  # Example of method chaining: melt, rename, query df_chain_melted = (     df_chain     .melt(var_name='Variable', value_name='Value')     .rename(columns={'Variable': 'var', 'Value': 'val'})     .query('val &gt; 3') ) display(df_chain_melted) var val 3 A 4 4 B 5 5 B 6 6 B 7 7 B 8 In\u00a0[19]: Copied! <pre>df_sample = pd.DataFrame({'ColA': [5, 2, 9, 1, 7, 3]})\nprint(\"Original Data:\")\ndisplay(df_sample)\n\n# Random sample of 3 rows\nsampled = df_sample.sample(n=3)\nprint(\"\\nRandom sample of 3 rows:\")\ndisplay(sampled)\n\n# 2 largest values in ColA\nlargest_two = df_sample.nlargest(2, 'ColA')\nprint(\"\\n2 largest in ColA:\")\ndisplay(largest_two)\n\n# 2 smallest values in ColA\nsmallest_two = df_sample.nsmallest(2, 'ColA')\nprint(\"\\n2 smallest in ColA:\")\ndisplay(smallest_two)\n</pre> df_sample = pd.DataFrame({'ColA': [5, 2, 9, 1, 7, 3]}) print(\"Original Data:\") display(df_sample)  # Random sample of 3 rows sampled = df_sample.sample(n=3) print(\"\\nRandom sample of 3 rows:\") display(sampled)  # 2 largest values in ColA largest_two = df_sample.nlargest(2, 'ColA') print(\"\\n2 largest in ColA:\") display(largest_two)  # 2 smallest values in ColA smallest_two = df_sample.nsmallest(2, 'ColA') print(\"\\n2 smallest in ColA:\") display(smallest_two) <pre>Original Data:\n</pre> ColA 0 5 1 2 2 9 3 1 4 7 5 3 <pre>\nRandom sample of 3 rows:\n</pre> ColA 3 1 0 5 5 3 <pre>\n2 largest in ColA:\n</pre> ColA 2 9 4 7 <pre>\n2 smallest in ColA:\n</pre> ColA 3 1 1 2 In\u00a0[20]: Copied! <pre>df_dup = pd.DataFrame({\n    'X': [1, 1, 2, 2, 3],\n    'Y': [10, 10, 20, 30, 30]\n})\nprint(\"Original Data:\")\ndisplay(df_dup)\n\ndf_no_dup = df_dup.drop_duplicates()\nprint(\"\\nAfter drop_duplicates:\")\ndisplay(df_no_dup)\n</pre> df_dup = pd.DataFrame({     'X': [1, 1, 2, 2, 3],     'Y': [10, 10, 20, 30, 30] }) print(\"Original Data:\") display(df_dup)  df_no_dup = df_dup.drop_duplicates() print(\"\\nAfter drop_duplicates:\") display(df_no_dup) <pre>Original Data:\n</pre> X Y 0 1 10 1 1 10 2 2 20 3 2 30 4 3 30 <pre>\nAfter drop_duplicates:\n</pre> X Y 0 1 10 2 2 20 3 2 30 4 3 30 In\u00a0[21]: Copied! <pre>df_counts = pd.Series(['apple', 'banana', 'apple', 'orange', 'banana'])\nprint(\"Original Series:\")\ndisplay(df_counts)\n\nprint(\"\\nValue counts:\")\ndisplay(df_counts.value_counts())\nprint(\"\\nNumber of unique values:\")\ndisplay(df_counts.nunique())\n</pre> df_counts = pd.Series(['apple', 'banana', 'apple', 'orange', 'banana']) print(\"Original Series:\") display(df_counts)  print(\"\\nValue counts:\") display(df_counts.value_counts()) print(\"\\nNumber of unique values:\") display(df_counts.nunique()) <pre>Original Series:\n</pre> <pre>0     apple\n1    banana\n2     apple\n3    orange\n4    banana\ndtype: object</pre> <pre>\nValue counts:\n</pre> <pre>apple     2\nbanana    2\norange    1\nName: count, dtype: int64</pre> <pre>\nNumber of unique values:\n</pre> <pre>3</pre> In\u00a0[22]: Copied! <pre>df_filter = pd.DataFrame({\n    'width_cm': [10, 15, 20],\n    'height_cm': [5, 8, 12],\n    'depth_m': [0.5, 0.8, 1.2]\n})\n\n# Filter columns that contain '_cm'\ncm_cols = df_filter.filter(regex='_cm$')\nprint(\"Original DataFrame:\")\ndisplay(df_filter)\nprint(\"\\nColumns that end with '_cm':\")\ndisplay(cm_cols)\n</pre> df_filter = pd.DataFrame({     'width_cm': [10, 15, 20],     'height_cm': [5, 8, 12],     'depth_m': [0.5, 0.8, 1.2] })  # Filter columns that contain '_cm' cm_cols = df_filter.filter(regex='_cm$') print(\"Original DataFrame:\") display(df_filter) print(\"\\nColumns that end with '_cm':\") display(cm_cols) <pre>Original DataFrame:\n</pre> width_cm height_cm depth_m 0 10 5 0.5 1 15 8 0.8 2 20 12 1.2 <pre>\nColumns that end with '_cm':\n</pre> width_cm height_cm 0 10 5 1 15 8 2 20 12 In\u00a0[23]: Copied! <pre>df_query = pd.DataFrame({\n    'col1': [5, 10, 15],\n    'col2': [2, 4, 6]\n})\nprint(\"Original Data:\")\ndisplay(df_query)\n\n# Filter rows where col1 &gt; 5 AND col2 &lt; 6\nfiltered_query = df_query.query(\"col1 &gt; 5 and col2 &lt; 6\")\nprint(\"\\nFiltered via query:\")\ndisplay(filtered_query)\n</pre> df_query = pd.DataFrame({     'col1': [5, 10, 15],     'col2': [2, 4, 6] }) print(\"Original Data:\") display(df_query)  # Filter rows where col1 &gt; 5 AND col2 &lt; 6 filtered_query = df_query.query(\"col1 &gt; 5 and col2 &lt; 6\") print(\"\\nFiltered via query:\") display(filtered_query) <pre>Original Data:\n</pre> col1 col2 0 5 2 1 10 4 2 15 6 <pre>\nFiltered via query:\n</pre> col1 col2 1 10 4 In\u00a0[24]: Copied! <pre>df_pivot_example = pd.DataFrame({\n    'month': ['Jan', 'Jan', 'Feb', 'Feb'],\n    'category': ['A', 'B', 'A', 'B'],\n    'value': [10, 20, 30, 40]\n})\n\nprint(\"Original DataFrame:\")\ndisplay(df_pivot_example)\n\n# pivot will fail if there are duplicate entries for month &amp; category\n# pivot_table can aggregate duplicates:\ndf_pivoted_tbl = df_pivot_example.pivot_table(\n    index='month',\n    columns='category',\n    values='value',\n    aggfunc='sum'\n)\nprint(\"\\nPivot Table with aggregation:\")\ndisplay(df_pivoted_tbl)\n</pre> df_pivot_example = pd.DataFrame({     'month': ['Jan', 'Jan', 'Feb', 'Feb'],     'category': ['A', 'B', 'A', 'B'],     'value': [10, 20, 30, 40] })  print(\"Original DataFrame:\") display(df_pivot_example)  # pivot will fail if there are duplicate entries for month &amp; category # pivot_table can aggregate duplicates: df_pivoted_tbl = df_pivot_example.pivot_table(     index='month',     columns='category',     values='value',     aggfunc='sum' ) print(\"\\nPivot Table with aggregation:\") display(df_pivoted_tbl) <pre>Original DataFrame:\n</pre> month category value 0 Jan A 10 1 Jan B 20 2 Feb A 30 3 Feb B 40 <pre>\nPivot Table with aggregation:\n</pre> category A B month Feb 30 40 Jan 10 20 In\u00a0[25]: Copied! <pre>df_cum = pd.DataFrame({'vals': [100, 200, 200, 300]})\nprint(\"Original Data:\")\ndisplay(df_cum)\n\n# rank with 'dense' method\ndf_cum['rank'] = df_cum['vals'].rank(method='dense')\nprint(\"\\nRank (dense) on 'vals':\")\ndisplay(df_cum)\n\n# shift by 1\ndf_cum['shifted_vals'] = df_cum['vals'].shift(1)\nprint(\"\\nAfter shifting 'vals' by 1:\")\ndisplay(df_cum)\n\n# cumsum\ndf_cum['cumulative_sum'] = df_cum['vals'].cumsum()\nprint(\"\\nCumulative sum of 'vals':\")\ndisplay(df_cum)\n</pre> df_cum = pd.DataFrame({'vals': [100, 200, 200, 300]}) print(\"Original Data:\") display(df_cum)  # rank with 'dense' method df_cum['rank'] = df_cum['vals'].rank(method='dense') print(\"\\nRank (dense) on 'vals':\") display(df_cum)  # shift by 1 df_cum['shifted_vals'] = df_cum['vals'].shift(1) print(\"\\nAfter shifting 'vals' by 1:\") display(df_cum)  # cumsum df_cum['cumulative_sum'] = df_cum['vals'].cumsum() print(\"\\nCumulative sum of 'vals':\") display(df_cum) <pre>Original Data:\n</pre> vals 0 100 1 200 2 200 3 300 <pre>\nRank (dense) on 'vals':\n</pre> vals rank 0 100 1.0 1 200 2.0 2 200 2.0 3 300 3.0 <pre>\nAfter shifting 'vals' by 1:\n</pre> vals rank shifted_vals 0 100 1.0 NaN 1 200 2.0 100.0 2 200 2.0 200.0 3 300 3.0 200.0 <pre>\nCumulative sum of 'vals':\n</pre> vals rank shifted_vals cumulative_sum 0 100 1.0 NaN 100 1 200 2.0 100.0 300 2 200 2.0 200.0 500 3 300 3.0 200.0 800"},{"location":"Cheat-Sheets/Pandas/#pandas","title":"Pandas\u00b6","text":"<p>Pandas is a fast, flexible, and expressive open-source data analysis/manipulation library built on top of NumPy in Python. It provides data structures like Series (1D) and DataFrame (2D) for handling tabular data, time series, and more. Essential for data cleaning, transformation, and exploration.</p> <ul> <li>Official Website: https://pandas.pydata.org/</li> <li>Installation:  (https://pandas.pydata.org/docs/getting_started/install.html)<pre>pip install pandas\n</pre> </li> <li>Documentation: https://pandas.pydata.org/docs/</li> <li>GitHub: https://pandas.pydata.org/docs/</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#pandas-data-structures","title":"Pandas Data Structures\u00b6","text":""},{"location":"Cheat-Sheets/Pandas/#series","title":"Series\u00b6","text":"<ul> <li>A one-dimensional labeled array capable of holding any data type.</li> <li>Created by passing a list or array of data, optionally with an index.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#dataframe","title":"DataFrame\u00b6","text":"<ul> <li>A two-dimensional labeled data structure with columns of potentially different types.</li> <li>You can think of it like a table (similar to a spreadsheet or SQL table). You can create a DataFrame from dictionaries, lists, NumPy arrays, and more.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#multiindex","title":"MultiIndex\u00b6","text":"<p>A MultiIndex (hierarchical index) allows you to store and work with higher-dimensional data in a 2D table by using multiple index levels on rows (and/or columns).</p>"},{"location":"Cheat-Sheets/Pandas/#selection-indexing","title":"Selection &amp; Indexing\u00b6","text":"<p>You can select data in a <code>DataFrame</code> or <code>Series</code> in multiple ways: by label, by position, or by boolean masking.</p>"},{"location":"Cheat-Sheets/Pandas/#reshaping-tidy-data","title":"Reshaping &amp; Tidy Data\u00b6","text":"<p>Common operations to reshape a DataFrame include <code>melt</code> (going from wide to long format), <code>pivot</code> (long to wide), and <code>concat</code>. \"Tidy\" data means each variable has its own column, each observation its own row.</p>"},{"location":"Cheat-Sheets/Pandas/#subsetting-data","title":"Subsetting Data\u00b6","text":"<p>Subsetting data means taking rows or columns that meet certain criteria. You can slice by row/column positions, select columns by name, or filter by conditions.</p>"},{"location":"Cheat-Sheets/Pandas/#summarizing-descriptive-statistics","title":"Summarizing &amp; Descriptive Statistics\u00b6","text":"<p>Pandas provides convenient methods to get summary statistics: <code>mean()</code>, <code>count()</code>, <code>describe()</code>, etc.</p>"},{"location":"Cheat-Sheets/Pandas/#handling-missing-data","title":"Handling Missing Data\u00b6","text":"<p>Pandas provides tools like <code>isnull()</code>, <code>notnull()</code>, <code>dropna()</code>, and <code>fillna()</code> to handle missing values.</p>"},{"location":"Cheat-Sheets/Pandas/#making-new-columns","title":"Making New Columns\u00b6","text":"<p>You can create or modify columns using vectorized operations, <code>assign()</code>, or direct assignment.</p>"},{"location":"Cheat-Sheets/Pandas/#group-data","title":"Group Data\u00b6","text":"<p>Grouping is done with <code>df.groupby()</code>, allowing you to compute aggregates on partitions of the data.</p>"},{"location":"Cheat-Sheets/Pandas/#windows-rolling-expanding","title":"Windows (Rolling, Expanding)\u00b6","text":"<p>Rolling windows let you apply operations over a fixed window size. Expanding windows accumulate all previous values.</p>"},{"location":"Cheat-Sheets/Pandas/#combining-merging-data","title":"Combining &amp; Merging Data\u00b6","text":"<p>You can combine multiple DataFrames using <code>pd.concat()</code>, <code>pd.merge()</code>, and different types of joins (<code>left</code>, <code>right</code>, <code>inner</code>, <code>outer</code>).</p>"},{"location":"Cheat-Sheets/Pandas/#plotting","title":"Plotting\u00b6","text":"<p>Pandas integrates well with <code>matplotlib</code> for quick plotting. You can do <code>df.plot()</code> or create specific plots like <code>df.plot.scatter()</code>, <code>df.plot.hist()</code>, etc.</p>"},{"location":"Cheat-Sheets/Pandas/#dataframe-series-info","title":"DataFrame &amp; Series Info\u00b6","text":"<p>You can retrieve metadata and info about a DataFrame or Series: shape, columns, index, dtypes, and null counts.</p>"},{"location":"Cheat-Sheets/Pandas/#applying-functions","title":"Applying Functions\u00b6","text":"<p>Use <code>df.apply()</code> to apply a function column-wise or row-wise, and <code>df.applymap()</code> for element-wise operations on an entire DataFrame. <code>Series.apply()</code> is element-wise by default.</p>"},{"location":"Cheat-Sheets/Pandas/#data-alignment","title":"Data Alignment\u00b6","text":"<p>When performing operations on two Series or DataFrames with different indexes, pandas aligns the data based on labels by default. Missing labels become <code>NaN</code>. You can use fill parameters to handle this.</p>"},{"location":"Cheat-Sheets/Pandas/#io-reading-writing-data","title":"I/O: Reading &amp; Writing Data\u00b6","text":"<p>You can read data from various formats (CSV, Excel, SQL, etc.) and write DataFrames out similarly.</p>"},{"location":"Cheat-Sheets/Pandas/#method-chaining","title":"Method Chaining\u00b6","text":"<p>Method chaining involves writing multiple pandas operations in a single expression by \"chaining\" them with dots (<code>.</code>). This often improves readability and can reduce the need for intermediate variables.</p>"},{"location":"Cheat-Sheets/Pandas/#sampling-nlargest-and-nsmallest","title":"Sampling, <code>nlargest</code>, and <code>nsmallest</code>\u00b6","text":"<ul> <li>df.sample(n=... or frac=...): Randomly sample a certain number or fraction of rows.</li> <li>df.nlargest(n, 'column'): Select the top n rows based on a column value, sorted descending.</li> <li>df.nsmallest(n, 'column'): Select the bottom n rows based on a column value, sorted ascending.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#drop-duplicates","title":"Drop Duplicates\u00b6","text":"<ul> <li>df.drop_duplicates(): Removes duplicate rows (or specified subset of columns).</li> <li>By default, it keeps the first occurrence and drops the rest. You can change this behavior with <code>keep='last'</code> or <code>keep=False</code>.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#value_counts-and-counting-uniques","title":"<code>value_counts</code> and Counting Uniques\u00b6","text":"<ul> <li>Series.value_counts(): Shows unique values in a Series and their frequency counts.</li> <li>Series.nunique(): Counts the number of unique values in the Series.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#regex-filtering","title":"Regex Filtering\u00b6","text":"<p>df.filter(regex=...): Allows selecting columns that match a certain regular expression pattern. Useful when you have many columns sharing naming patterns.</p>"},{"location":"Cheat-Sheets/Pandas/#using-dfquery","title":"Using <code>df.query(...)</code>\u00b6","text":"<p><code>df.query(expr)</code> uses a string expression to filter rows in a DataFrame. Column names must be valid Python identifiers (letters, numbers, underscores, no spaces) or else be backticked (e.g., `my column`).</p>"},{"location":"Cheat-Sheets/Pandas/#pivot-vs-pivot_table","title":"pivot vs pivot_table\u00b6","text":"<ul> <li>pivot: Reshapes a DataFrame without performing an aggregation. It requires that each index/column pair is unique.</li> <li>pivot_table: Allows grouping and aggregation when you have duplicate indices/columns.</li> </ul>"},{"location":"Cheat-Sheets/Pandas/#shift-rank-and-cumulative-operations","title":"shift, rank, and Cumulative Operations\u00b6","text":"<ul> <li>shift: Moves index by specified periods, introducing NaNs for missing positions.</li> <li>rank: Assigns numeric rank to each entry in a Series (with optional tie-breaking methods).</li> <li>cumsum, cummax, cummin, cumprod: Cumulative sums, maxima, minima, and products over rows or columns.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/","title":"PySpark Cheat Sheet","text":"<ul> <li>PySpark Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>SparkSession</li> <li>SparkContext</li> <li>Stopping SparkSession/SparkContext</li> </ul> </li> <li>Data Loading<ul> <li>Loading from Text Files</li> <li>Loading from CSV Files</li> <li>Loading from Parquet Files</li> <li>Loading from ORC Files</li> <li>Loading from Avro Files</li> <li>Loading from JDBC</li> <li>Loading from Delta Lake</li> </ul> </li> <li>DataFrames<ul> <li>Creating DataFrames</li> <li>DataFrame Operations</li> <li>Applying Python Functions (UDFs)</li> <li>Applying Pandas UDFs (Vectorized UDFs)</li> <li>GroupBy Operations</li> <li>SQL Queries</li> </ul> </li> <li>RDDs (Resilient Distributed Datasets)<ul> <li>Creating RDDs</li> <li>RDD Transformations</li> <li>RDD Actions</li> <li>Pair RDDs</li> </ul> </li> <li>Writing Data<ul> <li>Writing DataFrames</li> <li>Writing RDDs</li> </ul> </li> <li>Spark SQL<ul> <li>Creating Tables</li> <li>Inserting Data</li> <li>Selecting Data</li> <li>Filtering Data</li> <li>Aggregating Data</li> <li>Joining Tables</li> <li>Window Functions in SQL</li> </ul> </li> <li>Spark MLlib<ul> <li>Data Preparation</li> <li>Feature Extraction</li> <li>Feature Scaling</li> <li>Feature Selection</li> <li>Classification</li> <li>Regression</li> <li>Clustering</li> <li>Recommendation</li> <li>Evaluation</li> <li>Cross-Validation</li> <li>Pipelines</li> <li>Model Persistence</li> </ul> </li> <li>Structured Streaming<ul> <li>Reading Data</li> <li>Processing Data</li> <li>Writing Data</li> <li>Available Output Modes</li> <li>Available Sinks</li> </ul> </li> <li>Performance Tuning<ul> <li>Data Partitioning</li> <li>Caching</li> <li>Broadcast Variables</li> <li>Accumulators</li> <li>Memory Management</li> <li>Shuffle Optimization</li> <li>Data Serialization</li> <li>Garbage Collection</li> </ul> </li> <li>Common Issues and Debugging</li> <li>Spark Configuration<ul> <li>SparkConf Options</li> </ul> </li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the PySpark API, covering essential concepts, code snippets, and best practices for efficient data processing and machine learning with Apache Spark. It aims to be a one-stop reference for common tasks.</p>"},{"location":"Cheat-Sheets/PySpark/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/PySpark/#installation","title":"Installation","text":"<pre><code>pip install pyspark\n</code></pre> <p>Consider using a virtual environment:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Linux/macOS\nvenv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#sparksession","title":"SparkSession","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"MyPySparkApp\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\n# To use an existing SparkContext:\n# spark = SparkSession(sparkContext=sc)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#sparkcontext","title":"SparkContext","text":"<pre><code>from pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setAppName(\"MyPySparkApp\").setMaster(\"local[*]\")\nsc = SparkContext(conf=conf)\n\n# To use SparkSession:\n# from pyspark.sql import SparkSession\n# spark = SparkSession(sparkContext=sc)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#stopping-sparksessionsparkcontext","title":"Stopping SparkSession/SparkContext","text":"<pre><code>spark.stop()  # For SparkSession\nsc.stop()     # For SparkContext\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#data-loading","title":"Data Loading","text":""},{"location":"Cheat-Sheets/PySpark/#loading-from-text-files","title":"Loading from Text Files","text":"<pre><code># SparkContext\nlines = sc.textFile(\"path/to/my/textfile.txt\")\n\n# SparkSession\ndf = spark.read.text(\"path/to/my/textfile.txt\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-csv-files","title":"Loading from CSV Files","text":"<pre><code># SparkSession\ndf = spark.read.csv(\"path/to/my/csvfile.csv\", header=True, inferSchema=True)```\n\n### Loading from JSON Files\n\n```python\n# SparkSession\ndf = spark.read.json(\"path/to/my/jsonfile.json\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-parquet-files","title":"Loading from Parquet Files","text":"<pre><code># SparkSession\ndf = spark.read.parquet(\"path/to/my/parquetfile.parquet\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-orc-files","title":"Loading from ORC Files","text":"<pre><code># SparkSession\ndf = spark.read.orc(\"path/to/my/orcfile.orc\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-avro-files","title":"Loading from Avro Files","text":"<pre><code># SparkSession\ndf = spark.read.format(\"avro\").load(\"path/to/my/avrofile.avro\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-jdbc","title":"Loading from JDBC","text":"<pre><code># SparkSession\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydatabase\") \\\n    .option(\"dbtable\", \"mytable\") \\\n    .option(\"user\", \"myuser\") \\\n    .option(\"password\", \"mypassword\") \\\n    .load()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#loading-from-delta-lake","title":"Loading from Delta Lake","text":"<pre><code>from delta.tables import DeltaTable\n\ndeltaTable = DeltaTable.forPath(spark, \"path/to/my/delta_table\")\ndf = deltaTable.toDF()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#dataframes","title":"DataFrames","text":""},{"location":"Cheat-Sheets/PySpark/#creating-dataframes","title":"Creating DataFrames","text":"<p>From RDD:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Example\").getOrCreate()\ndata = [(\"Alice\", 30), (\"Bob\", 25)]\nrdd = spark.sparkContext.parallelize(data)\ndf = spark.createDataFrame(rdd, schema=[\"Name\", \"Age\"])\n</code></pre> <p>From List of Dictionaries:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Example\").getOrCreate()\ndata = [{\"Name\": \"Alice\", \"Age\": 30}, {\"Name\": \"Bob\", \"Age\": 25}]\ndf = spark.createDataFrame(data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#dataframe-operations","title":"DataFrame Operations","text":"<ul> <li><code>df.show()</code>: Displays the DataFrame.</li> <li><code>df.printSchema()</code>: Prints the schema of the DataFrame.</li> <li><code>df.columns</code>: Returns a list of column names.</li> <li><code>df.count()</code>: Returns the number of rows.</li> <li><code>df.describe()</code>: Computes summary statistics.</li> <li><code>df.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\", \"25%\", \"50%\", \"75%\")</code>: Computes descriptive statistics.</li> <li><code>df.select(\"column1\", \"column2\")</code>: Selects specific columns.</li> <li><code>df.withColumn(\"new_column\", df[\"column1\"] + df[\"column2\"])</code>: Adds a new column.</li> <li><code>df.withColumnRenamed(\"old_name\", \"new_name\")</code>: Renames a column.</li> <li><code>df.drop(\"column1\")</code>: Drops a column.</li> <li><code>df.filter(df[\"age\"] &gt; 25)</code>: Filters rows based on a condition.</li> <li><code>df.where(df[\"age\"] &gt; 25)</code>: Another way to filter rows.</li> <li><code>df.groupBy(\"column1\").count()</code>: Groups data and counts occurrences.</li> <li><code>df.orderBy(\"column1\", ascending=False)</code>: Orders data by a column.</li> <li><code>df.sort(\"column1\", ascending=False)</code>: Another way to order data.</li> <li><code>df.limit(10)</code>: Limits the number of rows.</li> <li><code>df.distinct()</code>: Removes duplicate rows.</li> <li><code>df.union(other_df)</code>: Unions two DataFrames (requires same schema).</li> <li><code>df.unionByName(other_df)</code>: Unions two DataFrames by column name.</li> <li><code>df.intersect(other_df)</code>: Returns the intersection of two DataFrames.</li> <li><code>df.subtract(other_df)</code>: Returns the rows in <code>df</code> but not in <code>other_df</code>.</li> <li><code>df.join(other_df, df[\"key\"] == other_df[\"key\"], how=\"inner\")</code>: Joins two DataFrames.</li> <li><code>df.crossJoin(other_df)</code>: Performs a Cartesian product join.</li> <li><code>df.agg({\"age\": \"avg\"})</code>: Performs aggregation functions (avg, min, max, sum, etc.).</li> <li><code>df.rollup(\"column1\", \"column2\")</code>: Creates rollup aggregates.</li> <li><code>df.cube(\"column1\", \"column2\")</code>: Creates cube aggregates.</li> <li><code>df.pivot(\"column1\", values=[\"value1\", \"value2\"])</code>: Pivots a DataFrame.</li> <li><code>df.sample(withReplacement=False, fraction=0.5, seed=None)</code>: Samples a fraction of rows.</li> <li><code>df.randomSplit([0.8, 0.2], seed=None)</code>: Splits the DataFrame into multiple DataFrames randomly.</li> <li><code>df.cache()</code>: Caches the DataFrame in memory.</li> <li><code>df.persist(StorageLevel.MEMORY_AND_DISK)</code>: Persists the DataFrame with a specific storage level.</li> <li><code>df.unpersist()</code>: Removes the DataFrame from the cache.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#applying-python-functions-udfs","title":"Applying Python Functions (UDFs)","text":"<pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef to_upper(s):\n    return s.upper()\n\nto_upper_udf = udf(to_upper, StringType())\n\ndf = df.withColumn(\"upper_name\", to_upper_udf(df[\"Name\"]))\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#applying-pandas-udfs-vectorized-udfs","title":"Applying Pandas UDFs (Vectorized UDFs)","text":"<pre><code>from pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import StringType\nimport pandas as pd\n\n@pandas_udf(StringType())\ndef to_upper_pandas(series: pd.Series) -&gt; pd.Series:\n    return series.str.upper()\n\ndf = df.withColumn(\"upper_name\", to_upper_pandas(df[\"Name\"]))\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#groupby-operations","title":"GroupBy Operations","text":"<pre><code>from pyspark.sql.functions import avg, max, min, sum, count\n\n# Group by a single column\ngrouped_df = df.groupBy(\"Department\")\ngrouped_df.count().show()\n\n# Group by multiple columns\ngrouped_df = df.groupBy(\"Department\", \"City\")\ngrouped_df.agg(avg(\"Salary\"), sum(\"Bonus\")).show()\n\n# Applying aggregation functions\nfrom pyspark.sql.functions import col\ndf.groupBy(\"Department\") \\\n  .agg(avg(col(\"Salary\")).alias(\"Average Salary\"),\n       sum(col(\"Bonus\")).alias(\"Total Bonus\")) \\\n  .show()\n\n# Window functions\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import rank, dense_rank, row_number\n\nwindowSpec  = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\ndf.withColumn(\"rank\",rank().over(windowSpec)).show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#sql-queries","title":"SQL Queries","text":"<p>Register DataFrame as a temporary view:</p> <pre><code>df.createOrReplaceTempView(\"my_table\")\n</code></pre> <p>Run SQL queries:</p> <pre><code>result_df = spark.sql(\"SELECT Name, Age FROM my_table WHERE Age &gt; 25\")\nresult_df.show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#rdds-resilient-distributed-datasets","title":"RDDs (Resilient Distributed Datasets)","text":""},{"location":"Cheat-Sheets/PySpark/#creating-rdds","title":"Creating RDDs","text":"<p>From a List:</p> <pre><code>data = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n</code></pre> <p>From a Text File:</p> <pre><code>rdd = sc.textFile(\"path/to/my/textfile.txt\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#rdd-transformations","title":"RDD Transformations","text":"<ul> <li><code>rdd.map(lambda x: x * 2)</code>: Applies a function to each element.</li> <li><code>rdd.filter(lambda x: x &gt; 2)</code>: Filters elements based on a condition.</li> <li><code>rdd.flatMap(lambda x: x.split())</code>: Flattens and maps elements.</li> <li><code>rdd.distinct()</code>: Removes duplicate elements.</li> <li><code>rdd.sample(withReplacement=False, fraction=0.5)</code>: Samples elements.</li> <li><code>rdd.union(other_rdd)</code>: Unions two RDDs.</li> <li><code>rdd.intersection(other_rdd)</code>: Intersects two RDDs.</li> <li><code>rdd.subtract(other_rdd)</code>: Subtracts one RDD from another.</li> <li><code>rdd.cartesian(other_rdd)</code>: Computes the Cartesian product of two RDDs.</li> <li><code>rdd.sortBy(lambda x: x, ascending=False)</code>: Sorts elements.</li> <li><code>rdd.repartition(numPartitions=4)</code>: Changes the number of partitions.</li> <li><code>rdd.coalesce(numPartitions=1)</code>: Decreases the number of partitions.</li> <li><code>rdd.pipe(command)</code>: Pipes each element to a shell command.</li> <li><code>rdd.zip(other_rdd)</code>: Zips two RDDs together.</li> <li><code>rdd.zipWithIndex()</code>: Zips the RDD with its element indices.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#rdd-actions","title":"RDD Actions","text":"<ul> <li><code>rdd.collect()</code>: Returns all elements as a list.</li> <li><code>rdd.count()</code>: Returns the number of elements.</li> <li><code>rdd.first()</code>: Returns the first element.</li> <li><code>rdd.take(3)</code>: Returns the first N elements.</li> <li><code>rdd.top(3)</code>: Returns the top N elements.</li> <li><code>rdd.reduce(lambda x, y: x + y)</code>: Reduces elements using a function.</li> <li><code>rdd.fold(zeroValue, op)</code>: Folds elements using a function and a zero value.</li> <li><code>rdd.aggregate(zeroValue, seqOp, combOp)</code>: Aggregates elements using sequence and combination functions.</li> <li><code>rdd.foreach(lambda x: print(x))</code>: Applies a function to each element.</li> <li><code>rdd.saveAsTextFile(\"path/to/output\")</code>: Saves the RDD as a text file.</li> <li><code>rdd.saveAsPickleFile(\"path/to/output\")</code>: Saves the RDD as a serialized Python object file.</li> <li><code>rdd.countByKey()</code>: Returns the count of each key (for pair RDDs).</li> <li><code>rdd.collectAsMap()</code>: Returns the elements as a dictionary (for pair RDDs).</li> <li><code>rdd.lookup(key)</code>: Returns the values for a given key (for pair RDDs).</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#pair-rdds","title":"Pair RDDs","text":"<ul> <li><code>rdd.map(lambda x: (x, 1))</code>: Creates a pair RDD.</li> <li><code>rdd.reduceByKey(lambda x, y: x + y)</code>: Reduces values for each key.</li> <li><code>rdd.groupByKey()</code>: Groups values for each key.</li> <li><code>rdd.aggregateByKey(zeroValue, seqFunc, combFunc)</code>: Aggregates values for each key.</li> <li><code>rdd.foldByKey(zeroValue, func)</code>: Folds values for each key.</li> <li><code>rdd.combineByKey(createCombiner, mergeValue, mergeCombiners)</code>: Generic combine function for each key.</li> <li><code>rdd.sortByKey()</code>: Sorts by key.</li> <li><code>rdd.join(other_rdd)</code>: Joins two pair RDDs.</li> <li><code>rdd.leftOuterJoin(other_rdd)</code>: Performs a left outer join.</li> <li><code>rdd.rightOuterJoin(other_rdd)</code>: Performs a right outer join.</li> <li><code>rdd.fullOuterJoin(other_rdd)</code>: Performs a full outer join.</li> <li><code>rdd.cogroup(other_rdd)</code>: Groups values for each key in multiple RDDs.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#writing-data","title":"Writing Data","text":""},{"location":"Cheat-Sheets/PySpark/#writing-dataframes","title":"Writing DataFrames","text":"<p>To CSV:</p> <pre><code>df.write.csv(\"path/to/output/csv\", header=True, mode=\"overwrite\")\n</code></pre> <p>To JSON:</p> <pre><code>df.write.json(\"path/to/output/json\", mode=\"overwrite\")\n</code></pre> <p>To Parquet:</p> <pre><code>df.write.parquet(\"path/to/output/parquet\", mode=\"overwrite\")\n</code></pre> <p>To ORC:</p> <pre><code>df.write.orc(\"path/to/output/orc\", mode=\"overwrite\")\n</code></pre> <p>To Avro:</p> <pre><code>df.write.format(\"avro\").save(\"path/to/output/avro\", mode=\"overwrite\")\n</code></pre> <p>To JDBC:</p> <pre><code>df.write.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydatabase\") \\\n    .option(\"dbtable\", \"mytable\") \\\n    .option(\"user\", \"myuser\") \\\n    .option(\"password\", \"mypassword\") \\\n    .mode(\"overwrite\") \\\n    .save()\n</code></pre> <p>To Delta Lake:</p> <pre><code>df.write.format(\"delta\").mode(\"overwrite\").save(\"path/to/delta_table\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#writing-rdds","title":"Writing RDDs","text":"<pre><code>rdd.saveAsTextFile(\"path/to/output\")\nrdd.saveAsPickleFile(\"path/to/output\")\nrdd.saveAsSequenceFile(\"path/to/output\") # For Hadoop SequenceFile format\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#spark-sql","title":"Spark SQL","text":""},{"location":"Cheat-Sheets/PySpark/#creating-tables","title":"Creating Tables","text":"<p>From DataFrame:</p> <pre><code>df.write.saveAsTable(\"my_table\")\n</code></pre> <p>Using SQL:</p> <pre><code>spark.sql(\"CREATE TABLE my_table (name STRING, age INT) USING parquet\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#inserting-data","title":"Inserting Data","text":"<p>From DataFrame:</p> <pre><code>df.write.insertInto(\"my_table\")\n</code></pre> <p>Using SQL:</p> <pre><code>spark.sql(\"INSERT INTO my_table VALUES ('Alice', 30)\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#selecting-data","title":"Selecting Data","text":"<pre><code>spark.sql(\"SELECT * FROM my_table\").show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#filtering-data","title":"Filtering Data","text":"<pre><code>spark.sql(\"SELECT * FROM my_table WHERE age &gt; 25\").show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#aggregating-data","title":"Aggregating Data","text":"<pre><code>spark.sql(\"SELECT name, AVG(age) FROM my_table GROUP BY name\").show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#joining-tables","title":"Joining Tables","text":"<pre><code>spark.sql(\"SELECT * FROM table1 JOIN table2 ON table1.key = table2.key\").show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#window-functions-in-sql","title":"Window Functions in SQL","text":"<pre><code>spark.sql(\"\"\"\nSELECT\n    name,\n    age,\n    RANK() OVER (ORDER BY age DESC) as age_rank\nFROM\n    my_table\n\"\"\").show()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#spark-mllib","title":"Spark MLlib","text":""},{"location":"Cheat-Sheets/PySpark/#data-preparation","title":"Data Preparation","text":"<pre><code>from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n\n# StringIndexer\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nindexed_df = indexer.fit(df).transform(df)\n\n# VectorAssembler\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\noutput_df = assembler.transform(indexed_df)\n\n# StandardScaler\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=True, withMean=True)\nscalerModel = scaler.fit(output_df)\nscaled_df = scalerModel.transform(output_df)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#feature-extraction","title":"Feature Extraction","text":"<ul> <li><code>Tokenizer</code>: Splits strings into words.</li> <li><code>StopWordsRemover</code>: Removes stop words.</li> <li><code>CountVectorizer</code>: Converts text documents to vectors of term counts.</li> <li><code>IDF</code>: Computes Inverse Document Frequency.</li> <li><code>Word2Vec</code>: Learns vector representations of words.</li> <li><code>NGram</code>: Generates n-grams from input sequences.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#feature-scaling","title":"Feature Scaling","text":"<ul> <li><code>StandardScaler</code>: Standardizes features by removing the mean and scaling to unit variance.</li> <li><code>MinMaxScaler</code>: Transforms features by scaling each feature to a given range.</li> <li><code>MaxAbsScaler</code>: Scales each feature to the [-1, 1] range by dividing through the largest maximum absolute value in each feature.</li> <li><code>Normalizer</code>: Normalizes each sample to unit norm.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#feature-selection","title":"Feature Selection","text":"<ul> <li><code>VectorSlicer</code>: Creates a new feature vector by selecting a subset of features from an existing vector.</li> <li><code>RFormula</code>: Implements the R formula string syntax for selecting features.</li> <li><code>PCA</code>: Reduces the dimensionality of feature vectors using Principal Component Analysis.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#classification","title":"Classification","text":"<p>Logistic Regression:</p> <pre><code>from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nmodel = lr.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Decision Tree:</p> <pre><code>from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\nmodel = dt.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Random Forest:</p> <pre><code>from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\nmodel = rf.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Gradient-Boosted Trees (GBT):</p> <pre><code>from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\")\nmodel = gbt.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Multilayer Perceptron Classifier (MLPC):</p> <pre><code>from pyspark.ml.classification import MultilayerPerceptronClassifier\n\nlayers = [4, 5, 4, 3] # Input size, hidden layers, output size\nmlp = MultilayerPerceptronClassifier(layers=layers, featuresCol='features', labelCol='label')\nmodel = mlp.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#regression","title":"Regression","text":"<p>Linear Regression:</p> <pre><code>from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\nmodel = lr.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Decision Tree Regression:</p> <pre><code>from pyspark.ml.regression import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\nmodel = dt.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Random Forest Regression:</p> <pre><code>from pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\nmodel = rf.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre> <p>Gradient-Boosted Trees (GBT) Regression:</p> <pre><code>from pyspark.ml.regression import GBTRegressor\n\ngbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\nmodel = gbt.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#clustering","title":"Clustering","text":"<p>K-Means:</p> <pre><code>from pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=3, featuresCol=\"features\")\nmodel = kmeans.fit(data)\npredictions = model.transform(data)\n</code></pre> <p>Gaussian Mixture Model (GMM):</p> <pre><code>from pyspark.ml.clustering import GaussianMixture\n\ngmm = GaussianMixture().setK(2).setSeed(538009335)\nmodel = gmm.fit(data)\npredictions = model.transform(data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#recommendation","title":"Recommendation","text":"<p>Alternating Least Squares (ALS):</p> <pre><code>from pyspark.ml.recommendation import ALS\n\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\nmodel = als.fit(training_data)\npredictions = model.transform(test_data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#evaluation","title":"Evaluation","text":"<p>Classification Metrics:</p> <pre><code>from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %s\" % (accuracy))\n</code></pre> <p>Regression Metrics:</p> <pre><code>from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"RMSE = %s\" % (rmse))\n</code></pre> <p>Clustering Metrics:</p> <pre><code>from pyspark.ml.evaluation import ClusteringEvaluator\n\nevaluator = ClusteringEvaluator(featuresCol=\"features\")\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#cross-validation","title":"Cross-Validation","text":"<pre><code>from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .addGrid(lr.fitIntercept, [False, True]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)\n\ncvModel = crossval.fit(training_data)\npredictions = cvModel.transform(test_data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#pipelines","title":"Pipelines","text":"<pre><code>from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\n\n# Define stages\nindexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n\n# Create pipeline\npipeline = Pipeline(stages=[indexer, assembler, lr])\n\n# Fit the pipeline\nmodel = pipeline.fit(training_data)\n\n# Transform the data\npredictions = model.transform(test_data)\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#model-persistence","title":"Model Persistence","text":"<pre><code>model.save(\"path/to/my/model\")\nloaded_model = PipelineModel.load(\"path/to/my/model\")\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#structured-streaming","title":"Structured Streaming","text":""},{"location":"Cheat-Sheets/PySpark/#reading-data","title":"Reading Data","text":"<pre><code>df = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#processing-data","title":"Processing Data","text":"<pre><code>from pyspark.sql.functions import explode, split\n\nwords = df.select(explode(split(df.value, \" \")).alias(\"word\"))\nwordCounts = words.groupBy(\"word\").count()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#writing-data_1","title":"Writing Data","text":"<pre><code>query = wordCounts.writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\nquery.awaitTermination()\n</code></pre>"},{"location":"Cheat-Sheets/PySpark/#available-output-modes","title":"Available Output Modes","text":"<ul> <li><code>\"append\"</code>: Only new rows are written to the sink.</li> <li><code>\"complete\"</code>: All rows are written to the sink every time there are updates.</li> <li><code>\"update\"</code>: Only updated rows are written to the sink.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#available-sinks","title":"Available Sinks","text":"<ul> <li><code>\"console\"</code>: Prints to the console.</li> <li><code>\"memory\"</code>: Stores the output in memory.</li> <li><code>\"parquet\"</code>, <code>\"csv\"</code>, <code>\"json\"</code>, <code>\"jdbc\"</code>: Writes to files or databases.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#performance-tuning","title":"Performance Tuning","text":""},{"location":"Cheat-Sheets/PySpark/#data-partitioning","title":"Data Partitioning","text":"<ul> <li>Use <code>repartition()</code> or <code>coalesce()</code> to control the number of partitions.</li> <li>Partition data based on frequently used keys.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#caching","title":"Caching","text":"<ul> <li>Use <code>cache()</code> or <code>persist()</code> to store intermediate results in memory or on disk.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#broadcast-variables","title":"Broadcast Variables","text":"<ul> <li>Use <code>sc.broadcast()</code> to broadcast small datasets to all executors.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#accumulators","title":"Accumulators","text":"<ul> <li>Use <code>sc.accumulator()</code> to create global counters.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#memory-management","title":"Memory Management","text":"<ul> <li>Tune <code>spark.executor.memory</code> and <code>spark.driver.memory</code> to allocate sufficient memory.</li> <li>Avoid creating large objects in the driver.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#shuffle-optimization","title":"Shuffle Optimization","text":"<ul> <li>Tune <code>spark.sql.shuffle.partitions</code> to control the number of shuffle partitions.</li> <li>Use <code>mapPartitions</code> to perform operations on each partition.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#data-serialization","title":"Data Serialization","text":"<ul> <li>Use Kryo serialization for better performance.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>Tune garbage collection settings to reduce GC overhead.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#common-issues-and-debugging","title":"Common Issues and Debugging","text":"<ul> <li>Out of Memory Errors: Increase executor memory or reduce the amount of data being processed.</li> <li>Slow Performance: Analyze the Spark UI to identify bottlenecks.</li> <li>Serialization Errors: Ensure that all objects being serialized are serializable.</li> <li>Data Skew: Partition data to distribute it evenly across executors.</li> <li>Driver OOM: Increase driver memory or reduce the amount of data being collected to the driver.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#spark-configuration","title":"Spark Configuration","text":""},{"location":"Cheat-Sheets/PySpark/#sparkconf-options","title":"SparkConf Options","text":"<ul> <li><code>spark.app.name</code>: Application name.</li> <li><code>spark.master</code>: Spark master URL.</li> <li><code>spark.executor.memory</code>: Memory per executor.</li> <li><code>spark.driver.memory</code>: Memory for the driver process.</li> <li><code>spark.executor.cores</code>: Number of cores per executor.</li> <li><code>spark.default.parallelism</code>: Default number of partitions.</li> <li><code>spark.sql.shuffle.partitions</code>: Number of partitions to use when shuffling data for joins or aggregations.</li> <li><code>spark.serializer</code>: Serializer class name (e.g., <code>org.apache.spark.serializer.KryoSerializer</code>).</li> <li><code>spark.driver.maxResultSize</code>: Maximum size of the result that the driver can collect.</li> <li><code>spark.kryoserializer.buffer.max</code>: Maximum buffer size for Kryo serialization.</li> <li><code>spark.sql.adaptive.enabled</code>: Enables adaptive query execution.</li> <li><code>spark.sql.adaptive.coalescePartitions.enabled</code>: Enables adaptive partition coalescing.</li> </ul>"},{"location":"Cheat-Sheets/PySpark/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Use meaningful names for variables and functions.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use a consistent coding style.</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Use appropriate data types for your data.</li> <li>Optimize your Spark configuration for your workload.</li> <li>Use caching to improve performance.</li> <li>Use partitioning to distribute data evenly.</li> <li>Avoid shuffling data unnecessarily.</li> <li>Use broadcast variables for small datasets.</li> <li>Use accumulators for global counters.</li> <li>Use the Spark UI to monitor your application.</li> <li>Use a logging framework to log events and errors.</li> <li>Use a security framework to protect your data.</li> <li>Use a resource manager (e.g., YARN, Mesos, Kubernetes) to manage your cluster.</li> <li>Use a deployment tool to deploy your application to production.</li> <li>Monitor your application for performance issues.</li> <li>Use a CDN (Content Delivery Network) for static files.</li> <li>Optimize database queries.</li> <li>Use asynchronous tasks for long-running operations.</li> <li>Implement proper logging and error handling.</li> <li>Regularly update PySpark and its dependencies.</li> <li>Use a security scanner to identify potential vulnerabilities.</li> <li>Follow security best practices.</li> <li>Use a reverse proxy like Nginx or Apache in front of your Spark application.</li> <li>Use a load balancer for high availability.</li> <li>Automate deployments using tools like Fabric or Ansible.</li> <li>Use a monitoring tool like Prometheus or Grafana.</li> <li>Implement health checks for your application.</li> <li>Use a CDN for static assets.</li> <li>Cache frequently accessed data.</li> <li>Use a database connection pool.</li> <li>Optimize your database queries.</li> <li>Use a task queue for long-running tasks.</li> <li>Use a background worker for asynchronous tasks.</li> <li>Use a message queue for inter-process communication.</li> <li>Use a service discovery tool for microservices.</li> <li>Use a containerization tool like Docker.</li> <li>Use an orchestration tool like Kubernetes.</li> <li>Use Delta Lake for reliable data lakes.</li> <li>Use Apache Arrow for faster data transfer between Python and Spark.</li> <li>Use vectorized UDFs for better performance.</li> <li>Use adaptive query execution (AQE) to optimize queries at runtime.</li> <li>Use cost-based optimization (CBO) to choose the best query plan.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/","title":"PyTorch Cheat Sheet","text":"<ul> <li>PyTorch Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Importing PyTorch</li> </ul> </li> <li>Tensors<ul> <li>Creating Tensors</li> <li>Tensor Attributes</li> <li>Tensor Operations</li> <li>Data Types</li> <li>Device Management</li> <li>Moving Data Between CPU and GPU</li> </ul> </li> <li>Neural Networks<ul> <li>Defining a Model</li> <li>Layers</li> <li>Activation Functions</li> <li>Loss Functions</li> <li>Optimizers</li> <li>Optimizer Configuration</li> <li>Learning Rate Schedulers</li> <li>Metrics</li> </ul> </li> <li>Training<ul> <li>Training Loop</li> <li>DataLoaders</li> <li>Transforms</li> <li>Mixed Precision Training</li> </ul> </li> <li>Evaluation</li> <li>Prediction</li> <li>Saving and Loading Models<ul> <li>Save the Entire Model</li> <li>Load the Entire Model</li> <li>Save Model State Dictionary</li> <li>Load Model State Dictionary</li> </ul> </li> <li>CUDA (GPU Support)<ul> <li>Check CUDA Availability</li> <li>Set Device</li> <li>Move Tensors to GPU</li> <li>CUDA Best Practices</li> </ul> </li> <li>Distributed Training<ul> <li>DataParallel</li> <li>DistributedDataParallel (DDP)</li> <li>Distributed Data Loading</li> </ul> </li> <li>Autograd<ul> <li>Tracking Gradients</li> <li>Disabling Gradient Tracking</li> <li>Detaching Tensors</li> <li>Custom Autograd Functions</li> </ul> </li> <li>Data Augmentation</li> <li>Learning Rate Schedulers</li> <li>TensorBoard Integration</li> <li>ONNX Export</li> <li>TorchScript<ul> <li>Tracing</li> <li>Scripting</li> </ul> </li> <li>Deployment<ul> <li>Serving with Flask</li> <li>Serving with TorchServe</li> </ul> </li> <li>Distributed Training<ul> <li>DataParallel</li> <li>DistributedDataParallel (DDP)</li> <li>Gradient Clipping</li> <li>Weight Decay</li> <li>Early Stopping</li> <li>Learning Rate Finders</li> <li>Gradient Accumulation</li> </ul> </li> <li>Common Issues and Debugging</li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the PyTorch deep learning library, covering essential concepts, code snippets, and best practices for efficient model building, training, and deployment. It aims to be a one-stop reference for common tasks.</p>"},{"location":"Cheat-Sheets/PyTorch/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/PyTorch/#installation","title":"Installation","text":"<pre><code>pip install torch torchvision torchaudio\n</code></pre> <p>For CUDA support:</p> <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>Replace <code>cu121</code> with your CUDA version. Check the PyTorch website for the most up-to-date installation instructions.</p>"},{"location":"Cheat-Sheets/PyTorch/#importing-pytorch","title":"Importing PyTorch","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#tensors","title":"Tensors","text":""},{"location":"Cheat-Sheets/PyTorch/#creating-tensors","title":"Creating Tensors","text":"<p>From a List:</p> <pre><code>data = [1, 2, 3, 4, 5]\ntensor = torch.tensor(data)\n</code></pre> <p>From a NumPy Array:</p> <pre><code>import numpy as np\n\ndata = np.array([1, 2, 3, 4, 5])\ntensor = torch.from_numpy(data)\n</code></pre> <p>Zeros and Ones:</p> <pre><code>zeros = torch.zeros(size=(3, 4))\nones = torch.ones(size=(3, 4))\n</code></pre> <p>Full (fill with a specific value):</p> <pre><code>full = torch.full(size=(3, 4), fill_value=7)\n</code></pre> <p>Ranges:</p> <pre><code>arange = torch.arange(start=0, end=10, step=2) # 0, 2, 4, 6, 8\nlinspace = torch.linspace(start=0, end=1, steps=5) # 0.0, 0.25, 0.5, 0.75, 1.0\n</code></pre> <p>Random Numbers:</p> <pre><code>rand = torch.rand(size=(3, 4))  # Uniform distribution [0, 1)\nrandn = torch.randn(size=(3, 4)) # Standard normal distribution\nrandint = torch.randint(low=0, high=10, size=(3, 4)) # Integer values\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#tensor-attributes","title":"Tensor Attributes","text":"<pre><code>tensor.shape       # Shape of the tensor\ntensor.size()      # Same as shape\ntensor.ndim        # Number of dimensions\ntensor.dtype       # Data type of the tensor\ntensor.device      # Device where the tensor is stored (CPU or GPU)\ntensor.requires_grad # Whether gradients are tracked\ntensor.layout      # Memory layout (torch.strided, torch.sparse_coo)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#tensor-operations","title":"Tensor Operations","text":"<p>Arithmetic:</p> <pre><code>a = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\n\nc = a + b       # Element-wise addition\nd = a * b       # Element-wise multiplication\ne = a.add(b)    # In-place addition\nf = a.mul(b)    # In-place multiplication\ng = torch.add(a, b) # Functional form\nh = torch.mul(a, b) # Functional form\n</code></pre> <p>Slicing and Indexing:</p> <pre><code>tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\ntensor[0]       # First row\ntensor[:, 1]     # Second column\ntensor[0, 1]    # Element at row 0, column 1\ntensor[0:2, 1:3] # Slicing\n</code></pre> <p>Reshaping:</p> <pre><code>tensor = torch.arange(12)\nreshaped_tensor = tensor.reshape(3, 4)\ntransposed_tensor = tensor.T # For 2D tensors\nflattened_tensor = tensor.flatten() # Flatten to 1D\nviewed_tensor = tensor.view(3, 4) # Similar to reshape, but shares memory\n</code></pre> <p>Concatenation:</p> <pre><code>tensor1 = torch.tensor([[1, 2], [3, 4]])\ntensor2 = torch.tensor([[5, 6], [7, 8]])\n\nconcatenated_tensor = torch.cat((tensor1, tensor2), dim=0) # Concatenate along rows\nstacked_tensor = torch.stack((tensor1, tensor2), dim=0) # Stack along a new dimension\n</code></pre> <p>Matrix Multiplication:</p> <pre><code>a = torch.randn(3, 4)\nb = torch.randn(4, 5)\nc = torch.matmul(a, b) # Matrix multiplication\nd = a @ b # Matrix multiplication (shorthand)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#data-types","title":"Data Types","text":"<ul> <li><code>torch.float32</code> or <code>torch.float</code>: 32-bit floating point</li> <li><code>torch.float64</code> or <code>torch.double</code>: 64-bit floating point</li> <li><code>torch.float16</code> or <code>torch.half</code>: 16-bit floating point</li> <li><code>torch.bfloat16</code>: BFloat16 floating point (useful for mixed precision)</li> <li><code>torch.int8</code>: 8-bit integer (signed)</li> <li><code>torch.int16</code> or <code>torch.short</code>: 16-bit integer (signed)</li> <li><code>torch.int32</code> or <code>torch.int</code>: 32-bit integer (signed)</li> <li><code>torch.int64</code> or <code>torch.long</code>: 64-bit integer (signed)</li> <li><code>torch.uint8</code>: 8-bit integer (unsigned)</li> <li><code>torch.bool</code>: Boolean</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#device-management","title":"Device Management","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntensor = tensor.to(device)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#moving-data-between-cpu-and-gpu","title":"Moving Data Between CPU and GPU","text":"<pre><code>cpu_tensor = tensor.cpu()\ngpu_tensor = tensor.cuda() # or tensor.to('cuda')\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#neural-networks","title":"Neural Networks","text":""},{"location":"Cheat-Sheets/PyTorch/#defining-a-model","title":"Defining a Model","text":"<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = Net()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#layers","title":"Layers","text":"<ul> <li><code>nn.Linear</code>: Fully connected layer.</li> <li><code>nn.Conv1d</code>: 1D convolution layer.</li> <li><code>nn.Conv2d</code>: 2D convolution layer.</li> <li><code>nn.Conv3d</code>: 3D convolution layer.</li> <li><code>nn.ConvTranspose2d</code>: Transposed convolution layer (deconvolution).</li> <li><code>nn.MaxPool1d</code>, <code>nn.MaxPool2d</code>, <code>nn.MaxPool3d</code>: Max pooling layers.</li> <li><code>nn.AvgPool1d</code>, <code>nn.AvgPool2d</code>, <code>nn.AvgPool3d</code>: Average pooling layers.</li> <li><code>nn.AdaptiveAvgPool2d</code>: Adaptive average pooling layer.</li> <li><code>nn.ReLU</code>: ReLU activation function.</li> <li><code>nn.Sigmoid</code>: Sigmoid activation function.</li> <li><code>nn.Tanh</code>: Tanh activation function.</li> <li><code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code>, <code>nn.BatchNorm3d</code>: Batch normalization layers.</li> <li><code>nn.LayerNorm</code>: Layer normalization layer.</li> <li><code>nn.Dropout</code>: Dropout layer.</li> <li><code>nn.Embedding</code>: Embedding layer.</li> <li><code>nn.LSTM</code>: LSTM layer.</li> <li><code>nn.GRU</code>: GRU layer.</li> <li><code>nn.Transformer</code>: Transformer layer.</li> <li><code>nn.TransformerEncoder</code>, <code>nn.TransformerDecoder</code>: Transformer encoder and decoder layers.</li> <li><code>nn.MultiheadAttention</code>: Multi-head attention layer.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#activation-functions","title":"Activation Functions","text":"<ul> <li><code>torch.relu</code>: Rectified Linear Unit.</li> <li><code>torch.sigmoid</code>: Sigmoid function.</li> <li><code>torch.tanh</code>: Hyperbolic tangent function.</li> <li><code>torch.softmax</code>: Softmax function (for multi-class classification).</li> <li><code>torch.elu</code>: Exponential Linear Unit.</li> <li><code>torch.selu</code>: Scaled Exponential Linear Unit.</li> <li><code>torch.leaky_relu</code>: Leaky Rectified Linear Unit.</li> <li><code>torch.gelu</code>: Gaussian Error Linear Unit (GELU).</li> <li><code>torch.silu</code>: SiLU (Sigmoid Linear Unit) or Swish.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#loss-functions","title":"Loss Functions","text":"<ul> <li><code>nn.CrossEntropyLoss</code>: Cross-entropy loss (for multi-class classification).</li> <li><code>nn.BCELoss</code>: Binary cross-entropy loss (for binary classification).</li> <li><code>nn.BCEWithLogitsLoss</code>: Binary cross-entropy with logits (more stable).</li> <li><code>nn.MSELoss</code>: Mean squared error loss (for regression).</li> <li><code>nn.L1Loss</code>: Mean absolute error loss (for regression).</li> <li><code>nn.SmoothL1Loss</code>: Huber loss (for robust regression).</li> <li><code>nn.CTCLoss</code>: Connectionist Temporal Classification loss (for sequence labeling).</li> <li><code>nn.TripletMarginLoss</code>: Triplet margin loss (for learning embeddings).</li> <li><code>nn.CosineEmbeddingLoss</code>: Cosine embedding loss.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#optimizers","title":"Optimizers","text":"<ul> <li><code>optim.SGD</code>: Stochastic Gradient Descent.</li> <li><code>optim.Adam</code>: Adaptive Moment Estimation.</li> <li><code>optim.RMSprop</code>: Root Mean Square Propagation.</li> <li><code>optim.Adagrad</code>: Adaptive Gradient Algorithm.</li> <li><code>optim.Adadelta</code>: Adaptive Delta.</li> <li><code>optim.AdamW</code>: Adam with weight decay regularization.</li> <li><code>optim.SparseAdam</code>: Adam optimizer for sparse tensors.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#optimizer-configuration","title":"Optimizer Configuration","text":"<pre><code>optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<pre><code>from torch.optim.lr_scheduler import StepLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    # Training loop\n    scheduler.step()\n</code></pre> <p>Common Schedulers:</p> <ul> <li><code>StepLR</code>: Decays the learning rate by a factor every few steps.</li> <li><code>MultiStepLR</code>: Decays the learning rate at specified milestones.</li> <li><code>ExponentialLR</code>: Decays the learning rate exponentially.</li> <li><code>CosineAnnealingLR</code>: Uses a cosine annealing schedule.</li> <li><code>ReduceLROnPlateau</code>: Reduces the learning rate when a metric has stopped improving.</li> <li><code>CyclicLR</code>: Sets the learning rate cyclically.</li> <li><code>OneCycleLR</code>: Sets the learning rate according to the 1cycle policy.</li> <li><code>CosineAnnealingWarmRestarts</code>: Cosine annealing with warm restarts.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#metrics","title":"Metrics","text":"<ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-Score</li> <li>AUC (Area Under the Curve)</li> <li>IoU (Intersection over Union)</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#training","title":"Training","text":""},{"location":"Cheat-Sheets/PyTorch/#training-loop","title":"Training Loop","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Sample data\nX = torch.randn(100, 784)\ny = torch.randint(0, 10, (100,))\n\n# Create dataset and dataloader\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Model, loss, optimizer\nmodel = nn.Linear(784, 10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    for inputs, labels in dataloader:\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#dataloaders","title":"DataLoaders","text":"<pre><code>from torch.utils.data import Dataset, DataLoader\n\nclass MyDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ndataset = MyDataset(data, labels)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#transforms","title":"Transforms","text":"<pre><code>import torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\ntrainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n                                          shuffle=True, num_workers=4, pin_memory=True)\n</code></pre> <p>Common Augmentations:</p> <ul> <li><code>transforms.RandomHorizontalFlip</code>: Horizontally flips the image.</li> <li><code>transforms.RandomVerticalFlip</code>: Vertically flips the image.</li> <li><code>transforms.RandomRotation</code>: Rotates the image by a random angle.</li> <li><code>transforms.RandomAffine</code>: Applies random affine transformations.</li> <li><code>transforms.RandomPerspective</code>: Performs perspective transformation of the given image randomly with a given magnitude.</li> <li><code>transforms.RandomCrop</code>: Crops a random portion of the image.</li> <li><code>transforms.CenterCrop</code>: Crops the image from the center.</li> <li><code>transforms.ColorJitter</code>: Randomly changes the brightness, contrast, saturation, and hue of an image.</li> <li><code>transforms.RandomGrayscale</code>: Converts the image to grayscale with a certain probability.</li> <li><code>transforms.RandomErasing</code>: Randomly erases a rectangular region in the image.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>scaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(epochs):\n    for inputs, labels in dataloader:\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#evaluation","title":"Evaluation","text":"<pre><code>model.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # Disable gradient calculation\n    correct = 0\n    total = 0\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Accuracy: {100 * correct / total:.2f}%')\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#prediction","title":"Prediction","text":"<pre><code>model.eval()\nwith torch.no_grad():\n    input_tensor = torch.randn(1, 3, 224, 224).to(device)  # Example input\n    output = model(input_tensor)\n    predicted_class = torch.argmax(output).item()\n    print(f'Predicted class: {predicted_class}')\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#saving-and-loading-models","title":"Saving and Loading Models","text":""},{"location":"Cheat-Sheets/PyTorch/#save-the-entire-model","title":"Save the Entire Model","text":"<pre><code>torch.save(model, 'my_model.pth') # Saves the entire model object\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#load-the-entire-model","title":"Load the Entire Model","text":"<pre><code>model = torch.load('my_model.pth')\nmodel.eval()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#save-model-state-dictionary","title":"Save Model State Dictionary","text":"<pre><code>torch.save(model.state_dict(), 'model_state_dict.pth') # Saves only the model's learned parameters\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#load-model-state-dictionary","title":"Load Model State Dictionary","text":"<pre><code>model = Net()  # Instantiate the model\nmodel.load_state_dict(torch.load('model_state_dict.pth'))\nmodel.eval()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#cuda-gpu-support","title":"CUDA (GPU Support)","text":""},{"location":"Cheat-Sheets/PyTorch/#check-cuda-availability","title":"Check CUDA Availability","text":"<pre><code>torch.cuda.is_available()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#set-device","title":"Set Device","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#move-tensors-to-gpu","title":"Move Tensors to GPU","text":"<pre><code>tensor = tensor.to(device)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#cuda-best-practices","title":"CUDA Best Practices","text":"<ul> <li>Use pinned memory for data transfer: <code>torch.utils.data.DataLoader(..., pin_memory=True)</code></li> <li>Use asynchronous data transfer: <code>torch.cuda.Stream()</code></li> <li>Use mixed precision training: <code>torch.cuda.amp.autocast()</code> and <code>torch.cuda.amp.GradScaler()</code></li> <li>Use <code>torch.backends.cudnn.benchmark = True</code> for faster convolutions when input sizes are fixed.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#distributed-training","title":"Distributed Training","text":""},{"location":"Cheat-Sheets/PyTorch/#dataparallel","title":"DataParallel","text":"<pre><code>model = nn.DataParallel(model)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#distributeddataparallel-ddp","title":"DistributedDataParallel (DDP)","text":"<pre><code>import torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size):\n    setup(rank, world_size)\n\n    model = Net().to(rank)\n    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    # Training loop\n    cleanup()\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    mp.spawn(train,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#distributed-data-loading","title":"Distributed Data Loading","text":"<p>When using DDP, you'll want to use a <code>DistributedSampler</code> to ensure each process gets a unique subset of the data:</p> <pre><code>from torch.utils.data.distributed import DistributedSampler\n\ntrain_sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\ntrainloader = DataLoader(dataset, batch_size=32, shuffle=False, sampler=train_sampler) # shuffle=False is important here\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#autograd","title":"Autograd","text":""},{"location":"Cheat-Sheets/PyTorch/#tracking-gradients","title":"Tracking Gradients","text":"<pre><code>x = torch.randn(3, requires_grad=True)\ny = x + 2\nz = y * y * 3\nout = z.mean()\nout.backward()\nprint(x.grad)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#disabling-gradient-tracking","title":"Disabling Gradient Tracking","text":"<pre><code>with torch.no_grad():\n    y = x + 2\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#detaching-tensors","title":"Detaching Tensors","text":"<pre><code>y = x.detach()  # Creates a new tensor with the same content but no gradient history\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#custom-autograd-functions","title":"Custom Autograd Functions","text":"<pre><code>class MyReLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input &lt; 0] = 0\n        return grad_input\n\nmy_relu = MyReLU.apply\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#data-augmentation","title":"Data Augmentation","text":"<pre><code>import torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n</code></pre> <p>Common Augmentations:</p> <ul> <li><code>transforms.RandomHorizontalFlip</code>: Horizontally flips the image.</li> <li><code>transforms.RandomVerticalFlip</code>: Vertically flips the image.</li> <li><code>transforms.RandomRotation</code>: Rotates the image by a random angle.</li> <li><code>transforms.RandomAffine</code>: Applies random affine transformations.</li> <li><code>transforms.RandomPerspective</code>: Performs perspective transformation of the given image randomly with a given magnitude.</li> <li><code>transforms.RandomCrop</code>: Crops a random portion of the image.</li> <li><code>transforms.CenterCrop</code>: Crops the image from the center.</li> <li><code>transforms.ColorJitter</code>: Randomly changes the brightness, contrast, saturation, and hue of an image.</li> <li><code>transforms.RandomGrayscale</code>: Converts the image to grayscale with a certain probability.</li> <li><code>transforms.RandomErasing</code>: Randomly erases a rectangular region in the image.</li> <li><code>transforms.RandomResizedCrop</code>: Crops a random portion of the image and resizes it.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#learning-rate-schedulers_1","title":"Learning Rate Schedulers","text":"<pre><code>from torch.optim.lr_scheduler import StepLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    # Training loop\n    scheduler.step()\n</code></pre> <p>Common Schedulers:</p> <ul> <li><code>StepLR</code>: Decays the learning rate by a factor every few steps.</li> <li><code>MultiStepLR</code>: Decays the learning rate at specified milestones.</li> <li><code>ExponentialLR</code>: Decays the learning rate exponentially.</li> <li><code>CosineAnnealingLR</code>: Uses a cosine annealing schedule.</li> <li><code>ReduceLROnPlateau</code>: Reduces the learning rate when a metric has stopped improving.</li> <li><code>CyclicLR</code>: Sets the learning rate cyclically.</li> <li><code>OneCycleLR</code>: Sets the learning rate according to the 1cycle policy.</li> <li><code>CosineAnnealingWarmRestarts</code>: Cosine annealing with warm restarts.</li> <li><code>LambdaLR</code>: Allows defining a custom learning rate schedule using a lambda function.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#tensorboard-integration","title":"TensorBoard Integration","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter(\"runs/experiment_1\")\n\n# Log scalar values\nwriter.add_scalar('Loss/train', loss.item(), epoch)\nwriter.add_scalar('Accuracy/train', accuracy, epoch)\n\n# Log model graph\nwriter.add_graph(model, images)\n\n# Log images\nwriter.add_image('Image', img_grid, epoch)\n\n# Log histograms\nwriter.add_histogram('fc1.weight', model.fc1.weight, epoch)\n\n# Log embeddings\nwriter.add_embedding(features, metadata=labels, tag='my_embedding')\n\nwriter.close()\n</code></pre> <p>Run TensorBoard:</p> <pre><code>tensorboard --logdir=runs\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#onnx-export","title":"ONNX Export","text":"<pre><code>dummy_input = torch.randn(1, 3, 224, 224).to(device) # Example input\ntorch.onnx.export(model, dummy_input, \"model.onnx\", verbose=True, input_names=['input'], output_names=['output'], dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                                                                                                             'output' : {0 : 'batch_size'}})\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#torchscript","title":"TorchScript","text":""},{"location":"Cheat-Sheets/PyTorch/#tracing","title":"Tracing","text":"<pre><code>model.eval()\nexample = torch.rand(1, 3, 224, 224).to(device)\ntraced_script_module = torch.jit.trace(model, example)\ntraced_script_module.save(\"model_traced.pt\")\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#scripting","title":"Scripting","text":"<pre><code>@torch.jit.script\ndef scripted_function(x, y):\n    return x + y\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#deployment","title":"Deployment","text":""},{"location":"Cheat-Sheets/PyTorch/#serving-with-flask","title":"Serving with Flask","text":"<pre><code>from flask import Flask, request, jsonify\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n\napp = Flask(__name__)\nmodel = torch.load('my_model.pth')\nmodel.eval()\n\ndef transform_image(image):\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    image = Image.open(image).convert('RGB')\n    return transform(image).unsqueeze(0)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        if request.files.get('image'):\n            try:\n                img_tensor = transform_image(request.files['image'])\n                with torch.no_grad():\n                    prediction = model(img_tensor)\n                predicted_class = torch.argmax(prediction).item()\n                return jsonify({'prediction': str(predicted_class)})\n            except Exception as e:\n                return jsonify({'error': str(e)})\n        return jsonify({'error': 'No image found'})\n    return jsonify({'message': 'Use POST method'})\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#serving-with-torchserve","title":"Serving with TorchServe","text":"<ol> <li>Install TorchServe:</li> </ol> <pre><code>pip install torchserve torch-model-archiver\n</code></pre> <ol> <li>Create a model archive:</li> </ol> <pre><code>torch-model-archiver --model-name my_model --version 1.0 --model-file model.py --serialized-file model.pth --handler handler.py --extra-files index_to_name.json\n</code></pre> <ol> <li>Start TorchServe:</li> </ol> <pre><code>torchserve --start --model-store model_store --models my_model=my_model.mar\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#distributed-training_1","title":"Distributed Training","text":""},{"location":"Cheat-Sheets/PyTorch/#dataparallel_1","title":"DataParallel","text":"<pre><code>model = nn.DataParallel(model)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#distributeddataparallel-ddp_1","title":"DistributedDataParallel (DDP)","text":"<pre><code>import torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size):\n    setup(rank, world_size)\n\n    model = Net().to(rank)\n    ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    # Training loop\n    cleanup()\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    mp.spawn(train,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#gradient-clipping","title":"Gradient Clipping","text":"<pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2.0)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#weight-decay","title":"Weight Decay","text":"<p>Weight decay (L2 regularization) is often included directly in the optimizer:</p> <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#early-stopping","title":"Early Stopping","text":"<pre><code>patience = 10\nbest_val_loss = float('inf')\ncounter = 0\n\nfor epoch in range(num_epochs):\n    # Training and validation steps\n    val_loss = validate(model, validation_loader, criterion)\n\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        counter += 1\n\n    if counter &gt;= patience:\n        print(\"Early stopping triggered\")\n        break\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#learning-rate-finders","title":"Learning Rate Finders","text":"<pre><code># Requires a separate library like `torch_lr_finder`\nfrom torch_lr_finder import LRFinder\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=0.01)\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(trainloader, end_lr=1, num_iter=100)\nlr_finder.plot()\nlr_finder.reset()\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradient accumulation allows you to simulate larger batch sizes when you are limited by GPU memory. It works by accumulating gradients over multiple smaller batches before performing the optimization step.</p> <pre><code>accumulation_steps = 4 # Accumulate gradients over 4 batches\n\noptimizer.zero_grad() # Reset gradients before starting\n\nfor i, (inputs, labels) in enumerate(trainloader):\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss = loss / accumulation_steps # Normalize the loss\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0: # Every accumulation_steps batches\n        optimizer.step()        # Perform optimization step\n        optimizer.zero_grad()   # Reset gradients\n</code></pre>"},{"location":"Cheat-Sheets/PyTorch/#common-issues-and-debugging","title":"Common Issues and Debugging","text":"<ul> <li>CUDA Out of Memory Errors: Reduce batch size, use mixed precision training, use gradient checkpointing, use a smaller model, or use multiple GPUs.</li> <li>Slow Training: Profile your code to identify bottlenecks, use a GPU, use data parallelism or distributed training, optimize data loading, or use faster operations.</li> <li>NaN Losses: Reduce learning rate, use gradient clipping, use a different optimizer, check for numerical instability, or normalize your data.</li> <li>Overfitting: Use regularization techniques, data augmentation, early stopping, or reduce model complexity.</li> <li>Underfitting: Increase model capacity, train for longer, use a more complex optimizer, or add more features.</li> <li>Incorrect Tensor Shapes: Carefully check the shapes of your tensors and ensure they are compatible with the operations you are performing. Use <code>tensor.shape</code> to inspect tensor shapes.</li> <li>Incorrect Data Types: Ensure that your tensors have the correct data types (e.g., <code>torch.float32</code> for floating-point operations, <code>torch.long</code> for indices). Use <code>tensor.dtype</code> to inspect tensor data types.</li> <li>Device Mismatch: Ensure that all tensors and models are on the same device (CPU or GPU). Use <code>tensor.to(device)</code> and <code>model.to(device)</code> to move tensors and models to the correct device.</li> <li>Gradients Not Flowing: Check that <code>requires_grad=True</code> is set for the tensors you want to compute gradients for. Check that you are not detaching tensors from the computation graph unintentionally.</li> <li>Dead Neurons (ReLU): Use Leaky ReLU or other activation functions that allow a small gradient to flow even when the input is negative.</li> <li>Exploding Gradients: Use gradient clipping to limit the magnitude of gradients.</li> <li>Vanishing Gradients: Use skip connections (e.g., ResNet), batch normalization, or different activation functions.</li> <li>Data Loading Bottlenecks: Increase the number of worker processes in your <code>DataLoader</code> and use pinned memory.</li> <li>Incorrect Loss Function: Ensure that you are using the appropriate loss function for your task (e.g., <code>CrossEntropyLoss</code> for multi-class classification, <code>MSELoss</code> for regression).</li> <li>Incorrect Optimizer: Experiment with different optimizers and learning rates to find the best configuration for your task.</li> <li>Unstable Training: Use a smaller learning rate, increase the batch size, or use a more stable optimizer.</li> <li>Model Not Learning: Check your data for errors, ensure that your model is complex enough to learn the task, and try different hyperparameters.</li> </ul>"},{"location":"Cheat-Sheets/PyTorch/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Use meaningful names for variables and functions.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use a consistent coding style (e.g., PEP 8).</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Use appropriate data types for your data.</li> <li>Optimize your PyTorch configuration for your workload.</li> <li>Use caching to improve performance.</li> <li>Use a logging framework to log events and errors.</li> <li>Use a security framework to protect your data.</li> <li>Use a resource manager (e.g., YARN, Mesos, Kubernetes) to manage your cluster.</li> <li>Use a deployment tool to deploy your application to production.</li> <li>Monitor your application for performance issues.</li> <li>Use a CDN (Content Delivery Network) for static files.</li> <li>Optimize database queries.</li> <li>Use asynchronous tasks for long-running operations.</li> <li>Implement proper logging and error handling.</li> <li>Regularly update PyTorch and its dependencies.</li> <li>Use a security scanner to identify potential vulnerabilities.</li> <li>Follow security best practices.</li> <li>Use a reverse proxy like Nginx or Apache in front of your PyTorch application.</li> <li>Use a load balancer for high availability.</li> <li>Automate deployments using tools like Fabric or Ansible.</li> <li>Use a monitoring tool like Prometheus or Grafana.</li> <li>Implement health checks for your application.</li> <li>Use a CDN for static assets.</li> <li>Cache frequently accessed data.</li> <li>Use a database connection pool.</li> <li>Optimize your database queries.</li> <li>Use a task queue for long-running tasks.</li> <li>Use a background worker for asynchronous tasks.</li> <li>Use a message queue for inter-process communication.</li> <li>Use a service discovery tool for microservices.</li> <li>Use a containerization tool like Docker.</li> <li>Use an orchestration tool like Kubernetes.</li> <li>Use a model compression technique to reduce model size.</li> <li>Use quantization to reduce model size and improve inference speed.</li> <li>Use pruning to remove unnecessary connections from the model.</li> <li>Use knowledge distillation to transfer knowledge from a large model to a smaller model.</li> <li>Use a model deployment framework like TorchServe, TensorFlow Serving, or ONNX Runtime.</li> <li>Use a model monitoring tool to track model performance in production.</li> <li>Implement A/B testing to compare different model versions.</li> <li>Use a CI/CD pipeline to automate the model deployment process.</li> <li>Use a feature store to manage your features.</li> <li>Use a data catalog to manage your data.</li> <li>Use a data lineage tool to track the flow of data through your system.</li> <li>Use a data governance tool to ensure data quality and compliance.</li> <li>Use a model registry to manage your models.</li> <li>Use a model versioning tool to track changes to your models.</li> <li>Use a model explainability tool to understand why your model is making certain predictions.</li> <li>Use a model fairness tool to ensure that your model is not biased against certain groups of people.</li> <li>Use a model security tool to protect your model from adversarial attacks.</li> <li>Use a model privacy tool to protect the privacy of your data.</li> </ul>"},{"location":"Cheat-Sheets/Python/","title":"Python Cheat Sheet","text":"<ul> <li>Python Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Running Python Code</li> </ul> </li> <li>Basic Syntax<ul> <li>Comments</li> <li>Variables</li> <li>Data Types</li> <li>Operators</li> <li>Control Flow</li> <li>Functions</li> <li>Data Structures</li> <li>List Comprehensions</li> <li>Dictionary Comprehensions</li> <li>Set Comprehensions</li> <li>Generators</li> </ul> </li> <li>Modules and Packages<ul> <li>Importing Modules</li> <li>Creating Modules</li> <li>Packages</li> </ul> </li> <li>File I/O<ul> <li>Reading from a File</li> <li>Writing to a File</li> <li>Appending to a File</li> <li>Reading Lines from a File</li> </ul> </li> <li>String Formatting<ul> <li>f-strings (Python 3.6+)</li> <li>str.format()</li> <li>% Formatting</li> </ul> </li> <li>Decorators<ul> <li>Decorators with Arguments</li> </ul> </li> <li>Context Managers</li> <li>Object-Oriented Programming (OOP)<ul> <li>Classes and Objects</li> <li>Inheritance</li> <li>Encapsulation</li> <li>Polymorphism</li> <li>Class Methods and Static Methods</li> </ul> </li> <li>Metaclasses</li> <li>Abstract Base Classes (ABCs)</li> <li>Exception Handling<ul> <li>Raising Exceptions</li> <li>Custom Exceptions</li> </ul> </li> <li>Iterators and Generators<ul> <li>Iterators</li> <li>Generators</li> </ul> </li> <li>Descriptors</li> <li>Working with Dates and Times</li> <li>Working with CSV Files</li> <li>Working with JSON</li> <li>Working with Regular Expressions</li> <li>Working with OS</li> <li>Working with Collections</li> <li>Working with Itertools</li> <li>Working with Functools</li> <li>Concurrency and Parallelism<ul> <li>Threads</li> <li>Processes</li> <li>Asyncio</li> <li>ThreadPoolExecutor</li> <li>ProcessPoolExecutor</li> </ul> </li> <li>Type Hints</li> <li>Virtual Environments<ul> <li>Using venv (Built-in)</li> <li>Using Conda</li> </ul> </li> <li>Testing<ul> <li>Using unittest</li> <li>Using pytest</li> </ul> </li> <li>Logging</li> <li>Debugging<ul> <li>Using pdb (Python Debugger)</li> <li>Using print() Statements</li> </ul> </li> <li>Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the Python programming language, covering essential syntax, data structures, functions, modules, and best practices for efficient development. It aims to be a one-stop reference for common tasks.</p> Python Mindmap - Visual Overview <p></p>"},{"location":"Cheat-Sheets/Python/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/Python/#installation","title":"Installation","text":"<p>Check if Python is already installed:</p> <pre><code>python --version\npython3 --version\n</code></pre> <p>Install Python using a package manager (e.g., <code>apt</code>, <code>brew</code>, <code>choco</code>) or from the official website:</p> <ul> <li>Python Downloads</li> </ul>"},{"location":"Cheat-Sheets/Python/#running-python-code","title":"Running Python Code","text":"<p>Interactive Mode:</p> <pre><code>python\npython3\n</code></pre> <p>Run a Python Script:</p> <pre><code>python my_script.py\npython3 my_script.py\n</code></pre>"},{"location":"Cheat-Sheets/Python/#basic-syntax","title":"Basic Syntax","text":""},{"location":"Cheat-Sheets/Python/#comments","title":"Comments","text":"<pre><code># This is a single-line comment\n\n\"\"\"\nThis is a multi-line comment\n\"\"\"\n</code></pre>"},{"location":"Cheat-Sheets/Python/#variables","title":"Variables","text":"<pre><code>x = 10\nname = \"Alice\"\nis_active = True\n</code></pre>"},{"location":"Cheat-Sheets/Python/#data-types","title":"Data Types","text":"<p><code>int</code> - Integer numbers <pre><code>x = 42  # Immutable, supports +, -, *, /, //, %, **\n</code></pre></p> <p><code>float</code> - Floating-point numbers <pre><code>pi = 3.14  # Immutable, supports arithmetic ops, .is_integer()\n</code></pre></p> <p><code>str</code> - Strings (text) <pre><code>name = \"Alice\"  # Immutable, supports +, *, slicing, .upper(), .lower(), .split()\n</code></pre></p> <p><code>bool</code> - Boolean values <pre><code>is_active = True  # Subclass of int (True=1, False=0)\n</code></pre></p> <p><code>list</code> - Ordered, mutable collection <pre><code>items = [1, 2, 3]  # Mutable, supports indexing, .append(), .extend(), .pop()\n</code></pre></p> <p><code>tuple</code> - Ordered, immutable collection <pre><code>coords = (10, 20)  # Immutable, faster than lists, supports indexing\n</code></pre></p> <p><code>dict</code> - Key-value pairs <pre><code>user = {\"name\": \"Bob\", \"age\": 30}  # Mutable, supports .keys(), .values(), .items()\n</code></pre></p> <p><code>set</code> - Unordered, unique elements <pre><code>tags = {1, 2, 3}  # Mutable, supports .add(), .remove(), set operations (|, &amp;, -)\n</code></pre></p> <p><code>NoneType</code> - Absence of value <pre><code>result = None  # Singleton object, often used as default/placeholder\n</code></pre></p>"},{"location":"Cheat-Sheets/Python/#operators","title":"Operators","text":"<p>Arithmetic Operators <pre><code>x, y = 10, 3\nx + y    # 13 - Addition\nx - y    # 7  - Subtraction\nx * y    # 30 - Multiplication\nx / y    # 3.33 - Division (float)\nx // y   # 3  - Floor division (integer)\nx % y    # 1  - Modulus (remainder)\nx ** y   # 1000 - Exponentiation (power)\n</code></pre></p> <p>Comparison Operators <pre><code>x, y = 5, 3\nx == y   # False - Equal to\nx != y   # True  - Not equal to\nx &gt; y    # True  - Greater than\nx &lt; y    # False - Less than\nx &gt;= y   # True  - Greater than or equal to\nx &lt;= y   # False - Less than or equal to\n</code></pre></p> <p>Logical Operators <pre><code>x, y = True, False\nx and y  # False - Logical AND (both must be True)\nx or y   # True  - Logical OR (at least one must be True)\nnot x    # False - Logical NOT (negates the value)\n</code></pre></p> <p>Assignment Operators <pre><code>x = 10      # Simple assignment\nx += 5      # x = x + 5  (compound addition)\nx -= 3      # x = x - 3  (compound subtraction)\nx *= 2      # x = x * 2  (compound multiplication)\nx /= 4      # x = x / 4  (compound division)\nx //= 2     # x = x // 2 (compound floor division)\nx %= 3      # x = x % 3  (compound modulus)\nx **= 2     # x = x ** 2 (compound exponentiation)\n</code></pre></p> <p>Identity Operators <pre><code>a = [1, 2, 3]\nb = a\nc = [1, 2, 3]\na is b       # True  - Same object in memory\na is c       # False - Different objects (same values)\na is not c   # True  - Different objects\n</code></pre></p> <p>Membership Operators <pre><code>my_list = [1, 2, 3, 4, 5]\n3 in my_list        # True  - Value exists in sequence\n6 in my_list        # False - Value doesn't exist\n6 not in my_list    # True  - Value doesn't exist\n</code></pre></p> <p>Bitwise Operators (work on binary representations) <pre><code>a, b = 5, 3  # Binary: 101, 011\na &amp; b    # 1   - AND (001)\na | b    # 7   - OR (111)\na ^ b    # 6   - XOR (110)\n~a       # -6  - NOT (inverts all bits)\na &lt;&lt; 1   # 10  - Left shift (1010)\na &gt;&gt; 1   # 2   - Right shift (010)\n</code></pre></p>"},{"location":"Cheat-Sheets/Python/#control-flow","title":"Control Flow","text":"<p>If Statement:</p> <pre><code>x = 10\nif x &gt; 0:\n    print(\"Positive\")\nelif x == 0:\n    print(\"Zero\")\nelse:\n    print(\"Negative\")\n</code></pre> <p>For Loop:</p> <pre><code>for i in range(5):\n    print(i)\n</code></pre> <p>While Loop:</p> <pre><code>i = 0\nwhile i &lt; 5:\n    print(i)\n    i += 1\n</code></pre> <p>Break and Continue:</p> <pre><code>for i in range(10):\n    if i == 3:\n        break  # Exit the loop\n    if i == 1:\n        continue  # Skip to the next iteration\n    print(i)\n</code></pre> <p>Try-Except Block:</p> <pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nfinally:\n    print(\"This will always execute\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#functions","title":"Functions","text":"<p>Defining a Function:</p> <pre><code>def greet(name=\"World\"):\n    \"\"\"This function greets the person passed in as a parameter.\n    If no parameter is passed, it greets the world.\"\"\"\n    print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")\ngreet()\n</code></pre> <p>Function Arguments:</p> <p>Positional Arguments - Required, order matters <pre><code>def greet(name, age):\n    print(f\"{name} is {age} years old\")\ngreet(\"Alice\", 30)  # Must provide in order\n</code></pre></p> <p>Keyword Arguments - Named parameters, order flexible <pre><code>greet(age=30, name=\"Alice\")  # Order doesn't matter\n</code></pre></p> <p>Default Arguments - Optional with default values <pre><code>def greet(name, greeting=\"Hello\"):\n    print(f\"{greeting}, {name}!\")\ngreet(\"Alice\")           # Uses default greeting\ngreet(\"Bob\", \"Hi\")       # Overrides default\n</code></pre></p> <p><code>*args</code> - Variable positional arguments (tuple) <pre><code>def sum_all(*numbers):\n    return sum(numbers)\nsum_all(1, 2, 3, 4, 5)   # Can pass any number of args\n</code></pre></p> <p><code>**kwargs</code> - Variable keyword arguments (dict) <pre><code>def print_info(**info):\n    for key, value in info.items():\n        print(f\"{key}: {value}\")\nprint_info(name=\"Alice\", age=30, city=\"NYC\")\n</code></pre></p> <p>Combined Example - All argument types together <pre><code>def my_function(a, b=2, *args, **kwargs):\n    print(f\"a: {a}, b: {b}, args: {args}, kwargs: {kwargs}\")\n\nmy_function(1, 2, 3, 4, name=\"Alice\", age=30)\n# Output: a: 1, b: 2, args: (3, 4), kwargs: {'name': 'Alice', 'age': 30}\n</code></pre></p> <p>Lambda Functions:</p> <pre><code>square = lambda x: x ** 2\nprint(square(5))\n</code></pre>"},{"location":"Cheat-Sheets/Python/#data-structures","title":"Data Structures","text":"<p>Lists:</p> <pre><code>my_list = [1, 2, \"hello\", True]\nmy_list.append(5)\nmy_list.insert(2, \"new\")\nmy_list.remove(2)\nmy_list.pop(1)\nprint(my_list[0])\nprint(my_list[-1])\nprint(my_list[1:3])\n</code></pre> <p>Tuples:</p> <pre><code>my_tuple = (1, 2, \"hello\")\nprint(my_tuple[0])\n</code></pre> <p>Dictionaries:</p> <pre><code>my_dict = {\"name\": \"Alice\", \"age\": 30}\nmy_dict[\"city\"] = \"New York\"\nprint(my_dict[\"name\"])\nprint(my_dict.get(\"age\"))\nprint(my_dict.keys())\nprint(my_dict.values())\nprint(my_dict.items())\n</code></pre> <p>Sets:</p> <pre><code>my_set = {1, 2, 3, 4}\nmy_set.add(5)\nmy_set.remove(2)\nprint(my_set)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#list-comprehensions","title":"List Comprehensions","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\nsquares = [x ** 2 for x in numbers]\neven_squares = [x ** 2 for x in numbers if x % 2 == 0]\n</code></pre>"},{"location":"Cheat-Sheets/Python/#dictionary-comprehensions","title":"Dictionary Comprehensions","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\nsquare_dict = {x: x ** 2 for x in numbers}\n</code></pre>"},{"location":"Cheat-Sheets/Python/#set-comprehensions","title":"Set Comprehensions","text":"<pre><code>numbers = [1, 2, 2, 3, 4, 4, 5]\nunique_squares = {x ** 2 for x in numbers}\n</code></pre>"},{"location":"Cheat-Sheets/Python/#generators","title":"Generators","text":"<pre><code>def my_generator(n):\n    for i in range(n):\n        yield i ** 2\n\nfor value in my_generator(5):\n    print(value)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#modules-and-packages","title":"Modules and Packages","text":""},{"location":"Cheat-Sheets/Python/#importing-modules","title":"Importing Modules","text":"<pre><code>import math\nprint(math.sqrt(16))\n\nimport datetime\nnow = datetime.datetime.now()\nprint(now)\n\nfrom collections import Counter\n</code></pre>"},{"location":"Cheat-Sheets/Python/#creating-modules","title":"Creating Modules","text":"<p>Create a file named <code>my_module.py</code>:</p> <pre><code>def my_function():\n    print(\"Hello from my_module!\")\n\nmy_variable = 10\n</code></pre> <p>Import and use the module:</p> <pre><code>import my_module\n\nmy_module.my_function()\nprint(my_module.my_variable)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#packages","title":"Packages","text":"<p>Create a directory named <code>my_package</code> with an <code>__init__.py</code> file inside.</p> <p>Create modules inside the package (e.g., <code>my_package/module1.py</code>, <code>my_package/module2.py</code>).</p> <p>Import and use the package:</p> <pre><code>import my_package.module1\nfrom my_package import module2\n\nmy_package.module1.my_function()\nmodule2.another_function()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#file-io","title":"File I/O","text":""},{"location":"Cheat-Sheets/Python/#reading-from-a-file","title":"Reading from a File","text":"<pre><code>with open(\"my_file.txt\", \"r\") as f:\n    content = f.read()\n    print(content)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#writing-to-a-file","title":"Writing to a File","text":"<pre><code>with open(\"my_file.txt\", \"w\") as f:\n    f.write(\"Hello, file!\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#appending-to-a-file","title":"Appending to a File","text":"<pre><code>with open(\"my_file.txt\", \"a\") as f:\n    f.write(\"\\nAppending to the file.\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#reading-lines-from-a-file","title":"Reading Lines from a File","text":"<pre><code>with open(\"my_file.txt\", \"r\") as f:\n    for line in f:\n        print(line.strip())\n</code></pre>"},{"location":"Cheat-Sheets/Python/#string-formatting","title":"String Formatting","text":""},{"location":"Cheat-Sheets/Python/#f-strings-python-36","title":"f-strings (Python 3.6+)","text":"<pre><code>name = \"Alice\"\nage = 30\nprint(f\"My name is {name} and I am {age} years old.\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#strformat","title":"str.format()","text":"<pre><code>name = \"Alice\"\nage = 30\nprint(\"My name is {} and I am {} years old.\".format(name, age))\n</code></pre>"},{"location":"Cheat-Sheets/Python/#formatting","title":"% Formatting","text":"<pre><code>name = \"Alice\"\nage = 30\nprint(\"My name is %s and I am %d years old.\" % (name, age))\n</code></pre>"},{"location":"Cheat-Sheets/Python/#decorators","title":"Decorators","text":"<pre><code>def my_decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Before function execution\")\n        result = func(*args, **kwargs)\n        print(\"After function execution\")\n        return result\n    return wrapper\n\n@my_decorator\ndef say_hello(name):\n    print(f\"Hello, {name}!\")\n\nsay_hello(\"Alice\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#decorators-with-arguments","title":"Decorators with Arguments","text":"<pre><code>def repeat(num_times):\n    def decorator_repeat(func):\n        def wrapper(*args, **kwargs):\n            for _ in range(num_times):\n                result = func(*args, **kwargs)\n            return result\n        return wrapper\n    return decorator_repeat\n\n@repeat(num_times=3)\ndef greet(name):\n    print(f\"Hello {name}\")\n\ngreet(\"Alice\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#context-managers","title":"Context Managers","text":"<pre><code>with open(\"my_file.txt\", \"r\") as f:\n    content = f.read()\n    print(content)\n\n# Custom context manager\nclass MyContextManager:\n    def __enter__(self):\n        print(\"Entering the context\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(\"Exiting the context\")\n        if exc_type:\n            print(f\"An exception occurred: {exc_type}\")\n\n    def do_something(self):\n        print(\"Doing something in the context\")\n\nwith MyContextManager() as cm:\n    cm.do_something()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#object-oriented-programming-oop","title":"Object-Oriented Programming (OOP)","text":""},{"location":"Cheat-Sheets/Python/#classes-and-objects","title":"Classes and Objects","text":"<pre><code>class Dog:\n    def __init__(self, name, breed):\n        self.name = name\n        self.breed = breed\n\n    def bark(self):\n        print(\"Woof!\")\n\nmy_dog = Dog(\"Buddy\", \"Golden Retriever\")\nprint(my_dog.name)\nmy_dog.bark()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#inheritance","title":"Inheritance","text":"<pre><code>class Animal:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        raise NotImplementedError(\"Subclass must implement abstract method\")\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\ndog = Dog(\"Buddy\")\ncat = Cat(\"Whiskers\")\nprint(dog.speak())\nprint(cat.speak())\n</code></pre>"},{"location":"Cheat-Sheets/Python/#encapsulation","title":"Encapsulation","text":"<pre><code>class MyClass:\n    def __init__(self):\n        self._protected_variable = 10  # Protected variable (convention)\n        self.__private_variable = 20  # Private variable (name mangling)\n\n    def get_private(self): #getter\n        return self.__private_variable\n\n    def set_private(self, value): #setter\n        if value &gt; 0:\n            self.__private_variable = value\n\nobj = MyClass()\nprint(obj._protected_variable)\n# print(obj.__private_variable)  # AttributeError: 'MyClass' object has no attribute '__private_variable'\nprint(obj.get_private()) # Accessing private variable through a getter method.\nobj.set_private(30)\nprint(obj.get_private())\n</code></pre>"},{"location":"Cheat-Sheets/Python/#polymorphism","title":"Polymorphism","text":"<pre><code>class Animal:\n    def speak(self):\n        raise NotImplementedError(\"Subclass must implement abstract method\")\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\ndef animal_sound(animal):\n    print(animal.speak())\n\ndog = Dog(\"Buddy\")\ncat = Cat(\"Whiskers\")\nanimal_sound(dog)\nanimal_sound(cat)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#class-methods-and-static-methods","title":"Class Methods and Static Methods","text":"<pre><code>class MyClass:\n    class_variable = 0\n\n    def __init__(self, instance_variable):\n        self.instance_variable = instance_variable\n\n    @classmethod\n    def increment_class_variable(cls):\n        cls.class_variable += 1\n\n    @staticmethod\n    def static_method():\n        print(\"This is a static method\")\n\nMyClass.increment_class_variable()\nprint(MyClass.class_variable)\nMyClass.static_method()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#metaclasses","title":"Metaclasses","text":"<pre><code>class MyMetaclass(type):\n    def __new__(cls, name, bases, attrs):\n        attrs['attribute'] = 100\n        return super().__new__(cls, name, bases, attrs)\n\nclass MyClass(metaclass=MyMetaclass):\n    pass\n\nobj = MyClass()\nprint(obj.attribute)  # Output: 100\n</code></pre>"},{"location":"Cheat-Sheets/Python/#abstract-base-classes-abcs","title":"Abstract Base Classes (ABCs)","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass MyAbstractClass(ABC):\n    @abstractmethod\n    def my_method(self):\n        pass\n\nclass MyConcreteClass(MyAbstractClass):\n    def my_method(self):\n        print(\"Implementation of my_method\")\n\n# obj = MyAbstractClass()  # TypeError: Can't instantiate abstract class MyAbstractClass with abstract methods my_method\nobj = MyConcreteClass()\nobj.my_method()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#exception-handling","title":"Exception Handling","text":"<pre><code>try:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nelse:\n    print(\"No errors occurred\")\nfinally:\n    print(\"This will always execute\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#raising-exceptions","title":"Raising Exceptions","text":"<pre><code>def divide(x, y):\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n</code></pre>"},{"location":"Cheat-Sheets/Python/#custom-exceptions","title":"Custom Exceptions","text":"<pre><code>class MyCustomError(Exception):\n    pass\n\ndef my_function():\n    raise MyCustomError(\"Something went wrong\")\n</code></pre>"},{"location":"Cheat-Sheets/Python/#iterators-and-generators","title":"Iterators and Generators","text":""},{"location":"Cheat-Sheets/Python/#iterators","title":"Iterators","text":"<pre><code>my_list = [1, 2, 3]\nmy_iterator = iter(my_list)\nprint(next(my_iterator))\nprint(next(my_iterator))\nprint(next(my_iterator))\n</code></pre>"},{"location":"Cheat-Sheets/Python/#generators_1","title":"Generators","text":"<pre><code>def my_generator(n):\n    for i in range(n):\n        yield i ** 2\n\nfor value in my_generator(5):\n    print(value)\n\n# Generator expression\nsquares = (x**2 for x in range(5))\nfor square in squares:\n    print(square)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#descriptors","title":"Descriptors","text":"<pre><code>class MyDescriptor:\n    def __get__(self, instance, owner):\n        print(f\"Getting: instance={instance}, owner={owner}\")\n        return instance._value\n\n    def __set__(self, instance, value):\n        print(f\"Setting: instance={instance}, value={value}\")\n        instance._value = value\n\n    def __delete__(self, instance):\n        print(f\"Deleting: instance={instance}\")\n        del instance._value\n\nclass MyClass:\n    my_attribute = MyDescriptor()\n\nobj = MyClass()\nobj.my_attribute = 10\nprint(obj.my_attribute)\ndel obj.my_attribute\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-dates-and-times","title":"Working with Dates and Times","text":"<pre><code>import datetime\n\nnow = datetime.datetime.now()\nprint(now)\n\ntoday = datetime.date.today()\nprint(today)\n\n# Creating datetime objects\ndt = datetime.datetime(2024, 1, 1, 12, 30, 0)\n\n# Formatting datetime objects\nformatted_date = now.strftime(\"%Y-%m-%d %H:%M:%S\")\nprint(formatted_date)\n\n# Parsing strings into datetime objects\nparsed_date = datetime.datetime.strptime(\"2024-01-01 12:30:00\", \"%Y-%m-%d %H:%M:%S\")\nprint(parsed_date)\n\n# Time deltas\ndelta = datetime.timedelta(days=5, hours=3)\nnew_date = now + delta\nprint(new_date)\n\n# Working with timezones\nimport pytz\ntimezone = pytz.timezone(\"America/Los_Angeles\")\nlocalized_time = timezone.localize(datetime.datetime(2024, 1, 1, 12, 0, 0))\nprint(localized_time)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-csv-files","title":"Working with CSV Files","text":"<pre><code>import csv\n\n# Reading CSV files\nwith open('my_data.csv', 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        print(row)\n\n# Writing CSV files\ndata = [['Name', 'Age', 'City'],\n        ['Alice', 30, 'New York'],\n        ['Bob', 25, 'Paris']]\n\nwith open('output.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(data)\n\n# Reading CSV files as dictionaries\nwith open('my_data.csv', mode='r') as csv_file:\n    csv_reader = csv.DictReader(csv_file)\n    for row in csv_reader:\n        print(row['Name'], row['Age'], row['City'])\n\n# Writing CSV files from dictionaries\nfieldnames = ['Name', 'Age', 'City']\ndata = [\n    {'Name': 'Alice', 'Age': 30, 'City': 'New York'},\n    {'Name': 'Bob', 'Age': 25, 'City': 'Paris'}\n]\n\nwith open('output.csv', mode='w', newline='') as csv_file:\n    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n    writer.writeheader()\n    writer.writerows(data)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-json","title":"Working with JSON","text":"<pre><code>import json\n\n# Serializing Python objects to JSON\ndata = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\njson_string = json.dumps(data, indent=4) # indent for pretty printing\nprint(json_string)\n\n# Deserializing JSON to Python objects\nparsed_data = json.loads(json_string)\nprint(parsed_data[\"name\"])\n\n# Reading JSON from a file\nwith open(\"data.json\", \"r\") as f:\n    data = json.load(f)\n\n# Writing JSON to a file\nwith open(\"data.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-regular-expressions","title":"Working with Regular Expressions","text":"<pre><code>import re\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\npattern = r\"\\b\\w{5}\\b\"  # Matches 5-letter words\n\n# Search for a pattern\nmatch = re.search(pattern, text)\nif match:\n    print(match.group(0))\n\n# Find all occurrences of a pattern\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['quick', 'brown', 'jumps']\n\n# Replace occurrences of a pattern\nnew_text = re.sub(pattern, \"five\", text)\nprint(new_text)\n\n# Split a string by a pattern\nparts = re.split(r\"\\s+\", text) # Split by whitespace\nprint(parts)\n\n# Compile a pattern for reuse\ncompiled_pattern = re.compile(pattern)\nmatches = compiled_pattern.findall(text)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-os","title":"Working with OS","text":"<pre><code>import os\n\n# Get the current working directory\ncurrent_directory = os.getcwd()\nprint(current_directory)\n\n# Change the current working directory\nos.chdir(\"/path/to/new/directory\")\n\n# List files and directories\nfiles_and_dirs = os.listdir(\".\")\nprint(files_and_dirs)\n\n# Create a directory\nos.mkdir(\"my_new_directory\")\nos.makedirs(\"path/to/new/directory\") # Creates intermediate directories as needed\n\n# Remove a file\nos.remove(\"my_file.txt\")\n\n# Remove a directory\nos.rmdir(\"my_empty_directory\")\nimport shutil\nshutil.rmtree(\"my_directory\") # Removes a directory and its contents\n\n# Join path components\nnew_path = os.path.join(current_directory, \"my_folder\")\nprint(new_path)\n\n# Check if a path exists\nif os.path.exists(new_path):\n    print(\"Path exists\")\n\n# Check if a path is a file\nif os.path.isfile(\"my_file.txt\"):\n    print(\"It's a file\")\n\n# Check if a path is a directory\nif os.path.isdir(\"my_folder\"):\n    print(\"It's a directory\")\n\n# Get the file extension\nfilename, extension = os.path.splitext(\"my_file.txt\")\nprint(extension)\n\n# Get environment variables\nprint(os.environ.get(\"HOME\"))\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-collections","title":"Working with Collections","text":"<pre><code>import collections\n\n# Counter\nmy_list = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\ncount = collections.Counter(my_list)\nprint(count)\nprint(count.most_common(2))\n\n# defaultdict\nmy_dict = collections.defaultdict(int)\nmy_dict[\"a\"] += 1\nprint(my_dict[\"a\"])\nprint(my_dict[\"b\"])  # Accessing a missing key returns the default value\n\n# namedtuple\nPoint = collections.namedtuple(\"Point\", [\"x\", \"y\"])\np = Point(10, 20)\nprint(p.x, p.y)\n\n# deque\nmy_deque = collections.deque([1, 2, 3])\nmy_deque.append(4)\nmy_deque.appendleft(0)\nmy_deque.pop()\nmy_deque.popleft()\nprint(my_deque)\n\n# OrderedDict (less relevant in Python 3.7+ where dicts maintain insertion order)\nmy_ordered_dict = collections.OrderedDict()\nmy_ordered_dict['a'] = 1\nmy_ordered_dict['b'] = 2\nmy_ordered_dict['c'] = 3\nprint(my_ordered_dict)\n\n# ChainMap\ndict1 = {'a': 1, 'b': 2}\ndict2 = {'c': 3, 'd': 4}\nchain = collections.ChainMap(dict1, dict2)\nprint(chain['a'])\nprint(chain['c'])\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-itertools","title":"Working with Itertools","text":"<pre><code>import itertools\n\n# Count\nfor i in itertools.count(start=10, step=2):\n    if i &gt; 20:\n        break\n    print(i)\n\n# Cycle\ncount = 0\nfor item in itertools.cycle(['A', 'B', 'C']):\n    if count &gt; 5:\n        break\n    print(item)\n    count += 1\n\n# Repeat\nfor item in itertools.repeat(\"Hello\", 3):\n    print(item)\n\n# Chain\nlist1 = [1, 2, 3]\nlist2 = [4, 5, 6]\nfor item in itertools.chain(list1, list2):\n    print(item)\n\n# Combinations\nfor combo in itertools.combinations([1, 2, 3, 4], 2):\n    print(combo)\n\n# Permutations\nfor perm in itertools.permutations([1, 2, 3], 2):\n    print(perm)\n\n# Product\nfor prod in itertools.product([1, 2], ['a', 'b']):\n    print(prod)\n\n# Groupby\ndata = [('A', 1), ('A', 2), ('B', 3), ('B', 4), ('C', 5)]\nfor key, group in itertools.groupby(data, key=lambda x: x[0]):\n    print(key, list(group))\n\n# islice\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nfor item in itertools.islice(data, 2, 7, 2):  # start, stop, step\n    print(item)\n\n# starmap\ndata = [(1, 2), (3, 4), (5, 6)]\nfor result in itertools.starmap(lambda x, y: x * y, data):\n    print(result)\n\n# takewhile\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nfor item in itertools.takewhile(lambda x: x &lt; 5, data):\n    print(item)\n\n# dropwhile\nfor item in itertools.dropwhile(lambda x: x &lt; 5, data):\n    print(item)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#working-with-functools","title":"Working with Functools","text":"<pre><code>import functools\n\n# partial\ndef power(base, exponent):\n    return base ** exponent\n\nsquare = functools.partial(power, exponent=2)\ncube = functools.partial(power, exponent=3)\n\nprint(square(5))  # Output: 25\nprint(cube(2))    # Output: 8\n\n# lru_cache\n@functools.lru_cache(maxsize=None)\ndef fibonacci(n):\n    if n &lt; 2:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))\n\n# reduce\nnumbers = [1, 2, 3, 4, 5]\nproduct = functools.reduce(lambda x, y: x * y, numbers)\nprint(product)\n\n# wraps\ndef my_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        \"\"\"Wrapper function docstring\"\"\"\n        print(\"Before function execution\")\n        result = func(*args, **kwargs)\n        print(\"After function execution\")\n        return result\n    return wrapper\n\n@my_decorator\ndef say_hello(name):\n    \"\"\"This function greets the person passed in as a parameter.\"\"\"\n    print(f\"Hello, {name}!\")\n\nprint(say_hello.__name__) # Output: say_hello\nprint(say_hello.__doc__) # Output: This function greets the person passed in as a parameter.\n</code></pre>"},{"location":"Cheat-Sheets/Python/#concurrency-and-parallelism","title":"Concurrency and Parallelism","text":""},{"location":"Cheat-Sheets/Python/#threads","title":"Threads","text":"<pre><code>import threading\n\ndef my_task(name):\n    print(f\"Thread {name}: starting\")\n    # Perform some work\n    print(f\"Thread {name}: finishing\")\n\nthreads = []\nfor i in range(3):\n    t = threading.Thread(target=my_task, args=(i,))\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#processes","title":"Processes","text":"<pre><code>import multiprocessing\n\ndef my_task(name):\n    print(f\"Process {name}: starting\")\n    # Perform some work\n    print(f\"Process {name}: finishing\")\n\nprocesses = []\nfor i in range(3):\n    p = multiprocessing.Process(target=my_task, args=(i,))\n    processes.append(p)\n    p.start()\n\nfor p in processes:\n    p.join()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#asyncio","title":"Asyncio","text":"<pre><code>import asyncio\n\nasync def my_coroutine(name):\n    print(f\"Coroutine {name}: starting\")\n    await asyncio.sleep(1)\n    print(f\"Coroutine {name}: finishing\")\n\nasync def main():\n    tasks = [my_coroutine(i) for i in range(3)]\n    await asyncio.gather(*tasks)\n\nasyncio.run(main())\n</code></pre>"},{"location":"Cheat-Sheets/Python/#threadpoolexecutor","title":"ThreadPoolExecutor","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef task(n):\n    print(f\"Processing {n}\")\n    return n * 2\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    results = executor.map(task, range(5))\n    for result in results:\n        print(result)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#processpoolexecutor","title":"ProcessPoolExecutor","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor\n\ndef task(n):\n    print(f\"Processing {n}\")\n    return n * 2\n\nwith ProcessPoolExecutor(max_workers=3) as executor:\n    results = executor.map(task, range(5))\n    for result in results:\n        print(result)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#type-hints","title":"Type Hints","text":"<pre><code>def add(x: int, y: int) -&gt; int:\n    return x + y\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\nfrom typing import List, Tuple, Dict, Optional, Union, Any\n\nmy_list: List[int] = [1, 2, 3]\nmy_tuple: Tuple[str, int] = (\"Alice\", 30)\nmy_dict: Dict[str, int] = {\"a\": 1, \"b\": 2}\n\ndef process_item(item: Union[str, int]) -&gt; Optional[str]:\n    if isinstance(item, str):\n        return item.upper()\n    elif isinstance(item, int):\n        return str(item * 2)\n    else:\n        return None\n\ndef my_function(x: Any) -&gt; None:\n    pass\n</code></pre>"},{"location":"Cheat-Sheets/Python/#virtual-environments","title":"Virtual Environments","text":""},{"location":"Cheat-Sheets/Python/#using-venv-built-in","title":"Using venv (Built-in)","text":"<p>Creating a Virtual Environment</p> <pre><code>python -m venv myenv\n</code></pre> <p>Activating a Virtual Environment</p> <p>On Linux/macOS:</p> <pre><code>source myenv/bin/activate\n</code></pre> <p>On Windows:</p> <pre><code>myenv\\Scripts\\activate\n</code></pre> <p>Deactivating a Virtual Environment</p> <pre><code>deactivate\n</code></pre>"},{"location":"Cheat-Sheets/Python/#using-conda","title":"Using Conda","text":"<p>Creating a Conda Environment</p> <pre><code># Create environment with specific Python version\nconda create --name myenv python=3.11\n\n# Create environment with packages\nconda create --name myenv python=3.11 numpy pandas scikit-learn\n\n# Create from environment.yml file\nconda env create -f environment.yml\n</code></pre> <p>Activating a Conda Environment</p> <pre><code>conda activate myenv\n</code></pre> <p>Deactivating a Conda Environment</p> <pre><code>conda deactivate\n</code></pre> <p>Managing Conda Environments</p> <pre><code># List all environments\nconda env list\n\n# Remove an environment\nconda env remove --name myenv\n\n# Export environment to file\nconda env export &gt; environment.yml\n\n# Clone an environment\nconda create --name newenv --clone myenv\n</code></pre> <p>Installing Packages in Conda</p> <pre><code># Install packages\nconda install numpy pandas matplotlib\n\n# Install specific version\nconda install numpy=1.24.0\n\n# Install from conda-forge channel\nconda install -c conda-forge package_name\n\n# List installed packages\nconda list\n\n# Update a package\nconda update numpy\n\n# Update all packages\nconda update --all\n</code></pre>"},{"location":"Cheat-Sheets/Python/#testing","title":"Testing","text":""},{"location":"Cheat-Sheets/Python/#using-unittest","title":"Using <code>unittest</code>","text":"<pre><code>import unittest\n\nclass MyTestCase(unittest.TestCase):\n    def test_addition(self):\n        self.assertEqual(1 + 1, 2)\n\n    def test_subtraction(self):\n        self.assertNotEqual(5 - 2, 4)\n\n    def test_raises_exception(self):\n        with self.assertRaises(ValueError):\n            raise ValueError\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"Cheat-Sheets/Python/#using-pytest","title":"Using <code>pytest</code>","text":"<p>Installation:</p> <pre><code>pip install pytest\n</code></pre> <p>Test Example:</p> <pre><code># test_my_module.py\ndef add(x, y):\n    return x + y\n\ndef test_add():\n    assert add(1, 2) == 3\n    assert add(-1, 1) == 0\n</code></pre> <p>Run tests:</p> <pre><code>pytest\n</code></pre>"},{"location":"Cheat-Sheets/Python/#logging","title":"Logging","text":"<pre><code>import logging\n\n# Basic configuration\nlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# Create a logger\nlogger = logging.getLogger(__name__)\n\n# Log messages\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\nlogger.warning(\"This is a warning message\")\nlogger.error(\"This is an error message\")\nlogger.critical(\"This is a critical message\")\n\n# Logging to a file\nfile_handler = logging.FileHandler('my_log.log')\nfile_handler.setLevel(logging.WARNING)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfile_handler.setFormatter(formatter)\nlogger.addHandler(file_handler)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#debugging","title":"Debugging","text":""},{"location":"Cheat-Sheets/Python/#using-pdb-python-debugger","title":"Using <code>pdb</code> (Python Debugger)","text":"<pre><code>import pdb\n\ndef my_function(x, y):\n    z = x + y\n    pdb.set_trace()  # Set a breakpoint\n    return z\n\nmy_function(1, 2)\n</code></pre>"},{"location":"Cheat-Sheets/Python/#using-print-statements","title":"Using <code>print()</code> Statements","text":"<pre><code>def my_function(x, y):\n    print(f\"x: {x}, y: {y}\")\n    z = x + y\n    print(f\"z: {z}\")\n    return z\n</code></pre>"},{"location":"Cheat-Sheets/Python/#best-practices","title":"Best Practices","text":"<ul> <li>Use virtual environments to isolate project dependencies.</li> <li>Use meaningful names for variables and functions.</li> <li>Follow the DRY (Don't Repeat Yourself) principle.</li> <li>Write unit tests to ensure code quality.</li> <li>Use a consistent coding style (PEP 8).</li> <li>Document your code.</li> <li>Use a version control system (e.g., Git).</li> <li>Use appropriate data types for your data.</li> <li>Handle exceptions gracefully.</li> <li>Use logging to track events and errors.</li> <li>Use a security linter (e.g., Bandit) to identify potential vulnerabilities.</li> <li>Follow security best practices.</li> <li>Use a linter (like <code>flake8</code>) and formatter (like <code>black</code>) to ensure consistent code style.</li> <li>Use a code coverage tool (like <code>coverage.py</code>) to measure test coverage.</li> <li>Use a static analysis tool (like <code>mypy</code>) to check for type errors.</li> <li>Use a profiler to identify performance bottlenecks.</li> <li>Use a debugger to step through your code and inspect variables.</li> <li>Use a build tool (like <code>setuptools</code>) to package and distribute your code.</li> <li>Use a continuous integration (CI) system to automatically run tests and build your code.</li> <li>Use a continuous deployment (CD) system to automatically deploy your code to production.</li> <li>Use a monitoring tool to track the performance of your application in production.</li> <li>Use a configuration management tool (like Ansible, Chef, or Puppet) to manage your infrastructure.</li> <li>Use a containerization tool like Docker.</li> <li>Use an orchestration tool like Kubernetes.</li> </ul>"},{"location":"Cheat-Sheets/RegEx/","title":"Regular Expressions (RegEx) Cheat Sheet","text":"<ul> <li>Regular Expressions (RegEx) Cheat Sheet<ul> <li>Basic Syntax<ul> <li>Literals</li> <li>Special Characters</li> <li>Character Classes</li> <li>Predefined Character Classes</li> <li>Quantifiers</li> <li>Anchors</li> <li>Alternation</li> <li>Grouping and Capturing</li> <li>Lookarounds</li> <li>Flags (Modifiers)</li> <li>Character Properties (Unicode)</li> <li>Examples</li> </ul> </li> <li>Advanced Techniques<ul> <li>Atomic Groups</li> <li>Recursive Patterns</li> <li>Conditional Expressions</li> <li>Named Capture Groups</li> <li>Comments</li> <li>Free-Spacing Mode (x flag)</li> <li>Branch Reset Groups</li> <li>Subroutine Calls</li> </ul> </li> <li>Common RegEx Engines and Differences</li> <li>Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of regular expressions (RegEx), covering syntax, character classes, quantifiers, anchors, groups, flags, and advanced techniques. It aims to be a complete reference for using regular expressions, with a focus on Python examples.</p>"},{"location":"Cheat-Sheets/RegEx/#basic-syntax","title":"Basic Syntax","text":""},{"location":"Cheat-Sheets/RegEx/#literals","title":"Literals","text":"<p>Characters match themselves literally, except for special characters.</p> <ul> <li><code>abc</code>: Matches the literal string \"abc\".</li> </ul> <pre><code>import re\ntext = \"abc def ghi\"\npattern = r\"abc\"\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: abc\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#special-characters","title":"Special Characters","text":"<p>These characters have special meanings in RegEx:</p> <ul> <li><code>. ^ $ * + ? { } [ ] \\ | ( )</code></li> </ul> <p>To match these characters literally, escape them with a backslash (<code>\\</code>):</p> <ul> <li><code>\\.</code>: Matches a literal dot (<code>.</code>).</li> <li><code>\\\\</code>: Matches a literal backslash (<code>\\</code>).</li> </ul> <pre><code>import re\ntext = \"123.456\"\npattern = r\"\\.\"  # Matches a literal dot\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: .\n\ntext = \"path\\\\to\\\\file\"\npattern = r\"\\\\\" # Matches a literal backslash\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\") # Output: \\\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#character-classes","title":"Character Classes","text":"<ul> <li><code>.</code>: Matches any character except a newline (unless the <code>s</code> flag is used).</li> <li><code>[abc]</code>: Matches any character inside the brackets (a, b, or c).</li> <li><code>[^abc]</code>: Matches any character not inside the brackets.</li> <li><code>[a-z]</code>: Matches any character in the range a to z (lowercase).</li> <li><code>[A-Z]</code>: Matches any character in the range A to Z (uppercase).</li> <li><code>[0-9]</code>: Matches any digit.</li> <li><code>[a-zA-Z0-9]</code>: Matches any alphanumeric character.</li> </ul> <pre><code>import re\n\ntext = \"apple banana cherry\"\npattern = r\"[abc]\"  # Matches 'a', 'b', or 'c'\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['a', 'a', 'b', 'a', 'a', 'a', 'a', 'c']\n\ntext = \"apple banana cherry\"\npattern = r\"[^abc]\"  # Matches any character except 'a', 'b', or 'c'\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['p', 'p', 'l', 'e', ' ', 'n', 'n', ' ', 'h', 'e', 'r', 'r', 'y']\n\ntext = \"apple1 banana2 cherry3\"\npattern = r\"[0-9]\"  # Matches any digit\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['1', '2', '3']\n\ntext = \"Hello. World!\"\npattern = r\".\" # Matches any character except newline\nmatches = re.findall(pattern, text)\nprint(matches) # Output: ['H', 'e', 'l', 'l', 'o', '.', ' ', 'W', 'o', 'r', 'l', 'd', '!']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#predefined-character-classes","title":"Predefined Character Classes","text":"<ul> <li><code>\\d</code>: Matches any digit (equivalent to <code>[0-9]</code>).</li> <li><code>\\D</code>: Matches any non-digit (equivalent to <code>[^0-9]</code>).</li> <li><code>\\w</code>: Matches any word character (alphanumeric + underscore, equivalent to <code>[a-zA-Z0-9_]</code>).</li> <li><code>\\W</code>: Matches any non-word character (equivalent to <code>[^a-zA-Z0-9_]</code>).</li> <li><code>\\s</code>: Matches any whitespace character (space, tab, newline, etc.).</li> <li><code>\\S</code>: Matches any non-whitespace character.</li> <li><code>\\b</code>: Matches a word boundary.</li> <li><code>\\B</code>: Matches a non-word boundary.</li> <li><code>\\t</code>: Matches a tab character.</li> <li><code>\\n</code>: Matches a newline character.</li> <li><code>\\r</code>: Matches a carriage return character.</li> <li><code>\\f</code>: Matches a form feed character.</li> <li><code>\\v</code>: Matches a vertical tab character.</li> <li><code>\\0</code>: Matches a null character.</li> <li><code>[\\b]</code>: Matches a backspace character (inside a character class).</li> </ul> <pre><code>import re\n\ntext = \"123 abc 456\"\npattern = r\"\\d+\"  # Matches one or more digits\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['123', '456']\n\ntext = \"Hello World\"\npattern = r\"\\bWorld\\b\"  # Matches \"World\" at a word boundary\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: World\n\ntext = \"Hello\\tWorld\\n\"\npattern = r\"\\s+\"  # Matches one or more whitespace characters\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['\\t', '\\n']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#quantifiers","title":"Quantifiers","text":"<ul> <li><code>*</code>: Matches 0 or more occurrences.</li> <li><code>+</code>: Matches 1 or more occurrences.</li> <li><code>?</code>: Matches 0 or 1 occurrence.</li> <li><code>{n}</code>: Matches exactly <code>n</code> occurrences.</li> <li><code>{n,}</code>: Matches <code>n</code> or more occurrences.</li> <li><code>{n,m}</code>: Matches between <code>n</code> and <code>m</code> occurrences (inclusive).</li> <li><code>*?</code>, <code>+?</code>, <code>??</code>, <code>{n,}?</code>, <code>{n,m}?</code>: Non-greedy (lazy) versions.</li> </ul> <pre><code>import re\n\ntext = \"aaabbbccc\"\npattern = r\"a+\"  # Matches one or more 'a's\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['aaa']\n\ntext = \"ab abb abbb\"\npattern = r\"ab{2,4}\"  # Matches 'ab' with 2 to 4 'b's\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['abb', 'abbb']\n\ntext = \"color colour\"\npattern = r\"colou?r\"  # Matches 'color' or 'colour'\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['color', 'colour']\n\ntext = \"aaaa\"\npattern = r\"a*?\" # Matches 0 or more 'a', non-greedy\nmatches = re.findall(pattern, text)\nprint(matches) # Output: ['', 'a', '', 'a', '', 'a', '', 'a', '']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#anchors","title":"Anchors","text":"<ul> <li><code>^</code>: Matches the beginning of the string (or line).</li> <li><code>$</code>: Matches the end of the string (or line).</li> <li><code>\\A</code>: Matches the beginning of the string.</li> <li><code>\\Z</code>: Matches the end of the string, or before a newline at the end.</li> <li><code>\\z</code>: Matches the end of the string.</li> </ul> <pre><code>import re\n\ntext = \"hello world\"\npattern = r\"^hello\"  # Matches 'hello' at the beginning\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: hello\n\ntext = \"world\\nhello\"\npattern = r\"hello$\"  # Matches 'hello' at the end\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\") # Output: hello\n\ntext = \"hello world\\n\"\npattern = r\"\\Ahello\"\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\") # Output: hello\n\ntext = \"hello world\\n\"\npattern = r\"world\\Z\"\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\") # Output: world\n\ntext = \"hello world\"\npattern = r\"world\\z\"\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\") # Output: world\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#alternation","title":"Alternation","text":"<ul> <li><code>|</code>: Matches either the expression before or the expression after the <code>|</code>.</li> </ul> <pre><code>import re\n\ntext = \"cat and dog\"\npattern = r\"cat|dog\"  # Matches 'cat' or 'dog'\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['cat', 'dog']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#grouping-and-capturing","title":"Grouping and Capturing","text":"<ul> <li><code>( )</code>: Groups expressions and creates a capturing group.</li> <li><code>(?: )</code>: Groups expressions without creating a capturing group.</li> <li><code>\\1</code>, <code>\\2</code>, etc.: Backreferences to captured groups.</li> </ul> <pre><code>import re\n\ntext = \"apple apple\"\npattern = r\"(\\w+) \\1\"  # Matches repeated words\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: apple apple\nprint(match.group(1)) if match else print(\"No match\")  # Output: apple\n\ntext = \"12-34-56\"\npattern = r\"(\\d{2})-(?:(\\d{2})-(\\d{2}))\"  # Non-capturing group for the middle part\nmatch = re.search(pattern, text)\nif match:\n    print(match.groups())  # Output: ('12', '34', '56')\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#lookarounds","title":"Lookarounds","text":"<ul> <li><code>(?= )</code>: Positive lookahead.</li> <li><code>(?! )</code>: Negative lookahead.</li> <li><code>(?&lt;= )</code>: Positive lookbehind.</li> <li><code>(?&lt;! )</code>: Negative lookbehind.</li> </ul> <pre><code>import re\n\ntext = \"apple123 banana456\"\npattern = r\"\\w+(?=\\d+)\"  # Matches words followed by digits (positive lookahead)\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['apple', 'banana']\n\ntext = \"apple banana cherry\"\npattern = r\"\\w+(?!e)\"  # Matches words NOT followed by 'e' (negative lookahead)\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['appl', 'banan', 'banan', 'cherr', 'cherr']\n\ntext = \"123apple 456banana\"\npattern = r\"(?&lt;=\\d+)\\w+\"  # Matches words preceded by digits (positive lookbehind)\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['apple', 'banana']\n\ntext = \"apple banana cherry\"\npattern = r\"(?&lt;!a)\\w+\"  # Matches words NOT preceded by 'a' (negative lookbehind)\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['pple', 'banana', 'cherry']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#flags-modifiers","title":"Flags (Modifiers)","text":"<ul> <li><code>i</code>: Case-insensitive.</li> <li><code>m</code>: Multiline.</li> <li><code>s</code>: Dotall (single-line).</li> <li><code>g</code>: Global (find all matches).</li> <li><code>x</code>: Extended (allow whitespace and comments).</li> <li><code>u</code>: Unicode.</li> </ul> <pre><code>import re\n\ntext = \"Hello World\"\npattern = r\"world\"\nmatch = re.search(pattern, text, re.IGNORECASE)  # Case-insensitive\nprint(match.group(0)) if match else print(\"No match\")  # Output: World\n\ntext = \"Line 1\\nLine 2\\nLine 3\"\npattern = r\"^Line\"\nmatches = re.findall(pattern, text, re.MULTILINE)  # Multiline\nprint(matches)  # Output: ['Line', 'Line', 'Line']\n\ntext = \"Hello\\nWorld\"\npattern = r\"Hello.World\"\nmatch = re.search(pattern, text, re.DOTALL)  # Dotall\nprint(match.group(0)) if match else print(\"No match\")  # Output: Hello\\nWorld\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#character-properties-unicode","title":"Character Properties (Unicode)","text":"<ul> <li><code>\\p{Property}</code>: Matches a character with the specified Unicode property.</li> <li><code>\\P{Property}</code>: Matches a character without the specified Unicode property.</li> </ul> <pre><code>import re\n\ntext = \"Hello 123 \u3053\u3093\u306b\u3061\u306f\"\npattern = r\"\\p{L}+\"  # Matches one or more letters\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['Hello', '\u3053\u3093\u306b\u3061\u306f']\n\ntext = \"Hello 123 \u3053\u3093\u306b\u3061\u306f\"\npattern = r\"\\p{N}+\"  # Matches one or more numbers\nmatches = re.findall(pattern, text)\nprint(matches)  # Output: ['123']\n\ntext = \"Hello 123 \u3053\u3093\u306b\u3061\u306f\"\npattern = r\"\\P{L}+\"  # Matches one or more characters that are NOT letters\nmatches = re.findall(pattern, text)\nprint(matches) # Output: [' ', '123 ', ' ']\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#examples","title":"Examples","text":"<ul> <li>Email: <code>^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$</code></li> </ul> <pre><code>import re\nemail = \"test@example.com\"\npattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\nmatch = re.match(pattern, email)\nprint(bool(match))  # Output: True\n</code></pre> <ul> <li>URL: <code>^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$</code></li> </ul> <pre><code>import re\nurl = \"https://www.example.com/path/to/page.html\"\npattern = r\"^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?$\"\nmatch = re.match(pattern, url)\nprint(bool(match))  # Output: True\n</code></pre> <ul> <li>IP Address: <code>^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$</code></li> </ul> <pre><code>import re\nip = \"192.168.1.1\"\npattern = r\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\nmatch = re.match(pattern, ip)\nprint(bool(match))  # Output: True\n</code></pre> <ul> <li>Hex Color Code: <code>^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$</code></li> </ul> <pre><code>import re\nhex_code = \"#FF0000\"\npattern = r\"^#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3})$\"\nmatch = re.match(pattern, hex_code)\nprint(bool(match))  # Output: True\n</code></pre> <ul> <li>Date (YYYY-MM-DD): <code>^\\d{4}-\\d{2}-\\d{2}$</code></li> </ul> <pre><code>import re\ndate = \"2024-01-08\"\npattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\nmatch = re.match(pattern, date)\nprint(bool(match))  # Output: True\n</code></pre> <ul> <li>HTML Tag: <code>&lt;([a-z]+)([^&lt;]+)*(?:&gt;(.*)&lt;\\/\\1&gt;|\\s+\\/&gt;)</code></li> </ul> <pre><code>import re\nhtml = \"&lt;p&gt;This is a paragraph.&lt;/p&gt;\"\npattern = r\"&lt;([a-z]+)([^&lt;]+)*(?:&gt;(.*)&lt;\\/\\1&gt;|\\s+\\/&gt;)\"\nmatch = re.search(pattern, html)\nprint(match.group(1)) if match else print(\"No match\")  # Output: p\n</code></pre> <ul> <li>Phone Number (US): <code>\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}</code></li> </ul> <pre><code>import re\nphone = \"(555) 123-4567\"\npattern = r\"\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"\nmatch = re.search(pattern, phone)\nprint(bool(match))  # Output: True\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"Cheat-Sheets/RegEx/#atomic-groups","title":"Atomic Groups","text":"<ul> <li><code>(?&gt; )</code>: Atomic group.</li> </ul> <pre><code>import re\n\ntext = \"aaaaaaaaab\"\npattern = r\"a+b\"  # Regular +\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: ab\n\ntext = \"aaaaaaaaab\"\npattern = r\"a+?b\"  # Non-greedy +?\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: ab\n\ntext = \"aaaaaaaaab\"\npattern = r\"(?&gt;a+)b\"  # Atomic group\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: No match (because a+ consumed all 'a's)\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#recursive-patterns","title":"Recursive Patterns","text":"<ul> <li><code>(?R)</code> or <code>(?0)</code>: Recursively matches the entire pattern.</li> <li><code>(?1)</code>, <code>(?2)</code>, etc.: Recursively matches a specific capturing group.</li> </ul> <pre><code>import re\n\ntext = \"((abc)def(ghi))\"\npattern = r\"\\(([^()]|(?R))*\\)\"  # Matches balanced parentheses\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: ((abc)def(ghi))\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#conditional-expressions","title":"Conditional Expressions","text":"<ul> <li><code>(?(condition)yes-pattern|no-pattern)</code></li> </ul> <pre><code>import re\n\ntext = \"ab\"\npattern = r\"(?(?&lt;=a)b|c)\"  # If preceded by 'a', match 'b'; otherwise, match 'c'.\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: b\n\ntext = \"cb\"\npattern = r\"(?(?&lt;=a)b|c)\"  # If preceded by 'a', match 'b'; otherwise, match 'c'.\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: c\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#named-capture-groups","title":"Named Capture Groups","text":"<ul> <li><code>(?P&lt;name&gt; )</code>: Creates a named capturing group.</li> <li><code>(?P=name)</code>: Backreference to a named capturing group.</li> </ul> <pre><code>import re\n\ntext = \"apple apple\"\npattern = r\"(?P&lt;word&gt;\\w+) (?P=word)\"  # Matches repeated words\nmatch = re.search(pattern, text)\nprint(match.group('word')) if match else print(\"No match\")  # Output: apple\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#comments","title":"Comments","text":"<ul> <li><code>(?#comment)</code>: Inline comment.</li> </ul> <pre><code>import re\n\ntext = \"123-4567\"\npattern = r\"\\d{3}(?#This is a comment)-?\\d{4}\"\nmatch = re.search(pattern, text)\nprint(match.group(0)) if match else print(\"No match\")  # Output: 123-4567\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#free-spacing-mode-x-flag","title":"Free-Spacing Mode (<code>x</code> flag)","text":"<pre><code>import re\n\ntext = \"123-4567\"\npattern = r\"\"\"\n    \\d{3}  # Area code\n    -?     # Optional separator\n    \\d{4}  # Phone number\n\"\"\"\nmatch = re.search(pattern, text, re.VERBOSE)  # Use re.VERBOSE or re.X\nprint(match.group(0)) if match else print(\"No match\")  # Output: 123-4567\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#branch-reset-groups","title":"Branch Reset Groups","text":"<ul> <li><code>(?| )</code>: Resets the capture group numbering within each alternative.</li> </ul> <pre><code>import re\n\ntext = \"abc\"\npattern = r\"(?|(a)|(b)|(c))\"  # All alternatives capture to group 1\nmatch = re.search(pattern, text)\nprint(match.group(1)) if match else print(\"No match\")  # Output: a\n\ntext = \"def\"\npattern = r\"(?|(a)|(b)|(c))\"  # All alternatives capture to group 1\nmatch = re.search(pattern, text)\nprint(match.group(1)) if match else print(\"No match\")  # Output: None\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#subroutine-calls","title":"Subroutine Calls","text":"<ul> <li><code>(?&amp;name)</code>: Calls a named subroutine.</li> </ul> <pre><code>import re\ntext = \"((abc)def(ghi))\"\npattern = r\"\"\"\n(?(DEFINE)\n  (?P&lt;paren&gt;\\(([^()]|(?&amp;paren))*\\))  # Define a named subroutine 'paren'\n)\n^(?&amp;paren)$  # Call the subroutine\n\"\"\"\nmatch = re.search(pattern, text, re.VERBOSE)\nprint(match.group(0)) if match else print(\"No match\")  # Output: ((abc)def(ghi))\n</code></pre>"},{"location":"Cheat-Sheets/RegEx/#common-regex-engines-and-differences","title":"Common RegEx Engines and Differences","text":"<ul> <li>PCRE (Perl Compatible Regular Expressions): Widely used, feature-rich.</li> <li>JavaScript: Good support, but lookbehind assertions were limited (now widely supported).</li> <li>Python (<code>re</code> module): Excellent support, including Unicode properties.</li> <li>.NET: Powerful and feature-rich.</li> <li>Java: Good support, some syntax differences.</li> <li>POSIX: Basic and Extended Regular Expressions (BRE and ERE). Limited features.</li> </ul>"},{"location":"Cheat-Sheets/RegEx/#best-practices","title":"Best Practices","text":"<ul> <li>Be specific: Avoid overly broad patterns.</li> <li>Use character classes: <code>\\d</code> is more efficient than <code>[0-9]</code>.</li> <li>Use non-capturing groups <code>(?:...)</code> when you don't need the captured text.</li> <li>Be aware of greediness: Use non-greedy quantifiers (<code>*?</code>, <code>+?</code>, etc.).</li> <li>Test your regex: Use online tools (regex101.com, regexr.com) or Python's <code>re</code> module interactively.</li> <li>Comment complex regexes: Use the <code>x</code> flag (extended mode) for readability.</li> <li>Avoid catastrophic backtracking: Be careful with nested quantifiers.</li> <li>Escape special characters: Always escape special characters.</li> <li>Use raw strings in Python: Use <code>r\"\\d+\"</code> to avoid escaping backslashes.</li> <li>Consider alternatives: Sometimes, regular expressions are not the best tool.</li> <li>Know your engine: Be aware of the features and limitations of your RegEx engine.</li> </ul>"},{"location":"Cheat-Sheets/SQL/","title":"SQL Cheat Sheet","text":"<ul> <li>SQL Cheat Sheet<ul> <li>Data Types<ul> <li>Numeric</li> <li>String</li> <li>Date and Time</li> <li>Boolean</li> <li>Other</li> </ul> </li> <li>Data Definition Language (DDL)<ul> <li>CREATE TABLE</li> <li>ALTER TABLE</li> <li>DROP TABLE</li> <li>TRUNCATE TABLE</li> <li>CREATE INDEX</li> <li>DROP INDEX</li> <li>CREATE VIEW</li> <li>DROP VIEW</li> <li>DATABASE Operations</li> </ul> </li> <li>Data Manipulation Language (DML)<ul> <li>INSERT</li> <li>UPDATE</li> <li>DELETE</li> </ul> </li> <li>Data Query Language (DQL)<ul> <li>SELECT</li> <li>WHERE Clause</li> <li>ORDER BY Clause</li> <li>LIMIT and OFFSET Clauses</li> <li>Aggregate Functions</li> <li>GROUP BY Clause</li> <li>HAVING Clause</li> </ul> </li> <li>Order of execution</li> <li>Joins<ul> <li>INNER JOIN</li> <li>LEFT JOIN (LEFT OUTER JOIN)</li> <li>RIGHT JOIN (RIGHT OUTER JOIN)</li> <li>FULL JOIN (FULL OUTER JOIN)</li> <li>Self Join</li> <li>Cross Join</li> </ul> </li> <li>Set Operations<ul> <li>UNION</li> <li>UNION ALL</li> <li>INTERSECT</li> <li>EXCEPT</li> </ul> </li> <li>Subqueries</li> <li>Common Table Expressions (CTEs)</li> <li>Window Functions</li> <li>Transaction Control Language (TCL)<ul> <li>START TRANSACTION (or BEGIN)</li> <li>COMMIT</li> <li>ROLLBACK</li> <li>SAVEPOINT</li> <li>ROLLBACK TO SAVEPOINT</li> <li>SET TRANSACTION</li> </ul> </li> <li>String Functions</li> <li>Date and Time Functions</li> <li>Conditional Expressions<ul> <li>CASE</li> <li>IF (MySQL, SQL Server)</li> <li>COALESCE</li> <li>NULLIF</li> </ul> </li> <li>User-Defined Functions (UDFs)</li> <li>Stored Procedures</li> <li>Triggers</li> <li>Indexes<ul> <li>Creating Indexes</li> <li>Dropping Indexes</li> </ul> </li> <li>Views<ul> <li>Creating Views</li> <li>Dropping Views</li> </ul> </li> <li>Transactions</li> <li>Security</li> <li>Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of SQL (Structured Query Language), covering data types, Data Definition Language (DDL), Data Manipulation Language (DML), Data Query Language (DQL), Transaction Control Language (TCL), joins, subqueries, window functions, common table expressions (CTEs), and best practices. It aims to be a complete reference for writing and understanding SQL queries.  This cheat sheet is designed to be generally applicable across different SQL database systems (e.g., MySQL, PostgreSQL, SQL Server, Oracle, SQLite), but notes specific differences where significant.</p> SQL Cheat Sheet Images <p> https://www.sqltutorial.org/sql-cheat-sheet/ </p> <p> https://learnsql.com/blog/mysql-cheat-sheet/ </p> <p> https://www.datacamp.com/cheat-sheet/sql-basics-cheat-sheet </p> <p> https://www.datacamp.com/cheat-sheet/my-sql-basics-cheat-sheet </p>"},{"location":"Cheat-Sheets/SQL/#data-types","title":"Data Types","text":""},{"location":"Cheat-Sheets/SQL/#numeric","title":"Numeric","text":"<ul> <li><code>INT</code>, <code>INTEGER</code>: Integer values.</li> <li><code>SMALLINT</code>: Smaller integer values.</li> <li><code>BIGINT</code>: Larger integer values.</li> <li><code>TINYINT</code>: Very small integer values (MySQL, SQL Server).</li> <li><code>REAL</code>: Single-precision floating-point numbers.</li> <li><code>FLOAT(p)</code>: Floating-point number with precision <code>p</code>.</li> <li><code>DOUBLE PRECISION</code>: Double-precision floating-point numbers.</li> <li><code>DECIMAL(p, s)</code>, <code>NUMERIC(p, s)</code>: Fixed-point numbers with precision <code>p</code> and scale <code>s</code>.</li> </ul>"},{"location":"Cheat-Sheets/SQL/#string","title":"String","text":"<ul> <li><code>CHAR(n)</code>: Fixed-length character string of length <code>n</code>.</li> <li><code>VARCHAR(n)</code>: Variable-length character string with a maximum length of <code>n</code>.</li> <li><code>TEXT</code>: Variable-length character string with no specified maximum length (or a very large maximum).</li> <li><code>NCHAR(n)</code>, <code>NVARCHAR(n)</code>: Unicode character strings (for storing characters from different languages).</li> </ul>"},{"location":"Cheat-Sheets/SQL/#date-and-time","title":"Date and Time","text":"<ul> <li><code>DATE</code>: Date (YYYY-MM-DD).</li> <li><code>TIME</code>: Time (HH:MI:SS).</li> <li><code>DATETIME</code>, <code>TIMESTAMP</code>: Date and time.</li> <li><code>INTERVAL</code>: A period of time.</li> </ul>"},{"location":"Cheat-Sheets/SQL/#boolean","title":"Boolean","text":"<ul> <li><code>BOOLEAN</code>: True or False.  (Some databases, like MySQL, use <code>TINYINT(1)</code> to represent booleans).</li> </ul>"},{"location":"Cheat-Sheets/SQL/#other","title":"Other","text":"<ul> <li><code>BLOB</code>: Binary large object (for storing binary data).</li> <li><code>CLOB</code>: Character large object (for storing large text data).</li> <li><code>JSON</code>, <code>JSONB</code>: JSON data (supported by some databases like PostgreSQL).</li> <li><code>UUID</code>: Universally Unique Identifier (supported by some databases like PostgreSQL).</li> <li><code>ENUM</code>: Enumerated type (MySQL, PostgreSQL).</li> <li><code>ARRAY</code>: Array type (PostgreSQL).</li> </ul>"},{"location":"Cheat-Sheets/SQL/#data-definition-language-ddl","title":"Data Definition Language (DDL)","text":""},{"location":"Cheat-Sheets/SQL/#create-table","title":"CREATE TABLE","text":"<pre><code>CREATE TABLE table_name (\n    column1 datatype constraints,\n    column2 datatype constraints,\n    ...\n    PRIMARY KEY (column1),\n    FOREIGN KEY (column_fk) REFERENCES other_table(other_column)\n);\n\n-- Example\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    email VARCHAR(100) UNIQUE,\n    hire_date DATE,\n    salary DECIMAL(10, 2),\n    department_id INT,\n    FOREIGN KEY (department_id) REFERENCES departments(id)\n);\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#alter-table","title":"ALTER TABLE","text":"<pre><code>-- Add a column\nALTER TABLE table_name ADD COLUMN column_name datatype;\n\n-- Drop a column\nALTER TABLE table_name DROP COLUMN column_name;\n\n-- Modify a column\nALTER TABLE table_name MODIFY COLUMN column_name new_datatype;  -- MySQL, SQL Server\nALTER TABLE table_name ALTER COLUMN column_name TYPE new_datatype; -- PostgreSQL\n\n-- Add a constraint\nALTER TABLE table_name ADD CONSTRAINT constraint_name constraint_definition;\n\n-- Drop a constraint\nALTER TABLE table_name DROP CONSTRAINT constraint_name; -- Most databases\nALTER TABLE table_name DROP INDEX constraint_name; -- MySQL (for UNIQUE constraints)\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#drop-table","title":"DROP TABLE","text":"<pre><code>DROP TABLE table_name;\n\n-- Drop table only if it exists (avoids error if it doesn't)\nDROP TABLE IF EXISTS table_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#truncate-table","title":"TRUNCATE TABLE","text":"<pre><code>TRUNCATE TABLE table_name;  -- Removes all rows, faster than DELETE\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#create-index","title":"CREATE INDEX","text":"<pre><code>CREATE INDEX index_name ON table_name (column1, column2, ...);\n\n-- Unique index\nCREATE UNIQUE INDEX index_name ON table_name (column1);\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#drop-index","title":"DROP INDEX","text":"<pre><code>DROP INDEX index_name ON table_name; -- Most databases\nALTER TABLE table_name DROP INDEX index_name; -- MySQL\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#create-view","title":"CREATE VIEW","text":"<pre><code>CREATE VIEW view_name AS\nSELECT column1, column2, ...\nFROM table_name\nWHERE condition;\n\n-- Example\nCREATE VIEW employee_names AS\nSELECT first_name, last_name\nFROM employees;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#drop-view","title":"DROP VIEW","text":"<pre><code>DROP VIEW view_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#database-operations","title":"DATABASE Operations","text":"<pre><code>-- Create a new database\nCREATE DATABASE database_name;\n\n-- Delete an existing database\nDROP DATABASE database_name;\n\n-- Delete an existing database only if it exists (avoids error if it doesn't)\nDROP DATABASE IF EXISTS database_name;\n\n-- Select a database to use (syntax varies, common in MySQL)\nUSE database_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#data-manipulation-language-dml","title":"Data Manipulation Language (DML)","text":""},{"location":"Cheat-Sheets/SQL/#insert","title":"INSERT","text":"<pre><code>INSERT INTO table_name (column1, column2, ...) VALUES (value1, value2, ...);\n\n-- Insert multiple rows\nINSERT INTO table_name (column1, column2) VALUES\n(value1a, value2a),\n(value1b, value2b),\n(value1c, value2c);\n\n-- Insert from another table\nINSERT INTO table_name (column1, column2)\nSELECT column1, column2\nFROM other_table\nWHERE condition;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#update","title":"UPDATE","text":"<pre><code>UPDATE table_name\nSET column1 = value1, column2 = value2, ...\nWHERE condition;\n\n-- Example\nUPDATE employees\nSET salary = salary * 1.10\nWHERE department_id = 1;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#delete","title":"DELETE","text":"<pre><code>DELETE FROM table_name WHERE condition;\n\n-- Example\nDELETE FROM employees WHERE id = 123;\n\n-- Delete all rows (slower than TRUNCATE TABLE)\nDELETE FROM table_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#data-query-language-dql","title":"Data Query Language (DQL)","text":""},{"location":"Cheat-Sheets/SQL/#select","title":"SELECT","text":"<pre><code>SELECT column1, column2, ...\nFROM table_name\nWHERE condition\nORDER BY column1 ASC, column2 DESC\nLIMIT n OFFSET m;\n\n-- Select all columns\nSELECT * FROM table_name;\n\n-- Select with aliases\nSELECT column1 AS alias1, column2 AS alias2 FROM table_name;\n\n-- Select distinct values\nSELECT DISTINCT column1 FROM table_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#where-clause","title":"WHERE Clause","text":"<pre><code>SELECT * FROM table_name WHERE column1 = value1 AND column2 &gt; value2;\nSELECT * FROM table_name WHERE column1 IN (value1, value2, value3);\nSELECT * FROM table_name WHERE column1 BETWEEN value1 AND value2;\nSELECT * FROM table_name WHERE column1 LIKE 'pattern%'; -- % is a wildcard\nSELECT * FROM table_name WHERE column1 IS NULL;\nSELECT * FROM table_name WHERE column1 IS NOT NULL;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#order-by-clause","title":"ORDER BY Clause","text":"<pre><code>SELECT * FROM table_name ORDER BY column1 ASC, column2 DESC;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#limit-and-offset-clauses","title":"LIMIT and OFFSET Clauses","text":"<pre><code>SELECT * FROM table_name LIMIT 10;  -- Get the first 10 rows\nSELECT * FROM table_name LIMIT 10 OFFSET 5;  -- Get 10 rows starting from row 6\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#aggregate-functions","title":"Aggregate Functions","text":"<ul> <li><code>COUNT()</code>: Counts rows.</li> <li><code>SUM()</code>: Sums values.</li> <li><code>AVG()</code>: Calculates the average.</li> <li><code>MIN()</code>: Finds the minimum value.</li> <li><code>MAX()</code>: Finds the maximum value.</li> </ul> <pre><code>SELECT COUNT(*) FROM table_name;\nSELECT SUM(salary) FROM employees;\nSELECT AVG(age) FROM employees;\nSELECT MIN(hire_date) FROM employees;\nSELECT MAX(salary) FROM employees;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#group-by-clause","title":"GROUP BY Clause","text":"<pre><code>SELECT department_id, AVG(salary) AS average_salary\nFROM employees\nGROUP BY department_id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#having-clause","title":"HAVING Clause","text":"<pre><code>SELECT department_id, AVG(salary) AS average_salary\nFROM employees\nGROUP BY department_id\nHAVING AVG(salary) &gt; 50000;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#order-of-execution","title":"Order of execution","text":"Order of execution"},{"location":"Cheat-Sheets/SQL/#joins","title":"Joins","text":"<p>Visualise joins:</p> <ul> <li>https://joins.spathon.com/</li> <li>https://sql-joins.leopard.in.ua/</li> </ul> https://www.atlassian.com/data/sql/sql-join-types-explained-visually"},{"location":"Cheat-Sheets/SQL/#inner-join","title":"INNER JOIN","text":"<pre><code>SELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nINNER JOIN departments d ON e.department_id = d.id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#left-join-left-outer-join","title":"LEFT JOIN (LEFT OUTER JOIN)","text":"<pre><code>SELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nLEFT JOIN departments d ON e.department_id = d.id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#right-join-right-outer-join","title":"RIGHT JOIN (RIGHT OUTER JOIN)","text":"<pre><code>SELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nRIGHT JOIN departments d ON e.department_id = d.id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#full-join-full-outer-join","title":"FULL JOIN (FULL OUTER JOIN)","text":"<pre><code>-- Full outer join is not supported by all databases (e.g., MySQL).\n-- Use a combination of LEFT JOIN and RIGHT JOIN with UNION for equivalent functionality.\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nFULL OUTER JOIN departments d ON e.department_id = d.id;\n\n-- Equivalent in MySQL:\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nLEFT JOIN departments d ON e.department_id = d.id\nUNION\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nRIGHT JOIN departments d ON e.department_id = d.id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#self-join","title":"Self Join","text":"<pre><code>SELECT e1.first_name, e2.first_name AS manager_name\nFROM employees e1\nJOIN employees e2 ON e1.manager_id = e2.id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#cross-join","title":"Cross Join","text":"<pre><code>SELECT *\nFROM table1\nCROSS JOIN table2;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#set-operations","title":"Set Operations","text":""},{"location":"Cheat-Sheets/SQL/#union","title":"UNION","text":"<p>Combines the results of two <code>SELECT</code> statements and removes duplicate rows.</p> <pre><code>SELECT column1, column2 FROM table1\nUNION\nSELECT column1, column2 FROM table2;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#union-all","title":"UNION ALL","text":"<p>Combines the results of two <code>SELECT</code> statements, including duplicate rows.</p> <pre><code>SELECT column1, column2 FROM table1\nUNION ALL\nSELECT column1, column2 FROM table2;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#intersect","title":"INTERSECT","text":"<p>Returns the rows that are common to both <code>SELECT</code> statements.</p> <pre><code>SELECT column1, column2 FROM table1\nINTERSECT\nSELECT column1, column2 FROM table2;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#except","title":"EXCEPT","text":"<p>Returns the rows that are in the first <code>SELECT</code> statement but not in the second.</p> <pre><code>SELECT column1, column2 FROM table1\nEXCEPT\nSELECT column1, column2 FROM table2;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#subqueries","title":"Subqueries","text":"<pre><code>-- Subquery in WHERE clause\nSELECT *\nFROM employees\nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n\n-- Subquery in SELECT clause\nSELECT first_name, last_name,\n       (SELECT COUNT(*) FROM orders WHERE orders.employee_id = employees.id) AS order_count\nFROM employees;\n\n-- Subquery in FROM clause\nSELECT *\nFROM (SELECT first_name, last_name, salary FROM employees) AS employee_salaries\nWHERE salary &gt; 60000;\n\n-- Correlated subquery\nSELECT e.first_name, e.last_name\nFROM employees e\nWHERE e.salary &gt; (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id);\n\n-- EXISTS and NOT EXISTS\nSELECT *\nFROM employees e\nWHERE EXISTS (SELECT 1 FROM orders o WHERE o.employee_id = e.id);\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#common-table-expressions-ctes","title":"Common Table Expressions (CTEs)","text":"<pre><code>WITH employee_summary AS (\n    SELECT department_id, AVG(salary) AS avg_salary\n    FROM employees\n    GROUP BY department_id\n)\nSELECT d.department_name, es.avg_salary\nFROM departments d\nJOIN employee_summary es ON d.id = es.department_id;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#window-functions","title":"Window Functions","text":"<pre><code>SELECT\n    first_name,\n    last_name,\n    salary,\n    AVG(salary) OVER (PARTITION BY department_id) AS avg_salary_by_department,\n    RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) AS salary_rank\nFROM employees;\n</code></pre> <p>Common Window Functions:</p> <ul> <li><code>ROW_NUMBER()</code>: Assigns a unique sequential integer to each row within its partition.</li> <li><code>RANK()</code>: Assigns a rank to each row within its partition, with gaps in rank values.</li> <li><code>DENSE_RANK()</code>: Assigns a rank to each row within its partition, without gaps.</li> <li><code>NTILE(n)</code>: Divides the rows within a partition into <code>n</code> groups.</li> <li><code>LAG(column, offset, default)</code>: Accesses data from a previous row.</li> <li><code>LEAD(column, offset, default)</code>: Accesses data from a subsequent row.</li> <li><code>FIRST_VALUE(column)</code>: Returns the first value in a window frame.</li> <li><code>LAST_VALUE(column)</code>: Returns the last value in a window frame.</li> <li><code>NTH_VALUE(column, n)</code>: Returns the nth value in a window frame.</li> </ul>"},{"location":"Cheat-Sheets/SQL/#transaction-control-language-tcl","title":"Transaction Control Language (TCL)","text":""},{"location":"Cheat-Sheets/SQL/#start-transaction-or-begin","title":"START TRANSACTION (or BEGIN)","text":"<pre><code>START TRANSACTION;\n-- or\nBEGIN;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#commit","title":"COMMIT","text":"<pre><code>COMMIT;  -- Save changes\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#rollback","title":"ROLLBACK","text":"<pre><code>ROLLBACK;  -- Discard changes\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#savepoint","title":"SAVEPOINT","text":"<pre><code>SAVEPOINT savepoint_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#rollback-to-savepoint","title":"ROLLBACK TO SAVEPOINT","text":"<pre><code>ROLLBACK TO SAVEPOINT savepoint_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#set-transaction","title":"SET TRANSACTION","text":"<pre><code>SET TRANSACTION ISOLATION LEVEL READ COMMITTED; -- Example\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#string-functions","title":"String Functions","text":"<ul> <li><code>CONCAT(str1, str2, ...)</code>: Concatenates strings.</li> <li><code>LENGTH(str)</code> or <code>LEN(str)</code>: Returns the length of a string.</li> <li><code>SUBSTRING(str, start, length)</code> or <code>SUBSTR(str, start, length)</code>: Extracts a substring.</li> <li><code>UPPER(str)</code> or <code>UCASE(str)</code>: Converts a string to uppercase.</li> <li><code>LOWER(str)</code> or <code>LCASE(str)</code>: Converts a string to lowercase.</li> <li><code>TRIM(str)</code>: Removes leading and trailing whitespace.</li> <li><code>LTRIM(str)</code>: Removes leading whitespace.</li> <li><code>RTRIM(str)</code>: Removes trailing whitespace.</li> <li><code>REPLACE(str, old, new)</code>: Replaces occurrences of a substring.</li> <li><code>INSTR(str, substr)</code> or <code>POSITION(substr IN str)</code>: Returns the position of a substring.</li> <li><code>LEFT(str, length)</code>: Returns the leftmost characters of a string.</li> <li><code>RIGHT(str, length)</code>: Returns the rightmost characters of a string.</li> <li><code>LPAD(str, length, padstr)</code>: Left-pads a string.</li> <li><code>RPAD(str, length, padstr)</code>: Right-pads a string.</li> </ul>"},{"location":"Cheat-Sheets/SQL/#date-and-time-functions","title":"Date and Time Functions","text":"<ul> <li><code>NOW()</code>, <code>CURRENT_TIMESTAMP</code>: Returns the current date and time.</li> <li><code>CURDATE()</code>, <code>CURRENT_DATE</code>: Returns the current date.</li> <li><code>CURTIME()</code>, <code>CURRENT_TIME</code>: Returns the current time.</li> <li><code>DATE(expression)</code>: Extracts the date part of a date or datetime expression.</li> <li><code>TIME(expression)</code>: Extracts the time part of a time or datetime expression.</li> <li><code>YEAR(date)</code>, <code>MONTH(date)</code>, <code>DAY(date)</code>: Extracts the year, month, or day from a date.</li> <li><code>HOUR(time)</code>, <code>MINUTE(time)</code>, <code>SECOND(time)</code>: Extracts the hour, minute, or second from a time.</li> <li><code>EXTRACT(unit FROM datetime)</code>: Extracts a specific unit (e.g., <code>YEAR</code>, <code>MONTH</code>, <code>DAY</code>, <code>HOUR</code>, <code>MINUTE</code>, <code>SECOND</code>) from a date or timestamp.</li> <li><code>DATE_ADD(date, INTERVAL expr unit)</code>, <code>DATE_SUB(date, INTERVAL expr unit)</code>: Adds or subtracts a time interval (units: <code>DAY</code>, <code>WEEK</code>, <code>MONTH</code>, <code>YEAR</code>, etc.).</li> <li><code>DATEDIFF(date1, date2)</code>: Returns the difference between two dates (result unit varies by database, often days).</li> <li><code>TIMESTAMPDIFF(unit, datetime1, datetime2)</code>: Returns the difference between two datetimes in a specified unit (units: <code>MINUTE</code>, <code>HOUR</code>, <code>SECOND</code>, <code>DAY</code>, <code>MONTH</code>, <code>YEAR</code>).</li> <li><code>DATE_FORMAT(date, format)</code>: Formats a date according to the specified format string (format codes vary by database).</li> <li><code>DAYOFWEEK(date)</code>: Returns the day of the week as a number (e.g., 1=Sunday, 2=Monday...).</li> <li><code>WEEKOFYEAR(date)</code>: Returns the week number of the year.</li> <li><code>QUARTER(date)</code>: Returns the quarter of the year (1-4).</li> <li><code>WEEK(date)</code>: Returns the week number (behavior can vary based on mode/database).</li> </ul> <pre><code>-- Get current date, time, timestamp\nSELECT CURRENT_DATE();\nSELECT CURRENT_TIME();\nSELECT CURRENT_TIMESTAMP();\n\n-- Extract parts of a date/time\nSELECT DATE(CURRENT_TIMESTAMP());\nSELECT EXTRACT(YEAR FROM CURRENT_TIMESTAMP());\nSELECT EXTRACT(MONTH FROM CURRENT_TIMESTAMP());\nSELECT EXTRACT(DAY FROM CURRENT_TIMESTAMP());\nSELECT EXTRACT(HOUR FROM CURRENT_TIMESTAMP());\nSELECT EXTRACT(MINUTE FROM CURRENT_TIMESTAMP());\nSELECT EXTRACT(SECOND FROM CURRENT_TIMESTAMP());\n\n-- Get week/day information\nSELECT DAYOFWEEK(CURRENT_TIMESTAMP()); -- 1=Sunday, 2=Monday, ..., 7=Saturday (common convention)\nSELECT WEEKOFYEAR(CURRENT_TIMESTAMP());\nSELECT QUARTER(CURRENT_DATE());\nSELECT WEEK(CURRENT_DATE()); -- Behavior might depend on mode\n\n-- Date arithmetic\nSELECT DATE_ADD(CURRENT_DATE(), INTERVAL 4 DAY) AS four_days_from_today;\nSELECT DATE_ADD(CURRENT_DATE(), INTERVAL 1 DAY);\nSELECT DATE_ADD(CURRENT_DATE(), INTERVAL 2 WEEK);\nSELECT DATE_ADD(CURRENT_DATE(), INTERVAL 3 MONTH);\nSELECT DATE_ADD(CURRENT_DATE(), INTERVAL 4 YEAR);\n\n-- Date differences\nSELECT DATEDIFF(CURRENT_DATE(), '2023-01-01'); -- Difference in days (example)\nSELECT TIMESTAMPDIFF(HOUR, '2023-01-01 10:00:00', CURRENT_TIMESTAMP()); -- Difference in hours\n\n-- Formatting\nSELECT DATE_FORMAT(CURRENT_DATE(), '%Y-%m-%d'); -- Common format codes\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#conditional-expressions","title":"Conditional Expressions","text":""},{"location":"Cheat-Sheets/SQL/#case","title":"CASE","text":"<pre><code>SELECT\n    first_name,\n    last_name,\n    CASE\n        WHEN salary &gt; 80000 THEN 'High'\n        WHEN salary &gt; 50000 THEN 'Medium'\n        ELSE 'Low'\n    END AS salary_level\nFROM employees;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#if-mysql-sql-server","title":"IF (MySQL, SQL Server)","text":"<pre><code>SELECT first_name, last_name, IF(salary &gt; 50000, 'High', 'Low') AS salary_level\nFROM employees;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#coalesce","title":"COALESCE","text":"<pre><code>SELECT COALESCE(column1, column2, 'Default Value') AS result FROM table_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#nullif","title":"NULLIF","text":"<pre><code>SELECT NULLIF(column1, value) AS result FROM table_name;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#user-defined-functions-udfs","title":"User-Defined Functions (UDFs)","text":"<p>(Syntax varies significantly between database systems)</p> <p>Example (MySQL):</p> <pre><code>DELIMITER //\nCREATE FUNCTION my_function(param1 INT, param2 VARCHAR(255))\nRETURNS INT\nDETERMINISTIC\nBEGIN\n    -- Function logic\n    RETURN result;\nEND //\nDELIMITER ;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#stored-procedures","title":"Stored Procedures","text":"<p>(Syntax varies significantly between database systems)</p> <p>Example (MySQL):</p> <pre><code>DELIMITER //\nCREATE PROCEDURE my_procedure(IN param1 INT, OUT param2 VARCHAR(255))\nBEGIN\n    -- Procedure logic\n    SELECT column1 INTO param2 FROM table_name WHERE column2 = param1;\nEND //\nDELIMITER ;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#triggers","title":"Triggers","text":"<p>(Syntax varies significantly between database systems)</p> <p>Example (MySQL):</p> <pre><code>DELIMITER //\nCREATE TRIGGER my_trigger\nBEFORE INSERT ON employees\nFOR EACH ROW\nBEGIN\n    -- Trigger logic\n    SET NEW.created_at = NOW();\nEND //\nDELIMITER ;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#indexes","title":"Indexes","text":""},{"location":"Cheat-Sheets/SQL/#creating-indexes","title":"Creating Indexes","text":"<pre><code>CREATE INDEX idx_lastname ON employees (last_name);\nCREATE UNIQUE INDEX idx_email ON employees (email);\nCREATE INDEX idx_lastname_firstname ON employees (last_name, first_name);\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#dropping-indexes","title":"Dropping Indexes","text":"<pre><code>DROP INDEX idx_lastname ON employees; -- Standard SQL\nALTER TABLE employees DROP INDEX idx_lastname; -- MySQL\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#views","title":"Views","text":""},{"location":"Cheat-Sheets/SQL/#creating-views","title":"Creating Views","text":"<pre><code>CREATE VIEW high_salary_employees AS\nSELECT employee_id, first_name, last_name, salary\nFROM employees\nWHERE salary &gt; 80000;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#dropping-views","title":"Dropping Views","text":"<pre><code>DROP VIEW high_salary_employees;\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#transactions","title":"Transactions","text":"<pre><code>START TRANSACTION; -- or BEGIN;\n\n-- SQL statements\n\nCOMMIT; -- Save changes\n-- or\nROLLBACK; -- Discard changes\n</code></pre>"},{"location":"Cheat-Sheets/SQL/#security","title":"Security","text":"<ul> <li>User Management: <code>CREATE USER</code>, <code>ALTER USER</code>, <code>DROP USER</code>, <code>GRANT</code>, <code>REVOKE</code>.</li> <li>Permissions: Grant specific privileges (e.g., <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) to users or roles on database objects.</li> <li>Roles: Create roles to group privileges and assign them to users.</li> <li>Views: Use views to restrict access to sensitive data.</li> <li>Stored Procedures: Use stored procedures to encapsulate logic and control access.</li> <li>Encryption: Encrypt sensitive data at rest and in transit.</li> <li>Auditing: Enable auditing to track database activity.</li> <li>SQL Injection Prevention: Use parameterized queries or prepared statements to prevent SQL injection attacks.</li> </ul>"},{"location":"Cheat-Sheets/SQL/#best-practices","title":"Best Practices","text":"<ul> <li>Use meaningful names: Choose descriptive names for tables, columns, and other database objects.</li> <li>Normalize your database: Design your database schema to reduce data redundancy and improve data integrity.</li> <li>Use appropriate data types: Select data types that are appropriate for the data you are storing.</li> <li>Use indexes: Create indexes on columns that are frequently used in <code>WHERE</code> clauses and <code>JOIN</code> conditions.</li> <li>Optimize your queries: Write efficient queries that minimize the amount of data that needs to be processed.</li> <li>Use transactions: Use transactions to ensure data consistency and integrity.</li> <li>Back up your database: Regularly back up your database to prevent data loss.</li> <li>Secure your database: Implement appropriate security measures to protect your data.</li> <li>Use comments: Add comments to your SQL code to explain what it does.</li> <li>Use a consistent coding style: Follow a consistent coding style to make your code easier to read and maintain.</li> <li>Test your queries: Thoroughly test your queries to ensure they are working as expected.</li> <li>Use a database management tool: Use a tool like MySQL Workbench, pgAdmin, SQL Server Management Studio, or Dbeaver to manage your database.</li> <li>Use version control: Use a version control system (e.g., Git) to track changes to your database schema and code.</li> <li>Use an ORM (Object-Relational Mapper): Consider using an ORM (e.g., SQLAlchemy, Django ORM) to simplify database interactions.</li> <li>Avoid <code>SELECT *</code>: Explicitly list the columns you need to retrieve.</li> <li>Use <code>EXISTS</code> instead of <code>COUNT(*)</code> when checking for existence: <code>EXISTS</code> is often more efficient.</li> <li>Use <code>JOIN</code> instead of subqueries when possible: Joins are generally faster.</li> <li>Use <code>UNION ALL</code> instead of <code>UNION</code> when you don't need to remove duplicates: <code>UNION ALL</code> is faster.</li> <li>Use <code>CASE</code> expressions for conditional logic: <code>CASE</code> expressions are more flexible than <code>IF</code>.</li> <li>Use CTEs to improve readability: CTEs can make complex queries easier to understand.</li> <li>Use window functions for advanced analytics: Window functions allow you to perform calculations across rows.</li> <li>Use stored procedures and functions to encapsulate logic: This can improve code reusability and maintainability.</li> <li>Use triggers to automate tasks: Triggers can be used to automatically perform actions when certain events occur.</li> <li>Use views to simplify complex queries: Views can make it easier to access data from multiple tables.</li> <li>Use indexes to improve query performance: Indexes can significantly speed up queries that filter or sort data.</li> <li>Use explain plans to analyze query performance: Explain plans show you how the database is executing your queries.</li> <li>Use a database profiler to identify performance bottlenecks: Profilers can help you find slow queries and other performance issues.</li> <li>Use a database monitoring tool to track database performance: Monitoring tools can help you identify and resolve performance problems.</li> <li>Regularly update your database software: Updates often include performance improvements and security fixes.</li> <li>Follow database best practices: Each database system has its own set of best practices.</li> </ul>"},{"location":"Cheat-Sheets/Sk-learn/","title":"Scikit-learn Cheat Sheet","text":"<ul> <li>Scikit-learn Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Importing Scikit-learn</li> </ul> </li> <li>Data Preprocessing<ul> <li>Loading Data<ul> <li>Built-in Datasets</li> <li>From Pandas DataFrame</li> </ul> </li> <li>Splitting Data</li> <li>Feature Scaling<ul> <li>Standardization</li> <li>Min-Max Scaling</li> <li>Robust Scaling</li> <li>Normalization</li> </ul> </li> <li>Handling Missing Values<ul> <li>Imputation (SimpleImputer)</li> <li>Dropping Missing Values</li> </ul> </li> <li>Encoding Categorical Features<ul> <li>One-Hot Encoding</li> <li>Ordinal Encoding</li> <li>Label Encoding (for target variable)</li> </ul> </li> <li>Feature Engineering<ul> <li>Polynomial Features</li> <li>Custom Transformers</li> </ul> </li> <li>Feature Selection<ul> <li>VarianceThreshold</li> <li>SelectKBest</li> <li>SelectFromModel</li> <li>RFE (Recursive Feature Elimination)</li> </ul> </li> </ul> </li> <li>Model Selection and Training<ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Support Vector Machines (SVM)</li> <li>Decision Trees</li> <li>Random Forest</li> <li>Gradient Boosting</li> <li>K-Nearest Neighbors (KNN)</li> <li>Naive Bayes</li> <li>Clustering (K-Means)</li> <li>Principal Component Analysis (PCA)</li> <li>Model Persistence</li> </ul> </li> <li>Model Evaluation<ul> <li>Regression Metrics</li> <li>Classification Metrics</li> <li>ROC Curve and AUC</li> <li>Cross-Validation</li> <li>Learning Curves</li> <li>Validation Curves</li> </ul> </li> <li>Hyperparameter Tuning<ul> <li>GridSearchCV</li> <li>RandomizedSearchCV</li> </ul> </li> <li>Pipelines</li> <li>Ensemble Methods<ul> <li>Bagging</li> <li>Boosting (AdaBoost)</li> <li>Stacking</li> <li>Voting Classifier</li> </ul> </li> <li>Dimensionality Reduction<ul> <li>PCA</li> <li>Linear Discriminant Analysis (LDA)</li> <li>t-distributed Stochastic Neighbor Embedding (t-SNE)</li> <li>Non-negative Matrix Factorization (NMF)</li> </ul> </li> <li>Model Inspection<ul> <li>Feature Importances</li> <li>Partial Dependence Plots</li> <li>Permutation Importance</li> </ul> </li> <li>Calibration</li> <li>Dummy Estimators</li> <li>Multi-label Classification</li> <li>Multi-class and Multi-label Classification</li> <li>Outlier Detection</li> <li>Semi-Supervised Learning</li> <li>Tips and Best Practices</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of the Scikit-learn (sklearn) machine learning library, covering essential concepts, code snippets, and best practices for efficient model building, training, evaluation, and deployment. It aims to be a one-stop reference for common tasks.</p>"},{"location":"Cheat-Sheets/Sk-learn/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/Sk-learn/#installation","title":"Installation","text":"<pre><code>pip install scikit-learn\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#importing-scikit-learn","title":"Importing Scikit-learn","text":"<pre><code>import sklearn\nfrom sklearn import datasets  # For built-in datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"Cheat-Sheets/Sk-learn/#loading-data","title":"Loading Data","text":""},{"location":"Cheat-Sheets/Sk-learn/#built-in-datasets","title":"Built-in Datasets","text":"<pre><code>from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nboston = datasets.load_boston() # Now deprecated, use fetch_california_housing\ncalifornia_housing = datasets.fetch_california_housing()\nX = california_housing.data\ny = california_housing.target\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#from-pandas-dataframe","title":"From Pandas DataFrame","text":"<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"your_data.csv\")\nX = df.drop(\"target_column\", axis=1)\ny = df[\"target_column\"]\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#splitting-data","title":"Splitting Data","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% training, 20% testing\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#feature-scaling","title":"Feature Scaling","text":""},{"location":"Cheat-Sheets/Sk-learn/#standardization","title":"Standardization","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#min-max-scaling","title":"Min-Max Scaling","text":"<pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#robust-scaling","title":"Robust Scaling","text":"<pre><code>from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#normalization","title":"Normalization","text":"<pre><code>from sklearn.preprocessing import Normalizer\n\nnormalizer = Normalizer()\nX_train_normalized = normalizer.fit_transform(X_train)\nX_test_normalized = normalizer.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#handling-missing-values","title":"Handling Missing Values","text":""},{"location":"Cheat-Sheets/Sk-learn/#imputation-simpleimputer","title":"Imputation (SimpleImputer)","text":"<pre><code>from sklearn.impute import SimpleImputer\nimport numpy as np\n\nimputer = SimpleImputer(strategy=\"mean\")  # Replace missing values with the mean\n# Other strategies: \"median\", \"most_frequent\", \"constant\"\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)```\n\n#### Imputation (KNNImputer)\n\n```python\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=5)\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#dropping-missing-values","title":"Dropping Missing Values","text":"<pre><code>import pandas as pd\n# Assuming X_train and X_test are pandas DataFrames\nX_train_dropped = X_train.dropna()\nX_test_dropped = X_test.dropna()\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#encoding-categorical-features","title":"Encoding Categorical Features","text":""},{"location":"Cheat-Sheets/Sk-learn/#one-hot-encoding","title":"One-Hot Encoding","text":"<pre><code>from sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\n# Assuming X_train and X_test are pandas DataFrames\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # sparse=False for older versions\nX_train_encoded = encoder.fit_transform(X_train[['categorical_feature']])\nX_test_encoded = encoder.transform(X_test[['categorical_feature']])\n\n# Or, using pandas:\nX_train_encoded = pd.get_dummies(X_train, columns=['categorical_feature'])\nX_test_encoded = pd.get_dummies(X_test, columns=['categorical_feature'])\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#ordinal-encoding","title":"Ordinal Encoding","text":"<pre><code>from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder()\nX_train_encoded = encoder.fit_transform(X_train[['ordinal_feature']])\nX_test_encoded = encoder.transform(X_test[['ordinal_feature']])\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#label-encoding-for-target-variable","title":"Label Encoding (for target variable)","text":"<pre><code>from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny_train_encoded = encoder.fit_transform(y_train)\ny_test_encoded = encoder.transform(y_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#feature-engineering","title":"Feature Engineering","text":""},{"location":"Cheat-Sheets/Sk-learn/#polynomial-features","title":"Polynomial Features","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#custom-transformers","title":"Custom Transformers","text":"<pre><code>from sklearn.preprocessing import FunctionTransformer\nimport numpy as np\n\ndef log_transform(x):\n    return np.log1p(x)\n\nlog_transformer = FunctionTransformer(log_transform)\nX_train_log = log_transformer.transform(X_train)\nX_test_log = log_transformer.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#feature-selection","title":"Feature Selection","text":""},{"location":"Cheat-Sheets/Sk-learn/#variancethreshold","title":"VarianceThreshold","text":"<pre><code>from sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(threshold=0.1)  # Remove features with variance below 0.1\nX_train_selected = selector.fit_transform(X_train)\nX_test_selected = selector.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#selectkbest","title":"SelectKBest","text":"<pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(score_func=f_classif, k=5)  # Select top 5 features\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#selectfrommodel","title":"SelectFromModel","text":"<pre><code>from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression(penalty=\"l1\", solver='liblinear')\nselector = SelectFromModel(estimator)\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#rfe-recursive-feature-elimination","title":"RFE (Recursive Feature Elimination)","text":"<pre><code>from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression()\nselector = RFE(estimator, n_features_to_select=5)\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#model-selection-and-training","title":"Model Selection and Training","text":""},{"location":"Cheat-Sheets/Sk-learn/#linear-regression","title":"Linear Regression","text":"<pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#logistic-regression","title":"Logistic Regression","text":"<pre><code>from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='liblinear') # Add solver for smaller datasets\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#support-vector-machines-svm","title":"Support Vector Machines (SVM)","text":"<pre><code>from sklearn.svm import SVC, SVR\n\n# For classification\nmodel = SVC(kernel='linear', C=1.0)\nmodel.fit(X_train, y_train)\n\n# For regression\nmodel = SVR(kernel='linear', C=1.0)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#decision-trees","title":"Decision Trees","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\n# For classification\nmodel = DecisionTreeClassifier(max_depth=5)\nmodel.fit(X_train, y_train)\n\n# For regression\nmodel = DecisionTreeRegressor(max_depth=5)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#random-forest","title":"Random Forest","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n\n# For classification\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5)\nmodel.fit(X_train, y_train)\n\n# For regression\nmodel = RandomForestRegressor(n_estimators=100, max_depth=5)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#gradient-boosting","title":"Gradient Boosting","text":"<pre><code>from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n\n# For classification\nmodel = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel.fit(X_train, y_train)\n\n# For regression\nmodel = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)","text":"<pre><code>from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\n# For classification\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\n\n# For regression\nmodel = KNeighborsRegressor(n_neighbors=5)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#naive-bayes","title":"Naive Bayes","text":"<pre><code>from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#clustering-k-means","title":"Clustering (K-Means)","text":"<pre><code>from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=3, random_state=42, n_init = 'auto') # Added n_init\nmodel.fit(X_train)\nlabels = model.predict(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#model-persistence","title":"Model Persistence","text":"<pre><code>import joblib\n\n# Save the model\njoblib.dump(model, 'my_model.pkl')\n\n# Load the model\nloaded_model = joblib.load('my_model.pkl')\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#model-evaluation","title":"Model Evaluation","text":""},{"location":"Cheat-Sheets/Sk-learn/#regression-metrics","title":"Regression Metrics","text":"<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#classification-metrics","title":"Classification Metrics","text":"<pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#roc-curve-and-auc","title":"ROC Curve and AUC","text":"<pre><code>from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# For binary classification\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\n\nplt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#cross-validation","title":"Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n\n# K-Fold Cross-Validation\ncv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n\n# Stratified K-Fold (for classification)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(model, X, y, cv=cv)\n\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean cross-validation score: {cv_scores.mean():.2f}\")\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#learning-curves","title":"Learning Curves","text":"<pre><code>from sklearn.model_selection import learning_curve\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10))\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean, label='Training score')\nplt.plot(train_sizes, test_mean, label='Cross-validation score')\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\nplt.xlabel('Training examples')\nplt.ylabel('Score')\nplt.legend()\nplt.title('Learning Curve')\nplt.show()\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#validation-curves","title":"Validation Curves","text":"<pre><code>from sklearn.model_selection import validation_curve\nimport numpy as np\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    model, X, y, param_name=\"gamma\", param_range=param_range,\n    cv=5, scoring=\"accuracy\")\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, label='Training score')\nplt.plot(param_range, test_mean, label='Cross-validation score')\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1)\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.1)\nplt.xscale('log')\nplt.xlabel('Parameter Value')\nplt.ylabel('Score')\nplt.legend()\nplt.title('Validation Curve')\nplt.show()\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"Cheat-Sheets/Sk-learn/#gridsearchcv","title":"GridSearchCV","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': [0.1, 1, 'scale', 'auto']\n}\n\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=2)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")\nbest_model = grid_search.best_estimator_\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#randomizedsearchcv","title":"RandomizedSearchCV","text":"<pre><code>from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint\n\nparam_dist = {\n    'n_estimators': randint(10, 200),\n    'max_depth': [3, 5, 10, None],\n    'min_samples_split': randint(2, 11),\n    'min_samples_leaf': randint(1, 11),\n    'bootstrap': [True, False]\n}\n\nrandom_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist,\n                                   n_iter=20, cv=5, scoring='accuracy', random_state=42, verbose=2)\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {random_search.best_params_}\")\nprint(f\"Best cross-validation score: {random_search.best_score_:.2f}\")\nbest_model = random_search.best_estimator_\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#pipelines","title":"Pipelines","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#ensemble-methods","title":"Ensemble Methods","text":""},{"location":"Cheat-Sheets/Sk-learn/#bagging","title":"Bagging","text":"<pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbase_estimator = DecisionTreeClassifier(max_depth=5)\nbagging = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#boosting-adaboost","title":"Boosting (AdaBoost)","text":"<pre><code>from sklearn.ensemble import AdaBoostClassifier\n\nadaboost = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\nadaboost.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#stacking","title":"Stacking","text":"<pre><code>from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nestimators = [\n    ('svm', SVC(kernel='linear', C=1.0)),\n    ('dt', DecisionTreeClassifier(max_depth=5))\n]\nfinal_estimator = LogisticRegression()\n\nstacking = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\nstacking.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#voting-classifier","title":"Voting Classifier","text":"<pre><code>from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nestimator1 = LogisticRegression(solver='liblinear')\nestimator2 = SVC(kernel='linear', C=1.0, probability=True) # probability=True for soft voting\n\nvoting = VotingClassifier(estimators=[('lr', estimator1), ('svc', estimator2)], voting='soft') # 'hard' for majority voting\nvoting.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#dimensionality-reduction","title":"Dimensionality Reduction","text":""},{"location":"Cheat-Sheets/Sk-learn/#pca","title":"PCA","text":"<pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#linear-discriminant-analysis-lda","title":"Linear Discriminant Analysis (LDA)","text":"<pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_train_lda = lda.fit_transform(X_train, y_train)  # Supervised, needs y_train\nX_test_lda = lda.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-distributed Stochastic Neighbor Embedding (t-SNE)","text":"<pre><code>from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_train_tsne = tsne.fit_transform(X_train)  # Usually only fit_transform\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#non-negative-matrix-factorization-nmf","title":"Non-negative Matrix Factorization (NMF)","text":"<pre><code>from sklearn.decomposition import NMF\n\nnmf = NMF(n_components=2, random_state=42)\nX_train_nmf = nmf.fit_transform(X_train)\nX_test_nmf = nmf.transform(X_test)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#model-inspection","title":"Model Inspection","text":""},{"location":"Cheat-Sheets/Sk-learn/#feature-importances","title":"Feature Importances","text":"<pre><code># For tree-based models (RandomForest, GradientBoosting)\nimportances = model.feature_importances_\nprint(importances)\n\n# For linear models (LogisticRegression, LinearRegression)\ncoefficients = model.coef_\nprint(coefficients)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#partial-dependence-plots","title":"Partial Dependence Plots","text":"<pre><code>from sklearn.inspection import plot_partial_dependence\n\nplot_partial_dependence(model, X_train, features=[0, 1])  # Plot for features 0 and 1\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#permutation-importance","title":"Permutation Importance","text":"<pre><code>from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\nprint(result.importances_mean)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#calibration","title":"Calibration","text":"<pre><code>from sklearn.calibration import CalibratedClassifierCV\n\ncalibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5) # 'sigmoid' is another method\ncalibrated_model.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#dummy-estimators","title":"Dummy Estimators","text":"<pre><code>from sklearn.dummy import DummyClassifier, DummyRegressor\n\n# For classification\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\n\n# For regression\ndummy_reg = DummyRegressor(strategy=\"mean\")\ndummy_reg.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#multi-label-classification","title":"Multi-label Classification","text":"<pre><code>from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nmultilabel_model = MultiOutputClassifier(RandomForestClassifier())\nmultilabel_model.fit(X_train, y_train) # y_train is a 2D array of shape (n_samples, n_labels)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#multi-class-and-multi-label-classification","title":"Multi-class and Multi-label Classification","text":"<pre><code>from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\novr_model = OneVsRestClassifier(SVC(kernel='linear'))\novr_model.fit(X_train, y_train)\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#outlier-detection","title":"Outlier Detection","text":"<pre><code>from sklearn.ensemble import IsolationForest\n\noutlier_detector = IsolationForest(random_state=42)\noutlier_detector.fit(X_train)\noutliers = outlier_detector.predict(X_test) # 1 for inliers, -1 for outliers\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#semi-supervised-learning","title":"Semi-Supervised Learning","text":"<pre><code>from sklearn.semi_supervised import LabelPropagation\n\nlabel_prop_model = LabelPropagation()\nlabel_prop_model.fit(X_train, y_train) # y_train can contain -1 for unlabeled samples\n</code></pre>"},{"location":"Cheat-Sheets/Sk-learn/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Data Preprocessing: Always preprocess your data (scaling, encoding, handling missing values) before training a model.</li> <li>Cross-Validation: Use cross-validation to get a reliable estimate of your model's performance.</li> <li>Hyperparameter Tuning: Use <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> to find the best hyperparameters for your model.</li> <li>Pipelines: Use pipelines to streamline your workflow and prevent data leakage.</li> <li>Model Persistence: Save your trained models using <code>joblib</code> or <code>pickle</code>.</li> <li>Feature Importance: Use feature importance techniques to understand which features are most important for your model.</li> <li>Regularization: Use regularization techniques (L1, L2, Dropout) to prevent overfitting.</li> <li>Ensemble Methods: Combine multiple models to improve performance.</li> <li>Choose the Right Model: Select a model that is appropriate for your data and task.</li> <li>Evaluate Your Model: Use appropriate evaluation metrics for your task.</li> <li>Understand Your Data: Spend time exploring and understanding your data before building a model.</li> <li>Start Simple: Begin with a simple model and gradually increase complexity.</li> <li>Iterate: Machine learning is an iterative process. Experiment with different models, features, and hyperparameters.</li> <li>Document Your Work: Keep track of your experiments and results.</li> <li>Use Version Control: Use Git to track changes to your code.</li> <li>Use Virtual Environments: Isolate project dependencies.</li> <li>Read the Documentation: The Scikit-learn documentation is excellent.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/","title":"TensorFlow Cheat Sheet","text":"<ul> <li>TensorFlow Cheat Sheet<ul> <li>Getting Started<ul> <li>Installation</li> <li>Importing TensorFlow</li> <li>Checking Version</li> <li>Checking GPU Availability</li> </ul> </li> <li>Tensors<ul> <li>Creating Tensors</li> <li>Tensor Attributes</li> <li>Tensor Operations</li> <li>Indexing and Slicing</li> <li>Data Types</li> </ul> </li> <li>Variables</li> <li>Automatic Differentiation (Autograd)<ul> <li>Persistent Gradient Tape</li> <li>Watching Non-Variable Tensors</li> </ul> </li> <li>Keras API<ul> <li>Model Building<ul> <li>Sequential Model</li> <li>Functional API</li> <li>Model Subclassing</li> </ul> </li> <li>Layers</li> <li>Activation Functions</li> <li>Loss Functions</li> <li>Optimizers</li> <li>Metrics</li> <li>Model Compilation</li> <li>Training</li> <li>Evaluation</li> <li>Prediction</li> <li>Saving and Loading Models</li> <li>Callbacks</li> <li>Regularization</li> <li>Custom Layers</li> <li>Custom Loss Functions</li> <li>Custom Metrics</li> <li>Custom Training Loops</li> </ul> </li> <li>Data Input Pipelines (tf.data)<ul> <li>Creating Datasets</li> <li>Dataset Transformations</li> <li>Reading TFRecord Files</li> </ul> </li> <li>Distributed Training<ul> <li>MirroredStrategy</li> <li>MultiWorkerMirroredStrategy</li> <li>ParameterServerStrategy</li> <li>TPUStrategy</li> </ul> </li> <li>TensorFlow Hub<ul> <li>Using Pre-trained Models</li> </ul> </li> <li>TensorFlow Lite<ul> <li>Converting to TensorFlow Lite</li> <li>Quantization</li> <li>Inference with TensorFlow Lite</li> </ul> </li> <li>TensorFlow Serving<ul> <li>Exporting a SavedModel</li> <li>Serving with TensorFlow Serving</li> </ul> </li> <li>TensorFlow Extended (TFX)</li> <li>TensorFlow Probability<ul> <li>Installation</li> <li>Distributions</li> <li>Bijectors</li> <li>Markov Chain Monte Carlo (MCMC)</li> </ul> </li> <li>TensorFlow Datasets (TFDS)<ul> <li>Installation</li> <li>Loading Datasets</li> <li>Processing Datasets</li> </ul> </li> <li>TensorFlow Addons<ul> <li>Installation</li> <li>Usage (Example: WeightNormalization)</li> </ul> </li> <li>Eager Execution</li> <li>tf.function</li> <li>Custom Training with GradientTape</li> <li>Custom Callbacks</li> <li>Mixed Precision Training</li> <li>Profiling</li> <li>Best Practices</li> <li>Common Issues and Debugging</li> </ul> </li> </ul> <p>This cheat sheet provides an exhaustive overview of TensorFlow 2.x, covering essential concepts, code snippets, and best practices for efficient deep learning model building, training, evaluation, and deployment. It aims to be a one-stop reference for common tasks.</p>"},{"location":"Cheat-Sheets/tensorflow/#getting-started","title":"Getting Started","text":""},{"location":"Cheat-Sheets/tensorflow/#installation","title":"Installation","text":"<pre><code>pip install tensorflow\n</code></pre> <p>For GPU support:</p> <pre><code>pip install tensorflow-gpu  # (Deprecated in TF 2.10)\n# For newer versions, TensorFlow automatically uses GPU if available\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#importing-tensorflow","title":"Importing TensorFlow","text":"<pre><code>import tensorflow as tf\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#checking-version","title":"Checking Version","text":"<pre><code>print(tf.__version__)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#checking-gpu-availability","title":"Checking GPU Availability","text":"<pre><code>print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensors","title":"Tensors","text":""},{"location":"Cheat-Sheets/tensorflow/#creating-tensors","title":"Creating Tensors","text":"<pre><code># Constant tensors\na = tf.constant([[1, 2], [3, 4]])\nb = tf.zeros((2, 3))\nc = tf.ones((3, 2))\nd = tf.eye(3)  # Identity matrix\ne = tf.random.normal((2, 2))  # Normal distribution\nf = tf.random.uniform((2, 2))  # Uniform distribution\n\n# From NumPy arrays\nimport numpy as np\narr = np.array([1, 2, 3])\ntensor_from_np = tf.convert_to_tensor(arr)\n\n# Sequences\nrange_tensor = tf.range(start=0, limit=10, delta=2) # 0, 2, 4, 6, 8\nlinspace_tensor = tf.linspace(start=0.0, stop=1.0, num=5) # 0.0, 0.25, 0.5, 0.75, 1.0\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensor-attributes","title":"Tensor Attributes","text":"<pre><code>tensor = tf.constant([[1, 2], [3, 4]])\nprint(tensor.shape)       # Shape of the tensor\nprint(tensor.dtype)       # Data type of the tensor\nprint(tensor.device)      # Device where the tensor is stored (CPU or GPU)\nprint(tensor.numpy())     # Convert to a NumPy array\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensor-operations","title":"Tensor Operations","text":"<pre><code>a = tf.constant([[1, 2], [3, 4]])\nb = tf.constant([[5, 6], [7, 8]])\n\n# Element-wise operations\nc = a + b       # Addition\nd = a - b       # Subtraction\ne = a * b       # Multiplication\nf = a / b       # Division\ng = tf.add(a, b) # Functional form\nh = tf.multiply(a, b) # Functional form\n\n# Matrix multiplication\ni = tf.matmul(a, b)\n\n# Transpose\nj = tf.transpose(a)\n\n# Reshape\nk = tf.reshape(a, (1, 4))\n\n# Squeeze and Expand\nl = tf.squeeze(tf.constant([[[1], [2], [3]]])) # Removes dimensions of size 1\nm = tf.expand_dims(tf.constant([1, 2, 3]), axis=0) # Adds a dimension of size 1\n\n# Concatenation\nn = tf.concat([a, b], axis=0)  # Concatenate along rows\no = tf.stack([a, b], axis=0)   # Stack along a new dimension\n\n# Reduce operations\np = tf.reduce_sum(a)        # Sum of all elements\nq = tf.reduce_mean(a)       # Mean of all elements\nr = tf.reduce_max(a)        # Maximum element\ns = tf.reduce_min(a)        # Minimum element\nt = tf.reduce_prod(a)       # Product of all elements\n\n# Argmax and Argmin\nu = tf.argmax(a, axis=1)    # Index of the maximum element along axis 1\nv = tf.argmin(a, axis=0)    # Index of the minimum element along axis 0\n\n# Casting\nw = tf.cast(a, tf.float32)  # Cast to float32\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#indexing-and-slicing","title":"Indexing and Slicing","text":"<pre><code>tensor = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(tensor[0])       # First row\nprint(tensor[:, 1])     # Second column\nprint(tensor[0:2, 1:3]) # Slicing\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#data-types","title":"Data Types","text":"<ul> <li><code>tf.float16</code>, <code>tf.float32</code>, <code>tf.float64</code>: Floating-point numbers.</li> <li><code>tf.int8</code>, <code>tf.int16</code>, <code>tf.int32</code>, <code>tf.int64</code>: Signed integers.</li> <li><code>tf.uint8</code>, <code>tf.uint16</code>, <code>tf.uint32</code>, <code>tf.uint64</code>: Unsigned integers.</li> <li><code>tf.bool</code>: Boolean.</li> <li><code>tf.string</code>: String.</li> <li><code>tf.complex64</code>, <code>tf.complex128</code>: Complex numbers.</li> <li><code>tf.qint8</code>, <code>tf.qint32</code>, <code>tf.quint8</code>: Quantized integers.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#variables","title":"Variables","text":"<pre><code>var = tf.Variable([1.0, 2.0])\nvar.assign([3.0, 4.0])\nvar.assign_add([1.0, 1.0])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#automatic-differentiation-autograd","title":"Automatic Differentiation (Autograd)","text":"<pre><code>x = tf.Variable(3.0)\n\nwith tf.GradientTape() as tape:\n    y = x**2\n\ndy_dx = tape.gradient(y, x)\nprint(dy_dx.numpy())  # Output: 6.0\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#persistent-gradient-tape","title":"Persistent Gradient Tape","text":"<pre><code>x = tf.Variable(3.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x**2\n    z = y * 2\n\ndy_dx = tape.gradient(y, x)  # 6.0\ndz_dx = tape.gradient(z, x)  # 12.0\nprint(dy_dx.numpy())\nprint(dz_dx.numpy())\n\ndel tape  # Drop the reference to the tape\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#watching-non-variable-tensors","title":"Watching Non-Variable Tensors","text":"<pre><code>x = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = x * x\ndy_dx = tape.gradient(y, x)\nprint(dy_dx.numpy())\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#keras-api","title":"Keras API","text":""},{"location":"Cheat-Sheets/tensorflow/#model-building","title":"Model Building","text":""},{"location":"Cheat-Sheets/tensorflow/#sequential-model","title":"Sequential Model","text":"<pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,)),\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#functional-api","title":"Functional API","text":"<pre><code>from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\ninputs = Input(shape=(784,))\nx = Dense(128, activation='relu')(inputs)\noutputs = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#model-subclassing","title":"Model Subclassing","text":"<pre><code>import tensorflow as tf\n\nclass MyModel(tf.keras.Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n\n    def call(self, inputs, training=None): # Add training argument\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#layers","title":"Layers","text":"<ul> <li><code>tf.keras.layers.Dense</code>: Fully connected layer.</li> <li><code>tf.keras.layers.Conv2D</code>: 2D convolution layer.</li> <li><code>tf.keras.layers.MaxPooling2D</code>: Max pooling layer.</li> <li><code>tf.keras.layers.ReLU</code>: ReLU activation function.</li> <li><code>tf.keras.layers.Activation</code>: Applies an activation function.</li> <li><code>tf.keras.layers.Softmax</code>: Softmax activation function.</li> <li><code>tf.keras.layers.BatchNormalization</code>: Batch normalization layer.</li> <li><code>tf.keras.layers.Dropout</code>: Dropout layer.</li> <li><code>tf.keras.layers.Flatten</code>: Flattens the input.</li> <li><code>tf.keras.layers.Reshape</code>: Reshapes the input.</li> <li><code>tf.keras.layers.Embedding</code>: Embedding layer.</li> <li><code>tf.keras.layers.LSTM</code>: LSTM layer.</li> <li><code>tf.keras.layers.GRU</code>: GRU layer.</li> <li><code>tf.keras.layers.Bidirectional</code>: Bidirectional wrapper for RNNs.</li> <li><code>tf.keras.layers.Input</code>: Creates an input tensor.</li> <li><code>tf.keras.layers.Add</code>, <code>tf.keras.layers.Multiply</code>, <code>tf.keras.layers.Concatenate</code>: Merge layers.</li> <li><code>tf.keras.layers.GlobalAveragePooling2D</code>, <code>tf.keras.layers.GlobalMaxPooling2D</code>: Global pooling layers.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#activation-functions","title":"Activation Functions","text":"<ul> <li><code>'relu'</code>: Rectified Linear Unit.</li> <li><code>'sigmoid'</code>: Sigmoid function.</li> <li><code>'tanh'</code>: Hyperbolic tangent function.</li> <li><code>'softmax'</code>: Softmax function.</li> <li><code>'elu'</code>: Exponential Linear Unit.</li> <li><code>'selu'</code>: Scaled Exponential Linear Unit.</li> <li><code>'linear'</code>: Linear (identity) activation.</li> <li><code>'LeakyReLU'</code>: Leaky Rectified Linear Unit.</li> <li><code>'PReLU'</code>: Parametric Rectified Linear Unit.</li> <li><code>'gelu'</code>: Gaussian Error Linear Unit.</li> <li><code>'swish'</code>: Swish activation function.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#loss-functions","title":"Loss Functions","text":"<ul> <li><code>tf.keras.losses.CategoricalCrossentropy</code>: Categorical cross-entropy.</li> <li><code>tf.keras.losses.SparseCategoricalCrossentropy</code>: Sparse categorical cross-entropy.</li> <li><code>tf.keras.losses.BinaryCrossentropy</code>: Binary cross-entropy.</li> <li><code>tf.keras.losses.MeanSquaredError</code>: Mean squared error.</li> <li><code>tf.keras.losses.MeanAbsoluteError</code>: Mean absolute error.</li> <li><code>tf.keras.losses.Hinge</code>: Hinge loss.</li> <li><code>tf.keras.losses.KLDivergence</code>: Kullback-Leibler Divergence loss.</li> <li><code>tf.keras.losses.Huber</code>: Huber loss.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#optimizers","title":"Optimizers","text":"<ul> <li><code>tf.keras.optimizers.SGD</code>: Stochastic Gradient Descent.</li> <li><code>tf.keras.optimizers.Adam</code>: Adaptive Moment Estimation.</li> <li><code>tf.keras.optimizers.RMSprop</code>: Root Mean Square Propagation.</li> <li><code>tf.keras.optimizers.Adagrad</code>: Adaptive Gradient Algorithm.</li> <li><code>tf.keras.optimizers.Adadelta</code>: Adaptive Delta.</li> <li><code>tf.keras.optimizers.Adamax</code>: Adamax optimizer.</li> <li><code>tf.keras.optimizers.Nadam</code>: Nesterov Adam optimizer.</li> <li><code>tf.keras.optimizers.Ftrl</code>: Follow The Regularized Leader optimizer.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#metrics","title":"Metrics","text":"<ul> <li><code>tf.keras.metrics.Accuracy</code>: Accuracy.</li> <li><code>tf.keras.metrics.BinaryAccuracy</code>: Binary accuracy.</li> <li><code>tf.keras.metrics.CategoricalAccuracy</code>: Categorical accuracy.</li> <li><code>tf.keras.metrics.SparseCategoricalAccuracy</code>: Sparse categorical accuracy.</li> <li><code>tf.keras.metrics.TopKCategoricalAccuracy</code>: Top-K categorical accuracy.</li> <li><code>tf.keras.metrics.MeanSquaredError</code>: Mean squared error.</li> <li><code>tf.keras.metrics.MeanAbsoluteError</code>: Mean absolute error.</li> <li><code>tf.keras.metrics.Precision</code>: Precision.</li> <li><code>tf.keras.metrics.Recall</code>: Recall.</li> <li><code>tf.keras.metrics.AUC</code>: Area Under the Curve.</li> <li><code>tf.keras.metrics.F1Score</code>: F1 score.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#model-compilation","title":"Model Compilation","text":"<pre><code>model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#training","title":"Training","text":"<pre><code>import numpy as np\n\ndata = np.random.random((1000, 784))\nlabels = np.random.randint(10, size=(1000,))\none_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=10)\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32, validation_split=0.2)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#evaluation","title":"Evaluation","text":"<pre><code>loss, accuracy = model.evaluate(data, one_hot_labels)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#prediction","title":"Prediction","text":"<pre><code>predictions = model.predict(data)\npredicted_classes = np.argmax(predictions, axis=1)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code># Save the entire model\nmodel.save('my_model.h5')\n\n# Load the entire model\nloaded_model = tf.keras.models.load_model('my_model.h5')\n\n# Save model weights\nmodel.save_weights('my_model_weights.h5')\n\n# Load model weights\nmodel.load_weights('my_model_weights.h5')\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#callbacks","title":"Callbacks","text":"<ul> <li><code>tf.keras.callbacks.ModelCheckpoint</code>: Saves the model at certain intervals.</li> <li><code>tf.keras.callbacks.EarlyStopping</code>: Stops training when a monitored metric has stopped improving.</li> <li><code>tf.keras.callbacks.TensorBoard</code>: Enables visualization of metrics and more.</li> <li><code>tf.keras.callbacks.ReduceLROnPlateau</code>: Reduces the learning rate when a metric has stopped improving.</li> <li><code>tf.keras.callbacks.CSVLogger</code>: Streams epoch results to a CSV file.</li> <li><code>tf.keras.callbacks.LearningRateScheduler</code>: Schedules the learning rate.</li> <li><code>tf.keras.callbacks.TerminateOnNaN</code>: Terminates training when a NaN loss is encountered.</li> </ul> <pre><code>from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n\ncheckpoint_callback = ModelCheckpoint(filepath='./checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5',\n                                     save_best_only=True,\n                                     monitor='val_loss',\n                                     verbose=1)\n\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n\ntensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32,\n          validation_data=(val_data, one_hot_val_labels),\n          callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#regularization","title":"Regularization","text":"<ul> <li><code>tf.keras.regularizers.l1(0.01)</code>: L1 regularization.</li> <li><code>tf.keras.regularizers.l2(0.01)</code>: L2 regularization.</li> <li><code>tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)</code>: L1 and L2 regularization.</li> </ul> <pre><code>from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,),\n          kernel_regularizer=regularizers.l1(0.01),  # L1 regularization\n          bias_regularizer=regularizers.l2(0.01)),    # L2 regularization\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-layers","title":"Custom Layers","text":"<pre><code>import tensorflow as tf\n\nclass MyCustomLayer(tf.keras.layers.Layer):\n    def __init__(self, units=32):\n        super(MyCustomLayer, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n        self.b = self.add_weight(shape=(self.units,),\n                                 initializer='zeros',\n                                 trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>import tensorflow as tf\n\ndef my_custom_loss(y_true, y_pred):\n    squared_difference = tf.square(y_true - y_pred)\n    return tf.reduce_mean(squared_difference, axis=-1)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-metrics","title":"Custom Metrics","text":"<pre><code>import tensorflow as tf\n\nclass MyCustomMetric(tf.keras.metrics.Metric):\n    def __init__(self, name='my_custom_metric', **kwargs):\n        super(MyCustomMetric, self).__init__(name=name, **kwargs)\n        self.sum = self.add_weight(name='sum', initializer='zeros')\n        self.count = self.add_weight(name='count', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        values = tf.abs(y_true - y_pred)\n        if sample_weight is not None:\n            sample_weight = tf.cast(sample_weight, self.dtype)\n            values = tf.multiply(values, sample_weight)\n        self.sum.assign_add(tf.reduce_sum(values))\n        self.count.assign_add(tf.cast(tf.size(y_true), self.dtype))\n\n    def result(self):\n        return self.sum / self.count\n\n    def reset_state(self):\n        self.sum.assign(0.0)\n        self.count.assign(0.0)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-training-loops","title":"Custom Training Loops","text":"<pre><code>import tensorflow as tf\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\nmetric_fn = tf.keras.metrics.CategoricalAccuracy()\n\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_fn(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    metric_fn.update_state(labels, predictions)\n    return loss\n\nepochs = 10\nfor epoch in range(epochs):\n    for images, labels in dataset:\n        loss = train_step(images, labels)\n    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}, Accuracy: {metric_fn.result().numpy():.4f}\")\n    metric_fn.reset_state()\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#data-input-pipelines-tfdata","title":"Data Input Pipelines (tf.data)","text":""},{"location":"Cheat-Sheets/tensorflow/#creating-datasets","title":"Creating Datasets","text":"<pre><code>import tensorflow as tf\n\n# From NumPy arrays\ndataset = tf.data.Dataset.from_tensor_slices((data, one_hot_labels))\n\n# From a list of files\ndataset = tf.data.Dataset.list_files(\"path/to/data/*.tfrecord\")\n\n# From a generator\ndef my_generator():\n    for i in range(1000):\n        yield i, i**2\n\ndataset = tf.data.Dataset.from_generator(my_generator, output_types=(tf.int32, tf.int32))\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#dataset-transformations","title":"Dataset Transformations","text":"<ul> <li><code>dataset.batch(batch_size)</code>: Combines consecutive elements into batches.</li> <li><code>dataset.shuffle(buffer_size)</code>: Randomly shuffles the elements of the dataset.</li> <li><code>dataset.repeat(count=None)</code>: Repeats the dataset (indefinitely if <code>count</code> is None).</li> <li><code>dataset.map(map_func)</code>: Applies a function to each element.</li> <li><code>dataset.prefetch(buffer_size)</code>: Prefetches elements for performance.</li> <li><code>dataset.cache()</code>: Caches the elements of the dataset.</li> <li><code>dataset.filter(predicate)</code>: Filters elements based on a predicate.</li> <li><code>dataset.interleave(map_func, cycle_length=None, block_length=None)</code>: Maps <code>map_func</code> across the dataset and interleaves the results.</li> <li><code>dataset.flat_map(map_func)</code>: Maps <code>map_func</code> across the dataset and flattens the result.</li> <li><code>dataset.take(count)</code>: Creates a dataset with at most <code>count</code> elements.</li> <li><code>dataset.skip(count)</code>: Skips the first <code>count</code> elements.</li> <li><code>dataset.zip(datasets)</code>: Zips together multiple datasets.</li> </ul> <pre><code>dataset = tf.data.Dataset.from_tensor_slices((data, one_hot_labels))\ndataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#reading-tfrecord-files","title":"Reading TFRecord Files","text":"<pre><code>raw_dataset = tf.data.TFRecordDataset(\"my_data.tfrecord\")\n\n# Define a feature description\nfeature_description = {\n    'feature0': tf.io.FixedLenFeature([], tf.int64),\n    'feature1': tf.io.FixedLenFeature([], tf.string),\n    'feature2': tf.io.FixedLenFeature([10], tf.float32),\n}\n\ndef _parse_function(example_proto):\n  # Parse the input tf.train.Example proto using the feature description.\n  return tf.io.parse_single_example(example_proto, feature_description)\n\nparsed_dataset = raw_dataset.map(_parse_function)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#distributed-training","title":"Distributed Training","text":""},{"location":"Cheat-Sheets/tensorflow/#mirroredstrategy","title":"MirroredStrategy","text":"<pre><code>import tensorflow as tf\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#multiworkermirroredstrategy","title":"MultiWorkerMirroredStrategy","text":"<pre><code>import os, json\n\nos.environ['TF_CONFIG'] = json.dumps({\n    'cluster': {\n        'worker': [\"localhost:12345\", \"localhost:23456\"]\n    },\n    'task': {'type': 'worker', 'index': 0}\n})\n\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nwith strategy.scope():\n    # ... build and compile model ...\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#parameterserverstrategy","title":"ParameterServerStrategy","text":"<pre><code>import os, json\nos.environ['TF_CONFIG'] = json.dumps({\n    'cluster': {\n        'worker': [\"localhost:12345\", \"localhost:23456\"],\n        'ps': [\"localhost:34567\"]\n    },\n    'task': {'type': 'worker', 'index': 0}\n})\n\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\n\nwith strategy.scope():\n    # ... build and compile model ...\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tpustrategy","title":"TPUStrategy","text":"<pre><code>resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)\n\nwith strategy.scope():\n    # ... build and compile model ...\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-hub","title":"TensorFlow Hub","text":""},{"location":"Cheat-Sheets/tensorflow/#using-pre-trained-models","title":"Using Pre-trained Models","text":"<pre><code>import tensorflow_hub as hub\n\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n                   trainable=False),  # Feature extraction\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.build([None, 224, 224, 3])  # Build the model\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-lite","title":"TensorFlow Lite","text":""},{"location":"Cheat-Sheets/tensorflow/#converting-to-tensorflow-lite","title":"Converting to TensorFlow Lite","text":"<pre><code>converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#quantization","title":"Quantization","text":"<pre><code>converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_quant_model = converter.convert()\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#inference-with-tensorflow-lite","title":"Inference with TensorFlow Lite","text":"<pre><code>interpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Set input tensor\ninput_data = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n\ninterpreter.invoke()\n\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nprint(output_data)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-serving","title":"TensorFlow Serving","text":""},{"location":"Cheat-Sheets/tensorflow/#exporting-a-savedmodel","title":"Exporting a SavedModel","text":"<pre><code>tf.saved_model.save(model, \"path/to/saved_model\")\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#serving-with-tensorflow-serving","title":"Serving with TensorFlow Serving","text":"<ol> <li> <p>Install TensorFlow Serving:</p> <pre><code># See TensorFlow Serving installation guide for details\n</code></pre> </li> <li> <p>Start the server:</p> <pre><code>tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=my_model --model_base_path=/path/to/saved_model\n</code></pre> </li> <li> <p>Send requests (using <code>requests</code> library in Python):</p> </li> </ol> <pre><code>import requests\nimport json\n\ndata = json.dumps({\"instances\": [[1.0, 2.0, ...]]}) # Example input data\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/my_model:predict', data=data, headers=headers)\npredictions = json.loads(json_response.text)['predictions']\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-extended-tfx","title":"TensorFlow Extended (TFX)","text":"<p>TFX is a platform for building and deploying production ML pipelines.  It includes components for:</p> <ul> <li>Data validation (<code>tensorflow_data_validation</code>)</li> <li>Data transformation (<code>tensorflow_transform</code>)</li> <li>Model training (<code>tensorflow</code>)</li> <li>Model analysis (<code>tensorflow_model_analysis</code>)</li> <li>Model serving (<code>tensorflow_serving</code>)</li> <li>Pipeline orchestration (Apache Beam, Apache Airflow, Kubeflow Pipelines)</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-probability","title":"TensorFlow Probability","text":""},{"location":"Cheat-Sheets/tensorflow/#installation_1","title":"Installation","text":"<pre><code>pip install tensorflow-probability\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#distributions","title":"Distributions","text":"<pre><code>import tensorflow_probability as tfp\n\ntfd = tfp.distributions\n\n# Normal distribution\nnormal_dist = tfd.Normal(loc=0., scale=1.)\nsamples = normal_dist.sample(10)\nlog_prob = normal_dist.log_prob(0.)\n\n# Bernoulli distribution\nbernoulli_dist = tfd.Bernoulli(probs=0.7)\nsamples = bernoulli_dist.sample(10)\n\n# Categorical distribution\ncategorical_dist = tfd.Categorical(probs=[0.2, 0.3, 0.5])\nsamples = categorical_dist.sample(10)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#bijectors","title":"Bijectors","text":"<pre><code>import tensorflow_probability as tfp\n\ntfb = tfp.bijectors\n\n# Affine bijector\naffine_bijector = tfb.Affine(shift=2., scale_diag=[3., 4.])\ntransformed_tensor = affine_bijector.forward(tf.constant([[1., 2.]]))\n\n# Exp bijector\nexp_bijector = tfb.Exp()\ntransformed_tensor = exp_bijector.forward(tf.constant([0., 1., 2.]))\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<pre><code>import tensorflow_probability as tfp\n\ntfd = tfp.distributions\ntfm = tfp.mcmc\n\n# Define a target distribution (e.g., a normal distribution)\ntarget_log_prob_fn = tfd.Normal(loc=0., scale=1.).log_prob\n\n# Define a kernel (e.g., Hamiltonian Monte Carlo)\nkernel = tfm.HamiltonianMonteCarlo(\n    target_log_prob_fn=target_log_prob_fn,\n    step_size=0.1,\n    num_leapfrog_steps=3)\n\n# Run the MCMC sampler\nsamples, _ = tfm.sample_chain(\n    num_results=1000,\n    current_state=0.,\n    kernel=kernel)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-datasets-tfds","title":"TensorFlow Datasets (TFDS)","text":""},{"location":"Cheat-Sheets/tensorflow/#installation_2","title":"Installation","text":"<pre><code>pip install tensorflow-datasets\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#loading-datasets","title":"Loading Datasets","text":"<pre><code>import tensorflow_datasets as tfds\n\n# Load a dataset\n(ds_train, ds_test), ds_info = tfds.load(\n    'mnist',\n    split=['train', 'test'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\n\n# Print dataset information\nprint(ds_info)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#processing-datasets","title":"Processing Datasets","text":"<pre><code>def normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -&gt; `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label\n\nds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\nds_train = ds_train.batch(128)\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n\nds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_test = ds_test.batch(128)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tensorflow-addons","title":"TensorFlow Addons","text":""},{"location":"Cheat-Sheets/tensorflow/#installation_3","title":"Installation","text":"<pre><code>pip install tensorflow-addons\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#usage-example-weightnormalization","title":"Usage (Example: WeightNormalization)","text":"<pre><code>import tensorflow_addons as tfa\n\nmodel = tf.keras.Sequential([\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(64, activation=\"relu\"), data_init=False),\n    tf.keras.layers.Dense(10, activation=\"softmax\"),\n])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#eager-execution","title":"Eager Execution","text":"<p>Eager execution is enabled by default in TensorFlow 2.x.  You can check if it's enabled:</p> <pre><code>tf.executing_eagerly()  # Returns True\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#tffunction","title":"tf.function","text":"<pre><code>@tf.function\ndef my_function(x, y):\n    return x + y\n\n# Call the function\nresult = my_function(tf.constant(1), tf.constant(2))\nprint(result)\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-training-with-gradienttape","title":"Custom Training with GradientTape","text":"<pre><code>import tensorflow as tf\n\n# Define the model, optimizer, and loss function\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(10, input_shape=(784,), activation='softmax')])\noptimizer = tf.keras.optimizers.Adam()\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\n\n# Define a training step\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_fn(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    for images, labels in dataset:\n        loss = train_step(images, labels)\n    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#custom-callbacks","title":"Custom Callbacks","text":"<pre><code>class MyCustomCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        print(f\"Starting epoch {epoch}\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Finished epoch {epoch}, loss: {logs['loss']:.4f}\")\n\n    def on_train_batch_begin(self, batch, logs=None):\n        print(f\"Training: Starting batch {batch}\")\n\n    def on_train_batch_end(self, batch, logs=None):\n        print(f\"Training: Finished batch {batch}, loss: {logs['loss']:.4f}\")\n\nmodel.fit(data, one_hot_labels, epochs=10, callbacks=[MyCustomCallback()])\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>from tensorflow.keras.mixed_precision import experimental as mixed_precision\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_policy(policy)\n\n# Build model with mixed precision\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n    tf.keras.layers.Dense(10, activation='softmax', dtype='float32') # Output layer should be float32\n])\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\noptimizer = mixed_precision.LossScaleOptimizer(optimizer)\n\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)\n        scaled_loss = optimizer.get_scaled_loss(loss)\n    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#profiling","title":"Profiling","text":"<pre><code>import tensorflow as tf\n\n# Profile the training steps 2 to 5\ntf.profiler.experimental.start('logdir')\n\nfor step in range(10):\n    # Your training step here\n    with tf.profiler.experimental.Trace('train', step_num=step):\n        # ... your training code ...\n        pass\ntf.profiler.experimental.stop()\n</code></pre> <p>Then, use TensorBoard to visualize the profiling results:</p> <pre><code>tensorboard --logdir logdir\n</code></pre>"},{"location":"Cheat-Sheets/tensorflow/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>tf.data</code> for efficient input pipelines: <code>tf.data</code> provides optimized data loading and preprocessing.</li> <li>Use <code>tf.function</code> to compile your functions into graphs: This can significantly improve performance.</li> <li>Use mixed precision training on compatible GPUs: This can speed up training and reduce memory usage.</li> <li>Use distributed training strategies for large models and datasets: Distribute the workload across multiple GPUs or machines.</li> <li>Use TensorBoard to monitor training progress: Visualize metrics, graphs, and more.</li> <li>Save and restore your models regularly: Use checkpoints to save your model's progress.</li> <li>Use Keras whenever possible: The Keras API is generally easier to use and more intuitive than the lower-level TensorFlow APIs.</li> <li>Use pre-trained models and transfer learning: Leverage existing models to speed up development and improve performance.</li> <li>Regularize your models to prevent overfitting: Use techniques like dropout, L1/L2 regularization, and batch normalization.</li> <li>Tune your hyperparameters: Use techniques like grid search, random search, or Bayesian optimization to find the best hyperparameters for your model.</li> <li>Validate your models carefully: Use a separate validation set to evaluate your model's performance and prevent overfitting.</li> <li>Use appropriate data types: Use <code>tf.float32</code> for most computations, but consider <code>tf.float16</code> for mixed precision training.</li> <li>Vectorize your operations: Avoid using Python loops when possible; use TensorFlow's vectorized operations instead.</li> <li>Use XLA (Accelerated Linear Algebra) for further performance improvements: Add <code>@tf.function(experimental_compile=True)</code> to your functions.</li> <li>Profile your code: Use the TensorFlow Profiler to identify performance bottlenecks.</li> <li>Keep your TensorFlow version up-to-date: Newer versions often include performance improvements and bug fixes.</li> <li>Read the TensorFlow documentation: The TensorFlow documentation is comprehensive and well-written.</li> </ul>"},{"location":"Cheat-Sheets/tensorflow/#common-issues-and-debugging","title":"Common Issues and Debugging","text":"<ul> <li> <p>Out of Memory (OOM) Errors:</p> <ul> <li>Reduce batch size.</li> <li>Use mixed precision training (<code>tf.float16</code>).</li> <li>Use gradient accumulation.</li> <li>Use a smaller model.</li> <li>Use gradient checkpointing.</li> <li>Free up memory by deleting unused tensors and variables.</li> <li>Use <code>tf.config.experimental.set_memory_growth(gpu, True)</code> to allow GPU memory to grow as needed (instead of allocating all at once).</li> </ul> </li> <li> <p>NaN (Not a Number) Losses:</p> <ul> <li>Reduce the learning rate.</li> <li>Use gradient clipping.</li> <li>Check for numerical instability (e.g., division by zero, taking the logarithm of a non-positive number).</li> <li>Use a different optimizer.</li> <li>Initialize weights appropriately.</li> <li>Use batch normalization.</li> <li>Check your data for errors (e.g., NaN values).</li> </ul> </li> <li> <p>Slow Training:</p> <ul> <li>Use a GPU.</li> <li>Use <code>tf.data</code> for efficient input pipelines.</li> <li>Use mixed precision training.</li> <li>Use distributed training.</li> <li>Use XLA compilation.</li> <li>Profile your code to identify bottlenecks.</li> <li>Increase batch size (if memory allows).</li> <li>Use asynchronous data loading.</li> <li>Use prefetching.</li> </ul> </li> <li> <p>Shape Mismatches:</p> <ul> <li>Carefully check the shapes of your tensors and ensure they are compatible with the operations you are performing.</li> <li>Use <code>tf.shape</code> and <code>tf.reshape</code> to inspect and modify tensor shapes.</li> </ul> </li> <li> <p>Data Type Errors:</p> <ul> <li>Ensure that your tensors have the correct data types (e.g., <code>tf.float32</code> for floating-point operations, <code>tf.int64</code> for indices).</li> <li>Use <code>tf.cast</code> to convert between data types.</li> </ul> </li> <li> <p>Device Placement Errors:</p> <ul> <li>Ensure that all tensors and operations are placed on the same device (CPU or GPU).</li> <li>Use <code>tf.device</code> to explicitly specify the device.</li> <li>Use <code>tf.distribute.Strategy</code> for distributed training.</li> </ul> </li> <li> <p>Gradient Issues (Vanishing/Exploding Gradients):</p> <ul> <li>Use gradient clipping.</li> <li>Use batch normalization.</li> <li>Use skip connections (e.g., ResNet).</li> <li>Use a different activation function (e.g., ReLU, LeakyReLU).</li> <li>Use a smaller learning rate.</li> <li>Use a different optimizer.</li> </ul> </li> <li> <p>Overfitting:</p> <ul> <li>Use regularization techniques (L1/L2 regularization, dropout).</li> <li>Use data augmentation.</li> <li>Use early stopping.</li> <li>Reduce model complexity.</li> <li>Increase the amount of training data.</li> </ul> </li> <li> <p>Underfitting:</p> <ul> <li>Increase model capacity.</li> <li>Train for longer.</li> <li>Use a more complex optimizer.</li> <li>Add more features.</li> <li>Reduce regularization.</li> </ul> </li> <li> <p>Debugging with <code>tf.print</code>:</p> <pre><code>@tf.function\ndef my_function(x):\n    tf.print(\"x:\", x)  # Print the value of x\n    return x * 2\n</code></pre> </li> <li> <p>Debugging with <code>tf.debugging.assert_*</code>:</p> <pre><code>@tf.function\ndef my_function(x):\n    tf.debugging.assert_positive(x, message=\"x must be positive\")\n    return x * 2\n</code></pre> </li> <li> <p>Using the TensorFlow Debugger (tfdbg): (Less common with TF 2.x eager execution, but still useful for graph mode)</p> </li> <li> <p>Using Python's <code>pdb</code> debugger:  You can use <code>pdb.set_trace()</code> inside your <code>@tf.function</code> decorated functions, but you'll need to run your code with eager execution disabled (<code>tf.config.run_functions_eagerly(False)</code>) or use <code>tf.py_function</code>.</p> </li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/","title":"Home","text":""},{"location":"Deploying-ML-models/deploying-ml-models/#introduction","title":"Introduction","text":"<p>This is a completely open-source platform for maintaining curated list of interview questions and answers for people looking and preparing for data science opportunities.</p> <p>Not only this, the platform will also serve as one point destination for all your needs like tutorials, online materials, etc.</p> <p>This platform is maintained by you! \ud83e\udd17 You can help us by answering/ improving existing questions as well as by sharing any new questions that you faced during your interviews.</p>"},{"location":"Deploying-ML-models/deploying-ml-models/#contribute-to-the-platform","title":"Contribute to the platform","text":"<p>Contribution in any form will be deeply appreciated. \ud83d\ude4f</p>"},{"location":"Deploying-ML-models/deploying-ml-models/#add-questions","title":"Add questions","text":"<p>\u2753 Add your questions here. Please ensure to provide a detailed description to allow your fellow contributors to understand your questions and answer them to your satisfaction.</p> <p></p> <p>\ud83e\udd1d Please note that as of now, you cannot directly add a question via a pull request. This will help us to maintain the quality of the content for you.</p>"},{"location":"Deploying-ML-models/deploying-ml-models/#add-answerstopics","title":"Add answers/topics","text":"<p>\ud83d\udcdd These are the answers/topics that need your help at the moment</p> <ul> <li> Add documentation for the project</li> <li> Online Material for Learning</li> <li> Suggested Learning Paths</li> <li> Cheat Sheets<ul> <li> Django</li> <li> Flask</li> <li> Numpy</li> <li> Pandas</li> <li> PySpark</li> <li> Python</li> <li> RegEx</li> <li> SQL</li> </ul> </li> <li> NLP Interview Questions</li> <li> Add python common DSA interview questions</li> <li> Add Major ML topics<ul> <li> Linear Regression </li> <li> Logistic Regression </li> <li> SVM </li> <li> Random Forest </li> <li> Gradient boosting </li> <li> PCA </li> <li> Collaborative Filtering </li> <li> K-means clustering </li> <li> kNN </li> <li> ARIMA </li> <li> Neural Networks </li> <li> Decision Trees </li> <li> Overfitting, Underfitting</li> <li> Unbalanced, Skewed data</li> <li> Activation functions relu/ leaky relu</li> <li> Normalization</li> <li> DBSCAN </li> <li> Normal Distribution </li> <li> Precision, Recall </li> <li> Loss Function MAE, RMSE </li> </ul> </li> <li> Add Pandas questions</li> <li> Add NumPy questions</li> <li> Add TensorFlow questions</li> <li> Add PyTorch questions</li> <li> Add list of learning resources</li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/#reportsolve-issues","title":"Report/Solve Issues","text":"<p>\ud83d\udd27 To report any issues find me on LinkedIn or raise an issue on GitHub.</p> <p>\ud83d\udee0 You can also solve existing issues on GitHub and create a pull request.</p>"},{"location":"Deploying-ML-models/deploying-ml-models/#say-thanks","title":"Say Thanks","text":"<p>\ud83d\ude0a If this platform helped you in any way, it would be great if you could share it with others.</p> <p> </p> <pre><code>Check out this \ud83d\udc47 platform \ud83d\udc47 for data science content:\n\ud83d\udc49 https://singhsidhukuldeep.github.io/data-science-interview-prep/ \ud83d\udc48\n\n#data-science #machine-learning #interview-preparation \n</code></pre> <p>You can also star the repository on GitHub    and watch-out for any updates </p>"},{"location":"Deploying-ML-models/deploying-ml-models/#features","title":"Features","text":"<ul> <li> <p>\ud83c\udfa8 Beautiful: The design is built on top of most popular libraries like MkDocs and material which allows the platform to be responsive and to work on all sorts of devices \u2013 from mobile phones to wide-screens. The underlying fluid layout will always adapt perfectly to the available screen space.</p> </li> <li> <p>\ud83e\uddd0 Searchable: almost magically, all the content on the website is searchable without any further ado. The built-in search \u2013 server-less \u2013 is fast and accurate in responses to any of the queries.</p> </li> <li> <p>\ud83d\ude4c Accessible:</p> <ul> <li>Easy to use: \ud83d\udc4c The website is hosted on github-pages and is free and open to use to over 40 million users of GitHub in 100+ countries.</li> <li>Easy to contribute: \ud83e\udd1d The website embodies the concept of collaboration to the latter. Allowing anyone to add/improve the content. To make contributing easy, everything is written in MarkDown and then compiled to beautiful html.</li> </ul> </li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/#setup","title":"Setup","text":"<p>No setup is required for usage of the platform</p> <p>Important: It is strongly advised to use virtual environment and not change anything in <code>gh-pages</code></p>"},{"location":"Deploying-ML-models/deploying-ml-models/#linux-systems","title":"<code>Linux</code> Systems","text":"<pre><code>python3 -m venv ./venv\n\nsource venv/bin/activate\n\npip3 install -r requirements.txt\n</code></pre> <pre><code>deactivate\n</code></pre>"},{"location":"Deploying-ML-models/deploying-ml-models/#windows-systems","title":"<code>Windows</code> Systems","text":"<pre><code>python3 -m venv ./venv\n\nvenv\\Scripts\\activate\n\npip3 install -r requirements.txt\n</code></pre> <pre><code>venv\\Scripts\\deactivate\n</code></pre>"},{"location":"Deploying-ML-models/deploying-ml-models/#to-install-the-latest","title":"To install the latest","text":"<pre><code>pip3 install mkdocs\npip3 install mkdocs-material\n</code></pre>"},{"location":"Deploying-ML-models/deploying-ml-models/#useful-commands","title":"Useful Commands","text":"<ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>mkdocs gh-deploy</code> - Use\u00a0<code>mkdocs gh-deploy --help</code>\u00a0to get a full list of options available for the\u00a0<code>gh-deploy</code>\u00a0command.     Be aware that you will not be able to review the built site before it is pushed to GitHub. Therefore, you may want to verify any changes you make to the docs beforehand by using the\u00a0<code>build</code>\u00a0or\u00a0<code>serve</code>\u00a0commands and reviewing the built files locally.</li> <li><code>mkdocs new [dir-name]</code> - Create a new project. No need to create a new project</li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/#useful-documents","title":"Useful Documents","text":"<ul> <li> <p>\ud83d\udcd1 MkDocs: https://github.com/mkdocs/mkdocs</p> </li> <li> <p>\ud83c\udfa8 Theme: https://github.com/squidfunk/mkdocs-material</p> </li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/#faq","title":"FAQ","text":"<ul> <li> <p>Can I filter questions based on companies? \ud83e\udd2a</p> <p>As much as this platform aims to help you with your interview preparation, it is not a short-cut to crack one. Think of this platform as a practicing field to help you sharpen your skills for your interview processes. However, for your convenience we have sorted all the questions by topics for you. \ud83e\udd13</p> <p>This doesn't mean that such feature won't be added in the future.  \"Never say Never\"</p> <p>But as of now there is neither plan nor data to do so. \ud83d\ude22</p> </li> <li> <p>Why is this platform free? \ud83e\udd17</p> <p>Currently there is no major cost involved in maintaining this platform other than time and effort that is put in by every contributor.  If you want to help you can contribute here. </p> <p>If you still want to pay for something that is free, we would request you to donate it to a charity of your choice instead. \ud83d\ude07</p> </li> </ul>"},{"location":"Deploying-ML-models/deploying-ml-models/#credits","title":"Credits","text":""},{"location":"Deploying-ML-models/deploying-ml-models/#maintained-by","title":"Maintained by","text":"<p>\ud83d\udc68\u200d\ud83c\udf93 Kuldeep Singh Sidhu </p> <p>Github: github/singhsidhukuldeep <code>https://github.com/singhsidhukuldeep</code></p> <p>Website: Kuldeep Singh Sidhu (Website) <code>http://kuldeepsinghsidhu.com</code></p> <p>LinkedIn: Kuldeep Singh Sidhu (LinkedIn) <code>https://www.linkedin.com/in/singhsidhukuldeep/</code></p>"},{"location":"Deploying-ML-models/deploying-ml-models/#contributors","title":"Contributors","text":"<p>\ud83d\ude0e The full list of all the contributors is available here</p>"},{"location":"Deploying-ML-models/deploying-ml-models/#current-status","title":"Current Status","text":""},{"location":"Interview-Questions/AB-testing/","title":"A/B Testing Interview Questions","text":"<p>This document provides a curated list of A/B Testing and Experimentation interview questions. It covers statistical foundations, experimental design, metric selection, and advanced topics like interference (network effects) and sequential testing. Critical for roles at data-driven companies like Netflix, Airbnb, and Uber.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is A/B Testing? Optimizely Most Tech Companies Easy Basics 2 Explain Null Hypothesis (\\(H_0\\)) vs Alternative Hypothesis (\\(H_1\\)) Khan Academy Most Tech Companies Easy Statistics 3 What is a p-value? Explain it to a non-technical person. Harvard Business Review Google, Meta, Amazon Medium Statistics, Communication 4 What is Statistical Power? Machine Learning Plus Google, Netflix, Uber Medium Statistics 5 What is Type I error (False Positive) vs Type II error (False Negative)? Towards Data Science Most Tech Companies Easy Statistics 6 How do you calculate sample size for an experiment? Evan Miller Google, Amazon, Meta Medium Experimental Design 7 What is Minimum Detectable Effect (MDE)? StatsEngine Netflix, Airbnb Medium Experimental Design 8 Explain Confidence Intervals. Coursera Most Tech Companies Easy Statistics 9 Difference between One-tailed and Two-tailed tests. Investopedia Google, Amazon Easy Statistics 10 What is the Central Limit Theorem? Why is it important? Khan Academy Google, HFT Firms Medium Statistics 11 How long should you run an A/B test? CXL Airbnb, Booking.com Medium Experimental Design 12 Can you stop an experiment as soon as it reaches significance? (Peeking) Evan Miller Netflix, Uber, Airbnb Hard Pitfalls 13 What is SRM (Sample Ratio Mismatch)? How to debug? Microsoft Research Microsoft, LinkedIn Hard Debugging 14 What is Randomization Unit vs Analysis Unit? Udacity Uber, DoorDash Medium Experimental Design 15 How to handle outliers in A/B testing metrics? Towards Data Science Google, Meta Medium Data Cleaning 16 Mean vs Median: Which metric to use? Stack Overflow Most Tech Companies Easy Metrics 17 What are Guardrail Metrics? Airbnb Tech Blog Airbnb, Netflix Medium Metrics 18 What is a North Star Metric? Amplitude Product Roles Easy Metrics 19 Difference between Z-test and T-test. Statistics By Jim Google, Amazon Medium Statistics 20 How to test multiple variants? (A/B/n testing) VWO Booking.com, Expedia Medium Experimental Design 21 What is the Bonferroni Correction? Wikipedia Google, Meta Hard Statistics 22 What is A/A Testing? Why do it? Optimizely Microsoft, LinkedIn Medium Validity 23 Explain Covariate Adjustment (CUPED). Booking.com Data Booking.com, Microsoft, Meta Hard Optimization 24 How to measure retention in A/B tests? Reforge Netflix, Spotify Medium Metrics 25 What is a Novelty Effect? CXL Facebook, Instagram Medium Pitfalls 26 What is a Primacy Effect? CXL Facebook, Instagram Medium Pitfalls 27 How to handle interference (Network Effects)? Uber Eng Blog Uber, Lyft, DoorDash Hard Network Effects 28 What is a Switchback (Time-split) Experiment? DoorDash Eng DoorDash, Uber Hard Experimental Design 29 What is Cluster Randomization? Wikipedia Facebook, LinkedIn Hard Experimental Design 30 How to test on a 2-sided marketplace? Lyft Eng Uber, Lyft, Airbnb Hard Marketplace 31 Explain Bayesian A/B Testing vs Frequentist. VWO Stitch Fix, Netflix Hard Statistics 32 What is a Multi-Armed Bandit (MAB)? Towards Data Science Netflix, Amazon Hard Bandits 33 Thompson Sampling vs Epsilon-Greedy. GeeksforGeeks Netflix, Amazon Hard Bandits 34 How to deal with low traffic experiments? CXL Startups Medium Strategy 35 How to select metrics for a new feature? Product School Meta, Google Medium Metrics 36 What is Simpson's Paradox? Britannica Google, Amazon Medium Paradoxes 37 How to analyze ratio metrics (e.g., CTR)? Deltamethod Google, Meta Hard Statistics, Delta Method 38 What is Bootstrapping? When to use it? Investopedia Amazon, Netflix Medium Statistics 39 How to detect and handle Seasonality? Towards Data Science Retail/E-comm Medium Time Series 40 What is Change Aversion? Google UX Google, YouTube Medium UX 41 How to design an experiment for a search algorithm? Airbnb Eng Google, Airbnb, Amazon Hard Search, Ranking 42 How to test pricing changes? PriceIntelligently Uber, Airbnb Hard Pricing, Strategy 43 What is interference between experiments? Microsoft Exp Google, Meta, Microsoft Hard Platform 44 Explain Sequential Testing. Evan Miller Optimizely, Netflix Hard Statistics 45 What is Variance Reduction? Meta Research Meta, Microsoft, Booking Hard Optimization 46 How to handle attribution (First-touch vs Last-touch)? Google Analytics Marketing Tech Medium Marketing 47 How to validate if randomization worked? Stats StackExchange Most Tech Companies Easy Validity 48 What is stratification? Wikipedia Most Tech Companies Medium Sampling 49 When should you NOT A/B test? Reforge Product Roles Medium Strategy 50 How to estimate long-term impact from short-term tests? Netflix TechBlog Netflix, Meta Hard Strategy, Proxy Metrics 51 What is Binomial Distribution? Khan Academy Most Tech Companies Easy Statistics 52 What is Poisson Distribution? Khan Academy Uber, Lyft (Rides) Medium Statistics 53 Difference between Correlation and Causation. Khan Academy Most Tech Companies Easy Basics 54 What is a Confounding Variable? Scribbr Most Tech Companies Easy Causal Inference 55 Explain Regression Discontinuity Design (RDD). Wikipedia Economics/Policy Roles Hard Causal Inference 56 Explain Difference-in-Differences (DiD). Wikipedia Uber, Airbnb Hard Causal Inference 57 What is Propensity Score Matching? Wikipedia Meta, Netflix Hard Causal Inference 58 How to Handle Heterogeneous Treatment Effects? CausalML Uber, Meta Hard Causal ML 59 What is Interference in social networks? Meta Research Meta, LinkedIn, Snap Hard Network Effects 60 Explain the concept of \"Holdout Groups\". Airbnb Eng Amazon, Airbnb Medium Strategy 61 How to test infrastructure changes? (Canary Deployment) Google SRE Google, Netflix Medium DevOps/SRE 62 What is Client-side vs Server-side testing? Optimizely Full Stack Roles Medium Implementation 63 How to deal with flickering? VWO Frontend Roles Medium Implementation 64 What is a Trigger selection in A/B testing? Microsoft Exp Microsoft, Airbnb Hard Experimental Design 65 How to analyze user funnel drop-offs? Mixpanel Product Analysts Medium Analytics 66 What is Geometric Distribution? Wikipedia Most Tech Companies Medium Statistics 67 Explain Inverse Propensity Weighting (IPW). Wikipedia Causal Inference Roles Hard Causal Inference 68 How to calculate Standard Error of Mean (SEM)? Investopedia Most Tech Companies Easy Statistics 69 What is Statistical Significance vs Practical Significance? Towards Data Science Google, Meta Medium Strategy 70 How to handle cookies and tracking prevention (ITP)? WebKit AdTech, Marketing Hard Privacy 71 [HARD] Explain the Delta Method for ratio metrics. Deltamethod Google, Meta, Uber Hard Statistics 72 [HARD] How does Switchback testing solve interference? DoorDash Eng DoorDash, Uber Hard Experimental Design 73 [HARD] Derive the sample size formula. Stats Exchange Google, HFT Firms Hard Math 74 [HARD] How to implement CUPED in Python/SQL? Booking.com Booking, Microsoft Hard Optimization 75 [HARD] Explain Sequential Probability Ratio Test (SPRT). Wikipedia Optimizely, Netflix Hard Statistics 76 [HARD] How to estimate Network Effects (Cluster-Based)? MIT Paper Meta, LinkedIn Hard Network Effects 77 [HARD] Design an experiment for a 3-sided marketplace. Uber Eng Uber, DoorDash Hard Marketplace 78 [HARD] How to correct for multiple comparisons (FDR vs FWER)? Wikipedia Pharma, BioTech, Tech Hard Statistics 79 [HARD] Explain Instrumental Variables (IV). Wikipedia Economics, Uber Hard Causal Inference 80 [HARD] How to build an Experimentation Platform? Microsoft Exp Microsoft, Netflix, Airbnb Hard System Design 81 [HARD] How to handle user identity resolution across devices? Segment Meta, Google Hard Data Engineering 82 [HARD] What is \"Carryover Effect\" in Switchback tests? DoorDash Eng DoorDash, Uber Hard Pitfalls 83 [HARD] Explain \"Washout Period\". Clinical Trials DoorDash, Uber Hard Experimental Design 84 [HARD] How to test Ranking algorithms (Interleaving)? Netflix TechBlog Netflix, Google, Airbnb Hard Search/Ranking 85 [HARD] Explain Always-Valid Inference. Optimizely Optimizely, Netflix Hard Statistics 86 [HARD] How to measure cannibalization? Harvard Business Review Retail, E-comm Hard Strategy 87 [HARD] Explain Thompson Sampling Implementation. TDS Amazon, Netflix Hard Bandits 88 [HARD] How to detect Heterogeneous Treatment Effects (Causal Forest)? Wager &amp; Athey Uber, Meta Hard Causal ML 89 [HARD] How to handle \"dilution\" in experiment metrics? Reforge Product Roles Hard Metrics 90 [HARD] Explain Synthetic Control Method. Wikipedia Uber (City-level tests) Hard Causal Inference 91 [HARD] How to optimize for Long-term Customer Value (LTV)? ThetaCLV Subscription roles Hard Metrics 92 [HARD] Explain \"Winner's Curse\" in A/B testing. Airbnb Eng Airbnb, Booking Hard Bias 93 [HARD] How to handle heavy-tailed metric distributions? TDS HFT, Fintech Hard Statistics 94 [HARD] How to implement Stratified Sampling in SQL? Stack Overflow Data Eng Hard Sampling 95 [HARD] Explain \"Regression to the Mean\". Wikipedia Most Tech Companies Hard Statistics 96 [HARD] How to budget \"Error Rate\" across the company? Microsoft Exp Microsoft, Google Hard Strategy 97 [HARD] How to detect bot traffic in experiments? Google Analytics Security, Fraud Hard Data Quality 98 [HARD] Explain \"Interaction Effects\" in Factorial Designs. Wikipedia Meta, Google Hard Statistics 99 [HARD] How to use Surrogate Metrics? Netflix TechBlog Netflix Hard Metrics 100 [HARD] How to implement A/B testing in a Microservices architecture? Split.io Netflix, Uber Hard Engineering"},{"location":"Interview-Questions/AB-testing/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/AB-testing/#1-power-analysis-and-sample-size-python","title":"1. Power Analysis and Sample Size (Python)","text":"<p>Calculating the required sample size before starting an experiment.</p> <pre><code>from statsmodels.stats.power import TTestIndPower\nimport numpy as np\n\n# Parameters\neffect_size = 0.1  # Cohen's d (Standardized difference)\nalpha = 0.05       # Significance level (5%)\npower = 0.8        # Power (80%)\n\nanalysis = TTestIndPower()\nsample_size = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha)\n\nprint(f\"Required sample size per group: {int(np.ceil(sample_size))}\")\n</code></pre>"},{"location":"Interview-Questions/AB-testing/#2-bayesian-ab-test-beta-binomial","title":"2. Bayesian A/B Test (Beta-Binomial)","text":"<p>Updating beliefs about conversion rates.</p> <pre><code>from scipy.stats import beta\n\n# Prior: Uniform distribution (Beta(1,1))\nalpha_prior = 1\nbeta_prior = 1\n\n# Data: Group A\nconversions_A = 120\nfailures_A = 880\n\n# Data: Group B\nconversions_B = 140\nfailures_B = 860\n\n# Posterior\nposterior_A = beta(alpha_prior + conversions_A, beta_prior + failures_A)\nposterior_B = beta(alpha_prior + conversions_B, beta_prior + failures_B)\n\n# Probability B &gt; A (Approximate via simulation)\nsamples = 100000\nprob_b_better = (posterior_B.rvs(samples) &gt; posterior_A.rvs(samples)).mean()\n\nprint(f\"Probability B is better than A: {prob_b_better:.4f}\")\n</code></pre>"},{"location":"Interview-Questions/AB-testing/#3-bootstrap-confidence-interval","title":"3. Bootstrap Confidence Interval","text":"<p>Calculating CI for non-normal metrics (e.g., Revenue per User).</p> <pre><code>import numpy as np\n\ndata_control = np.random.lognormal(mean=2, sigma=1, size=1000)\ndata_variant = np.random.lognormal(mean=2.1, sigma=1, size=1000)\n\ndef bootstrap_mean_diff(data1, data2, n_bootstrap=1000):\n    diffs = []\n    for _ in range(n_bootstrap):\n        # Sample with replacement\n        sample1 = np.random.choice(data1, len(data1), replace=True)\n        sample2 = np.random.choice(data2, len(data2), replace=True)\n        diffs.append(sample2.mean() - sample1.mean())\n    return np.percentile(diffs, [2.5, 97.5])\n\nci = bootstrap_mean_diff(data_control, data_variant)\nprint(f\"95% CI for difference: {ci}\")\n</code></pre>"},{"location":"Interview-Questions/AB-testing/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Explain the difference between Type I and Type II errors.</li> <li>How do you design an experiment to test a change in the Search Ranking algorithm?</li> <li>How to handle multiple metrics in an experiment? (Overall Evaluation Criterion).</li> <li>Explain the trade-off between sample size and experiment duration.</li> <li>Deriving the variance of the difference between two means.</li> <li>How to detect if your randomization algorithm is broken?</li> <li>Explain how you would test a feature with strong network effects.</li> <li>How to measure the long-term impact of a UI change?</li> <li>What metric would you use for a \"User Happiness\" experiment?</li> <li>Explain the concept of \"Regression to the Mean\" in the context of A/B testing.</li> </ul>"},{"location":"Interview-Questions/AB-testing/#questions-asked-in-meta-facebook-interview","title":"Questions asked in Meta (Facebook) interview","text":"<ul> <li>How to measure network effects in a social network experiment?</li> <li>Explain Cluster-based randomization. Why use it?</li> <li>How to handle \"Novelty Effect\" when launching a new feature?</li> <li>Explain CUPED (Controlled-experiment Using Pre-Experiment Data).</li> <li>How to design an experiment for the News Feed ranking?</li> <li>What are the potential bounds of network interference?</li> <li>How to detect if an experiment has a Sample Ratio Mismatch (SRM)?</li> <li>Explain the difference between Average Treatment Effect (ATE) and Conditional ATE (CATE).</li> <li>How to optimize for long-term user retention?</li> <li>Design a test to measure the impact of ads on user engagement.</li> </ul>"},{"location":"Interview-Questions/AB-testing/#questions-asked-in-netflix-interview","title":"Questions asked in Netflix interview","text":"<ul> <li>How to A/B test a new recommendation algorithm?</li> <li>Explain \"Interleaving\" in ranking experiments.</li> <li>How to choose between \"member-level\" vs \"profile-level\" assignment?</li> <li>How to estimate the causal impact of a TV show launch on subscriptions? (Quasi-experiment).</li> <li>Explain the concept of \"Proxy Metrics\".</li> <li>How to handle outlier users (e.g., bots, heavy users) in analysis?</li> <li>Explain \"Switchback\" testing infrastructure.</li> <li>How to balance \"Exploration\" vs \"Exploitation\" (Bandits)?</li> <li>Design a test for artwork personalization (thumbnails).</li> <li>How to measure the \"Incremental Reach\" of a marketing campaign?</li> </ul>"},{"location":"Interview-Questions/AB-testing/#questions-asked-in-uberlyft-interview-marketplace","title":"Questions asked in Uber/Lyft interview (Marketplace)","text":"<ul> <li>How to test changes in a two-sided marketplace (Rider vs Driver)?</li> <li>Explain \"Switchback\" designs for marketplace experiments.</li> <li>How to handle \"Spillover\" or \"Cannibalization\" effects?</li> <li>Explain \"Difference-in-Differences\" method.</li> <li>How to measure the impact of surge pricing changes?</li> <li>Explain \"Synthetic Control\" methods for city-level tests.</li> <li>How to calculate \"Marketplace Liquidity\" metrics?</li> <li>Design an experiment to reduce driver cancellations.</li> <li>How to test a new matching algorithm?</li> <li>Explain Interference in a geo-spatial context.</li> </ul>"},{"location":"Interview-Questions/AB-testing/#additional-resources","title":"Additional Resources","text":"<ul> <li>Microsoft Experimentation Platform (Exp) - Best technical papers.</li> <li>Netflix Tech Blog - Experimentation - Real-world case studies.</li> <li>Causal Inference for the Brave and True - Python handbook.</li> <li>Trustworthy Online Controlled Experiments (Book) - The \"Bible\" of A/B testing (Kohavi).</li> <li>Uber Engineering - Data - Marketplace testing concepts.</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/","title":"Data Science Interview Question Resources","text":"<p>About This Guide</p> <p>A comprehensive collection of community-recommended resources for data science interview preparation. These sources are valued by real practitioners, featuring GitHub repositories with significant stars, practice platforms used by the community, and actual interview experiences from top companies.</p> <p>Made with \u2764\ufe0f for the data science community</p>"},{"location":"Interview-Questions/Interview-Question-Resources/#quick-reference-table","title":"\ud83d\udcca Quick Reference Table","text":"<p>All topics have 5+ high-quality sources with direct links to interview questions:</p> Topic Total Sources Direct Question Links GitHub Repos Practice Platforms Python 8+ 6 links 4 repos (8.8k\u2b50) LeetCode, HackerRank, StrataScratch Machine Learning 10+ 6 links 6 repos (3.5k\u2b50) Blind, Books NLP 7+ 6 links 5 repos Coursera, Glassdoor GenAI 6+ 6 links 2 repos (5k\u2b50) DataCamp, Medium Deep Learning 8+ 6 links 4 repos Blind, Glassdoor Probability 10+ 8 links 3 repos (8.8k\u2b50) StrataScratch, DataLemur Pandas 9+ 7 links 4 repos (20k\u2b50) InterviewQuery, DataCamp NumPy 8+ 7 links 4 repos (20k\u2b50) InterviewBit, MLStack.Cafe SQL 12+ 6 links 6 repos (3.5k\u2b50) DataLemur, StrataScratch, LeetCode"},{"location":"Interview-Questions/Interview-Question-Resources/#python","title":"\ud83d\udc0d Python","text":"Python Interview Resources - 20+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars Description 1 youssefHosni/DS-Interview-QA 1.5k+ \u2b50 Curated list of Python interview Q&amp;A for data scientists 2 alexeygrigorev/data-science-interviews 8.5k+ \u2b50 Comprehensive interview questions including Python coding 3 kojino/120-DS-Questions 8.8k+ \u2b50 Answers to 120 commonly asked data science interview questions 4 Devinterview-io/data-scientist-questions Active Data scientist interview questions 2025 5 Devinterview-io/python-questions Active Python interview questions for developers 6 Tanu-N-Prabhu/Python Coding Interview Prep Community Beginner to advanced Python coding questions 7 rbhatia46/DS-Interview-Resources 500+ \u2b50 Curated data science interview resources 8 khanhnamle1994/cracking-ds-interview 3k+ \u2b50 Comprehensive DS interview preparation 9 amitshekhariitbhu/ml-questions 500+ \u2b50 ML interview questions including Python"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms","title":"\ud83d\udcbb Practice Platforms","text":"# Platform Description 10 LeetCode Python 2000+ Python coding problems 11 HackerRank Python Python practice for data science roles 12 StrataScratch Real Python interview questions from companies 13 Kaggle Python data science practice"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 14 Python Coding Questions Practice Coding practice resources 15 Python Interview Questions Guide Interview preparation 16 Data Scientist Python Interview Essentials DS-specific Python 17 How to Prepare for DS Python Interviews at FAANG FAANG preparation 18 LeetCode Resources for DS LeetCode strategy 19 Entry-Level DS Interview Questions Real interview questions 20 78 Python DS Practice Problems Practice problems"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources","title":"\ud83d\udcd6 Additional Resources","text":"# Source Description 21 DataInterview - Reddit DS Interview Reddit company interview insights 22 LinkedIn - Youssef Hosni Curated question collection"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>Python Interview Questions &amp; Answers for Data Scientists</li> <li>DataCamp Top Python Interview Questions</li> <li>Analytics Vidhya Python Coding Questions</li> <li>Top 15 Data Science Coding Interview Questions</li> <li>Interview Query Python DS Questions</li> <li>InterviewBit Data Science Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#machine-learning","title":"\ud83e\udd16 Machine Learning","text":"Machine Learning Interview Resources - 30+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_1","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars Description 1 alirezadir/ML-Interviews 3.5k+ \u2b50 Guide based on offers from Meta, Google, Amazon, Apple, Roku 2 khangich/ml-interview 1k+ \u2b50 Real questions from FAANG, Snapchat, LinkedIn 3 amitshekhariitbhu/ml-questions 500+ \u2b50 Your cheat sheet for machine learning interview 4 andrewekhalel/MLQuestions 1.5k+ \u2b50 ML and computer vision engineer technical questions 5 jl33-ai/1000-ml-questions 500+ \u2b50 Prepare for ML, programming, and quant interviews 6 youssefHosni/DS-Interview-QA (ML Section) 1.5k+ \u2b50 ML interview questions &amp; answers for data scientists 7 khanhnamle1994/cracking-ds-interview 3k+ \u2b50 Cheatsheets, books, questions, and portfolio 8 QuickLearner171998/ml-interview-prep Community Comprehensive ML interview preparation 9 aishwaryanr/awesome-genai-guide 5k+ \u2b50 Generative AI guide with ML fundamentals 10 rbhatia46/DS-Interview-Resources 500+ \u2b50 Curated ML interview resources"},{"location":"Interview-Questions/Interview-Question-Resources/#books-comprehensive-guides","title":"\ud83d\udcda Books &amp; Comprehensive Guides","text":"# Source Description 11 Chip Huyen's ML Interviews Book 200+ knowledge questions with difficulty levels"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_1","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 12 ML Interview Prep Resources Comprehensive ML prep strategies 13 ML Interview Questions &amp; Answers Community-recommended resources 14 Best ML Interview Preparation Discussion on best prep resources 15 ML Engineer Interview Experience Real ML theory interview questions 16 Common ML Interview Questions Community-sourced common questions 17 ML System Design Questions ML system design focus 18 Entry-Level ML Interviews Entry-level expectations"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-tools","title":"\ud83d\udcbb Practice Platforms &amp; Tools","text":"# Platform Description 19 Prepfully - ML Interview Questions Real ML questions from top companies 20 Glassdoor ML Interviews Actual ML interview experiences"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_1","title":"\ud83d\udcd6 Additional Resources","text":"# Source Description 21 LinkedIn - ML Interview Guide Curated ML question collection 22 Blind - Toughest ML Questions Real tough ML questions from community 23 Blind - ML Prep Thread ML and data science interview preparation"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_1","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>Springboard ML Interview Questions</li> <li>365 Data Science ML Questions &amp; Answers</li> <li>GeeksforGeeks ML Interview Questions</li> <li>DataInterview ML Questions</li> <li>Data Science and ML Interview Tips for New Grads</li> <li>BrainTrust Data Scientists Interview Questions</li> <li>InterviewBit ML Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#natural-language-processing-nlp","title":"\ud83d\udcac Natural Language Processing (NLP)","text":"NLP Interview Resources - 20+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_2","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars/Type Description 1 Devinterview-io/nlp-questions Active NLP interview questions for 2025 2 MukundAabha/DS-ML-DL-NLP-Qus Community Comprehensive cheat sheet with interview questions 3 rbhatia46/DS-Interview-Resources 500+ \u2b50 Curated sources including NLP resources 4 masmahbubalom/InterviewQuestions Active Collection of DS, AI, ML, DL, NLP, CV questions 5 youssefHosni/DS-Interview-QA 1.5k+ \u2b50 Part of comprehensive DS interview collection 6 andrewekhalel/MLQuestions 1.5k+ \u2b50 Includes NLP technical questions"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_2","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 7 NLP Interview Preparation NLP-specific interview prep 8 NLP Engineer Interview Questions Real NLP engineer questions 9 NLP Research Interview Experience Research-focused NLP interviews 10 NLP Coding Questions NLP coding challenges 11 LLM Interview Questions Large language model questions"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-tools_1","title":"\ud83d\udcbb Practice Platforms &amp; Tools","text":"# Platform Description 12 Prepfully - NLP Questions Real NLP interview questions from companies 13 Coursera NLP Questions 14 common NLP interview questions 14 Glassdoor NLP Roles Real interview experiences for NLP roles"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_2","title":"\ud83d\udcd6 Additional Resources","text":"# Source Description 15 LinkedIn - NLP Interview Guide Curated NLP question collection 16 Medium - NLP Interview Prep Comprehensive NLP interview guide"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_2","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>365 Data Science NLP Interview Questions</li> <li>GeeksforGeeks Advanced NLP Interview Questions</li> <li>ProjectPro NLP Interview Questions &amp; Answers</li> <li>Sprintzeal NLP Interview Questions</li> <li>MLStack.Cafe NLP Interview Questions</li> <li>Analytics Vidhya Top 100 Data Science Questions</li> <li>InterviewBit NLP Questions</li> <li>DataCamp NLP Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#generative-ai-genai","title":"\u2728 Generative AI (GenAI)","text":"Generative AI Interview Resources - 20+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_3","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars Description 1 aishwaryanr/awesome-genai-guide 5k+ \u2b50 One stop repository for generative AI research &amp; interviews 2 rbhatia46/DS-Interview-Resources 500+ \u2b50 Frequently updated with new GenAI resources 3 Devinterview-io/generative-ai-questions Active Generative AI questions for 2025 4 youssefHosni/DS-Interview-QA 1.5k+ \u2b50 Includes GenAI section"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_3","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 5 GenAI Interview Preparation GenAI interview strategies 6 LLM Interview Questions Large language model focus 7 Generative AI Career Discussion Career insights and questions 8 GenAI Engineer Interview Tips Practical interview tips"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-tools_2","title":"\ud83d\udcbb Practice Platforms &amp; Tools","text":"# Platform Description 9 Prepfully - GenAI Questions Real GenAI interview questions 10 Glassdoor - GenAI Roles Actual interview experiences"},{"location":"Interview-Questions/Interview-Question-Resources/#blog-posts-articles","title":"\ud83d\udcdd Blog Posts &amp; Articles","text":"# Source Description 11 DataCamp Top 30 generative AI questions for 2025 12 Analytics Vidhya Updated May 2025 with MCQs 13 ProjectPro Most asked generative AI interview questions"},{"location":"Interview-Questions/Interview-Question-Resources/#community-insights","title":"\ud83d\udca1 Community Insights","text":"# Platform Description 14 Medium - GenAI Prep GenAI engineer interview prep guide 15 LinkedIn - GenAI Questions Curated GenAI collection"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_3","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>DataCamp GenAI Interview Questions</li> <li>Top 25 GenAI Interview Questions &amp; In-Depth Answers</li> <li>Data Science Dojo Interview Questions for AI Scientists</li> <li>GeeksforGeeks Generative AI Questions with Answers</li> <li>Edureka Generative AI Interview Questions</li> <li>Verve Copilot Top 30 Gen AI Questions</li> <li>InterviewBit Generative AI Questions</li> <li>Simplilearn GenAI Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#deep-learning","title":"\ud83e\udde0 Deep Learning","text":"Deep Learning Interview Resources - 25+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_4","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars/Type Description 1 Devinterview-io/dl-questions Active Deep learning questions for 2025 2 youssefHosni/DS-Interview-QA (DL) 1.5k+ \u2b50 Deep learning Q&amp;A for data scientists 3 andrewekhalel/MLQuestions 1.5k+ \u2b50 ML and DL technical interview questions 4 Sroy20/ml-interview-questions Community Curated deep learning questions 5 alirezadir/ML-Interviews 3.5k+ \u2b50 Includes DL sections from FAANG interviews 6 rbhatia46/DS-Interview-Resources 500+ \u2b50 Includes DL resources"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_4","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 7 Deep Learning Interview Prep DL interview strategies 8 DL Interview Questions Thread Community-sourced DL questions 9 Neural Networks Interview Questions NN-specific questions 10 Computer Vision Interview Prep CV and DL questions 11 Deep Learning Engineer Expectations Role expectations and questions"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-tools_3","title":"\ud83d\udcbb Practice Platforms &amp; Tools","text":"# Platform Description 12 Prepfully - Deep Learning Real DL interview questions 13 Glassdoor - DL Roles Real deep learning interview experiences"},{"location":"Interview-Questions/Interview-Question-Resources/#educational-resources","title":"\ud83d\udcd6 Educational Resources","text":"# Source Description 14 DataCamp Top 20 deep learning interview Q&amp;A 15 InterviewBit Comprehensive DL interview guide 16 LinkedIn - DL Questions Curated DL collection"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_3","title":"\ud83d\udca1 Additional Resources","text":"# Source Description 17 Blind - Hardest DL Questions Hardest DS/ML/DL questions from community 18 Medium - DL Interview Guide Comprehensive DL guide"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_4","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>Analytics Vidhya Top 30 Deep Learning Questions</li> <li>GeeksforGeeks Deep Learning Interview Questions</li> <li>DataCamp Top 20 Deep Learning Q&amp;A</li> <li>Exponent Top Deep Learning Questions</li> <li>PWSkills Deep Learning Interview Questions</li> <li>SynergisticIT Deep Learning Q&amp;A</li> <li>Simplilearn Deep Learning Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#probability-statistics","title":"\ud83d\udcca Probability &amp; Statistics","text":"Probability &amp; Statistics Interview Resources - 30+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_5","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars Description 1 kojino/120-DS-Questions (Prob) 8.8k+ \u2b50 Probability questions from 120 DS questions 2 alexeygrigorev/data-science-interviews 8.5k+ \u2b50 Statistics and probability questions 3 youssefHosni/DS-Interview-QA (Stats) 1.5k+ \u2b50 Statistics interview questions 4 khanhnamle1994/cracking-ds-interview 3k+ \u2b50 Includes probability &amp; stats sections 5 rbhatia46/DS-Interview-Resources 500+ \u2b50 Probability and statistics resources"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_5","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 6 Probability Interview Questions Probability-specific prep 7 Statistics Questions for DS Interviews Statistics focus 8 Bayes Theorem Interview Questions Bayes theorem applications 9 Stats Fundamentals for DS Core statistics concepts 10 Probability Puzzles Thread Probability puzzles 11 A/B Testing Questions A/B testing and hypothesis testing"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms_1","title":"\ud83d\udcbb Practice Platforms","text":"# Platform Description 12 StrataScratch Real probability &amp; statistics questions 13 DataInterview 120 statistics questions for FAANGs 14 DataLemur Top 20 statistics questions asked 15 Prepfully - Statistics Real statistics interview questions"},{"location":"Interview-Questions/Interview-Question-Resources/#educational-resources_1","title":"\ud83d\udcd6 Educational Resources","text":"# Source Description 16 NickSingh.com Questions from FAANG &amp; Wall Street 17 DataCamp Top 35 statistics questions 2025 18 GeeksforGeeks Top 50+ statistics questions 19 Analytics Vidhya 25 probability and statistics questions"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_4","title":"\ud83d\udca1 Additional Resources","text":"# Source Description 20 LinkedIn - Stats Questions Curated statistics collection 21 Medium - Probability Guide Comprehensive probability guide 22 Glassdoor - Quant Roles Quant-focused probability questions"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_5","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>StrataScratch 30 Probability &amp; Statistics Questions</li> <li>NickSingh.com 40 Prob &amp; Stats Questions from FAANG</li> <li>14 Probability Problems for Data Science</li> <li>Top Important Probability Questions</li> <li>FinalRound AI Probability Interview Questions</li> <li>DataLemur Statistics Interview Questions</li> <li>InterviewBit Probability Questions</li> <li>Exponent Top Statistics &amp; DS Questions</li> <li>365 Data Science Statistics Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#pandas","title":"\ud83d\udc3c Pandas","text":"Pandas Interview Resources - 25+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_6","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars/Type Description 1 Devinterview-io/pandas-questions Active Pandas questions for ML/DS interviews 2025 2 FavioVazquez/ds-cheatsheets 20k+ \u2b50 List of data science cheatsheets including Pandas 3 pandas-dev/pandas (Official) Official Official pandas cheat sheet 4 aihubprojects/pandas-cheatsheet Community Python, NumPy, Pandas cheatsheet collection 5 rbhatia46/DS-Interview-Resources 500+ \u2b50 Includes Pandas resources 6 Gist - Quick Reference Community Super quick cheatsheet for common tasks"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_6","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 7 Pandas Interview Questions Thread Pandas-specific prep 8 Data Manipulation Questions Pandas data manipulation 9 Pandas Coding Challenges Hands-on Pandas practice 10 DataFrame Operations Questions DataFrame manipulation"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms_2","title":"\ud83d\udcbb Practice Platforms","text":"# Platform Description 11 InterviewQuery Top 27 Pandas questions with answers 12 DataCamp Top 26 Pandas interview Q&amp;A 13 StrataScratch Pandas questions from real companies 14 Prepfully - Pandas Pandas practice interviews"},{"location":"Interview-Questions/Interview-Question-Resources/#educational-resources_2","title":"\ud83d\udcd6 Educational Resources","text":"# Source Description 15 InterviewBit Pandas interview guide with downloadable PDF 16 GeeksforGeeks Top 50 Pandas questions 2024"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_5","title":"\ud83d\udca1 Additional Resources","text":"# Source Description 17 LinkedIn - Pandas Guide Curated Pandas collection 18 Medium - Pandas Interview Prep Comprehensive Pandas guide 19 Glassdoor - Data Analyst Roles Real Pandas interview questions"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_6","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>DataCamp Top Python Pandas Interview Q&amp;A</li> <li>StrataScratch Python Pandas Questions</li> <li>GeeksforGeeks Pandas Interview Questions</li> <li>Pandas Interview Questions &amp; Answers (Medium)</li> <li>Interview Query Pandas Questions</li> <li>InterviewBit Pandas Questions</li> <li>MLStack.Cafe Pandas Python Questions</li> <li>Analytics Vidhya Pandas Questions</li> <li>Simplilearn Pandas Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#numpy","title":"\ud83d\udd22 NumPy","text":"NumPy Interview Resources - 25+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_7","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars/Type Description 1 Devinterview-io/numpy-questions Active NumPy questions for ML/DS interviews 2025 2 FavioVazquez/ds-cheatsheets 20k+ \u2b50 Includes comprehensive NumPy cheatsheets 3 aihubprojects/numpy-cheatsheet Community NumPy cheatsheet with examples 4 tpn/pdfs Collection Technically-oriented PDF collection 5 rbhatia46/DS-Interview-Resources 500+ \u2b50 Includes NumPy resources 6 numpy/numpy (Official Docs) 28k+ \u2b50 Official NumPy documentation and resources"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_7","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 7 NumPy Interview Questions NumPy-specific prep 8 NumPy vs Lists Questions Performance comparisons 9 Array Operations Questions NumPy operations 10 NumPy Broadcasting Questions Broadcasting concepts"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms_3","title":"\ud83d\udcbb Practice Platforms","text":"# Platform Description 11 InterviewQuery Top 19 NumPy questions updated for 2025 12 DataCamp Top 20 NumPy questions: basic to advanced 13 Prepfully - NumPy NumPy practice questions"},{"location":"Interview-Questions/Interview-Question-Resources/#educational-resources_3","title":"\ud83d\udcd6 Educational Resources","text":"# Source Description 14 InterviewBit NumPy interview guide updated Dec 2024 15 MLStack.Cafe 27 advanced NumPy interview questions 16 GeeksforGeeks Comprehensive NumPy questions"},{"location":"Interview-Questions/Interview-Question-Resources/#additional-resources_6","title":"\ud83d\udca1 Additional Resources","text":"# Source Description 17 LinkedIn - NumPy Guide Curated NumPy collection 18 Medium - NumPy Interview Prep Comprehensive NumPy guide 19 Glassdoor - Python Developer Roles Real NumPy questions"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_7","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>DataCamp NumPy Interview Questions</li> <li>GeeksforGeeks NumPy Interview Questions</li> <li>GitHub NumPy Interview Questions</li> <li>InterviewBit NumPy Questions</li> <li>Verve Copilot 30 Most Common NumPy Questions</li> <li>MLStack.Cafe NumPy Interview Questions</li> <li>NumPy Interview Questions &amp; Answers (Medium)</li> <li>Analytics Vidhya NumPy Questions</li> <li>Simplilearn NumPy Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#sql","title":"\ud83d\uddc4\ufe0f SQL","text":"SQL Interview Resources - 35+ Community Sources"},{"location":"Interview-Questions/Interview-Question-Resources/#github-repositories-community-driven_8","title":"\ud83d\udcda GitHub Repositories (Community-Driven)","text":"# Repository Stars Description 1 shawlu95/Beyond-LeetCode-SQL 3.5k+ \u2b50 Analysis of SQL LeetCode &amp; classic interview questions 2 mdh266/SQL-Practice 500+ \u2b50 Solutions from LeetCode, HackerRank &amp; DataLemur 3 Thomas-George-T/HackerRank-SQL 1k+ \u2b50 All SQL HackerRank challenges using MySQL 4 mrinal1704/SQL-Leetcode 100+ \u2b50 All 117 LeetCode questions with solutions 5 ManikantaSanjay/LeetCode-SQL-70 Active Collection to ace coding interviews 6 TulipAggarwal/LeetCode-SQL50 Active Wide array of SQL concepts 7 alexeygrigorev/data-science-interviews 8.5k+ \u2b50 Includes SQL interview questions 8 rbhatia46/DS-Interview-Resources 500+ \u2b50 SQL resources for DS roles"},{"location":"Interview-Questions/Interview-Question-Resources/#reddit-community-discussions_8","title":"\ud83d\udcac Reddit Community Discussions","text":"# Discussion Focus Area 9 SQL Interview Questions Thread SQL interview prep 10 Best Resources for SQL Practice Practice platforms 11 SQL Window Functions Questions Window functions focus 12 Common SQL Mistakes in Interviews Interview pitfalls 13 SQL for Data Analyst Roles Data analyst focus 14 Advanced SQL Questions Advanced concepts 15 SQL vs NoSQL Interview Questions Database comparisons"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-most-popular","title":"\ud83d\udcbb Practice Platforms (Most Popular)","text":"# Platform Description 16 LeetCode SQL 200+ SQL problems from real interviews 17 DataLemur Real SQL questions from FAANG companies 18 StrataScratch 1000+ questions from 150 companies 19 HackerRank SQL Comprehensive SQL challenges 20 Prepfully - SQL Company-specific SQL questions 21 InterviewQuery SQL for data science roles"},{"location":"Interview-Questions/Interview-Question-Resources/#educational-resources_4","title":"\ud83d\udcd6 Educational Resources","text":"# Source Description 22 DataCamp Beginner to intermediate SQL Q&amp;A 23 GeeksforGeeks Top 50+ SQL questions 24 InterviewBit Comprehensive SQL interview guide"},{"location":"Interview-Questions/Interview-Question-Resources/#community-resources","title":"\ud83d\udca1 Community Resources","text":"# Source Description 25 Blind - SQL Interview Real SQL interview experiences (Reddit company) 26 KDnuggets Practical SQL &amp; Python questions 27 Glassdoor - SQL Developer Real SQL interview experiences 28 LinkedIn - SQL Questions Curated SQL collection"},{"location":"Interview-Questions/Interview-Question-Resources/#direct-links-to-interview-questions_8","title":"\ud83d\udd17 Direct Links to Interview Questions","text":"<ul> <li>Interview Query Data Science SQL Questions</li> <li>DataLemur SQL Practice Questions</li> <li>DataCamp Top SQL Interview Questions &amp; Answers</li> <li>StrataScratch SQL Interview Questions Guide</li> <li>DataInterview Top 100 SQL Questions</li> <li>365 Data Science SQL Interview Questions</li> <li>GeeksforGeeks SQL Questions</li> <li>Analytics Vidhya SQL Questions</li> <li>Simplilearn SQL Interview Questions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#comprehensive-multi-topic-resources","title":"\ud83d\udcda Comprehensive Multi-Topic Resources","text":"All-in-One Data Science Interview Resources"},{"location":"Interview-Questions/Interview-Question-Resources/#top-github-collections","title":"\ud83c\udf1f Top GitHub Collections","text":"# Repository Stars Description 1 alexeygrigorev/data-science-interviews 8.5k+ \u2b50 Technical questions covering SQL, Python, coding 2 kojino/120-DS-Questions 8.8k+ \u2b50 120 commonly asked DS questions with answers 3 youssefHosni/DS-Interview-QA 1.5k+ \u2b50 Curated list across 6 categories 4 khanhnamle1994/cracking-ds-interview 3k+ \u2b50 Cheatsheets, books, questions, portfolio 5 rbhatia46/DS-Interview-Resources 500+ \u2b50 Potential sources, frequently updated 6 PavelGrigoryevDS/awesome-data-analysis 1k+ \u2b50 500+ curated resources for beginners &amp; experts 7 benthecoder/ds-interview-resources 500+ \u2b50 Collection of awesome DS interview resources 8 ajitsingh98/DS-Interview-QA 500+ \u2b50 1000+ most asked DS questions"},{"location":"Interview-Questions/Interview-Question-Resources/#real-interview-experiences","title":"\ud83d\udcbc Real Interview Experiences","text":"Glassdoor &amp; Blind - Actual Interview Questions"},{"location":"Interview-Questions/Interview-Question-Resources/#glassdoor-interview-experiences","title":"\ud83c\udfe2 Glassdoor Interview Experiences","text":"# Company Key Topics Covered 1 Google Data Scientist ML algorithms, SQL, Python, A/B testing 2 Meta Data Scientist Data cleaning, modeling, system design, case studies 3 Amazon Data Scientist Pandas coding, regularization, bias/variance, A/B testing 4 LinkedIn Data Scientist Probability sampling, logistic regression algorithms 5 Reddit Data Scientist SQL-heavy, nested subqueries, CTEs, window functions"},{"location":"Interview-Questions/Interview-Question-Resources/#blind-community-discussions","title":"\ud83d\udcac Blind Community Discussions","text":"# Discussion Topic Topics Covered 1 Toughest DS/ML Questions SGD, logistic regression, regularizers, PCA 2 Hardest DS/ML Questions Advanced ML theory and implementation 3 DS Interview Question Bank Community-sourced question bank 4 ML Interview Prep Comprehensive ML/DS prep strategies 5 Google ML Interview Gradient descent, normalization, regularization, embeddings"},{"location":"Interview-Questions/Interview-Question-Resources/#practice-platforms-comparison","title":"\ud83c\udfaf Practice Platforms Comparison","text":"Best Platforms for Each Topic"},{"location":"Interview-Questions/Interview-Question-Resources/#for-sql-practice","title":"\ud83d\uddc4\ufe0f For SQL Practice","text":"Platform Best For Community Size Cost DataLemur Real FAANG SQL questions 20k+ users Free tier available StrataScratch Data science-specific SQL 20k+ users Subscription required LeetCode Coding challenges + SQL Millions Free tier available HackerRank Company assessments 28M+ developers Free"},{"location":"Interview-Questions/Interview-Question-Resources/#for-pythoncoding-practice","title":"\ud83d\udcbb For Python/Coding Practice","text":"Platform Best For Community Size Cost LeetCode Algorithmic coding Millions Free tier available HackerRank Data science challenges 28M+ developers Free StrataScratch DS-specific Python 20k+ users Subscription required"},{"location":"Interview-Questions/Interview-Question-Resources/#for-mltheory-questions","title":"\ud83e\udd16 For ML/Theory Questions","text":"Resource Type Best For Cost GitHub Repos Free comprehensive resources Free Blind/Glassdoor Real interview experiences Free Books (Chip Huyen) In-depth knowledge One-time purchase"},{"location":"Interview-Questions/Interview-Question-Resources/#interview-preparation-strategy","title":"\ud83d\udcd6 Interview Preparation Strategy","text":"How to Use These Resources - Structured 7-Week Plan"},{"location":"Interview-Questions/Interview-Question-Resources/#phase-1-foundation-building-weeks-1-2","title":"\ud83d\udcc5 Phase 1: Foundation Building (Weeks 1-2)","text":"<p>Focus Areas:</p> <ul> <li>Start with GitHub repositories like alexeygrigorev/data-science-interviews</li> <li>Review Python and SQL basics using community cheatsheets</li> <li>Practice 5-10 easy SQL questions daily on LeetCode or DataLemur</li> </ul> <p>Daily Schedule:</p> <ul> <li>Morning: 1 hour theory review (GitHub repos)</li> <li>Afternoon: 1 hour SQL practice</li> <li>Evening: 30 min Python coding problems</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#phase-2-concept-mastery-weeks-3-4","title":"\ud83d\udcc5 Phase 2: Concept Mastery (Weeks 3-4)","text":"<p>Focus Areas:</p> <ul> <li>Deep dive into ML concepts using alirezadir/ML-Interviews</li> <li>Study probability/statistics from kojino/120-DS-Questions</li> <li>Practice medium difficulty SQL and Python problems</li> </ul> <p>Daily Schedule:</p> <ul> <li>Morning: 1.5 hours ML theory</li> <li>Afternoon: 1 hour probability/statistics</li> <li>Evening: 45 min medium-level coding</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#phase-3-company-specific-prep-weeks-5-6","title":"\ud83d\udcc5 Phase 3: Company-Specific Prep (Weeks 5-6)","text":"<p>Focus Areas:</p> <ul> <li>Read Glassdoor interview experiences for target companies</li> <li>Review Blind discussions for insider insights</li> <li>Practice on StrataScratch for real company questions</li> </ul> <p>Daily Schedule:</p> <ul> <li>Morning: Company-specific question review</li> <li>Afternoon: Mock interviews (timed)</li> <li>Evening: Review and improve solutions</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#phase-4-mock-interviews-week-7","title":"\ud83d\udcc5 Phase 4: Mock Interviews (Week 7+)","text":"<p>Focus Areas:</p> <ul> <li>Do timed practice sessions</li> <li>Review GitHub repos for advanced topics</li> <li>Focus on weak areas identified during practice</li> </ul> <p>Daily Schedule:</p> <ul> <li>Full mock interview sessions (2-3 hours)</li> <li>Detailed review and improvement</li> <li>Target weak areas</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#common-pitfalls-best-practices","title":"\u26a0\ufe0f Common Pitfalls &amp; Best Practices","text":"Common Pitfalls to Avoid Community Recommendations <p>Based on Reddit, Blind, and GitHub discussions, the most recommended resources are:</p>"},{"location":"Interview-Questions/Interview-Question-Resources/#what-not-to-do","title":"\u274c What NOT to Do","text":"<ul> <li>Don't just read questions - Actually code the solutions</li> <li>Don't skip probability/statistics - Very common in interviews</li> <li>Don't ignore SQL - It's tested even for ML roles</li> <li>Don't only study theory - Practice is equally important</li> <li>Don't rely solely on SEO blogs - Use community resources</li> <li>Don't memorize without understanding - Interviewers can tell</li> <li>Don't practice only easy problems - Mix difficulty levels</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#top-picks-by-category","title":"\ud83c\udfc6 Top Picks by Category","text":"Category Top Recommendation Why? GitHub alexeygrigorev/data-science-interviews (8.5k\u2b50) Most comprehensive, actively maintained SQL Practice DataLemur + StrataScratch Real company questions, DS-focused ML Theory Chip Huyen's ML Interviews Book 200+ questions with difficulty levels Real Questions Glassdoor + Blind community Actual interview experiences Python LeetCode Easy-Medium problems Best for algorithmic thinking Comprehensive khanhnamle1994/cracking-the-data-science-interview All-in-one resource"},{"location":"Interview-Questions/Interview-Question-Resources/#pro-tips-from-the-community","title":"\ud83d\udca1 Pro Tips from the Community","text":"<ul> <li>Use Spaced Repetition: Review questions after 1 day, 1 week, 1 month</li> <li>Join Study Groups: Reddit r/datascience, Discord communities</li> <li>Track Your Progress: Use spreadsheets to monitor weak areas</li> <li>Focus on Fundamentals: Master basics before advanced topics</li> <li>Practice Explaining: Use the Feynman technique</li> </ul>"},{"location":"Interview-Questions/Interview-Question-Resources/#contributing","title":"\ud83d\udcdd Contributing","text":"<p>Found a Great Resource?</p> <p>Many of these GitHub repos accept contributions. If you've found a resource that the community loves:</p> <ul> <li>Fork the respective repository</li> <li>Add your finding with proper documentation</li> <li>Submit a pull request</li> <li>Help the community grow!</li> </ul>"},{"location":"Interview-Questions/Interview-Questions/","title":"Interview Questions (Intro)","text":"<p>These are currently most commonly asked questions. Questions can be removed if they are no longer popular in interview circles and added as new question banks are released.</p>"},{"location":"Interview-Questions/LangChain/","title":"LangChain Interview Questions","text":"<p>This document provides a curated list of LangChain interview questions commonly asked in technical interviews for LLM Engineer, AI Engineer, GenAI Developer, and Machine Learning roles.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is LangChain and why is it used? LangChain Docs Google, Amazon, Meta, OpenAI Easy Basics 2 Explain core components of LangChain LangChain Docs Google, Amazon, Meta Easy Architecture 3 What are LLMs and Chat Models in LangChain? LangChain Docs Google, Amazon, OpenAI Easy LLMs 4 How to use prompt templates? LangChain Docs Most Tech Companies Easy Prompts 5 Difference between PromptTemplate and ChatPromptTemplate LangChain Docs Google, Amazon, OpenAI Easy Prompts 6 How to implement output parsers? LangChain Docs Google, Amazon, Meta Medium Parsing 7 What are chains in LangChain? LangChain Docs Google, Amazon, Meta Medium Chains 8 How to implement memory in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Memory 9 Difference between ConversationBufferMemory and ConversationSummaryMemory LangChain Docs Google, Amazon Medium Memory 10 How to implement RAG (Retrieval Augmented Generation)? LangChain Docs Google, Amazon, Meta, OpenAI Medium RAG 11 What are document loaders? LangChain Docs Most Tech Companies Easy Loaders 12 What are text splitters and why are they needed? LangChain Docs Google, Amazon, OpenAI Medium Chunking 13 Difference between RecursiveCharacterTextSplitter and TokenTextSplitter LangChain Docs Google, Amazon Medium Chunking 14 How to choose optimal chunk size? LangChain Docs Google, Amazon, OpenAI Hard Optimization 15 What are embeddings in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Embeddings 16 How to use OpenAI embeddings vs HuggingFace embeddings? LangChain Docs Google, Amazon Medium Embeddings 17 What are vector stores? LangChain Docs Google, Amazon, Meta Medium VectorDB 18 How to use FAISS for vector storage? LangChain Docs Google, Amazon Medium FAISS 19 Difference between Chroma, Pinecone, and Weaviate LangChain Docs Google, Amazon, OpenAI Medium VectorDB 20 What are retrievers in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Retrievers 21 How to implement semantic search? LangChain Docs Google, Amazon, OpenAI Medium Search 22 What is similarity search vs MMR? LangChain Docs Google, Amazon Medium Search 23 How to implement hybrid search? LangChain Docs Google, Amazon, OpenAI Hard Search 24 What are agents in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Agents 25 How to implement ReAct agents? LangChain Docs Google, Amazon, OpenAI Medium Agents 26 What are tools in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Tools 27 How to create custom tools? LangChain Docs Google, Amazon, OpenAI Medium Tools 28 What is function calling in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Functions 29 What is structured output in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Output 30 How to use Pydantic with LangChain? LangChain Docs Google, Amazon, Microsoft Medium Validation 31 What is LCEL (LangChain Expression Language)? LangChain Docs Google, Amazon, OpenAI Medium LCEL 32 How to use the pipe operator in LCEL? LangChain Docs Google, Amazon Easy LCEL 33 What is RunnablePassthrough? LangChain Docs Google, Amazon Medium LCEL 34 What is RunnableParallel? LangChain Docs Google, Amazon Medium LCEL 35 How to implement streaming responses? LangChain Docs Google, Amazon, OpenAI Medium Streaming 36 What is LangSmith and why is it useful? LangSmith Docs Google, Amazon, OpenAI Medium Observability 37 How to trace and debug LangChain applications? LangSmith Docs Google, Amazon Medium Debugging 38 What is LangServe? LangServe Docs Google, Amazon Medium Deployment 39 How to deploy LangChain apps as REST APIs? LangServe Docs Google, Amazon, Microsoft Medium Deployment 40 What are callbacks in LangChain? LangChain Docs Google, Amazon Medium Callbacks 41 How to handle rate limiting with LLMs? LangChain Docs Google, Amazon, OpenAI Medium Limits 42 What are fallbacks in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Fallbacks 43 What is caching in LangChain? LangChain Docs Google, Amazon, OpenAI Medium Caching 44 How to implement semantic caching? LangChain Docs Google, Amazon Hard Caching 45 What is ConversationalRetrievalChain? LangChain Docs Google, Amazon, OpenAI Medium RAG 46 How to implement multi-turn conversations with RAG? LangChain Docs Google, Amazon, OpenAI Hard RAG 47 What is self-querying retrieval? LangChain Docs Google, Amazon Hard Retrieval 48 How to implement metadata filtering in RAG? LangChain Docs Google, Amazon, OpenAI Hard Filtering 49 What is parent document retriever? LangChain Docs Google, Amazon Hard Retrieval 50 How to implement multi-vector retrieval? LangChain Docs Google, Amazon Hard Retrieval 51 What is contextual compression? LangChain Docs Google, Amazon Hard Compression 52 How to implement re-ranking in RAG? LangChain Docs Google, Amazon, OpenAI Hard Reranking 53 What is HyDE (Hypothetical Document Embeddings)? LangChain Docs Google, Amazon Hard HyDE 54 How to implement SQL database agent? LangChain Docs Google, Amazon, Microsoft Medium SQL 55 What is summarization chain? LangChain Docs Google, Amazon, OpenAI Medium Summary 56 Difference between stuff, map_reduce, and refine chains LangChain Docs Google, Amazon, OpenAI Medium Chains 57 How to implement extraction with LangChain? LangChain Docs Google, Amazon Medium Extraction 58 How to implement chatbot with LangChain? LangChain Docs Most Tech Companies Medium Chatbot 59 What are few-shot prompts? LangChain Docs Google, Amazon, OpenAI Medium Few-Shot 60 How to implement dynamic few-shot selection? LangChain Docs Google, Amazon Hard Few-Shot 61 How to handle long contexts? LangChain Docs Google, Amazon, OpenAI Hard Context 62 How to implement token counting? LangChain Docs Google, Amazon, OpenAI Easy Tokens 63 [HARD] How to implement advanced RAG with query decomposition? LangChain Docs Google, Amazon, OpenAI Hard Advanced RAG 64 [HARD] How to implement FLARE (Forward-Looking Active Retrieval)? LangChain Docs Google, Amazon Hard FLARE 65 [HARD] How to implement corrective RAG? LangChain Docs Google, Amazon Hard CRAG 66 [HARD] How to handle hallucination detection? Towards Data Science Google, Amazon, OpenAI Hard Hallucination 67 [HARD] How to implement citation/source attribution? LangChain Docs Google, Amazon, OpenAI Hard Citation 68 [HARD] How to implement multi-agent systems? LangChain Docs Google, Amazon, OpenAI Hard Multi-Agent 69 [HARD] How to implement plan-and-execute agents? LangChain Docs Google, Amazon Hard Planning 70 [HARD] How to implement autonomous agents? LangChain Docs Google, Amazon, OpenAI Hard Autonomous 71 [HARD] How to implement RAG evaluation metrics? RAGAS Google, Amazon, OpenAI Hard Evaluation 72 [HARD] How to implement faithfulness scoring? RAGAS Google, Amazon Hard Faithfulness 73 [HARD] How to implement context precision/recall? RAGAS Google, Amazon Hard Metrics 74 [HARD] How to implement production-ready RAG pipelines? LangChain Docs Google, Amazon, OpenAI Hard Production 75 [HARD] How to implement load balancing across LLM providers? LangChain Docs Google, Amazon Hard Load Balance 76 [HARD] How to implement cost optimization strategies? LangChain Docs Google, Amazon, OpenAI Hard Cost 77 [HARD] How to implement multi-modal RAG? LangChain Docs Google, Amazon, OpenAI Hard Multi-Modal 78 [HARD] How to implement knowledge graph RAG? LangChain Docs Google, Amazon Hard KG-RAG 79 [HARD] How to secure LangChain applications? LangChain Docs Google, Amazon, Microsoft Hard Security 80 [HARD] How to implement prompt injection prevention? OWASP LLM Google, Amazon, OpenAI Hard Security 81 [HARD] How to implement PII detection and redaction? LangChain Docs Google, Amazon, Apple Hard Privacy 82 [HARD] How to implement guardrails? Guardrails AI Google, Amazon, OpenAI Hard Guardrails 83 [HARD] How to implement async LangChain operations? LangChain Docs Google, Amazon Hard Async 84 [HARD] How to implement A/B testing for prompts? LangSmith Docs Google, Amazon, OpenAI Hard A/B Testing 85 [HARD] How to implement human-in-the-loop systems? LangChain Docs Google, Amazon, OpenAI Hard HITL 86 [HARD] How to implement agentic RAG? LangChain Docs Google, Amazon, OpenAI Hard Agentic RAG 87 [HARD] How to implement tool use evaluation? LangSmith Docs Google, Amazon Hard Tool Eval 88 [HARD] How to handle context window limitations? LangChain Docs Google, Amazon, OpenAI Hard Context 89 [HARD] How to implement continuous evaluation? LangSmith Docs Google, Amazon Hard Evaluation 90 [HARD] How to implement fine-tuning integration? LangChain Docs Google, Amazon, OpenAI Hard Fine-Tuning 91 [HARD] How to implement batch processing efficiently? LangChain Docs Google, Amazon Hard Batch 92 [HARD] How to implement constitutional AI principles? Anthropic Google, Amazon, Anthropic Hard Constitutional 93 [HARD] How to implement router chains? LangChain Docs Google, Amazon Medium Routing 94 [HARD] How to implement graph transformers? LangChain Docs Google, Amazon Hard Graph 95 [HARD] How to implement open source LLMs with LangChain? LangChain Docs Google, Amazon, Meta Medium Open Source 96 [HARD] How to implement custom recursive splitters? LangChain Docs Google, Amazon Hard Chunking 97 [HARD] How to implement dense vs sparse retrieval? LangChain Docs Google, Amazon Hard Retrieval 98 [HARD] How to implement hypothetical questions generation? LangChain Docs Google, Amazon Hard RAG 99 [HARD] How to implement step-back prompting? LangChain Docs Google, Amazon Hard Prompting 100 [HARD] How to implement chain-of-note prompting? LangChain Docs Google, Amazon Hard Prompting 101 [HARD] How to implement skeletal-of-thought? LangChain Docs Google, Amazon Hard Prompting 102 [HARD] How to implement program-of-thought? LangChain Docs Google, Amazon Hard Prompting 103 [HARD] How to implement self-consistency in agents? LangChain Docs Google, Amazon Hard Agents 104 [HARD] How to implement reflection in agents? LangChain Docs Google, Amazon Hard Agents 105 [HARD] How to implement multimodal agents? LangChain Docs Google, Amazon Hard Multimodal 106 [HARD] How to implement streaming tool calls? LangChain Docs Google, Amazon Hard Streaming 107 [HARD] How to implement tool choice forcing? LangChain Docs Google, Amazon Medium Tools 108 [HARD] How to implement parallel function calling? LangChain Docs Google, Amazon Hard Parallel 109 [HARD] How to implement extraction from images? LangChain Docs Google, Amazon Hard Multimodal 110 [HARD] How to implement tagging with specific taxonomy? LangChain Docs Google, Amazon Medium Tagging"},{"location":"Interview-Questions/LangChain/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/LangChain/#1-basic-rag-pipeline-with-lcel","title":"1. Basic RAG Pipeline with LCEL","text":"<pre><code>from langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nvectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\n\nretrieval_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nretrieval_chain.invoke(\"where did harrison work?\")\n</code></pre>"},{"location":"Interview-Questions/LangChain/#2-custom-agent-with-tool-use","title":"2. Custom Agent with Tool Use","text":"<pre><code>from langchain.agents import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate\n\n@tool\ndef multiply(first_int: int, second_int: int) -&gt; int:\n    \"\"\"Multiply two integers together.\"\"\"\n    return first_int * second_int\n\ntools = [multiply]\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant\"),\n    (\"user\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\"),\n])\n\nagent = create_tool_calling_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nagent_executor.invoke({\"input\": \"what is 5 times 8?\"})\n</code></pre>"},{"location":"Interview-Questions/LangChain/#3-structured-output-extraction","title":"3. Structured Output Extraction","text":"<pre><code>from typing import List\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\nclass Person(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n\nclass People(BaseModel):\n    people: List[Person]\n\nllm = ChatOpenAI()\nstructured_llm = llm.with_structured_output(People)\n\ntext = \"Alice is 30 years old and Bob is 25.\"\nstructured_llm.invoke(text)\n</code></pre>"},{"location":"Interview-Questions/LangChain/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>How would you design a production-ready RAG system?</li> <li>Explain query decomposition strategies for complex questions</li> <li>Write code to implement multi-vector retrieval</li> <li>How would you handle hallucination in production systems?</li> <li>Explain the tradeoffs between different chunking strategies</li> <li>How would you implement citation and source attribution?</li> <li>Write code to implement corrective RAG</li> <li>How would you optimize latency for real-time applications?</li> <li>Explain how to implement multi-modal document understanding</li> <li>How would you implement A/B testing for RAG systems?</li> </ul>"},{"location":"Interview-Questions/LangChain/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Write code to implement a customer service chatbot with RAG</li> <li>How would you implement product recommendation using LangChain?</li> <li>Explain how to handle high-throughput scenarios</li> <li>Write code to implement semantic caching</li> <li>How would you implement cost optimization for LLM usage?</li> <li>Explain the difference between retrieval strategies</li> <li>Write code to implement SQL database agent</li> <li>How would you handle multiple document types?</li> <li>Explain how to implement batch processing</li> <li>How would you implement monitoring and alerting?</li> </ul>"},{"location":"Interview-Questions/LangChain/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>Write code to implement content moderation with LangChain</li> <li>How would you implement multi-agent collaboration?</li> <li>Explain how to handle multi-turn conversations</li> <li>Write code to implement social content analysis</li> <li>How would you implement user intent classification?</li> <li>Explain the security considerations for LLM applications</li> <li>Write code to implement plan-and-execute agents</li> <li>How would you handle adversarial inputs?</li> <li>Explain how to implement guardrails</li> <li>How would you scale LangChain applications?</li> </ul>"},{"location":"Interview-Questions/LangChain/#questions-asked-in-openai-interview","title":"Questions asked in OpenAI interview","text":"<ul> <li>Explain the LangChain ecosystem architecture</li> <li>Write code to implement advanced function calling</li> <li>How would you evaluate RAG system quality?</li> <li>Explain the differences between agent types</li> <li>Write code to implement autonomous task completion</li> <li>How would you implement self-healing agents?</li> <li>Explain how to optimize prompt engineering</li> <li>Write code to implement structured output extraction</li> <li>How would you handle context window limitations?</li> <li>Explain how to implement tool use evaluation</li> </ul>"},{"location":"Interview-Questions/LangChain/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Design an enterprise document Q&amp;A system</li> <li>How would you integrate Azure OpenAI with LangChain?</li> <li>Explain how to handle rate limiting and quotas</li> <li>Write code to implement effective memory management</li> <li>How would you ensure data privacy in RAG applications?</li> <li>Explain the role of LangSmith in production monitoring</li> <li>Write code to implement a custom retriever</li> <li>How would you evaluate the faithfulness of generated answers?</li> <li>Explain strategies for reducing LLM costs</li> <li>How would you implement role-based access control?</li> </ul>"},{"location":"Interview-Questions/LangChain/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official LangChain Documentation</li> <li>LangChain Cookbook</li> <li>Pinecone Learning Center</li> <li>DeepLearning.AI LangChain Courses</li> <li>LangSmith Documentation</li> </ul>"},{"location":"Interview-Questions/LangGraph/","title":"LangGraph Interview Questions","text":"<p>This document provides a curated list of LangGraph interview questions commonly asked in technical interviews for AI Engineer, Agentic AI Developer, and Senior Machine Learning Engineer roles. It covers fundamental concepts of stateful agents, graph-based orchestration, cyclic workflows, and multi-agent systems.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is LangGraph and how does it differ from LangChain? LangGraph Docs Google, Amazon, Meta, OpenAI Easy Basics 2 Explain the core concept of a StateGraph LangGraph Docs Google, Amazon, Meta Easy Core Concepts 3 What is the \"State\" in LangGraph? LangGraph Docs Google, Amazon, OpenAI Easy State Management 4 How do nodes and edges work in LangGraph? LangGraph Docs Most Tech Companies Easy Graph Theory 5 What is the difference between conditional edges and normal edges? LangGraph Docs Google, Amazon, OpenAI Medium Graph Control Flow 6 How to implement a basic cyclic graph? LangGraph Docs Google, Amazon, Meta Medium Cycles 7 What is the <code>END</code> node and why is it important? LangGraph Docs Google, Amazon, Meta Easy Graph Termination 8 How to define a custom state schema? LangGraph Docs Google, Amazon, OpenAI Medium State Schema 9 How to use TypedDict for state definition? LangGraph Docs Google, Amazon Easy State Definition 10 What is the difference between <code>MessageGraph</code> and <code>StateGraph</code>? LangGraph Docs Google, Amazon, Meta, OpenAI Medium Graph Types 11 How to implement persistence (checkpointer) in LangGraph? LangGraph Docs Google, Amazon, OpenAI, Anthropic Hard Persistence 12 What is a compiled graph? LangGraph Docs Google, Amazon Easy Compilation 13 How to stream output from a LangGraph workflow? LangGraph Docs Google, Amazon, OpenAI Medium Streaming 14 How to handle user input in a loop (Human-in-the-loop)? LangGraph Docs Google, Amazon, Meta, OpenAI Hard HITL 15 How to implement breakpoints in LangGraph? LangGraph Docs Google, Amazon, OpenAI Hard Debugging 16 What is time travel in LangGraph debugging? LangGraph Docs Google, Amazon Hard Debugging 17 How to modify state during a breakpoint? LangGraph Docs Google, Amazon, OpenAI Hard State Mutation 18 How to implement a tool-calling agent with LangGraph? LangGraph Docs Most Tech Companies Medium Agents 19 How to handle tool execution errors in the graph? LangGraph Docs Google, Amazon, Meta Medium Error Handling 20 How to implement a multi-agent system (e.g., Researcher &amp; Writer)? LangGraph Docs Google, Amazon, Meta, OpenAI Hard Multi-Agent 21 How to coordinate shared state between multiple agents? LangGraph Docs Google, Amazon, OpenAI Hard Shared State 22 What is the supervisor pattern in multi-agent systems? LangGraph Docs Google, Amazon, Meta Hard Multi-Agent Patterns 23 How to implement a hierarchical agent team? LangGraph Docs Google, Amazon, OpenAI Hard Multi-Agent Patterns 24 How to implement the Plan-and-Execute pattern? LangGraph Docs Google, Amazon, Meta Hard Planning 25 How to implement Reflection (Self-Correction) loops? LangGraph Docs Google, Amazon, OpenAI, Anthropic Hard Reliability 26 How to manage conversation history in the state? LangGraph Docs Most Tech Companies Medium Memory 27 How to use <code>Annotated</code> for reducer functions (e.g., <code>operator.add</code>)? LangGraph Docs Google, Amazon, Meta Medium State Reducers 28 How to implement parallel execution branches? LangGraph Docs Google, Amazon, OpenAI Medium Parallelism 29 How to implement map-reduce workflows in LangGraph? LangGraph Docs Google, Amazon, Meta Hard Workflow Patterns 30 How to optimize graph execution latency? LangGraph Docs Google, Amazon Hard Optimization 31 How to visualize the graph structure? LangGraph Docs Google, Amazon Easy Visualization 32 How to export the graph as an image? LangGraph Docs Google, Amazon Easy Visualization 33 How to integrate LangGraph with LangSmith? LangSmith Docs Google, Amazon, OpenAI Medium Observability 34 How to test individual nodes in isolation? LangGraph Docs Google, Amazon, Microsoft Medium Testing 35 How to implement end-to-end testing for graphs? LangGraph Docs Google, Amazon, Meta Hard Testing 36 How to mock tools during testing? LangGraph Docs Google, Amazon Medium Testing 37 How to handle long-running workflows? LangGraph Docs Google, Amazon, OpenAI Hard Production 38 How to deploy LangGraph applications? LangChain Docs Google, Amazon, Microsoft Medium Deployment 39 How to use LangGraph Cloud? LangChain Docs Google, Amazon Medium Cloud 40 How to implement asynchronous nodes? LangGraph Docs Google, Amazon, Meta Medium Async 41 How to handle rate limits in graph execution? LangGraph Docs Google, Amazon, OpenAI Medium Reliability 42 What is \"recursion limit\" in LangGraph and how to configure it? LangGraph Docs Google, Amazon, Meta Medium Configuration 43 How to implement subgraphs (graphs within graphs)? LangGraph Docs Google, Amazon, OpenAI Hard Composition 44 How to pass configuration to the graph run? LangGraph Docs Google, Amazon Medium Configuration 45 How to use <code>configurable</code> parameters in nodes? LangGraph Docs Google, Amazon Medium Configuration 46 How to implement semantic routing? LangGraph Docs Google, Amazon, OpenAI Hard Routing 47 How to implement dynamic edge routing based on LLM output? LangGraph Docs Google, Amazon, Meta Hard Routing 48 How to handle \"stuck\" agents? LangGraph Docs Google, Amazon, OpenAI Hard Reliability 49 How to implement fallback nodes? LangGraph Docs Google, Amazon, Meta Medium Reliability 50 How to integrate external databases with LangGraph state? LangGraph Docs Google, Amazon Hard Integration 51 How to implement RAG within a LangGraph node? LangGraph Docs Google, Amazon, OpenAI Medium RAG 52 How to implement \"Corrective RAG\" (CRAG) using LangGraph? LangGraph Docs Google, Amazon, OpenAI Hard Advanced RAG 53 How to implement \"Self-RAG\" using LangGraph? LangGraph Docs Google, Amazon, OpenAI Hard Advanced RAG 54 How to implement \"Adaptive RAG\" using LangGraph? LangGraph Docs Google, Amazon, OpenAI Hard Advanced RAG 55 How to manage vector store connections in nodes? LangGraph Docs Google, Amazon Medium Infrastructure 56 How to implement message trimming/summarization in the loop? LangGraph Docs Google, Amazon, OpenAI Hard Context Management 57 [HARD] How to implement multi-turn negotiation between agents? LangGraph Docs Google, Amazon, OpenAI Hard Multi-Agent 58 [HARD] How to implement a coding agent with execution sandbox? LangGraph Docs Google, Amazon, Meta Hard Agents 59 [HARD] How to design a graph for long-horizon task planning? LangGraph Docs Google, Amazon, OpenAI Hard Planning 60 [HARD] How to implement Monte Carlo Tree Search (MCTS) with LangGraph? LangGraph Docs Google, Amazon, DeepMind Hard Advanced Algorithms 61 [HARD] How to implement collaborative filtering with agent teams? LangGraph Docs Google, Amazon, Netflix Hard Multi-Agent 62 [HARD] How to separate \"read\" and \"write\" paths in the graph state? LangGraph Docs Google, Amazon Hard Architecture 63 [HARD] How to implement granular access control for nodes? LangGraph Docs Google, Amazon, Microsoft Hard Security 64 [HARD] How to securely pass API keys throughout the graph execution? LangGraph Docs Google, Amazon Hard Security 65 [HARD] How to implement custom checkpointers (e.g., Redis/Postgres)? LangGraph Docs Google, Amazon Hard Infrastructure 66 [HARD] How to migrate state schema versions in production? LangGraph Docs Google, Amazon, Meta Hard DevOps 67 [HARD] How to implement distributed graph execution? LangGraph Docs Google, Amazon Hard Scalability 68 [HARD] How to optimize state size for large-scale graph runs? LangGraph Docs Google, Amazon, OpenAI Hard Optimization 69 [HARD] How to implement a \"Teacher-Student\" training loop with agents? LangGraph Docs Google, Amazon, OpenAI Hard Training 70 [HARD] How to implement dynamic graph modification at runtime? LangGraph Docs Google, Amazon Hard Metaprogramming 71 [HARD] How to implement A/B testing for graph paths? LangGraph Docs Google, Amazon, Netflix Hard Experimentation 72 [HARD] How to evaluate agent performance over multiple graph runs? LangSmith Docs Google, Amazon, OpenAI Hard Evaluation 73 [HARD] How to implement \"Language Agent Tree Search\" (LATS)? LangGraph Docs Google, Amazon, OpenAI Hard Advanced Agents 74 [HARD] How to recover from crashes mid-execution (Hydration)? LangGraph Docs Google, Amazon, Meta Hard Reliability 75 [HARD] How to implement competitive multi-agent environments? LangGraph Docs Google, Amazon, OpenAI Hard Multi-Agent 76 [HARD] How to implement consensus voting mechanisms? LangGraph Docs Google, Amazon Hard Multi-Agent 77 [HARD] How to implement privacy-preserving state sharing? LangGraph Docs Google, Amazon, Apple Hard Privacy 78 [HARD] How to implement custom streaming protocols for frontend UI? LangGraph Docs Google, Amazon Hard Integration 79 [HARD] How to implement efficient batch processing for graphs? LangGraph Docs Google, Amazon Hard Performance 80 [HARD] How to implement graph-level caching strategies? LangGraph Docs Google, Amazon Hard Performance 81 [HARD] How to implement cross-graph communication? LangGraph Docs Google, Amazon Hard Architecture 82 [HARD] How to implement formal verification for graph logic? LangGraph Docs Google, Amazon, Microsoft Hard Reliability 83 [HARD] How to implement secure sandboxing for tool execution nodes? LangGraph Docs Google, Amazon, OpenAI Hard Security 84 [HARD] How to implement cost-aware routing (cheaper vs better models)? LangGraph Docs Google, Amazon, OpenAI Hard Cost Optimization 85 [HARD] How to implement \"Shadow Mode\" for testing new graph versions? LangGraph Docs Google, Amazon, Meta Hard Deployment 86 [HARD] How to implement automated regression testing for agents? LangGraph Docs Google, Amazon Hard Testing 87 [HARD] How to implement state rollback mechanisms? LangGraph Docs Google, Amazon Hard Reliability 88 [HARD] How to implement event-driven graph triggers? LangGraph Docs Google, Amazon Hard Architecture 89 [HARD] How to implement fine-grained observability/telemetry? LangSmith Docs Google, Amazon Hard Observability 90 [HARD] How to implement customized human-approval workflows? LangGraph Docs Google, Amazon, OpenAI Hard HITL 91 [HARD] How to implement \"Generative Agents\" simulation? LangGraph Docs Google, Amazon, OpenAI Hard Simulation 92 [HARD] How to implement specialized expert router architectures? LangGraph Docs Google, Amazon Hard Architecture 93 [HARD] How to implement dynamic tool selection/pruning? LangGraph Docs Google, Amazon Hard Optimization 94 [HARD] How to implement context-aware memory compression? LangGraph Docs Google, Amazon, OpenAI Hard Memory 95 [HARD] How to implement asynchronous human feedback collection? LangGraph Docs Google, Amazon Hard HITL 96 [HARD] How to implement graph versioning and rollback? LangGraph Docs Google, Amazon Hard DevOps 97 [HARD] How to implement custom retry and backoff strategies? LangGraph Docs Google, Amazon Medium Reliability 98 [HARD] How to implement multi-user collaboration on the same graph state? LangGraph Docs Google, Amazon, Figma Hard Collaboration 99 [HARD] How to implement compliance auditing for agent actions? LangGraph Docs Google, Amazon, Microsoft Hard Compliance 100 [HARD] How to implement secure secret management in graph config? LangGraph Docs Google, Amazon Hard Security"},{"location":"Interview-Questions/LangGraph/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/LangGraph/#1-basic-stategraph-definition","title":"1. Basic StateGraph Definition","text":"<pre><code>from typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph import StateGraph, END\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n\ndef agent(state):\n    # Agent logic here\n    return {\"messages\": [\"Agent response\"]}\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"agent\", agent)\nworkflow.set_entry_point(\"agent\")\nworkflow.add_edge(\"agent\", END)\n\napp = workflow.compile()\n</code></pre>"},{"location":"Interview-Questions/LangGraph/#2-multi-agent-coordinator-supervisor","title":"2. Multi-Agent Coordinator (Supervisor)","text":"<pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n\nmembers = [\"researcher\", \"coder\"]\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    \" following workers: {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\noptions = [\"FINISH\"] + members\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_prompt),\n    (\"user\", \"{messages}\"),\n    (\"system\", \"Given the conversation above, who should act next? or should we FINISH?\"),\n])\n\nsupervisor_chain = (\n    prompt\n    | ChatOpenAI(model=\"gpt-4-turbo\").bind_functions(functions=[function_def], function_call=\"route\")\n    | JsonOutputFunctionsParser()\n)\n</code></pre>"},{"location":"Interview-Questions/LangGraph/#3-human-in-the-loop-with-checkpointer","title":"3. Human-in-the-loop with Checkpointer","text":"<pre><code>from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory, interrupt_before=[\"human_review\"])\n\n# Run until interruption\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\nfor event in graph.stream(inputs, thread):\n    pass\n\n# Review and continue\nfull_state = graph.get_state(thread)\n# ... human reviews state ...\ngraph.stream(None, thread) # Resume execution\n</code></pre>"},{"location":"Interview-Questions/LangGraph/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Design a multi-agent system for software development (Coder, Reviewer, Tester)</li> <li>How would you debug an infinite loop in a cyclic graph?</li> <li>Implement a human-in-the-loop workflow for content approval</li> <li>How to optimize state management for very long conversations?</li> <li>Explain the supervisor pattern trade-offs vs hierarchical teams</li> <li>How would you implement \"Self-Refining\" agents?</li> <li>Write code to implement a custom persisted checkpointer</li> <li>How to handle race conditions in parallel branches?</li> <li>Explain strategies for detailed observability in agent networks</li> <li>How to implement cost controls for autonomous agents?</li> </ul>"},{"location":"Interview-Questions/LangGraph/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Design a customer support agent system with escalation paths</li> <li>How would you implement a \"Plan-and-Execute\" architecture?</li> <li>Explain how to handle tool failures gracefully in a graph</li> <li>How to implement efficient memory management for agents?</li> <li>Explain the difference between compiled vs dynamic graphs</li> <li>How to implement reliable event-driven triggers?</li> <li>Write code for a custom state reducer function</li> <li>How to implement secure sandboxed code execution?</li> <li>Explain strategies for A/B testing agent workflows</li> <li>How to implement automated regression tests for graphs?</li> </ul>"},{"location":"Interview-Questions/LangGraph/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>Design a social simulation using Generative Agents</li> <li>How would you implement \"Reflection\" to improve agent quality?</li> <li>Explain how to manage shared state in a complex graph</li> <li>How to implement dynamic routing based on content classification?</li> <li>Explain the benefits of functional state management</li> <li>How to implement privacy-preserving collaborative agents?</li> <li>Write code to implement semantic routing logic</li> <li>How to implement effective human-feedback loops?</li> <li>Explain strategies for preventing agent hallucination loops</li> <li>How to scale graph execution to millions of users?</li> </ul>"},{"location":"Interview-Questions/LangGraph/#questions-asked-in-openai-interview","title":"Questions asked in OpenAI interview","text":"<ul> <li>Design an autonomous research assistant using LangGraph</li> <li>How would you implement \"Language Agent Tree Search\" (LATS)?</li> <li>Explain how to control the \"recursion limit\" effectively</li> <li>How to implement Time Travel debugging?</li> <li>Explain the \"Teacher-Student\" training pattern for agents</li> <li>How to implement secure tool use verification?</li> <li>Write code to implement a subgraph pattern</li> <li>How to implement context-aware token usage optimization?</li> <li>Explain strategies for evaluating multi-agent systems</li> <li>How to implement \"Shadow Mode\" deployment safely?</li> </ul>"},{"location":"Interview-Questions/LangGraph/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Design an enterprise document processing workflow</li> <li>How would you integrate legacy SQL databases with LangGraph?</li> <li>Explain how to implement role-based access control (RBAC) in graphs</li> <li>How to implement reliable state durability and recovery?</li> <li>Explain the integration of LangGraph with copilots</li> <li>How to implement compliance logging for all agent decisions?</li> <li>Write code to implement parallel map-reduce processing</li> <li>How to implement secure API key handling in shared graphs?</li> <li>Explain strategies for versioning agent behaviors</li> <li>How to implement cross-geography distributed execution?</li> </ul>"},{"location":"Interview-Questions/LangGraph/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official LangGraph Documentation</li> <li>LangChain Academy: Intro to LangGraph</li> <li>LangGraph Tutorials</li> <li>Multi-Agent Systems with LangGraph</li> <li>LangSmith Evaluation</li> </ul>"},{"location":"Interview-Questions/Machine-Learning/","title":"Machine Learning Interview Questions","text":"<p>This document provides a curated list of 100 Machine Learning interview questions commonly asked in technical interviews. It covers topics ranging from basic ML concepts and data preprocessing to deep learning, reinforcement learning, and advanced optimization techniques. The list is updated frequently to serve as a comprehensive reference for interview preparation.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 Bias-Variance Tradeoff Machine Learning Mastery Google, Facebook, Amazon Medium Model Evaluation, Generalization 2 Regularization Techniques (L1, L2) Machine Learning Mastery Google, Amazon, Microsoft Medium Overfitting, Generalization 3 Cross-Validation Scikit-Learn Cross Validation Google, Facebook, Amazon Easy Model Evaluation 4 Overfitting and Underfitting Analytics Vidhya Google, Amazon, Facebook Easy Model Evaluation 5 Gradient Descent Towards Data Science Google, Amazon, Microsoft Medium Optimization 6 Supervised vs Unsupervised Learning IBM Cloud Learn Google, Facebook, Amazon Easy ML Basics 7 Classification vs Regression Towards Data Science Google, Amazon, Facebook Easy ML Basics 8 Evaluation Metrics: Precision, Recall, F1-score Towards Data Science Google, Amazon, Microsoft Medium Model Evaluation 9 Decision Trees Machine Learning Mastery Google, Amazon, Facebook Medium Tree-based Models 10 Ensemble Learning: Bagging and Boosting Towards Data Science Google, Amazon, Microsoft Medium Ensemble Methods 11 Random Forest Towards Data Science Google, Amazon, Facebook Medium Ensemble, Decision Trees 12 Support Vector Machines (SVM) Machine Learning Mastery Google, Facebook, Amazon Hard Classification, Kernel Methods 13 k-Nearest Neighbors (k-NN) Towards Data Science Google, Amazon, Facebook Easy Instance-based Learning 14 Dimensionality Reduction: PCA Towards Data Science Google, Amazon, Microsoft Medium Dimensionality Reduction 15 Handling Missing Data Machine Learning Mastery Google, Amazon, Facebook Easy Data Preprocessing 16 Parametric vs Non-Parametric Models Towards Data Science Google, Amazon Medium Model Types 17 Neural Networks: Basics Towards Data Science Google, Facebook, Amazon Medium Deep Learning 18 Convolutional Neural Networks (CNNs) Towards Data Science Google, Facebook, Amazon Hard Deep Learning, Computer Vision 19 Recurrent Neural Networks (RNNs) and LSTMs Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Sequence Models 20 Reinforcement Learning Basics Towards Data Science Google, Amazon, Facebook Hard Reinforcement Learning 21 Hyperparameter Tuning Machine Learning Mastery Google, Amazon, Microsoft Medium Model Optimization 22 Feature Engineering Towards Data Science Google, Amazon, Facebook Medium Data Preprocessing 23 ROC Curve and AUC Towards Data Science Google, Amazon, Microsoft Medium Model Evaluation 24 Regression Evaluation Metrics Scikit-Learn Google, Amazon, Facebook Medium Model Evaluation, Regression 25 Curse of Dimensionality Machine Learning Mastery Google, Amazon, Facebook Hard Data Preprocessing 26 Logistic Regression Towards Data Science Google, Amazon, Facebook Easy Classification, Regression 27 Linear Regression Analytics Vidhya Google, Amazon, Facebook Easy Regression 28 Loss Functions in ML Towards Data Science Google, Amazon, Microsoft Medium Optimization, Model Evaluation 29 Gradient Descent Variants Machine Learning Mastery Google, Amazon, Facebook Medium Optimization 30 Data Normalization and Standardization Machine Learning Mastery Google, Amazon, Facebook Easy Data Preprocessing 31 k-Means Clustering Towards Data Science Google, Amazon, Facebook Medium Clustering 32 Other Clustering Techniques Analytics Vidhya Google, Amazon, Facebook Medium Clustering 33 Anomaly Detection Towards Data Science Google, Amazon, Facebook Hard Outlier Detection 34 Learning Rate in Optimization Machine Learning Mastery Google, Amazon, Microsoft Medium Optimization 35 Deep Learning vs. Traditional ML IBM Cloud Learn Google, Amazon, Facebook Medium Deep Learning, ML Basics 36 Dropout in Neural Networks Towards Data Science Google, Amazon, Facebook Medium Deep Learning, Regularization 37 Backpropagation Analytics Vidhya Google, Amazon, Facebook Hard Deep Learning, Neural Networks 38 Role of Activation Functions Machine Learning Mastery Google, Amazon, Facebook Medium Neural Networks 39 Word Embeddings and Their Use Towards Data Science Google, Amazon, Facebook Medium NLP, Deep Learning 40 Transfer Learning Machine Learning Mastery Google, Amazon, Facebook Medium Deep Learning, Model Reuse 41 Bayesian Optimization for Hyperparameters Towards Data Science Google, Amazon, Microsoft Hard Hyperparameter Tuning, Optimization 42 Model Interpretability: SHAP and LIME Towards Data Science Google, Amazon, Facebook Hard Model Interpretability, Explainability 43 Ensemble Methods: Stacking and Blending Machine Learning Mastery Google, Amazon, Microsoft Hard Ensemble Methods 44 Gradient Boosting Machines (GBM) Basics Towards Data Science Google, Amazon, Facebook Medium Ensemble, Boosting 45 Extreme Gradient Boosting (XGBoost) Overview Towards Data Science Google, Amazon, Facebook Medium Ensemble, Boosting 46 LightGBM vs XGBoost Comparison Analytics Vidhya Google, Amazon Medium Ensemble, Boosting 47 CatBoost: Handling Categorical Features Towards Data Science Google, Amazon, Facebook Medium Ensemble, Categorical Data 48 Time Series Forecasting with ARIMA Analytics Vidhya Google, Amazon, Facebook Hard Time Series, Forecasting 49 Time Series Forecasting with LSTM Towards Data Science Google, Amazon, Facebook Hard Time Series, Deep Learning 50 Robust Scaling Techniques Towards Data Science Google, Amazon, Facebook Medium Data Preprocessing 51 Data Imputation Techniques in ML Machine Learning Mastery Google, Amazon, Facebook Medium Data Preprocessing 52 Handling Imbalanced Datasets: SMOTE and Others Towards Data Science Google, Amazon, Facebook Hard Data Preprocessing, Classification 53 Bias in Machine Learning: Fairness and Ethics Towards Data Science Google, Amazon, Facebook Hard Ethics, Fairness 54 Model Deployment: From Prototype to Production Towards Data Science Google, Amazon, Facebook Medium Deployment 55 Online Learning Algorithms Towards Data Science Google, Amazon, Microsoft Hard Online Learning 56 Concept Drift in Machine Learning Towards Data Science Google, Amazon, Facebook Hard Model Maintenance 57 Transfer Learning in NLP: BERT, GPT Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 58 Natural Language Processing: Text Preprocessing Analytics Vidhya Google, Amazon, Facebook Easy NLP, Data Preprocessing 59 Text Vectorization: TF-IDF vs Word2Vec Towards Data Science Google, Amazon, Facebook Medium NLP, Feature Extraction 60 Transformer Architecture and Self-Attention Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 61 Understanding BERT for NLP Tasks Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 62 Understanding GPT Models Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 63 Data Augmentation Techniques in ML Towards Data Science Google, Amazon, Facebook Medium Data Preprocessing 64 Adversarial Machine Learning: Attack and Defense Towards Data Science Google, Amazon, Facebook Hard Security, ML 65 Explainable AI (XAI) in Practice Towards Data Science Google, Amazon, Facebook Hard Model Interpretability 66 Federated Learning: Concepts and Challenges Towards Data Science Google, Amazon, Facebook Hard Distributed Learning 67 Multi-Task Learning in Neural Networks Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Multi-Task 68 Metric Learning and Siamese Networks Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Metric Learning 69 Deep Reinforcement Learning: DQN Overview Towards Data Science Google, Amazon, Facebook Hard Reinforcement Learning, Deep Learning 70 Policy Gradient Methods in Reinforcement Learning Towards Data Science Google, Amazon, Facebook Hard Reinforcement Learning 71 Actor-Critic Methods in RL Towards Data Science Google, Amazon, Facebook Hard Reinforcement Learning 72 Monte Carlo Methods in Machine Learning Towards Data Science Google, Amazon, Facebook Medium Optimization, Probabilistic Methods 73 Expectation-Maximization Algorithm Towards Data Science Google, Amazon, Facebook Hard Clustering, Probabilistic Models 74 Gaussian Mixture Models (GMM) Towards Data Science Google, Amazon, Facebook Medium Clustering, Probabilistic Models 75 Bayesian Inference in ML Towards Data Science Google, Amazon, Facebook Hard Bayesian Methods 76 Markov Chain Monte Carlo (MCMC) Methods Towards Data Science Google, Amazon, Facebook Hard Bayesian Methods, Probabilistic Models 77 Variational Autoencoders (VAEs) Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Generative Models 78 Generative Adversarial Networks (GANs) Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Generative Models 79 Conditional GANs for Data Generation Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Generative Models 80 Sequence-to-Sequence Models in NLP Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 81 Attention Mechanisms in Seq2Seq Models Towards Data Science Google, Amazon, Facebook Hard NLP, Deep Learning 82 Capsule Networks: An Introduction Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Neural Networks 83 Self-Supervised Learning in Deep Learning Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Unsupervised Learning 84 Zero-Shot and Few-Shot Learning Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Transfer Learning 85 Meta-Learning: Learning to Learn Towards Data Science Google, Amazon, Facebook Hard Deep Learning, Optimization 86 Hyperparameter Sensitivity Analysis Towards Data Science Google, Amazon, Facebook Medium Hyperparameter Tuning 87 High-Dimensional Feature Selection Techniques Towards Data Science Google, Amazon, Facebook Hard Feature Engineering, Dimensionality Reduction 88 Multi-Label Classification Techniques Towards Data Science Google, Amazon, Facebook Hard Classification, Multi-Output 89 Ordinal Regression in Machine Learning Towards Data Science Google, Amazon, Facebook Medium Regression, Classification 90 Survival Analysis in ML Towards Data Science Google, Amazon, Facebook Hard Statistics, ML 91 Semi-Supervised Learning Methods Towards Data Science Google, Amazon, Facebook Hard Unsupervised Learning, ML Basics 92 Unsupervised Feature Learning Towards Data Science Google, Amazon, Facebook Medium Unsupervised Learning, Feature Extraction 93 Clustering Evaluation Metrics: Silhouette, Davies-Bouldin Towards Data Science Google, Amazon, Facebook Medium Clustering, Evaluation 94 Dimensionality Reduction: t-SNE and UMAP Towards Data Science Google, Amazon, Facebook Medium Dimensionality Reduction 95 Probabilistic Graphical Models: Bayesian Networks Towards Data Science Google, Amazon, Facebook Hard Probabilistic Models, Graphical Models 96 Hidden Markov Models (HMMs) in ML Towards Data Science Google, Amazon, Facebook Hard Probabilistic Models, Sequence Modeling 97 Recommender Systems: Collaborative Filtering Towards Data Science Google, Amazon, Facebook Medium Recommender Systems 98 Recommender Systems: Content-Based Filtering Towards Data Science Google, Amazon, Facebook Medium Recommender Systems 99 Anomaly Detection in Time Series Data Towards Data Science Google, Amazon, Facebook Hard Time Series, Anomaly Detection 100 Optimization Algorithms Beyond Gradient Descent (Adam, RMSProp, etc.) Towards Data Science Google, Amazon, Facebook Medium Optimization, Deep Learning"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Bias-Variance Tradeoff  </li> <li>Cross-Validation  </li> <li>Overfitting and Underfitting  </li> <li>Gradient Descent  </li> <li>Neural Networks: Basics  </li> <li>Convolutional Neural Networks (CNNs)  </li> <li>Recurrent Neural Networks (RNNs) and LSTMs  </li> <li>Reinforcement Learning Basics  </li> <li>Hyperparameter Tuning  </li> <li>Transfer Learning  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-facebook-interview","title":"Questions asked in Facebook interview","text":"<ul> <li>Bias-Variance Tradeoff  </li> <li>Cross-Validation  </li> <li>Overfitting and Underfitting  </li> <li>Neural Networks: Basics  </li> <li>Convolutional Neural Networks (CNNs)  </li> <li>Recurrent Neural Networks (RNNs) and LSTMs  </li> <li>Support Vector Machines (SVM)  </li> <li>k-Nearest Neighbors (k-NN)  </li> <li>Feature Engineering  </li> <li>Dropout in Neural Networks  </li> <li>Backpropagation  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Bias-Variance Tradeoff  </li> <li>Regularization Techniques (L1, L2)  </li> <li>Cross-Validation  </li> <li>Overfitting and Underfitting  </li> <li>Decision Trees  </li> <li>Ensemble Learning: Bagging and Boosting  </li> <li>Random Forest  </li> <li>Support Vector Machines (SVM)  </li> <li>Neural Networks: Basics  </li> <li>Hyperparameter Tuning  </li> <li>ROC Curve and AUC  </li> <li>Logistic Regression  </li> <li>Data Normalization and Standardization  </li> <li>k-Means Clustering  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Regularization Techniques (L1, L2)  </li> <li>Gradient Descent  </li> <li>Convolutional Neural Networks (CNNs)  </li> <li>Recurrent Neural Networks (RNNs) and LSTMs  </li> <li>Support Vector Machines (SVM)  </li> <li>Hyperparameter Tuning  </li> <li>ROC Curve and AUC  </li> <li>Loss Functions in ML  </li> <li>Learning Rate in Optimization  </li> <li>Bayesian Optimization for Hyperparameters  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-uber-interview","title":"Questions asked in Uber interview","text":"<ul> <li>Reinforcement Learning Basics  </li> <li>Anomaly Detection  </li> <li>Gradient Descent Variants  </li> <li>Model Deployment: From Prototype to Production  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-swiggy-interview","title":"Questions asked in Swiggy interview","text":"<ul> <li>Handling Missing Data  </li> <li>Data Imputation Techniques in ML  </li> <li>Feature Engineering  </li> <li>Model Interpretability: SHAP and LIME  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-flipkart-interview","title":"Questions asked in Flipkart interview","text":"<ul> <li>Ensemble Methods: Stacking and Blending  </li> <li>Time Series Forecasting with ARIMA  </li> <li>Time Series Forecasting with LSTM  </li> <li>Model Deployment: From Prototype to Production  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-ola-interview","title":"Questions asked in Ola interview","text":"<ul> <li>Time Series Forecasting with LSTM  </li> <li>Data Normalization and Standardization  </li> <li>Recurrent Neural Networks (RNNs) and LSTMs  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-paytm-interview","title":"Questions asked in Paytm interview","text":"<ul> <li>Model Deployment: From Prototype to Production  </li> <li>Online Learning Algorithms  </li> <li>Handling Imbalanced Datasets: SMOTE and Others  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-oyo-interview","title":"Questions asked in OYO interview","text":"<ul> <li>Data Preprocessing Techniques  </li> <li>Ensemble Learning: Bagging and Boosting  </li> <li>Regularization Techniques (L1, L2)  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-whatsapp-interview","title":"Questions asked in WhatsApp interview","text":"<ul> <li>Neural Networks: Basics  </li> <li>Convolutional Neural Networks (CNNs)  </li> <li>Recurrent Neural Networks (RNNs) and LSTMs  </li> <li>Dropout in Neural Networks  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-slack-interview","title":"Questions asked in Slack interview","text":"<ul> <li>Bias-Variance Tradeoff  </li> <li>Cross-Validation  </li> <li>Feature Engineering  </li> <li>Transfer Learning  </li> </ul>"},{"location":"Interview-Questions/Machine-Learning/#questions-asked-in-airbnb-interview","title":"Questions asked in Airbnb interview","text":"<ul> <li>Bias-Variance Tradeoff  </li> <li>Hyperparameter Tuning  </li> <li>Transfer Learning  </li> <li>Model Interpretability: SHAP and LIME  </li> </ul> <p>Note: The practice links are curated from reputable sources such as Machine Learning Mastery, Towards Data Science, Analytics Vidhya, and Scikit-learn. You can update/contribute to these lists or add new ones as more resources become available.</p>"},{"location":"Interview-Questions/Natural-Language-Processing/","title":"Natural Language Processing (NLP) Interview Questions","text":"<p>This document provides a curated list of 100 NLP interview questions commonly asked in technical interviews. Covering topics from the fundamentals of text processing to deep learning\u2013based language models, this list is updated frequently and is intended to serve as a comprehensive reference for interview preparation.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is Natural Language Processing? Analytics Vidhya NLP Basics Google, Facebook, Amazon Easy NLP Basics 2 Explain Tokenization. Towards Data Science \u2013 Tokenization Google, Amazon, Facebook Easy Preprocessing 3 What is Stop Word Removal and why is it important? TDS \u2013 Stop Words Google, Facebook, Amazon Easy Preprocessing 4 Explain Stemming. TDS \u2013 Stemming Google, Amazon, Microsoft Easy Preprocessing 5 Explain Lemmatization. Analytics Vidhya \u2013 Lemmatization Google, Facebook, Amazon Easy Preprocessing 6 What is the Bag-of-Words Model? TDS \u2013 Bag-of-Words Google, Facebook, Amazon Easy Text Representation 7 Explain TF-IDF and its applications. TDS \u2013 TF-IDF Google, Amazon, Microsoft Easy Feature Extraction 8 What are Word Embeddings? TDS \u2013 Word Embeddings Google, Facebook, Amazon Medium Embeddings 9 Explain the Word2Vec algorithm. TDS \u2013 Word2Vec Google, Amazon, Facebook Medium Embeddings 10 Explain GloVe embeddings. TDS \u2013 GloVe Google, Facebook, Amazon Medium Embeddings 11 What is FastText and how does it differ from Word2Vec? TDS \u2013 FastText Google, Facebook, Amazon Medium Embeddings 12 What is one-hot encoding in NLP? Analytics Vidhya \u2013 NLP Encoding Google, Amazon, Facebook Easy Text Representation 13 What is an n-gram Language Model? TDS \u2013 N-grams Google, Facebook, Amazon Medium Language Modeling 14 Explain Language Modeling. TDS \u2013 Language Modeling Google, Amazon, Microsoft Medium Language Modeling 15 How are Recurrent Neural Networks (RNNs) used in NLP? TDS \u2013 RNNs for NLP Google, Facebook, Amazon Medium Deep Learning, Sequence Models 16 Explain Long Short-Term Memory (LSTM) Networks in NLP. TDS \u2013 LSTM Google, Amazon, Facebook Medium Deep Learning, Sequence Models 17 What are Gated Recurrent Units (GRU) and their benefits? TDS \u2013 GRU Google, Facebook, Amazon Medium Deep Learning, Sequence Models 18 What is the Transformer architecture? TDS \u2013 Transformers Google, Facebook, Amazon Hard Deep Learning, Transformers 19 What is BERT and how does it work? TDS \u2013 BERT Google, Facebook, Amazon Hard Language Models, Transformers 20 What is GPT and what are its applications in NLP? TDS \u2013 GPT Google, Facebook, Amazon Hard Language Models, Transformers 21 Explain the Attention Mechanism in NLP. TDS \u2013 Attention Google, Amazon, Facebook Hard Deep Learning, Transformers 22 What is Self-Attention? TDS \u2013 Self-Attention Google, Facebook, Amazon Hard Deep Learning, Transformers 23 Explain Sequence-to-Sequence Models. TDS \u2013 Seq2Seq Google, Facebook, Amazon Medium Deep Learning, Generation 24 What is Machine Translation? TDS \u2013 Machine Translation Google, Amazon, Facebook Medium Applications 25 Explain Sentiment Analysis. Analytics Vidhya \u2013 Sentiment Analysis Google, Facebook, Amazon Easy Applications 26 What is Named Entity Recognition (NER)? TDS \u2013 NER Google, Amazon, Facebook Easy Applications 27 What is Part-of-Speech Tagging? TDS \u2013 POS Tagging Google, Facebook, Amazon Easy Linguistic Processing 28 Explain Dependency Parsing. TDS \u2013 Dependency Parsing Google, Amazon, Microsoft Medium Parsing 29 What is Constituency Parsing? TDS \u2013 Constituency Parsing Google, Facebook, Amazon Medium Parsing 30 Explain Semantic Role Labeling. TDS \u2013 Semantic Role Labeling Google, Amazon, Facebook Hard Parsing, Semantics 31 What is Text Classification? Analytics Vidhya \u2013 Text Classification Google, Facebook, Amazon Easy Applications 32 What is Topic Modeling? TDS \u2013 Topic Modeling Google, Amazon, Facebook Medium Unsupervised Learning 33 Explain Latent Dirichlet Allocation (LDA). TDS \u2013 LDA Google, Amazon, Facebook Medium Topic Modeling 34 Explain Latent Semantic Analysis (LSA). TDS \u2013 LSA Google, Facebook, Amazon Medium Topic Modeling 35 What is Text Summarization? Analytics Vidhya \u2013 Summarization Google, Facebook, Amazon Medium Applications 36 Differentiate between Extractive and Abstractive Summarization. TDS \u2013 Summarization Google, Amazon, Facebook Hard Applications 37 What are Language Generation Models? TDS \u2013 Language Generation Google, Facebook, Amazon Hard Generation 38 Explain Sequence Labeling. TDS \u2013 Sequence Labeling Google, Amazon, Facebook Medium Applications 39 What is a Conditional Random Field (CRF) in NLP? TDS \u2013 CRF Google, Facebook, Amazon Hard Sequence Modeling 40 What is Word Sense Disambiguation? TDS \u2013 WSD Google, Amazon, Facebook Hard Semantics 41 Explain the concept of Perplexity in Language Models. TDS \u2013 Perplexity Google, Facebook, Amazon Medium Language Modeling 42 What is Text Normalization? Analytics Vidhya \u2013 NLP Preprocessing Google, Amazon, Facebook Easy Preprocessing 43 What is Noise Removal in Text Processing? TDS \u2013 NLP Preprocessing Google, Facebook, Amazon Easy Preprocessing 44 Explain the importance of punctuation in NLP. TDS \u2013 NLP Basics Google, Amazon, Facebook Easy Preprocessing 45 What is Document Classification? Analytics Vidhya \u2013 Document Classification Google, Facebook, Amazon Easy Applications 46 Explain the Vector Space Model. TDS \u2013 Vector Space Google, Amazon, Facebook Medium Text Representation 47 What is Cosine Similarity in Text Analysis? TDS \u2013 Cosine Similarity Google, Facebook, Amazon Medium Similarity Measures 48 What is Semantic Similarity? TDS \u2013 Semantic Similarity Google, Amazon, Facebook Medium Semantics 49 What is Text Clustering? TDS \u2013 Text Clustering Google, Facebook, Amazon Medium Unsupervised Learning 50 Explain Hierarchical Clustering for Text. TDS \u2013 Hierarchical Clustering Google, Amazon, Facebook Medium Unsupervised Learning 51 What is DBSCAN in the context of NLP? TDS \u2013 DBSCAN Google, Facebook, Amazon Medium Unsupervised Learning 52 Explain the process of Fine-tuning Pre-trained Language Models. TDS \u2013 Fine-tuning NLP Google, Amazon, Facebook Hard Transfer Learning 53 What is Transfer Learning in NLP? Analytics Vidhya \u2013 Transfer Learning Google, Facebook, Amazon Medium Transfer Learning 54 What is Zero-Shot Classification in NLP? TDS \u2013 Zero-Shot Learning Google, Amazon, Facebook Hard Transfer Learning 55 What is Few-Shot Learning in NLP? TDS \u2013 Few-Shot Learning Google, Facebook, Amazon Hard Transfer Learning 56 Explain Adversarial Attacks on NLP Models. TDS \u2013 Adversarial NLP Google, Facebook, Amazon Hard Security, Robustness 57 Discuss Bias in NLP Models. TDS \u2013 NLP Bias Google, Amazon, Facebook Hard Ethics, Fairness 58 What are Ethical Considerations in NLP? Analytics Vidhya \u2013 Ethical NLP Google, Facebook, Amazon Hard Ethics 59 What is Language Detection? TDS \u2013 Language Detection Google, Amazon, Facebook Easy Applications 60 Explain Transliteration in NLP. TDS \u2013 Transliteration Google, Facebook, Amazon Medium Applications 61 What is Language Identification? Analytics Vidhya \u2013 NLP Basics Google, Amazon, Facebook Easy Applications 62 Explain Query Expansion in Information Retrieval. TDS \u2013 Information Retrieval Google, Facebook, Amazon Medium IR, NLP 63 What is Textual Entailment? TDS \u2013 Textual Entailment Google, Amazon, Facebook Hard Semantics 64 What is Natural Language Inference (NLI)? TDS \u2013 NLI Google, Facebook, Amazon Hard Semantics 65 What are Dialog Systems in NLP? Analytics Vidhya \u2013 Dialog Systems Google, Facebook, Amazon Medium Conversational AI 66 Explain Chatbot Architecture. TDS \u2013 Chatbots Google, Amazon, Facebook Medium Conversational AI 67 What is Intent Detection in Chatbots? TDS \u2013 Intent Detection Google, Facebook, Amazon Medium Conversational AI 68 What is Slot Filling in Conversational Agents? TDS \u2013 Slot Filling Google, Amazon, Facebook Medium Conversational AI 69 Explain Conversation Modeling. TDS \u2013 Conversation Modeling Google, Facebook, Amazon Hard Conversational AI 70 How is Sentiment Analysis performed using lexicons? Analytics Vidhya \u2013 Sentiment Analysis Google, Facebook, Amazon Easy Applications 71 Explain deep learning techniques for sentiment analysis. TDS \u2013 Deep Sentiment Google, Amazon, Facebook Medium Deep Learning, Applications 72 What is Sequence-to-Sequence Learning for Chatbots? TDS \u2013 Seq2Seq Chatbots Google, Facebook, Amazon Hard Conversational AI 73 Explain the role of Attention in Machine Translation. TDS \u2013 Attention in MT Google, Amazon, Facebook Hard Deep Learning, Translation 74 What is Multi-Head Attention? TDS \u2013 Multi-Head Attention Google, Facebook, Amazon Hard Transformers 75 Explain the Encoder-Decoder Architecture. TDS \u2013 Encoder-Decoder Google, Amazon, Facebook Hard Deep Learning, Transformers 76 What is Beam Search in NLP? TDS \u2013 Beam Search Google, Facebook, Amazon Medium Decoding, Generation 77 Explain Back-Translation for Data Augmentation. TDS \u2013 Back-Translation Google, Amazon, Facebook Hard Data Augmentation 78 How does GPT generate text? TDS \u2013 GPT Generation Google, Facebook, Amazon Hard Language Models, Generation 79 What is Fine-tuning in Language Models? TDS \u2013 Fine-tuning Google, Facebook, Amazon Hard Transfer Learning 80 What is a Context Window in Language Models? TDS \u2013 Context Window Google, Amazon, Facebook Medium Language Modeling 81 Explain the Transformer Decoder. TDS \u2013 Transformer Decoder Google, Facebook, Amazon Hard Transformers 82 Discuss the importance of Embedding Layers in NLP. TDS \u2013 Embedding Layers Google, Facebook, Amazon Medium Deep Learning, Embeddings 83 What is Positional Encoding in Transformers? TDS \u2013 Positional Encoding Google, Facebook, Amazon Medium Transformers 84 What is Masked Language Modeling? TDS \u2013 Masked LM Google, Facebook, Amazon Hard Transformers, Pre-training 85 Explain Next Sentence Prediction in BERT. TDS \u2013 Next Sentence Prediction Google, Facebook, Amazon Hard BERT, Pre-training 86 What are Pre-trained Language Models? Analytics Vidhya \u2013 Pre-trained Models Google, Facebook, Amazon Easy Transfer Learning 87 Explain Open-Domain Question Answering in NLP. TDS \u2013 Question Answering Google, Facebook, Amazon Hard Applications, QA 88 What is Retrieval-Based NLP? TDS \u2013 Retrieval-Based Google, Facebook, Amazon Medium Applications, QA 89 Explain Extractive Question Answering. TDS \u2013 Extractive QA Google, Facebook, Amazon Hard Applications, QA 90 What is Abstractive Question Answering? TDS \u2013 Abstractive QA Google, Facebook, Amazon Hard Applications, QA 91 What is Machine Reading Comprehension? TDS \u2013 MRC Google, Facebook, Amazon Hard Applications, QA 92 What are Attention Heads in Transformers? TDS \u2013 Attention Heads Google, Facebook, Amazon Hard Transformers 93 Explain Sequence Transduction. TDS \u2013 Sequence Transduction Google, Facebook, Amazon Hard Deep Learning, Generation 94 Discuss the role of GPUs in NLP model training. Analytics Vidhya \u2013 NLP Infrastructure Google, Facebook, Amazon Medium Infrastructure 95 What is Subword Tokenization (BPE, SentencePiece)? TDS \u2013 Subword Tokenization Google, Facebook, Amazon Medium Preprocessing, Tokenization 96 What is a Language Corpus and why is it important? Analytics Vidhya \u2013 Language Corpora Google, Facebook, Amazon Easy NLP Resources 97 What are the challenges in Low-Resource Languages? TDS \u2013 Low-Resource NLP Google, Facebook, Amazon Hard Applications, Ethics 98 How do you handle Out-of-Vocabulary words in NLP? TDS \u2013 OOV Handling Google, Facebook, Amazon Medium Preprocessing, Embeddings 99 What are Transformer Variants and how do they differ? TDS \u2013 Transformer Variants Google, Facebook, Amazon Hard Transformers, Models 100 What are the Future Trends in Natural Language Processing? Analytics Vidhya \u2013 Future of NLP Google, Facebook, Amazon Medium Trends, Research"},{"location":"Interview-Questions/Natural-Language-Processing/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>What is Natural Language Processing?  </li> <li>Explain Tokenization.  </li> <li>What is TF-IDF and its applications.  </li> <li>What are Word Embeddings?  </li> <li>What is BERT and how does it work?  </li> <li>Explain the Attention Mechanism.  </li> <li>What is Machine Translation?  </li> <li>Explain Text Summarization.  </li> <li>What is Sentiment Analysis?  </li> <li>What is Named Entity Recognition (NER)?</li> </ul>"},{"location":"Interview-Questions/Natural-Language-Processing/#questions-asked-in-facebook-interview","title":"Questions asked in Facebook interview","text":"<ul> <li>Explain Tokenization.  </li> <li>What is Stop Word Removal?  </li> <li>Explain Stemming and Lemmatization.  </li> <li>What is the Bag-of-Words Model?  </li> <li>What are Word Embeddings (Word2Vec/GloVe/FastText)?  </li> <li>Explain the Transformer architecture.  </li> <li>What is GPT and its applications in NLP?  </li> <li>Explain the Attention Mechanism.  </li> <li>What is Sequence-to-Sequence Modeling?  </li> <li>What are Dialog Systems in NLP?</li> </ul>"},{"location":"Interview-Questions/Natural-Language-Processing/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>What is Natural Language Processing?  </li> <li>Explain TF-IDF and its applications.  </li> <li>What is Text Classification?  </li> <li>What is Topic Modeling (LDA/LSA)?  </li> <li>Explain Sentiment Analysis.  </li> <li>What is Named Entity Recognition (NER)?  </li> <li>Explain Language Modeling.  </li> <li>What is Transfer Learning in NLP?  </li> <li>What is Fine-tuning Pre-trained Language Models?  </li> <li>What are Pre-trained Language Models?</li> </ul>"},{"location":"Interview-Questions/Natural-Language-Processing/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>What is Natural Language Processing?  </li> <li>Explain Language Modeling and Perplexity.  </li> <li>What is the Transformer architecture?  </li> <li>What is BERT and how does it work?  </li> <li>Explain Dependency Parsing.  </li> <li>What is Text Summarization?  </li> <li>Explain Question Answering systems.  </li> <li>What is Subword Tokenization?  </li> <li>How do you handle Out-of-Vocabulary words?  </li> <li>Discuss challenges in low-resource languages.</li> </ul>"},{"location":"Interview-Questions/Natural-Language-Processing/#questions-asked-in-other-interviews","title":"Questions asked in other interviews","text":"<p>Uber / Flipkart / Ola: - Explain the Encoder-Decoder Architecture. - What is Beam Search in NLP? - How does GPT generate text? - What is Fine-tuning in Language Models?</p> <p>Swiggy / Paytm / OYO: - What is Noise Removal in Text Processing? - Explain Named Entity Recognition (NER). - What are Ethical Considerations in NLP? - How do you handle bias in NLP models?</p> <p>WhatsApp / Slack / Airbnb: - What is Natural Language Inference (NLI)? - Explain the Attention Mechanism. - What are Dialog Systems in NLP? - Discuss the future trends in NLP.</p>"},{"location":"Interview-Questions/NumPy/","title":"NumPy Interview Questions","text":"<p>This document provides a curated list of NumPy interview questions commonly asked in technical interviews for Data Science, Quantitative Analyst, Machine Learning Engineer, and High-Performance Computing roles. It covers everything from array manipulation to advanced linear algebra and memory management.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is NumPy and why is it faster than lists? NumPy Docs Google, Amazon, Meta, Netflix Easy Basics, Performance 2 Difference between list vs NumPy array GeeksforGeeks Google, Amazon, Microsoft Easy Data Structures 3 How to create specific arrays (zeros, ones, eye)? NumPy Docs Most Tech Companies Easy Array Creation 4 What is broadcasting in NumPy? NumPy Docs Google, Amazon, Meta, Apple Medium Broadcasting, Vectorization 5 How to handle shapes and reshaping? NumPy Docs Most Tech Companies Easy Array Manipulation 6 What are ufuncs (universal functions)? NumPy Docs Google, Amazon, OpenAI Medium ufuncs, Vectorization 7 How to check memory usage of an array? Stack Overflow Google, Amazon, Netflix Easy Memory, Performance 8 Difference between flatten() and ravel() Stack Overflow Google, Amazon, Meta Medium Array Manipulation 9 How to perform matrix multiplication? NumPy Docs Most Tech Companies Easy Linear Algebra 10 What is dot product vs cross product? NumPy Docs Google, Amazon, Meta Medium Linear Algebra 11 How to stack arrays (vstack, hstack)? NumPy Docs Google, Amazon, Microsoft Easy Array Manipulation 12 What is broadcasting error? Stack Overflow Google, Amazon, Meta Easy Debugging 13 How to generate random numbers? NumPy Docs Most Tech Companies Easy Random Sampling 14 Difference between rand(), randn(), randint() GeeksforGeeks Google, Amazon, Meta Easy Random Sampling 15 How to set random seed? NumPy Docs Google, Amazon, Netflix Easy Reproducibility 16 How to find unique values and counts? NumPy Docs Google, Amazon, Meta Easy Array Operations 17 How to calculate mean, median, std? NumPy Docs Most Tech Companies Easy Statistics 18 How to perform element-wise comparison? NumPy Docs Google, Amazon, Meta Easy Boolean Operations 19 How to filter array with boolean indexing? NumPy Docs Most Tech Companies Easy Indexing 20 How to use where() for conditional selection? NumPy Docs Google, Amazon, Meta Medium Conditional Logic 21 How to sort arrays? NumPy Docs Most Tech Companies Easy Sorting 22 Difference between sort() methods (quicksort etc)? NumPy Docs Google, Amazon, HFT Firms Medium Algorithms 23 How to get indices of sorted elements (argsort)? NumPy Docs Google, Amazon, Meta Medium Sorting, Indexing 24 How to find min/max values and their indices? NumPy Docs Most Tech Companies Easy Statistics 25 How to calculate percentiles and quantiles? NumPy Docs Google, Amazon, Netflix, Apple Medium Statistics 26 How to save and load arrays (.npy, .npz)? NumPy Docs Google, Amazon, Meta Easy File I/O 27 How to read text/CSV with NumPy? NumPy Docs Google, Amazon, Microsoft Medium File I/O 28 What is the difference between copy and view? NumPy Docs Google, Amazon, Meta Hard Memory Management 29 How to transpose a matrix? NumPy Docs Most Tech Companies Easy Linear Algebra 30 How to compute inverse of a matrix? NumPy Docs Google, Amazon, Meta Medium Linear Algebra 31 How to solve linear equations? NumPy Docs Google, Amazon, Meta Medium Linear Algebra 32 How to calculate eigenvalues and eigenvectors? NumPy Docs Google, Amazon, HFT Firms Hard Linear Algebra 33 How to compute determinant? NumPy Docs Google, Amazon, Meta Easy Linear Algebra 34 How to perform singular value decomposition (SVD)? NumPy Docs Google, Amazon, Netflix Hard Linear Algebra 35 How to calculate inner and outer products? NumPy Docs Google, Amazon, Meta Medium Linear Algebra 36 How to use nan-safe functions (nanmean, etc)? NumPy Docs Google, Amazon, Netflix Medium Missing Data 37 How to replace values meeting a condition? NumPy Docs Google, Amazon, Meta Easy Array Manipulation 38 How to pad an array? NumPy Docs Google, Amazon, CV Companies Medium Image Processing 39 How to repeat elements or arrays? NumPy Docs Google, Amazon, Meta Easy Array Manipulation 40 How to split arrays? NumPy Docs Google, Amazon, Meta Easy Array Manipulation 41 How to use meshgrid? NumPy Docs Google, Amazon, Meta Medium Plotting, Geometry 42 How to perform cumulative sum/product? NumPy Docs Google, Amazon, Meta Easy Statistics 43 How to use diff() for discrete difference? NumPy Docs Google, Amazon, HFT Firms Medium Time Series 44 How to compute histogram? NumPy Docs Google, Amazon, Meta Medium Statistics 45 How to digitize/bin data? NumPy Docs Google, Amazon, Meta Medium Statistics 46 How to set print options? NumPy Docs Google, Amazon Easy Display 47 How to use apply_along_axis? NumPy Docs Google, Amazon, Meta Medium Iteration 48 How to handle complex numbers? NumPy Docs Google, Amazon, HFT Firms Medium Data Types 49 How to change data type (astype)? NumPy Docs Most Tech Companies Easy Data Types 50 What are structured arrays? NumPy Docs Google, Amazon, HFT Firms Hard Advanced Data Types 51 What is None vs np.nan? Stack Overflow Google, Amazon, Microsoft Easy Basics 52 How to check if array is empty? NumPy Docs Google, Amazon Easy Basics 53 How to use expand_dims() and squeeze()? NumPy Docs Google, Amazon, Meta, CV Companies Medium Shape Manipulation 54 How to use vectorization for performance? Real Python Google, Amazon, Meta Medium Performance 55 How to optimize memory with strides? NumPy Docs Google, Amazon, HFT Firms Hard Internals 56 How to use matrix power? NumPy Docs Google, Amazon Easy Linear Algebra 57 How to compute trace? NumPy Docs Google, Amazon Easy Linear Algebra 58 How to compute norm of vector/matrix? NumPy Docs Google, Amazon, Meta Medium Linear Algebra 59 How to solve least squares problem? NumPy Docs Google, Amazon, Meta Medium Optimization 60 How to use clip() to limit values? NumPy Docs Google, Amazon, Meta Easy Array Manipulation 61 How to use roll() to shift elements? NumPy Docs Google, Amazon, Meta Medium Array Manipulation 62 How to use tile() to construct array? NumPy Docs Google, Amazon Medium Array Manipulation 63 How to use logical operations (and, or, xor)? NumPy Docs Google, Amazon, Meta Easy Logic 64 How to use isclose() for float comparison? NumPy Docs Google, Amazon, Meta Medium Logic, Precision 65 How to use allclose() for array comparison? NumPy Docs Google, Amazon, Meta Medium Logic, Testin 66 How to perform set operations (union, intersect)? NumPy Docs Google, Amazon, Meta Medium Set Operations 67 How to use indices() to return grid indices? NumPy Docs Google, Amazon Hard Advanced Indexing 68 How to use unravel_index()? NumPy Docs Google, Amazon Medium Shape Manipulation 69 How to use ravel_multi_index()? NumPy Docs Google, Amazon Hard Shape Manipulation 70 How to use diagonal() to extract diagonals? NumPy Docs Google, Amazon Easy Linear Algebra 71 How to create mask arrays? NumPy Docs Google, Amazon, Meta Medium Masked Arrays 72 How to use polyfit() and polyval()? NumPy Docs Google, Amazon, Meta Medium Curve Fitting 73 How to perform convolution? NumPy Docs Google, Amazon, CV Companies Hard Signal Processing 74 How to use correlate()? NumPy Docs Google, Amazon Hard Signal Processing 75 How to use fft() for Fourier Transform? NumPy Docs Google, Amazon, HFT Firms Hard Signal Processing 76 How to use piecewise() functions? NumPy Docs Google, Amazon Medium Advanced Logic 77 How to use select() for multiple conditions? NumPy Docs Google, Amazon Medium Advanced Logic 78 How to use einsum() for Einstein summation? NumPy Docs Google, Amazon, Meta, Research Hard Advanced Linear Algebra 79 How to use tensordot()? NumPy Docs Google, Amazon, Research Hard Deep Learning 80 How to use kronecker product (kron)? NumPy Docs Google, Amazon Medium Linear Algebra 81 How to use gradient() to compute gradient? NumPy Docs Google, Amazon, Meta Medium Calculus 82 How to use trapz() for integration? NumPy Docs Google, Amazon Medium Calculus 83 How to use interp() for linear interpolation? NumPy Docs Google, Amazon Medium Math 84 How to Use broadcasting with newaxis? NumPy Docs Google, Amazon, Meta Medium Broadcasting 85 How to use array_split()? NumPy Docs Google, Amazon Easy Array Manipulation 86 How to use column_stack() and row_stack()? NumPy Docs Google, Amazon Easy Array Manipulation 87 How to use dstack() (depth stacking)? NumPy Docs Google, Amazon, CV Companies Medium Array Manipulation 88 How to use vsplit() and hsplit()? NumPy Docs Google, Amazon Easy Array Manipulation 89 How to use rollaxis() vs moveaxis()? NumPy Docs Google, Amazon, Research Medium Shape Manipulation 90 How to use swapaxes()? NumPy Docs Google, Amazon Easy Shape Manipulation 91 How to use fromiter() to create array? NumPy Docs Google, Amazon Medium Array Creation 92 How to use frombuffer()? NumPy Docs Google, Amazon, HFT Firms Hard Internals, I/O 93 How to use partition() and argpartition()? NumPy Docs Google, Amazon Medium Sorting 94 How to use searchsorted() for binary search? NumPy Docs Google, Amazon, HFT Firms Medium Algorithms 95 How to use extract() based on condition? NumPy Docs Google, Amazon Medium Filtering 96 How to use count_nonzero()? NumPy Docs Google, Amazon Easy Basics 97 How to use copysign()? NumPy Docs Google, Amazon Medium Math 98 How to use fmax() and fmin()? NumPy Docs Google, Amazon Medium Math 99 How to use nan_to_num()? NumPy Docs Google, Amazon, Netflix Medium Data Cleaning 100 How to use correlate() vs convolve()? NumPy Docs Google, Amazon Hard Signal Processing 101 [HARD] How to implement custom ufuncs? NumPy Docs Google, Amazon, Research Hard Extending NumPy 102 [HARD] Explain C vs Fortran memory layout (contiguous)? NumPy Docs HFT Firms, Google, Amazon Hard Internals, Performance 103 [HARD] How to use <code>as_strided</code> for sliding windows? NumPy Docs HFT Firms, Google, Amazon Hard Internals 104 [HARD] How to map large files with <code>memmap</code>? NumPy Docs Google, Amazon, Netflix Hard Big Data, I/O 105 [HARD] Explain <code>einsum</code> index notation differences? NumPy Docs Google, DeepMind, OpenAI Hard Advanced Math 106 [HARD] How to efficiently broadcast without allocation? NumPy Docs Google, Amazon Hard Performance 107 [HARD] How to link with optimized BLAS/LAPACK? NumPy Docs Google, Amazon, Research Hard Performance, Build 108 [HARD] How to use Structured Arrays for mixed data? NumPy Docs Google, Amazon, HFT Firms Hard Advanced Data Types 109 [HARD] How to vectorizing non-trivial objects properly? NumPy Docs Google, Amazon Hard Performance 110 [HARD] How to manage floating point precision issues? NumPy Docs HFT Firms, Research Hard Numerics 111 [HARD] How to implement cache blocking for operations? Intel Guides HFT Firms, HPC Hard CPU Arch, Performance 112 [HARD] How to use Numba <code>@jit</code> with structured arrays? Numba Docs Google, HFT Firms Hard Optimization 113 [HARD] Explain the difference between <code>Generator</code> vs <code>RandomState</code>? NumPy Docs Google, Amazon, Research Hard Randomness 114 [HARD] How to implement thread-safe random number generation? NumPy Docs Google, Amazon, HFT Firms Hard Parallelism 115 [HARD] How to use <code>np.frompyfunc</code> vs <code>np.vectorize</code>? Stack Overflow Google, Amazon Hard Performance 116 [HARD] How to debug stride-related issues? NumPy Docs HFT Firms, Google Hard Debugging 117 [HARD] How to optimize reduction operations (<code>keepdims</code>)? NumPy Docs Google, Amazon Hard Optimization 118 [HARD] How to interface NumPy with C/C++ pointers? NumPy Docs HFT Firms, Google, Amazon Hard Interop 119 [HARD] How to use bitwise operations on packed arrays? NumPy Docs Google, Amazon Hard Optimization 120 [HARD] How to implement boolean masking without copies? NumPy Docs Google, Amazon Hard Memory"},{"location":"Interview-Questions/NumPy/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/NumPy/#1-advanced-broadcasting","title":"1. Advanced Broadcasting","text":"<pre><code>import numpy as np\n\n# Calculating distance matrix between two sets of points\n# A: (3, 2), B: (4, 2)\nA = np.array([[1,1], [2,2], [3,3]])\nB = np.array([[4,4], [5,5], [6,6], [7,7]])\n\n# Shape manipulation for broadcasting\n# shape (3,1,2) - shape (1,4,2) -&gt; shape (3,4,2)\ndiff = A[:, np.newaxis, :] - B[np.newaxis, :, :]\n\n# Summing squares along last axis: shape (3,4)\ndists = np.sum(diff**2, axis=-1)\nprint(dists)\n</code></pre>"},{"location":"Interview-Questions/NumPy/#2-efficient-sliding-window-stride-tricks","title":"2. Efficient Sliding Window (Stride Tricks)","text":"<pre><code>import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef sliding_window(arr, window_size):\n    \"\"\"\n    Efficiently create sliding windows without copying data.\n    \"\"\"\n    stride = arr.strides[0]\n    shape = (len(arr) - window_size + 1, window_size)\n    strides = (stride, stride)\n    return as_strided(arr, shape=shape, strides=strides)\n\narr = np.arange(10)\nprint(sliding_window(arr, 3))\n</code></pre>"},{"location":"Interview-Questions/NumPy/#3-einstein-summation","title":"3. Einstein Summation","text":"<pre><code>import numpy as np\n\nA = np.random.rand(2, 3)\nB = np.random.rand(3, 4)\nC = np.random.rand(2, 4)\n\n# Matrix multiplication: A @ B\nres_matmul = np.einsum('ik,kj-&gt;ij', A, B)\n\n# Dot product of rows in A and C\nres_dot = np.einsum('ij,ij-&gt;i', A, C)\n\nprint(\"Matmul shape:\", res_matmul.shape)\nprint(\"Row dot shape:\", res_dot.shape)\n</code></pre>"},{"location":"Interview-Questions/NumPy/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>How would you implement convolution from scratch using stride tricks?</li> <li>Explain the memory layout of C vs Fortran arrays in NumPy</li> <li>Write code to efficiently calculate pairwise distances</li> <li>How to handle numerical stability in large matrix operations?</li> <li>Explain broadcasting rules with examples</li> <li>How would you optimize a slow loop over arrays?</li> <li>Write code to perform image padding manually</li> <li>How to implement moving average without loops?</li> <li>Explain the usage of einsum vs dot product</li> <li>How to handle large datasets that don't fit in RAM?</li> </ul>"},{"location":"Interview-Questions/NumPy/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Write code to implement sparse matrix multiplication</li> <li>How would you generate non-uniform random numbers?</li> <li>Explain vectorized boolean operations</li> <li>Write code to filter values without creating a copy</li> <li>How to optimize array concatenation in a loop?</li> <li>Explain eigen decomposition implementation details</li> <li>Write code to solve system of linear equations</li> <li>How to handle missing values in numeric arrays?</li> <li>Explain performance difference between float32 vs float64</li> <li>Write code to normalize a matrix row-wise</li> </ul>"},{"location":"Interview-Questions/NumPy/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>How would you implement efficient array sorting?</li> <li>Explain structured arrays and their use cases</li> <li>Write code to compute histograms on multidimensional data</li> <li>How to implement custom reduction functions?</li> <li>Explain caching effects on array operations</li> <li>Write code to rotate an image represented as an array</li> <li>How to handle overflow in integer arrays?</li> <li>Explain how ufuncs work internally</li> <li>Write code to efficiently slicing multi-dimensional arrays</li> <li>How to implement vectorized string operations?</li> </ul>"},{"location":"Interview-Questions/NumPy/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Explain the role of BLAS/LAPACK in NumPy</li> <li>Write code to compute the inverse of a matrix</li> <li>How to create a view of an array with different data type?</li> <li>Explain memory mapping for large files</li> <li>Write code to perform fast fourier transform</li> <li>How to implement a custom random number generator?</li> <li>Explain broadcasting errors and how to fix them</li> <li>Write code to compute cross-correlation</li> <li>How to optimize dot product for sparse vectors?</li> <li>Explain how to use <code>np.where</code> for complex conditions</li> </ul>"},{"location":"Interview-Questions/NumPy/#questions-asked-in-hft-firms-eg-jane-street-citadel","title":"Questions asked in HFT Firms (e.g., Jane Street, Citadel)","text":"<ul> <li>How to optimize stride usage for cache locality?</li> <li>Write code to implement order management system logic with arrays</li> <li>Explain floating point precision pitfalls in financial calc</li> <li>How to minimize memory allocations in critical paths?</li> <li>Write code to implement rolling window statistics efficiently</li> <li>how to use Numba to accelerate NumPy logic?</li> <li>Explain SIMD instructions usage in NumPy</li> <li>Write code to process tick data efficiently</li> <li>How to handle NaN propagation in accumulation?</li> <li>Explain the difference between <code>np.random.rand</code> and <code>np.random.Generator</code></li> </ul>"},{"location":"Interview-Questions/NumPy/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official NumPy Documentation</li> <li>From Python to NumPy</li> <li>100 NumPy Exercises</li> <li>Scipy Lecture Notes</li> <li>NumPy Visualization</li> </ul>"},{"location":"Interview-Questions/Pandas/","title":"Pandas Interview Questions","text":"<p>This document provides a curated list of Pandas interview questions commonly asked in technical interviews for Data Science, Data Analysis, Machine Learning, and Python Developer roles. It covers fundamental concepts to advanced data manipulation techniques, including rigorous \"brutally difficult\" questions for senior roles.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is Pandas and why is it used? Pandas Docs Google, Amazon, Meta, Netflix Easy Basics, Introduction 2 Difference between Series and DataFrame GeeksforGeeks Google, Amazon, Meta, Microsoft Easy Data Structures 3 How to create a DataFrame from dictionary? Pandas Docs Amazon, Google, Flipkart Easy DataFrame Creation 4 Difference between loc and iloc Stack Overflow Google, Amazon, Meta, Apple, Netflix Easy Indexing, Selection 5 How to read CSV, Excel, JSON files? Pandas Docs Most Tech Companies Easy Data I/O 6 How to handle missing values (NaN)? Real Python Google, Amazon, Meta, Netflix, Apple Medium Missing Data, fillna, dropna 7 Difference between dropna() and fillna() Pandas Docs Amazon, Google, Microsoft Easy Missing Data 8 Explain GroupBy in Pandas Real Python Google, Amazon, Meta, Netflix, Apple Medium GroupBy, Aggregation 9 How to merge two DataFrames? Pandas Docs Google, Amazon, Meta, Microsoft Medium Merging, Joining 10 Difference between merge(), join(), concat() Stack Overflow Google, Amazon, Meta Medium Merging, Joining, Concatenation 11 How to apply a function to DataFrame? Pandas Docs Google, Amazon, Meta, Netflix Medium apply, applymap, map 12 Difference between apply(), map(), applymap() GeeksforGeeks Google, Amazon, Microsoft Medium Data Transformation 13 How to rename columns in DataFrame? Pandas Docs Most Tech Companies Easy Column Operations 14 How to sort DataFrame by column values? Pandas Docs Most Tech Companies Easy Sorting 15 How to filter rows based on conditions? Pandas Docs Google, Amazon, Meta, Netflix Easy Filtering, Boolean Indexing 16 How to remove duplicate rows? Pandas Docs Amazon, Google, Microsoft Easy Data Cleaning, Deduplication 17 How to change data types of columns? Pandas Docs Most Tech Companies Easy Data Types 18 What is the difference between copy() and view? Stack Overflow Google, Amazon, Meta Medium Memory Management 19 Explain pivot tables in Pandas Pandas Docs Amazon, Google, Microsoft, Netflix Medium Pivot Tables, Reshaping 20 Difference between pivot() and pivot_table() Stack Overflow Google, Amazon, Meta Medium Reshaping 21 How to handle datetime data in Pandas? Pandas Docs Google, Amazon, Netflix, Meta Medium DateTime, Time Series 22 How to create a date range? Pandas Docs Amazon, Netflix, Google Easy DateTime 23 What is MultiIndex (Hierarchical Indexing)? Pandas Docs Google, Amazon, Meta Hard MultiIndex, Hierarchical Data 24 How to reset and set index? Pandas Docs Most Tech Companies Easy Indexing 25 How to perform rolling window calculations? Pandas Docs Google, Amazon, Netflix, Apple Medium Rolling Windows, Time Series 26 How to calculate moving averages? GeeksforGeeks Google, Amazon, Netflix, Apple Medium Rolling Windows, Finance 27 How to perform resampling on time series? Pandas Docs Google, Amazon, Netflix Medium Resampling, Time Series 28 Difference between transform() and apply() Stack Overflow Google, Amazon, Meta Hard GroupBy, Data Transformation 29 How to create bins with cut() and qcut()? Pandas Docs Google, Amazon, Meta Medium Discretization, Binning 30 How to handle categorical data? Pandas Docs Google, Amazon, Meta, Netflix Medium Categorical Data, Memory 31 How to one-hot encode categorical data? Pandas Docs Google, Amazon, Meta, Microsoft Easy Feature Engineering, ML 32 How to read data from SQL database? Pandas Docs Amazon, Google, Microsoft Medium Database I/O 33 How to export DataFrame to various formats? Pandas Docs Most Tech Companies Easy Data Export 34 How to handle large datasets efficiently? Towards Data Science Google, Amazon, Netflix, Meta Hard Performance, Memory Optimization 35 What is Categorical dtype and when to use it? Pandas Docs Google, Amazon, Meta Medium Data Types, Memory Optimization 36 How to optimize memory usage in Pandas? Medium Google, Amazon, Netflix Hard Memory Optimization 37 Difference between inplace=True and returning copy Stack Overflow Most Tech Companies Easy DataFrame Modification 38 How to use query() method for filtering? Pandas Docs Google, Amazon, Meta Easy Filtering, Query 39 How to work with string data (str accessor)? Pandas Docs Google, Amazon, Meta, Netflix Medium String Operations 40 How to use str accessor methods? Pandas Docs Amazon, Google, Microsoft Medium String Operations 41 How to split and expand string columns? GeeksforGeeks Amazon, Google, Meta Medium String Operations, Data Cleaning 42 How to use melt() for unpivoting data? Pandas Docs Google, Amazon, Meta Medium Reshaping, Unpivoting 43 How to use stack() and unstack()? Pandas Docs Google, Amazon, Meta Medium Reshaping, MultiIndex 44 How to cross-tabulate with crosstab()? Pandas Docs Google, Amazon, Meta Medium Cross Tabulation, Analysis 45 How to calculate correlations? Pandas Docs Google, Amazon, Meta, Netflix Easy Statistical Analysis 46 How to calculate descriptive statistics? Pandas Docs Most Tech Companies Easy Statistical Analysis 47 How to use agg() for multiple aggregations? Pandas Docs Google, Amazon, Meta, Netflix Medium Aggregation 48 How to use named aggregations? Pandas Docs Google, Amazon, Meta Medium GroupBy, Named Aggregation 49 How to handle timezone-aware datetime? Pandas Docs Google, Amazon, Netflix Medium DateTime, Timezones 50 How to interpolate missing values? Pandas Docs Google, Amazon, Netflix Medium Missing Data, Interpolation 51 How to forward fill and backward fill? Pandas Docs Amazon, Netflix, Google Easy Missing Data, Time Series 52 How to use where() and mask() methods? Pandas Docs Google, Amazon, Meta Medium Conditional Operations 53 How to clip values in DataFrame? Pandas Docs Amazon, Google, Meta Easy Data Transformation 54 How to rank values in Pandas? Pandas Docs Google, Amazon, Meta, Netflix Easy Ranking 55 How to calculate percentage change? Pandas Docs Google, Amazon, Netflix, Apple Easy Time Series, Finance 56 How to shift and lag data? Pandas Docs Google, Amazon, Netflix Easy Time Series, Lag Features 57 How to calculate cumulative statistics? Pandas Docs Google, Amazon, Meta, Netflix Easy Cumulative Operations 58 How to use explode() for list columns? Pandas Docs Google, Amazon, Meta Medium List Operations, Data Preprocessing 59 How to sample data from DataFrame? Pandas Docs Google, Amazon, Meta, Netflix Easy Sampling 60 How to detect and handle outliers? Towards Data Science Google, Amazon, Meta, Netflix Medium Outlier Detection, Data Cleaning 61 How to normalize/standardize data? GeeksforGeeks Google, Amazon, Meta, Microsoft Medium Feature Engineering, ML 62 How to use eval() for efficient operations? Pandas Docs Google, Amazon, Meta Hard Performance Optimization 63 How to perform element-wise operations? Pandas Docs Most Tech Companies Easy Arithmetic Operations 64 Why vectorized operations are faster than loops? Real Python Google, Amazon, Meta Medium Performance, Vectorization 65 How to profile Pandas code performance? Pandas Docs Google, Amazon, Netflix Hard Performance Profiling 66 How to use pipe() for method chaining? Pandas Docs Google, Amazon, Meta Medium Method Chaining 67 How to handle SettingWithCopyWarning? Pandas Docs Google, Amazon, Meta, Microsoft Medium Common Errors, Debugging 68 How to compare two DataFrames? Pandas Docs Amazon, Google, Microsoft Medium Data Comparison, Validation 69 How to combine DataFrames with different schemas? Stack Overflow Google, Amazon, Meta Medium Merging, Schema Alignment 70 How to create conditional columns? GeeksforGeeks Most Tech Companies Easy Data Transformation 71 How to use np.where() with Pandas? Real Python Google, Amazon, Meta, Netflix Easy Conditional Operations 72 How to use np.select() for multiple conditions? Stack Overflow Google, Amazon, Meta Medium Conditional Operations 73 How to count value frequencies? Pandas Docs Most Tech Companies Easy Data Exploration 74 How to find unique values and nunique()? Pandas Docs Most Tech Companies Easy Data Exploration 75 How to check for null values? Pandas Docs Most Tech Companies Easy Missing Data 76 How to use any() and all() methods? Pandas Docs Google, Amazon, Meta Easy Boolean Operations 77 How to select specific columns? Pandas Docs Most Tech Companies Easy Column Selection 78 How to drop columns or rows? Pandas Docs Most Tech Companies Easy Data Cleaning 79 How to use assign() for creating new columns? Pandas Docs Google, Amazon, Meta Easy Column Creation 80 How to use idxmax() and idxmin()? Pandas Docs Google, Amazon, Meta, Netflix Easy Indexing 81 Why is iterating over rows slow? Stack Overflow Google, Amazon, Meta Medium Performance 82 How to use iterrows() and itertuples()? Pandas Docs Amazon, Google, Microsoft Easy Iteration 83 How to vectorize custom functions? Real Python Google, Amazon, Meta Hard Performance Optimization 84 How to use Pandas with NumPy? Pandas Docs Google, Amazon, Meta, Netflix Easy NumPy Integration 85 How to flatten hierarchical index? Stack Overflow Google, Amazon, Meta Medium MultiIndex 86 How to group by multiple columns? Pandas Docs Most Tech Companies Easy GroupBy 87 How to filter groups after GroupBy? Pandas Docs Google, Amazon, Meta Medium GroupBy, Filtering 88 How to get first/last n rows per group? Stack Overflow Google, Amazon, Meta, Netflix Medium GroupBy 89 How to handle JSON with nested structures? Pandas Docs Amazon, Google, Meta Medium JSON Processing 90 How to read/write Parquet files? Pandas Docs Google, Amazon, Netflix, Meta Easy File I/O, Big Data 91 Difference between Parquet, CSV, and Feather Towards Data Science Google, Amazon, Netflix Medium File Formats, Performance 92 How to use chunksize for large files? Pandas Docs Google, Amazon, Netflix, Meta Medium Large Data Processing 93 How to use nsmallest() and nlargest()? Pandas Docs Google, Amazon, Meta Easy Selection 94 How to calculate weighted average? Stack Overflow Google, Amazon, Netflix, Apple Medium Aggregation, Finance 95 How to perform window functions like SQL? Pandas Docs Google, Amazon, Meta, Netflix Medium Window Functions 96 How to join on nearest key (asof join)? Pandas Docs Google, Amazon, Netflix, Apple Hard Joining, Time Series 97 How to use combine_first() for data merging? Pandas Docs Amazon, Google, Microsoft Medium Merging 98 How to create period indices? Pandas Docs Google, Amazon, Netflix Medium Time Series 99 How to use Timedelta for time differences? Pandas Docs Google, Amazon, Netflix Easy DateTime 100 How to set display options globally? Pandas Docs Most Tech Companies Easy Display Options 101 What is method chaining and when to use it? Tom Augspurger Blog Google, Amazon, Meta Medium Method Chaining, Clean Code 102 How to calculate month-over-month change? StrataScratch Google, Amazon, Meta, Netflix Medium Time Series, Analytics 103 How to find customers with highest orders? DataLemur Amazon, Google, Meta, Netflix Medium GroupBy, Aggregation 104 [HARD] How to calculate retention metrics efficiently? StrataScratch Meta, Netflix, Amazon, Google Hard Cohort Analysis, Time Series 105 [HARD] How to implement A/B test analysis? Towards Data Science Meta, Google, Netflix, Amazon Hard Statistical Analysis, Testing 106 [HARD] How to optimize memory with <code>category</code> types? Pandas Docs Google, Amazon, Netflix Hard Memory Optimization 107 [HARD] How to implement cohort analysis? Towards Data Science Meta, Netflix, Amazon, Google Hard Cohort Analysis 108 [HARD] How to calculate funnel drop-off rates? StrataScratch Meta, Google, Amazon, Netflix Hard Funnel Analysis, Analytics 109 [HARD] How to implement custom testing using <code>assert_frame_equal</code>? Pandas Docs Google, Amazon, Microsoft Hard Testing, Quality 110 [HARD] How to handle sparse data structures? Pandas Docs Google, Amazon, Netflix Hard Sparse Data, Memory 111 [HARD] How to use Numba/JIT with Pandas? Pandas Docs Google, Amazon, Hedge Funds Hard Performance 112 [HARD] How to implement custom accessors? Pandas Docs Google, Amazon, Meta Hard Extending Pandas 113 [HARD] How to use Swifter for parallel processing? Swifter Docs Google, Amazon, Uber Hard Parallelism 114 [HARD] Explain Pandas Block Manager structure Pandas Wiki Google, Amazon, Meta Hard Internals 115 [HARD] How Copy-on-Write (CoW) works in Pandas 2.0+? Pandas Docs Google, Meta, Microsoft Hard Internals, Performance 116 [HARD] How to use PyArrow backend for performance? Pandas Docs Google, Amazon, Databricks Hard Performance, Arrow 117 [HARD] How to implement custom index types? Pandas Docs Google, Amazon Hard Extending Pandas 118 [HARD] How to optimize MultiIndex slicing performance? Pandas Docs Google, Amazon, Hedge Funds Hard Optimization 119 [HARD] <code>groupby().transform()</code> internal mechanics vs <code>apply()</code> Pandas Docs Google, Amazon, Meta Hard Deep Dive 120 [HARD] How to implement rolling window with <code>raw=True</code>? Pandas Docs Google, Amazon, Hedge Funds Hard Optimization 121 [HARD] How to extend Pandas with custom plotting backends? Pandas Docs Google, Amazon Hard Extending Pandas 122 [HARD] How to handle time series offset aliases? Pandas Docs Google, Amazon, Hedge Funds Hard Time Series 123 [HARD] How to use Dask DataFrames for out-of-core computing? Dask Docs Google, Amazon, Netflix Hard Big Data 124 [HARD] How to optimize chained assignment performance? Pandas Docs Google, Amazon, Meta Hard Optimization 125 [HARD] Nullable integers/floats implementation? Pandas Docs Google, Amazon, Microsoft Hard Internals 126 [HARD] How to use Cython with Pandas? Pandas Docs Google, Amazon, HFT Firms Hard Performance 127 [HARD] Comparison of Parquet vs Feather vs ORC? Apache Arrow Google, Amazon, Netflix Hard Systems"},{"location":"Interview-Questions/Pandas/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/Pandas/#1-memory-optimization","title":"1. Memory Optimization","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Typical large dataframe creation\ndf = pd.DataFrame({\n    'category': np.random.choice(['A', 'B', 'C'], size=1000000),\n    'value': np.random.randn(1000000)\n})\n\n# Memory usage before optimization\nprint(df.memory_usage(deep=True).sum() / 1024**2, \"MB\")\n\n# Optimize by converting object to category\ndf['category'] = df['category'].astype('category')\n\n# Memory usage after optimization\nprint(df.memory_usage(deep=True).sum() / 1024**2, \"MB\")\n</code></pre>"},{"location":"Interview-Questions/Pandas/#2-method-chaining-for-clean-code","title":"2. Method Chaining for Clean Code","text":"<pre><code># Instead of multiple intermediate variables\ndf = (\n    pd.read_csv('data.csv')\n    .query('status == \"active\"')\n    .assign(\n        year=lambda x: pd.to_datetime(x['date']).dt.year,\n        total_cost=lambda x: x['price'] * x['quantity']\n    )\n    .groupby(['year', 'region'])\n    .agg(total_revenue=('total_cost', 'sum'))\n    .reset_index()\n    .sort_values('total_revenue', ascending=False)\n)\n</code></pre>"},{"location":"Interview-Questions/Pandas/#3-parallel-processing-with-swifter","title":"3. Parallel Processing with Swifter","text":"<pre><code>import pandas as pd\nimport swifter\n\ndf = pd.DataFrame({'text': ['some text'] * 100000})\n\ndef heavy_processing(text):\n    # Simulate heavy work\n    return text.upper()[::-1]\n\n# Automatic parallelization\ndf['processed'] = df['text'].swifter.apply(heavy_processing)\n</code></pre>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>How would you optimize a Pandas operation running slowly on large dataset?</li> <li>Explain the difference between merge() and join()</li> <li>Write code to calculate rolling averages with different window sizes</li> <li>How would you handle a DataFrame with 100 million rows?</li> <li>Explain memory optimization techniques</li> <li>Write code to perform complex GroupBy with multiple aggregations</li> <li>Explain the internal data structure of DataFrame</li> <li>How would you implement feature engineering pipelines?</li> <li>Write code to calculate year-over-year growth</li> <li>Explain vectorized operations and their importance</li> <li>How to handle SettingWithCopyWarning?</li> <li>Write code to perform window functions similar to SQL</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Write code to merge multiple DataFrames with different schemas</li> <li>How would you calculate year-over-year growth?</li> <li>Explain how to handle time series data with irregular intervals</li> <li>Write code to identify and remove duplicate records</li> <li>How would you implement a moving average crossover strategy?</li> <li>Explain the difference between transform() and apply()</li> <li>Write code to pivot data for sales analysis</li> <li>How would you handle categorical variables with high cardinality?</li> <li>Explain how to optimize for memory efficiency</li> <li>Write code to perform cohort analysis</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>Write code to analyze user engagement data</li> <li>How would you calculate conversion funnels?</li> <li>Explain how to handle large-scale data processing</li> <li>Write code to resample time series data</li> <li>How would you implement A/B testing analysis?</li> <li>Explain method chaining and its benefits</li> <li>Write code to calculate retention metrics</li> <li>How would you handle hierarchical data structures?</li> <li>Explain vectorization benefits over loops</li> <li>Write code to analyze network data</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Explain the SettingWithCopyWarning and how to avoid it</li> <li>Write code to perform window functions similar to SQL</li> <li>How would you handle timezone conversions?</li> <li>Explain the difference between views and copies</li> <li>Write code to implement custom aggregation functions</li> <li>How would you optimize Pandas for production?</li> <li>Explain multi-level indexing use cases</li> <li>Write code to compare two DataFrames</li> <li>How would you handle missing data in time series?</li> <li>Explain eval() and query() methods</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-netflix-interview","title":"Questions asked in Netflix interview","text":"<ul> <li>Write code to analyze viewing patterns and user behavior</li> <li>How would you calculate streaming quality metrics?</li> <li>Explain how to handle messy data from multiple sources</li> <li>Write code to implement collaborative filtering preprocessing</li> <li>How would you analyze content performance across regions?</li> <li>Explain time series decomposition</li> <li>Write code to calculate customer lifetime value</li> <li>How would you handle data for recommendation systems?</li> <li>Explain rolling window calculations for real-time analytics</li> <li>Write code to analyze A/B test results</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-apple-interview","title":"Questions asked in Apple interview","text":"<ul> <li>Write code to perform data validation on imported data</li> <li>How would you implement data quality checks?</li> <li>Explain how to handle multi-format data imports</li> <li>Write code to analyze product performance metrics</li> <li>How would you implement data anonymization?</li> <li>Explain best practices for production Pandas code</li> <li>Write code to create automated data reports</li> <li>How would you handle data versioning?</li> <li>Explain memory management for large DataFrames</li> <li>Write code to implement time-based partitioning</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-flipkart-interview","title":"Questions asked in Flipkart interview","text":"<ul> <li>Write code to analyze e-commerce transaction data</li> <li>How would you calculate GMV metrics?</li> <li>Explain handling high-cardinality categorical data</li> <li>Write code to analyze customer purchase patterns</li> <li>How would you implement product recommendation preprocessing?</li> <li>Explain data aggregation for dashboard analytics</li> </ul>"},{"location":"Interview-Questions/Pandas/#questions-asked-in-linkedin-interview","title":"Questions asked in LinkedIn interview","text":"<ul> <li>Write code to analyze professional network connections</li> <li>How would you calculate engagement metrics for posts?</li> <li>Explain how to handle user activity data</li> <li>Write code to implement skill-based matching</li> <li>How would you analyze job posting performance?</li> <li>Explain data preprocessing for NLP tasks</li> </ul>"},{"location":"Interview-Questions/Pandas/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Pandas Documentation</li> <li>Minimally Sufficient Pandas</li> <li>Modern Pandas</li> <li>Python Data Science Handbook</li> <li>Effective Pandas (Book)</li> </ul>"},{"location":"Interview-Questions/Probability/","title":"Probability Interview Questions","text":"<p>This document provides a curated list of common probability interview questions frequently asked in technical interviews. It covers basic probability concepts, probability distributions, key theorems, and real-world applications. Use the practice links to explore detailed explanations and examples.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 Basic Probability Concepts: Definitions of Sample Space, Event, Outcome Wikipedia: Probability Google, Amazon, Microsoft Easy Fundamental Concepts 2 Conditional Probability and Independence Khan Academy: Conditional Probability Google, Facebook, Amazon Medium Conditional Probability, Independence 3 Bayes\u2019 Theorem: Statement and Application Wikipedia: Bayes' Theorem Google, Amazon, Microsoft Medium Bayesian Inference 4 Law of Total Probability Wikipedia: Law of Total Probability Google, Facebook Medium Theoretical Probability 5 Expected Value and Variance Khan Academy: Expected Value Google, Amazon, Facebook Medium Random Variables, Moments 6 Probability Distributions: Discrete vs. Continuous Wikipedia: Probability Distribution Google, Amazon, Microsoft Easy Distributions 7 Binomial Distribution: Definition and Applications Khan Academy: Binomial Distribution Amazon, Facebook Medium Discrete Distributions 8 Poisson Distribution: Characteristics and Uses Wikipedia: Poisson Distribution Google, Amazon Medium Discrete Distributions 9 Exponential Distribution: Properties and Applications Wikipedia: Exponential Distribution Google, Amazon Medium Continuous Distributions 10 Normal Distribution and the Central Limit Theorem Khan Academy: Normal Distribution Google, Microsoft, Facebook Medium Continuous Distributions, CLT 11 Law of Large Numbers Wikipedia: Law of Large Numbers Google, Amazon Medium Statistical Convergence 12 Covariance and Correlation: Definitions and Differences Khan Academy: Covariance and Correlation Google, Facebook Medium Statistics, Dependency 13 Moment Generating Functions (MGFs) Wikipedia: Moment-generating function Amazon, Microsoft Hard Random Variables, Advanced Concepts 14 Markov Chains: Basics and Applications Wikipedia: Markov chain Google, Amazon, Facebook Hard Stochastic Processes 15 Introduction to Stochastic Processes Wikipedia: Stochastic process Google, Microsoft Hard Advanced Probability 16 Difference Between Independent and Mutually Exclusive Events Wikipedia: Independent events Google, Facebook Easy Fundamental Concepts 17 Geometric Distribution: Concept and Use Cases Wikipedia: Geometric distribution Amazon, Microsoft Medium Discrete Distributions 18 Hypergeometric Distribution: When to Use It Wikipedia: Hypergeometric distribution Google, Amazon Medium Discrete Distributions 19 Confidence Intervals: Definition and Calculation Khan Academy: Confidence intervals Microsoft, Facebook Medium Inferential Statistics 20 Hypothesis Testing: p-values, Type I and Type II Errors Khan Academy: Hypothesis testing Google, Amazon, Facebook Medium Inferential Statistics 21 Chi-Squared Test: Basics and Applications Wikipedia: Chi-squared test Amazon, Microsoft Medium Inferential Statistics 22 Permutations and Combinations Khan Academy: Permutations and Combinations Google, Facebook Easy Combinatorics 23 The Birthday Problem and Its Implications Wikipedia: Birthday problem Google, Amazon Medium Probability Puzzles 24 The Monty Hall Problem Wikipedia: Monty Hall problem Google, Facebook Medium Probability Puzzles, Conditional Probability 25 Marginal vs. Conditional Probabilities Khan Academy: Conditional Probability Google, Amazon Medium Theoretical Concepts 26 Real-World Application of Bayes\u2019 Theorem Towards Data Science: Bayes\u2019 Theorem Applications Google, Amazon Medium Bayesian Inference 27 Probability Mass Function (PMF) vs. Probability Density Function (PDF) Wikipedia: Probability density function Amazon, Facebook Medium Distributions 28 Cumulative Distribution Function (CDF): Definition and Uses Wikipedia: Cumulative distribution function Google, Microsoft Medium Distributions 29 Determining Independence of Events Khan Academy: Independent Events Google, Amazon Easy Fundamental Concepts 30 Entropy in Information Theory Wikipedia: Entropy (information theory) Google, Facebook Hard Information Theory, Probability 31 Joint Probability Distributions Khan Academy: Joint Probability Microsoft, Amazon Medium Multivariate Distributions 32 Conditional Expectation Wikipedia: Conditional expectation Google, Facebook Hard Advanced Concepts 33 Sampling Methods: With and Without Replacement Khan Academy: Sampling Amazon, Microsoft Easy Sampling, Combinatorics 34 Risk Modeling Using Probability Investopedia: Risk Analysis Google, Amazon Medium Applications, Finance 35 In-Depth: Central Limit Theorem and Its Importance Khan Academy: Central Limit Theorem Google, Microsoft Medium Theoretical Concepts, Distributions 36 Variance under Linear Transformations Wikipedia: Variance Amazon, Facebook Hard Advanced Statistics 37 Quantiles: Definition and Interpretation Khan Academy: Percentiles Google, Amazon Medium Descriptive Statistics 38 Common Probability Puzzles and Brain Teasers Brilliant.org: Probability Puzzles Google, Facebook Medium Puzzles, Recreational Mathematics 39 Real-World Applications of Probability in Data Science Towards Data Science (Search for probability applications in DS) Google, Amazon, Facebook Medium Applications, Data Science 40 Advanced Topic: Introduction to Stochastic Calculus Wikipedia: Stochastic calculus Microsoft, Amazon Hard Advanced Probability, Finance"},{"location":"Interview-Questions/Probability/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Bayes\u2019 Theorem: Statement and Application  </li> <li>Conditional Probability and Independence  </li> <li>The Birthday Problem  </li> <li>The Monty Hall Problem  </li> <li>Normal Distribution and the Central Limit Theorem  </li> <li>Law of Large Numbers  </li> </ul>"},{"location":"Interview-Questions/Probability/#questions-asked-in-facebook-interview","title":"Questions asked in Facebook interview","text":"<ul> <li>Conditional Probability and Independence  </li> <li>Bayes\u2019 Theorem  </li> <li>Chi-Squared Test  </li> <li>The Monty Hall Problem  </li> <li>Entropy in Information Theory  </li> </ul>"},{"location":"Interview-Questions/Probability/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Basic Probability Concepts  </li> <li>Bayes\u2019 Theorem  </li> <li>Expected Value and Variance  </li> <li>Binomial and Poisson Distributions  </li> <li>Permutations and Combinations  </li> <li>Real-World Applications of Bayes\u2019 Theorem  </li> </ul>"},{"location":"Interview-Questions/Probability/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Bayes\u2019 Theorem  </li> <li>Markov Chains  </li> <li>Stochastic Processes  </li> <li>Central Limit Theorem  </li> <li>Variance under Linear Transformations  </li> </ul>"},{"location":"Interview-Questions/Probability/#custom-questions","title":"Custom Questions","text":""},{"location":"Interview-Questions/Probability/#average-score-on-a-dice-role-of-at-most-3-times","title":"Average score on a dice role of at most 3 times","text":"<p>Question</p> <p>Consider a fair 6-sided dice.  Your aim is to get the highest score you can, in at-most 3 roles.</p> <p>A score is defined as the number that appears on the face of the dice facing up after the role.  You can role at most 3 times but every time you role it is up to you to decide whether you want to role again.</p> <p>The last score will be counted as your final score.</p> <ul> <li>Find the average score if you rolled the dice only once?</li> <li>Find the average score that you can get with at most 3 roles?</li> <li>If the dice is fair, why is the average score for at most 3 roles and 1 role not the same?</li> </ul> Hint 1 <p>Find what is the expected score on single role</p> <p>And for cases when scores of single role &lt; <code>expected score on single role</code>  is when you will go for next role</p> <p>Eg: if expected score of single role comes out to be 4.5,  you will only role next turn for 1,2,3,4 and not for 5,6</p> Answer <p>If you role a fair dice once you can get:</p> Score Probability 1 \u2159 2 \u2159 3 \u2159 4 \u2159 5 \u2159 6 \u2159 <p>So your average score with one role is: </p> <p><code>sum of(score * scores's probability)</code> = (1+2+3+4+5+6)*(\u2159) = (21/6) = 3.5</p> <p>The average score if you rolled the dice only once is 3.5</p> <p>For at most 3 roles, let's try back-tracking. Let's say just did your second role and you have to decide whether to do your 3<sup>rd</sup> role!</p> <p>We just found out if we role dice once on average we can expect score of 3.5. So we will only role the 3<sup>rd</sup> time if score on 2<sup>nd</sup> role is less than 3.5 i.e (1,2 or 3)</p> <p>Possibilities</p> 2<sup>nd</sup> role score Probability 3<sup>rd</sup> role score Probability 1 \u2159 3.5 \u2159 2 \u2159 3.5 \u2159 3 \u2159 3.5 \u2159 4 \u2159 NA We won't role 5 \u2159 NA 3<sup>rd</sup> time if we 6 \u2159 NA get score &gt;3 on 2<sup>nd</sup> <p>So if we had 2 roles, average score would be:</p> <pre><code>[We role again if current score is less than 3.4]\n(3.5)*(1/6) + (3.5)*(1/6) + (3.5)*(1/6) \n+\n(4)*(1/6) + (5)*(1/6) + (6)*(1/6) [Decide not to role again]\n=\n1.75 + 2.5 = 4.25\n</code></pre> <p>The average score if you rolled the dice twice is 4.25</p> <p>So now if we look from the perspective of first role. We will only role again if our score is less than 4.25 i.e 1,2,3 or 4</p> <p>Possibilities</p> 1<sup>st</sup> role score Probability 2<sup>nd</sup> and 3<sup>rd</sup> role score Probability 1 \u2159 4.25 \u2159 2 \u2159 4.25 \u2159 3 \u2159 4.25 \u2159 4 \u2159 4.25 \u2159 5 \u2159 NA We won't role again if we 6 \u2159 NA get score &gt;4.25 on 1<sup>st</sup> <p>So if we had 3 roles, average score would be:</p> <p><pre><code>[We role again if current score is less than 4.25]\n(4.25)*(1/6) + (4.25)*(1/6) + (4.25)*(1/6) + (4.25)*(1/6) \n+\n(5)*(1/6) + (6)*(1/6) [[Decide not to role again]\n=\n17/6 + 11/6 = 4.66\n</code></pre> The average score if you rolled the dice only once is 4.66</p> <p>The average score for at most 3 roles and 1 role is not the same because although the dice is fair the event of rolling the dice is no longer independent. The scores would have been the same if we rolled the dice 2<sup>nd</sup> and 3<sup>rd</sup> time without considering what we got in the last roll i.e. if the event of rolling the dice was independent.</p>"},{"location":"Interview-Questions/Python/","title":"Python Interview Questions","text":"<p>This document provides a curated list of Python interview questions commonly asked in technical interviews for Data Scientists, Backend Engineers, and Python Developers. It covers core concepts, internals, concurrency, and advanced language features.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is Python? Interpreted or Compiled? Python Docs Google, Amazon, Meta Easy Basics 2 What is PEP 8? PEP 8 Most Tech Companies Easy Standards 3 Mutable vs Immutable types in Python Real Python Google, Amazon, Microsoft Easy Data Structures 4 Explain List vs Tuple GeeksforGeeks Most Tech Companies Easy Data Structures 5 What is a Dictionary in Python? Python Docs Most Tech Companies Easy Data Structures 6 How is memory managed in Python? Real Python Google, Amazon, Meta, Netflix Medium Internals, Memory 7 What is the Global Interpreter Lock (GIL)? Real Python Google, Amazon, Meta, Apple Hard Internals, Concurrency 8 Explain Garbage Collection in Python Python Docs Google, Amazon, Spotify Medium Internals, GC 9 What are decorators? Real Python Google, Amazon, Meta, Netflix Medium Functions, Advanced 10 Difference between <code>@staticmethod</code> and <code>@classmethod</code> Stack Overflow Google, Amazon, Meta Easy OOP 11 What are lambda functions? Real Python Most Tech Companies Easy Functions 12 Explain <code>*args</code> and <code>**kwargs</code> Real Python Most Tech Companies Easy Functions 13 What are generators and the <code>yield</code> keyword? Real Python Google, Amazon, Meta, Netflix Medium Iterators, Generators 14 Difference between range() and xrange() GeeksforGeeks Legacy Companies Easy Python 2 vs 3 15 What is a docstring? Python Docs Most Tech Companies Easy Documentation 16 How to copy an object? (Deep vs Shallow copy) Real Python Google, Amazon, Meta Medium Objects, Memory 17 What is <code>__init__</code>? Python Docs Most Tech Companies Easy OOP 18 What is <code>__str__</code> vs <code>__repr__</code>? Stack Overflow Google, Amazon, Meta Medium OOP, Magic Methods 19 How typically does inheritance work in Python? Real Python Most Tech Companies Easy OOP 20 What is Multiple Inheritance and MRO? Python Docs Google, Amazon, Meta Hard OOP, MRO 21 Explain <code>is</code> vs <code>==</code> Real Python Most Tech Companies Easy Operators 22 What are packing and unpacking? Real Python Google, Amazon, Meta Medium Basics 23 How to handle exceptions? (try/except/finally) Python Docs Most Tech Companies Easy Error Handling 24 What are assertions? Real Python Google, Amazon, Meta Medium Debugging 25 What implies <code>pass</code> statement? Python Docs Most Tech Companies Easy Control Flow 26 What are Context Managers (<code>with</code> statement)? Real Python Google, Amazon, Meta, Netflix Medium Context Managers 27 Difference between lists and arrays (array module)? GeeksforGeeks Google, Amazon Easy Data Structures 28 What is a Set? Python Docs Most Tech Companies Easy Data Structures 29 How does Python handle large numbers? Python Docs Google, Amazon, HFT Firms Medium Internals 30 What are namespaces? Real Python Google, Amazon, Meta Medium Scoping 31 Explain Local, Global, and Nonlocal scope Real Python Google, Amazon, Meta Medium Scoping 32 What is a module vs a package? Real Python Most Tech Companies Easy Modules 33 How to use <code>pip</code>? Python Docs Most Tech Companies Easy Packaging 34 What is venv/virtualenv? Real Python Most Tech Companies Easy Environment 35 How to read/write files? Python Docs Most Tech Companies Easy I/O 36 What is pickling and unpickling? Python Docs Google, Amazon, Meta Medium Serialization 37 What are iterators and lazy evaluation? Real Python Google, Amazon, Meta Medium Iterators 38 What is the <code>zip()</code> function? Real Python Most Tech Companies Easy Built-ins 39 What is <code>map()</code> and <code>filter()</code>? Real Python Google, Amazon, Meta Easy Functional Programming 40 What is <code>functools.reduce()</code>? Real Python Google, Amazon, Meta Medium Functional Programming 41 Difference between .py and .pyc files Stack Overflow Google, Amazon, Meta Medium Internals 42 What is <code>__name__ == \"__main__\"</code>? Real Python Most Tech Companies Easy Modules 43 How to create a singleton class? GeeksforGeeks Google, Amazon, Meta Hard Patterns 44 What are Metaclasses? Real Python Google, Amazon, Meta, Netflix Hard Metaclasses 45 What is <code>__slots__</code>? GeeksforGeeks Google, Amazon, HFT Firms Hard Optimization, Memory 46 Difference between <code>func</code> and <code>func()</code> Stack Overflow Google, Amazon Easy Functions 47 What is slicing? Python Docs Most Tech Companies Easy Data Structures 48 Negative indexing in Python GeeksforGeeks Most Tech Companies Easy Indexing 49 What is a hash map in Python? Python Docs Most Tech Companies Easy Data Structures 50 Does Python support pointer arithmetic? Stack Overflow Google, Amazon Medium Internals 51 What are default arguments? Pitfalls? Real Python Google, Amazon, Meta Medium Functions, Pitfalls 52 What is <code>collections</code> module? Python Docs Google, Amazon, Meta Medium Standard Library 53 Explain <code>defaultdict</code> and <code>Counter</code> Real Python Google, Amazon, Meta Medium Standard Library 54 What is <code>NamedTuple</code>? Real Python Google, Amazon, Meta Medium Data Structures 55 What is <code>itertools</code>? Real Python Google, Amazon, Meta, HFT Firms Hard Iterators 56 What are Threading vs Multiprocessing? Real Python Google, Amazon, Meta, Netflix Medium Concurrency 57 What is Asyncio? Real Python Google, Amazon, Meta, Netflix Hard Concurrency 58 Difference between <code>await</code> and <code>yield</code> Stack Overflow Google, Amazon, Meta Hard Concurrency 59 What is a coroutine? Python Docs Google, Amazon, Meta Hard Concurrency 60 How to debug python code? (pdb) Real Python Most Tech Companies Medium Debugging 61 What are type hints (Type Annotation)? Real Python Google, Amazon, Meta, Microsoft Medium Typing 62 What is <code>MyPy</code>? MyPy Docs Google, Amazon, Meta Medium Typing 63 What is a Data Class? Real Python Google, Amazon, Meta Medium Data Structures 64 Difference between <code>copy()</code> and <code>deepcopy()</code> Python Docs Google, Amazon, Meta Medium Memory 65 How to reverse a list? Real Python Most Tech Companies Easy Data Structures 66 String formatting options (f-strings vs format) Real Python Most Tech Companies Easy Strings 67 What is <code>sys.path</code>? Python Docs Google, Amazon, Meta Medium Modules 68 What is <code>__call__</code> method? GeeksforGeeks Google, Amazon, Meta Medium OOP 69 What is <code>__new__</code> vs <code>__init__</code>? Stack Overflow Google, Amazon, Meta Hard OOP 70 What is Monkey Patching? Stack Overflow Google, Amazon, Meta Medium Dynamic Programming 71 What is Duck Typing? Real Python Google, Amazon, Meta Medium OOP 72 What is <code>dir()</code> function? Python Docs Most Tech Companies Easy Introspection 73 What is <code>help()</code> function? Python Docs Most Tech Companies Easy Introspection 74 What is <code>enumerate()</code>? Real Python Most Tech Companies Easy Iteration 75 How to merge two dicts? Real Python Most Tech Companies Easy Data Structures 76 Comprehensions (List, Dict, Set) Real Python Most Tech Companies Medium Syntax 77 What is <code>__future__</code> module? Python Docs Google, Amazon Medium Compatibility 78 What is <code>pd.DataFrame</code> vs Python List? Pandas Docs Most Tech Companies Easy Data Analysis 79 How to handle circular imports? Stack Overflow Google, Amazon, Meta Medium Modules 80 What is <code>getattr</code>, <code>setattr</code>, <code>hasattr</code>? Python Docs Google, Amazon, Meta Medium Introspection 81 What is <code>__dict__</code> attribute? Python Docs Google, Amazon, Meta Medium Internals 82 Explain the <code>with</code> statement protocol (<code>__enter__</code>, <code>__exit__</code>) Real Python Google, Amazon, Meta Hard Context Managers 83 What are property decorators? Real Python Google, Amazon, Meta Medium OOP 84 What is Operator Overloading? GeeksforGeeks Google, Amazon, Meta Medium OOP 85 What is ternary operator in Python? Real Python Most Tech Companies Easy Syntax 86 How to optimize Python code speed? Wiki Google, Amazon, HFT Firms Hard Performance 87 Why is Python slow? Real Python Google, Amazon, Meta Medium Performance 88 What is PyPy? PyPy Google, Amazon Hard Interpreters 89 What is Cython? Cython Google, Amazon, HFT Firms Hard Performance 90 Difference between <code>os</code> and <code>sys</code> modules? Stack Overflow Google, Amazon Medium Standard Library 91 What is <code>re</code> module (Regular Expressions)? Real Python Most Tech Companies Medium Standard Library 92 [HARD] How does Reference Counting vs Garbage Collection work? DevGuide Google, Meta, Netflix Hard Internals 93 [HARD] How to implement custom Metaclass? Real Python Google, Meta, Frameworks Hard Metaprogramming 94 [HARD] Explain method resolution order (MRO) C3 algorithm Python Docs Google, Meta Hard OOP Internals 95 [HARD] How to avoid the GIL (multiprocessing, C extensions)? Real Python Google, Amazon, HFT Firms Hard Performance 96 [HARD] Memory leaks in Python: Causes and fixes TechBlog Google, Meta, Netflix Hard Memory 97 [HARD] How <code>asyncio</code> event loop works internally Real Python Google, Meta, Netflix Hard Asyncio Internals 98 [HARD] Difference between Threading and Asyncio concurrency models Real Python Google, Meta, Netflix Hard Concurrency 99 [HARD] How to implement non-blocking I/O? Python Docs Google, Amazon Hard I/O 100 [HARD] What is <code>__import__</code> vs <code>importlib</code>? Python Docs Google, Meta, Frameworks Hard Internals 101 [HARD] How are Python dictionaries implemented (Hash Table)? PyCon Talk Google, Meta, Amazon Hard Internals 102 [HARD] Explain descriptor protocol Real Python Google, Meta, Frameworks Hard Descriptors 103 [HARD] How to use <code>sys.settrace</code> for debugging/profiling? Python Docs Google, Meta Hard Internals 104 [HARD] Difference between Process and Thread in Python context Real Python Google, Amazon, Meta Hard OS Concepts 105 [HARD] How to manage weak references (<code>weakref</code>)? Python Docs Google, Meta Hard Memory 106 [HARD] What is the Disassembler (<code>dis</code> module)? Python Docs Google, Meta Hard Bytecode 107 [HARD] How to optimize dictionary memory usage? Stack Overflow Google, Amazon Hard Memory 108 [HARD] Explain Coroutines vs Generators Real Python Google, Meta Hard Concurrency 109 [HARD] How to implement custom context managers (contextlib)? Python Docs Google, Amazon Hard Advanced Patterns 110 [HARD] How to perform zero-copy data transfer (Buffer Protocol)? Python Docs Google, HFT Firms Hard Performance"},{"location":"Interview-Questions/Python/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/Python/#1-decorator-for-timing-functions","title":"1. Decorator for Timing Functions","text":"<pre><code>import time\nimport functools\n\ndef timer_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n        return result\n    return wrapper\n\n@timer_decorator\ndef complex_calculation(n):\n    return sum(i**2 for i in range(n))\n\ncomplex_calculation(1000000)\n</code></pre>"},{"location":"Interview-Questions/Python/#2-context-manager-for-files","title":"2. Context Manager for Files","text":"<pre><code>class FileManager:\n    def __init__(self, filename, mode):\n        self.filename = filename\n        self.mode = mode\n        self.file = None\n\n    def __enter__(self):\n        self.file = open(self.filename, self.mode)\n        return self.file\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        if self.file:\n            self.file.close()\n\n# Usage\n# with FileManager('test.txt', 'w') as f:\n#     f.write('Hello, World!')\n</code></pre>"},{"location":"Interview-Questions/Python/#3-asynchronous-pattern","title":"3. Asynchronous Pattern","text":"<pre><code>import asyncio\n\nasync def fetch_data(delay, id):\n    print(f\"Fetching data {id}...\")\n    await asyncio.sleep(delay)  # Simulate I/O op\n    print(f\"Data {id} fetched\")\n    return {\"id\": id, \"data\": \"sample\"}\n\nasync def main():\n    # Run tasks concurrently\n    tasks = [fetch_data(1, 1), fetch_data(2, 2), fetch_data(1.5, 3)]\n    results = await asyncio.gather(*tasks)\n    print(results)\n\n# asyncio.run(main())\n</code></pre>"},{"location":"Interview-Questions/Python/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Explain the Global Interpreter Lock (GIL) and its impact on multi-threading.</li> <li>How does Python's garbage collection mechanism work? (Reference counting vs Generational GC).</li> <li>Write a custom decorator that caches function results (Memoization).</li> <li>How would you debug a memory leak in a long-running Python process?</li> <li>Explain the method resolution order (MRO) works with multiple inheritance.</li> <li>Write code to implement a thread-safe singleton.</li> <li>How to optimize a CPU-bound Python script?</li> <li>Explain the key differences between Python 2 and Python 3.</li> <li>How to implement a context manager using <code>contextlib</code>.</li> <li>Write code to parse a large log file without loading it entirely into memory.</li> </ul>"},{"location":"Interview-Questions/Python/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>How are dictionaries implemented in Python? (Hash collision handling).</li> <li>Explain the difference between <code>__new__</code> and <code>__init__</code>.</li> <li>Write code to flatten a deeply nested dictionary.</li> <li>How does <code>asyncio</code> differ from threading? When to use which?</li> <li>Explain the concept of metaclasses and a use case.</li> <li>Write a generator that yields Fibonacci numbers indefinitely.</li> <li>How to handle circular imports in a large project?</li> <li>Explain the descriptor protocol and how properties work.</li> <li>How would you implement a custom iterator?</li> <li>Write code to validate and parse JSON data using <code>dataclasses</code> or <code>pydantic</code>.</li> </ul>"},{"location":"Interview-Questions/Python/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Write code to reverse a string without using built-in methods.</li> <li>Explain the difference between deep copy and shallow copy.</li> <li>How to handle exceptions in a large-scale application?</li> <li>Write code to find the most frequent element in a list efficiently.</li> <li>Explain the use of <code>*args</code> and <code>**kwargs</code>.</li> <li>How to implement a producer-consumer problem using <code>queue</code>?</li> <li>Explain the difference between <code>@staticmethod</code>, <code>@classmethod</code>, and instance methods.</li> <li>Write code to sort a list of dictionaries by a specific key.</li> <li>How does variable scope work in Python (LEGB rule)?</li> <li>Explain what <code>if __name__ == \"__main__\":</code> does.</li> </ul>"},{"location":"Interview-Questions/Python/#questions-asked-in-netflix-interview","title":"Questions asked in Netflix interview","text":"<ul> <li>How to optimize Python for high-throughput network applications?</li> <li>Explain the internals of CPython execution loop.</li> <li>Write code to implement a rate limiter using Redis and Python.</li> <li>How to handle dependency management in a microservices architecture?</li> <li>Explain how <code>gunicorn</code> or <code>uwsgi</code> works with Python web apps.</li> <li>Write code to implement async HTTP requests using <code>aiohttp</code>.</li> <li>How to profile Python code to find bottlenecks? (cProfile, py-spy).</li> <li>Explain the challenge of serialization (pickling) in distributed systems.</li> <li>How to implement rigorous unit testing with <code>pytest</code>?</li> <li>Write code to process a stream of data using generators.</li> </ul>"},{"location":"Interview-Questions/Python/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Python Documentation</li> <li>Real Python Tutorials</li> <li>Fluent Python (Book)</li> <li>Python Internals (GitHub)</li> <li>Hitchhiker's Guide to Python</li> </ul>"},{"location":"Interview-Questions/SQL-Interview-Questions/","title":"SQL Interview Questions","text":"<p>This document provides a curated list of SQL interview questions commonly asked in technical interviews. It covers topics ranging from basic SQL syntax and data types to advanced concepts like joins, subqueries, window functions, and database design. The list is updated frequently to serve as a comprehensive reference for interview preparation.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 Difference between <code>DELETE</code>, <code>TRUNCATE</code>, and <code>DROP</code> GeeksforGeeks Most Tech Companies Easy DDL, DML 2 Types of SQL Joins (INNER, LEFT, RIGHT, FULL) W3Schools Google, Amazon, Meta Easy Joins 3 What is Normalization? Explain different forms. StudyTonight Microsoft, Oracle, IBM Medium Database Design, Normalization 4 Explain Primary Key vs Foreign Key vs Unique Key GeeksforGeeks Most Tech Companies Easy Constraints, Database Design 5 What are Indexes and why are they important? Essential SQL Google, Amazon, Netflix Medium Performance Optimization, Indexes 6 Write a query to find the Nth highest salary. LeetCode Amazon, Microsoft, Uber Medium Subqueries, Window Functions, Ranking 7 Explain ACID properties in Databases. GeeksforGeeks Oracle, SAP, Banks Medium Transactions, Database Fundamentals 8 What is a Subquery? Types of Subqueries. SQLTutorial.org Meta, Google, LinkedIn Medium Subqueries, Query Structure 9 Difference between <code>UNION</code> and <code>UNION ALL</code>. W3Schools Most Tech Companies Easy Set Operations 10 What are Window Functions? Give examples. Mode Analytics Netflix, Airbnb, Spotify Hard Window Functions, Advanced SQL 11 Explain Common Table Expressions (CTEs). SQLShack Microsoft, Google Medium CTEs, Query Readability 12 How to handle NULL values in SQL? SQL Authority Most Tech Companies Easy NULL Handling, Functions (COALESCE, ISNULL) 13 What is SQL Injection and how to prevent it? OWASP All Security-Conscious Medium Security, Best Practices 14 Difference between <code>GROUP BY</code> and <code>PARTITION BY</code>. Stack Overflow Advanced Roles Hard Aggregation, Window Functions 15 Write a query to find duplicate records in a table. GeeksforGeeks Data Quality Roles Medium Aggregation, GROUP BY, HAVING 16 Difference between <code>WHERE</code> and <code>HAVING</code> clause. SQLTutorial.org Most Tech Companies Easy Filtering, Aggregation 17 What are Triggers? Give an example. GeeksforGeeks Database Roles Medium Triggers, Automation 18 Explain different types of relationships (1:1, 1:N, N:M). Lucidchart Most Tech Companies Easy Database Design, Relationships 19 What is a View in SQL? W3Schools Google, Microsoft Easy Views, Abstraction 20 How to optimize a slow SQL query? [Several Resources] Performance Engineers Hard Performance Tuning, Optimization 21 Difference between <code>ROW_NUMBER()</code>, <code>RANK()</code>, <code>DENSE_RANK()</code>. SQLShack Data Analysts, Scientists Medium Window Functions, Ranking 22 What is Database Denormalization? When to use it? GeeksforGeeks Performance-critical Apps Medium Database Design, Performance 23 Explain Stored Procedures. Advantages? SQLTutorial.org Oracle, Microsoft Medium Stored Procedures, Reusability 24 How does <code>BETWEEN</code> operator work? W3Schools Most Tech Companies Easy Operators, Filtering 25 What is the <code>CASE</code> statement used for? W3Schools Most Tech Companies Easy Conditional Logic 26 Explain Self Join with an example. GeeksforGeeks Amazon, Meta Medium Joins 27 What is the purpose of <code>DISTINCT</code> keyword? W3Schools Most Tech Companies Easy Deduplication, Querying 28 How to find the second highest value? [Various Methods] Common Interview Q Medium Subqueries, Window Functions 29 What is Referential Integrity? Techopedia Database Roles Medium Constraints, Data Integrity 30 Explain <code>EXISTS</code> and <code>NOT EXISTS</code> operators. SQLTutorial.org Google, LinkedIn Medium Subqueries, Operators 31 What is a Schema in a database? Wikipedia Most Tech Companies Easy Database Concepts 32 Difference between <code>CHAR</code> and <code>VARCHAR</code> data types. GeeksforGeeks Most Tech Companies Easy Data Types, Storage 33 How to concatenate strings in SQL? Database.Guide Most Tech Companies Easy String Manipulation 34 What is Data Warehousing? IBM BI Roles, Data Engineers Medium Data Warehousing, BI 35 Explain ETL (Extract, Transform, Load) process. AWS Data Engineers Medium ETL, Data Integration 36 What are Aggregate Functions? List some. W3Schools Most Tech Companies Easy Aggregation 37 How to handle transactions (COMMIT, ROLLBACK)? SQLTutorial.org Database Developers Medium Transactions, ACID 38 What is Database Sharding? DigitalOcean Scalability Roles Hard Scalability, Database Architecture 39 Explain Database Replication. Wikipedia High Availability Roles Hard High Availability, Replication 40 What is the <code>LIKE</code> operator used for? W3Schools Most Tech Companies Easy Pattern Matching, Filtering 41 Difference between <code>COUNT(*)</code> and <code>COUNT(column)</code>. Stack Overflow Most Tech Companies Easy Aggregation, NULL Handling 42 What is a Candidate Key? GeeksforGeeks Database Design Roles Medium Keys, Database Design 43 Explain Super Key. GeeksforGeeks Database Design Roles Medium Keys, Database Design 44 What is Composite Key? GeeksforGeeks Database Design Roles Medium Keys, Database Design 45 How to get the current date and time in SQL? [Varies by RDBMS] Most Tech Companies Easy Date/Time Functions 46 What is the purpose of <code>ALTER TABLE</code> statement? W3Schools Database Admins/Devs Easy DDL, Schema Modification 47 Explain <code>CHECK</code> constraint. W3Schools Database Design Roles Easy Constraints, Data Integrity 48 What is <code>DEFAULT</code> constraint? W3Schools Database Design Roles Easy Constraints, Default Values 49 How to create a temporary table? [Varies by RDBMS] Developers Medium Temporary Storage, Complex Queries 50 What is SQL Injection? (Revisited for emphasis) OWASP All Roles Medium Security 51 Explain Cross Join. When is it useful? W3Schools Specific Scenarios Medium Joins, Cartesian Product 52 What is the difference between Function and Stored Procedure? GeeksforGeeks Database Developers Medium Functions, Stored Procedures 53 How to find the length of a string? [Varies by RDBMS] Most Tech Companies Easy String Functions 54 What is the <code>HAVING</code> clause used for? W3Schools Most Tech Companies Easy Filtering Aggregates 55 Explain database locking mechanisms. Wikipedia Database Admins/Archs Hard Concurrency Control 56 What are Isolation Levels in transactions? GeeksforGeeks Database Developers Hard Transactions, Concurrency 57 How to perform conditional aggregation? SQL Authority Data Analysts Medium Aggregation, Conditional Logic 58 What is a Pivot Table in SQL? SQLShack Data Analysts, BI Roles Hard Data Transformation, Reporting 59 Explain the <code>MERGE</code> statement. Microsoft Docs SQL Server Devs Medium DML, Upsert Operations 60 How to handle errors in SQL (e.g., TRY...CATCH)? Microsoft Docs SQL Server Devs Medium Error Handling 61 What is Dynamic SQL? Pros and Cons? SQLShack Advanced SQL Devs Hard Dynamic Queries, Flexibility, Security 62 Explain Full-Text Search. Wikipedia Search Functionality Medium Indexing, Searching Text 63 How to work with JSON data in SQL? [Varies by RDBMS] Modern App Devs Medium JSON Support, Data Handling 64 What is Materialized View? Wikipedia Performance Optimization Hard Views, Performance 65 Difference between OLTP and OLAP. GeeksforGeeks DB Architects, BI Roles Medium Database Systems, Use Cases 66 How to calculate running totals? Mode Analytics Data Analysts Medium Window Functions, Aggregation 67 What is a Sequence in SQL? Oracle Docs Oracle/Postgres Devs Medium Sequence Generation 68 Explain Recursive CTEs. SQLTutorial.org Advanced SQL Devs Hard CTEs, Hierarchical Data 69 How to find the median value in SQL? Stack Overflow Data Analysts Hard Statistics, Window Functions 70 What is Query Execution Plan? Wikipedia Performance Tuning Medium Query Optimization, Performance 71 How to use <code>COALESCE</code> or <code>ISNULL</code>? W3Schools Most Tech Companies Easy NULL Handling 72 What is B-Tree Index? Wikipedia Database Internals Medium Indexes, Data Structures 73 Explain Hash Index. PostgreSQL Docs Database Internals Medium Indexes, Data Structures 74 Difference between Clustered and Non-Clustered Index. GeeksforGeeks Database Performance Medium Indexes, Performance 75 How to grant and revoke permissions? W3Schools Database Admins Easy Security, Access Control 76 What is SQL Profiler / Tracing? Microsoft Docs Performance Tuning Medium Monitoring, Debugging 77 Explain database constraints (NOT NULL, UNIQUE, etc.). W3Schools Most Tech Companies Easy Constraints, Data Integrity 78 How to update multiple rows with different values? Stack Overflow Developers Medium DML, Updates 79 What is database normalization (revisited)? StudyTonight All Roles Medium Database Design 80 Explain 1NF, 2NF, 3NF, BCNF. GeeksforGeeks Database Design Roles Medium Normalization Forms 81 How to delete duplicate rows? GeeksforGeeks Data Cleaning Roles Medium DML, Data Quality 82 What is the <code>INTERSECT</code> operator? W3Schools Set Operations Roles Medium Set Operations 83 What is the <code>EXCEPT</code> / <code>MINUS</code> operator? W3Schools Set Operations Roles Medium Set Operations 84 How to handle large objects (BLOB, CLOB)? Oracle Docs Specific Applications Medium Data Types, Large Data 85 What is database connection pooling? Wikipedia Application Developers Medium Performance, Resource Management 86 Explain CAP Theorem. Wikipedia Distributed Systems Hard Distributed Databases, Tradeoffs 87 How to perform date/time arithmetic? [Varies by RDBMS] Most Tech Companies Easy Date/Time Functions 88 What is a correlated subquery? GeeksforGeeks Advanced SQL Users Medium Subqueries, Performance Considerations 89 How to use <code>GROUPING SETS</code>, <code>CUBE</code>, <code>ROLLUP</code>? SQLShack BI / Analytics Roles Hard Advanced Aggregation 90 What is Parameter Sniffing (SQL Server)? Brent Ozar SQL Server DBAs/Devs Hard Performance Tuning (SQL Server) 91 How to create and use User-Defined Functions (UDFs)? [Varies by RDBMS] Database Developers Medium Functions, Reusability 92 What is database auditing? Wikipedia Security/Compliance Roles Medium Security, Monitoring 93 Explain optimistic vs. pessimistic locking. Stack Overflow Concurrent Applications Hard Concurrency Control 94 How to handle deadlocks? Microsoft Docs Database Admins/Devs Hard Concurrency, Error Handling 95 What is NoSQL? How does it differ from SQL? MongoDB Modern Data Roles Medium Database Paradigms 96 Explain eventual consistency. Wikipedia Distributed Systems Hard Distributed Databases, Consistency Models 97 How to design a schema for a specific scenario (e.g., social media)? [Design Principles] System Design Interviews Hard Database Design, Modeling 98 What are spatial data types and functions? PostGIS GIS Applications Hard Spatial Data, GIS 99 How to perform fuzzy string matching in SQL? Stack Overflow Data Matching Roles Hard String Matching, Extensions 100 What is Change Data Capture (CDC)? Wikipedia Data Integration/Sync Hard Data Replication, Event Streaming 101 Explain Graph Databases and their use cases. Neo4j Specialized Roles Hard Graph Databases, Data Modeling"},{"location":"Interview-Questions/SQL-Interview-Questions/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/SQL-Interview-Questions/#1-nth-highest-salary-using-dense_rank","title":"1. Nth Highest Salary using DENSE_RANK()","text":"<p>Finding the Nth highest salary is a classic problem. <code>DENSE_RANK()</code> is preferred over <code>ROW_NUMBER()</code> or <code>RANK()</code> because it handles ties without skipping ranks.</p> <pre><code>WITH RankedSalaries AS (\n    SELECT \n        salary,\n        DENSE_RANK() OVER (ORDER BY salary DESC) as rank_num\n    FROM employees\n)\nSELECT DISTINCT salary\nFROM RankedSalaries\nWHERE rank_num = :N;\n</code></pre>"},{"location":"Interview-Questions/SQL-Interview-Questions/#2-recursive-cte-employee-hierarchy","title":"2. Recursive CTE: Employee Hierarchy","text":"<p>Finding all subordinates of a manager (or traversing a graph/tree structure).</p> <pre><code>WITH RECURSIVE Hierarchy AS (\n    -- Anchor member: Start with the top-level manager\n    SELECT employee_id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive member: Join with the previous level\n    SELECT e.employee_id, e.name, e.manager_id, h.level + 1\n    FROM employees e\n    INNER JOIN Hierarchy h ON e.manager_id = h.employee_id\n)\nSELECT * FROM Hierarchy;\n</code></pre>"},{"location":"Interview-Questions/SQL-Interview-Questions/#3-running-total-and-moving-average","title":"3. Running Total and Moving Average","text":"<p>Using Window Functions for time-series analysis.</p> <pre><code>SELECT \n    date,\n    sales,\n    SUM(sales) OVER (ORDER BY date) as running_total,\n    AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as 7_day_moving_avg\nFROM daily_sales;\n</code></pre>"},{"location":"Interview-Questions/SQL-Interview-Questions/#4-pivot-data-case-when-aggregation","title":"4. Pivot Data (<code>CASE WHEN</code> Aggregation)","text":"<p>Transforming rows into columns (e.g., monthly sales side-by-side).</p> <pre><code>SELECT \n    product_id,\n    SUM(CASE WHEN month = 'Jan' THEN sales ELSE 0 END) as Jan_Sales,\n    SUM(CASE WHEN month = 'Feb' THEN sales ELSE 0 END) as Feb_Sales,\n    SUM(CASE WHEN month = 'Mar' THEN sales ELSE 0 END) as Mar_Sales\nFROM monthly_sales\nGROUP BY product_id;\n</code></pre>"},{"location":"Interview-Questions/SQL-Interview-Questions/#5-gap-analysis-identifying-missing-sequences","title":"5. Gap Analysis (Identifying Missing Sequences)","text":"<p>Finding gaps in sequential data (e.g., missing ID numbers).</p> <pre><code>WITH LaggedData AS (\n    SELECT \n        id, \n        LEAD(id) OVER (ORDER BY id) as next_id\n    FROM sequences\n)\nSELECT \n    id + 1 as gap_start, \n    next_id - 1 as gap_end\nFROM LaggedData\nWHERE next_id - id &gt; 1;\n</code></pre>"},{"location":"Interview-Questions/SQL-Interview-Questions/#additional-resources","title":"Additional Resources","text":"<ul> <li>PostgreSQL Documentation - The gold standard for SQL reference.</li> <li>LeetCode Database Problems - Best for practice.</li> <li>Mode Analytics SQL Tutorial - Excellent for data analysis focus.</li> <li>Use The Index, Luke! - Deep dive into SQL performance and indexing.</li> <li>Modern SQL - Features of newer SQL standards.</li> </ul>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-google-interviews","title":"Questions asked in Google interviews","text":"<ol> <li>Explain window functions and their applications in analytical queries.</li> <li>Write a query to find users who have logged in on consecutive days.</li> <li>How would you optimize a slow-performing query that involves multiple joins?</li> <li>Explain the difference between <code>RANK()</code>, <code>DENSE_RANK()</code>, and <code>ROW_NUMBER()</code>.</li> <li>Write a query to calculate a running total or moving average.</li> <li>How would you handle hierarchical data in SQL?</li> <li>Explain Common Table Expressions (CTEs) and their benefits.</li> <li>What are the performance implications of using subqueries vs. joins?</li> <li>How would you design a database schema for a specific application?</li> <li>Explain how indexes work and when they should be used.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-amazon-interviews","title":"Questions asked in Amazon interviews","text":"<ol> <li>Write a query to find the nth highest salary in a table.</li> <li>How would you identify and remove duplicate records?</li> <li>Explain the difference between <code>UNION</code> and <code>UNION ALL</code>.</li> <li>Write a query to pivot data from rows to columns.</li> <li>How would you handle time-series data in SQL?</li> <li>Explain the concept of database sharding.</li> <li>Write a query to find users who purchased products in consecutive months.</li> <li>How would you implement a recommendation system using SQL?</li> <li>Explain how you would optimize a query for large datasets.</li> <li>Write a query to calculate year-over-year growth.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-microsoft-interviews","title":"Questions asked in Microsoft interviews","text":"<ol> <li>Explain database normalization and denormalization.</li> <li>How would you implement error handling in SQL?</li> <li>Write a query to find departments with above-average salaries.</li> <li>Explain the different types of joins and their use cases.</li> <li>How would you handle slowly changing dimensions?</li> <li>Write a query to implement a pagination system.</li> <li>Explain transaction isolation levels.</li> <li>How would you design a database for high availability?</li> <li>Write a query to find the most frequent values in a column.</li> <li>Explain the differences between clustered and non-clustered indexes.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-meta-interviews","title":"Questions asked in Meta interviews","text":"<ol> <li>Write a query to analyze user engagement metrics.</li> <li>How would you implement a friend recommendation algorithm?</li> <li>Explain how you would handle large-scale data processing.</li> <li>Write a query to identify trending content.</li> <li>How would you design a database schema for a social media platform?</li> <li>Explain the concept of data partitioning.</li> <li>Write a query to calculate the conversion rate between different user actions.</li> <li>How would you implement A/B testing analysis using SQL?</li> <li>Explain how you would handle real-time analytics.</li> <li>Write a query to identify anomalies in user behavior.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-netflix-interviews","title":"Questions asked in Netflix interviews","text":"<ol> <li>Write a query to analyze streaming patterns and user retention.</li> <li>How would you implement a content recommendation system?</li> <li>Explain how you would handle data for personalized user experiences.</li> <li>Write a query to identify viewing trends across different demographics.</li> <li>How would you design a database for content metadata?</li> <li>Explain how you would optimize queries for real-time recommendations.</li> <li>Write a query to calculate user engagement metrics.</li> <li>How would you implement A/B testing for UI changes?</li> <li>Explain how you would handle data for regional content preferences.</li> <li>Write a query to identify factors affecting user churn.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-apple-interviews","title":"Questions asked in Apple interviews","text":"<ol> <li>Explain database security best practices.</li> <li>How would you design a database for an e-commerce platform?</li> <li>Write a query to analyze product performance.</li> <li>Explain how you would handle data migration.</li> <li>How would you implement data validation in SQL?</li> <li>Write a query to track user interactions with products.</li> <li>Explain how you would optimize database performance.</li> <li>How would you implement data archiving strategies?</li> <li>Write a query to analyze customer feedback data.</li> <li>Explain how you would handle internationalization in databases.</li> </ol>"},{"location":"Interview-Questions/SQL-Interview-Questions/#questions-asked-in-linkedin-interviews","title":"Questions asked in LinkedIn interviews","text":"<ol> <li>Write a query to implement a connection recommendation system.</li> <li>How would you design a database schema for professional profiles?</li> <li>Explain how you would handle data for skill endorsements.</li> <li>Write a query to analyze user networking patterns.</li> <li>How would you implement job recommendation algorithms?</li> <li>Explain how you would handle data for company pages.</li> <li>Write a query to identify trending job skills.</li> <li>How would you implement search functionality for profiles?</li> <li>Explain how you would handle data privacy requirements.</li> <li>Write a query to analyze user engagement with content.</li> </ol>"},{"location":"Interview-Questions/Scikit-Learn/","title":"Scikit-Learn Interview Questions","text":"<p>This document provides a curated list of Scikit-Learn interview questions commonly asked in technical interviews for Machine Learning Engineer, Data Scientist, and AI/ML roles. It covers fundamental concepts to advanced machine learning techniques, model evaluation, and production deployment.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 What is Scikit-Learn and why is it popular? Scikit-Learn Docs Google, Amazon, Meta, Netflix Easy Basics, Introduction 2 Explain the Scikit-Learn API design (fit, transform, predict) Scikit-Learn Docs Google, Amazon, Meta, Microsoft Easy API Design, Estimators 3 What are estimators, transformers, and predictors? Scikit-Learn Docs Google, Amazon, Meta Easy Core Concepts 4 How to split data into train and test sets? Scikit-Learn Docs Most Tech Companies Easy Data Splitting, train_test_split 5 What is cross-validation and why is it important? Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Cross-Validation, Model Evaluation 6 Difference between KFold, StratifiedKFold, GroupKFold Scikit-Learn Docs Google, Amazon, Meta Medium Cross-Validation Strategies 7 How to implement GridSearchCV for hyperparameter tuning? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Hyperparameter Tuning 8 Difference between GridSearchCV and RandomizedSearchCV Scikit-Learn Docs Google, Amazon, Meta Medium Hyperparameter Tuning 9 What is a Pipeline and why should we use it? Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Pipeline, Preprocessing 10 How to create a custom transformer? Scikit-Learn Docs Google, Amazon, Meta, Microsoft Medium Custom Transformers 11 Explain StandardScaler vs MinMaxScaler vs RobustScaler Scikit-Learn Docs Google, Amazon, Meta, Netflix Easy Feature Scaling 12 What is feature scaling and when is it necessary? Scikit-Learn Docs Most Tech Companies Easy Feature Scaling 13 How to handle missing values in Scikit-Learn? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Missing Data, Imputation 14 Difference between SimpleImputer and IterativeImputer Scikit-Learn Docs Google, Amazon, Meta Medium Imputation Strategies 15 How to encode categorical variables? Scikit-Learn Docs Most Tech Companies Easy Encoding, Categorical Data 16 Difference between LabelEncoder and OneHotEncoder Scikit-Learn Docs Google, Amazon, Meta, Netflix Easy Categorical Encoding 17 What is OrdinalEncoder and when to use it? Scikit-Learn Docs Google, Amazon, Meta Easy Ordinal Encoding 18 How to implement feature selection? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Feature Selection 19 Explain SelectKBest and mutual_info_classif Scikit-Learn Docs Google, Amazon, Meta Medium Feature Selection 20 What is Recursive Feature Elimination (RFE)? Scikit-Learn Docs Google, Amazon, Meta, Microsoft Medium Feature Selection, RFE 21 How to implement Linear Regression? Scikit-Learn Docs Most Tech Companies Easy Linear Regression 22 What is Ridge Regression and when to use it? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Regularization, Ridge 23 What is Lasso Regression and when to use it? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Regularization, Lasso 24 Difference between Ridge (L2) and Lasso (L1) Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Regularization 25 What is ElasticNet regression? Scikit-Learn Docs Google, Amazon, Meta Medium ElasticNet, Regularization 26 How to implement Logistic Regression? Scikit-Learn Docs Most Tech Companies Easy Logistic Regression, Classification 27 Explain the solver options in Logistic Regression Scikit-Learn Docs Google, Amazon, Meta Medium Optimization Solvers 28 How to implement Decision Trees? Scikit-Learn Docs Most Tech Companies Easy Decision Trees 29 What are the hyperparameters for Decision Trees? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Hyperparameters, Trees 30 How to implement Random Forest? Scikit-Learn Docs Most Tech Companies Medium Random Forest, Ensemble 31 Difference between bagging and boosting Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Ensemble Methods 32 How to implement Gradient Boosting? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Gradient Boosting 33 Difference between GradientBoosting and HistGradientBoosting Scikit-Learn Docs Google, Amazon, Meta Medium Gradient Boosting Variants 34 How to implement Support Vector Machines (SVM)? Scikit-Learn Docs Google, Amazon, Meta, Microsoft Medium SVM, Classification 35 Explain different kernel functions in SVM Scikit-Learn Docs Google, Amazon, Meta Medium SVM Kernels 36 How to implement K-Nearest Neighbors (KNN)? Scikit-Learn Docs Most Tech Companies Easy KNN, Classification 37 What is the curse of dimensionality? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Dimensionality, KNN 38 How to implement Naive Bayes classifiers? Scikit-Learn Docs Most Tech Companies Easy Naive Bayes 39 Difference between GaussianNB, MultinomialNB, BernoulliNB Scikit-Learn Docs Google, Amazon, Meta Medium Naive Bayes Variants 40 How to implement K-Means clustering? Scikit-Learn Docs Most Tech Companies Easy K-Means, Clustering 41 How to determine optimal number of clusters? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Elbow Method, Silhouette 42 What is DBSCAN and when to use it? Scikit-Learn Docs Google, Amazon, Meta Medium DBSCAN, Clustering 43 Difference between K-Means and DBSCAN Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Clustering Comparison 44 How to implement Hierarchical Clustering? Scikit-Learn Docs Google, Amazon, Meta Medium Hierarchical Clustering 45 How to implement PCA (Principal Component Analysis)? Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium PCA, Dimensionality Reduction 46 How to choose number of components in PCA? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium PCA, Variance Explained 47 What is t-SNE and when to use it? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium t-SNE, Visualization 48 Difference between PCA and t-SNE Scikit-Learn Docs Google, Amazon, Meta Medium Dimensionality Reduction 49 What is accuracy and when is it misleading? Scikit-Learn Docs Most Tech Companies Easy Metrics, Accuracy 50 Explain precision, recall, and F1-score Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Classification Metrics 51 What is the ROC curve and AUC? Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium ROC, AUC 52 When to use precision vs recall? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Metrics Tradeoff 53 What is the confusion matrix? Scikit-Learn Docs Most Tech Companies Easy Confusion Matrix 54 What is mean squared error (MSE) and RMSE? Scikit-Learn Docs Most Tech Companies Easy Regression Metrics 55 What is R\u00b2 score (coefficient of determination)? Scikit-Learn Docs Most Tech Companies Easy Regression Metrics 56 How to handle imbalanced datasets? Scikit-Learn Docs Google, Amazon, Meta, Netflix, Apple Medium Imbalanced Data, class_weight 57 What is SMOTE and how does it work? Imbalanced-Learn Google, Amazon, Meta Medium Oversampling, SMOTE 58 How to implement ColumnTransformer? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Column Transformers 59 What is FeatureUnion and when to use it? Scikit-Learn Docs Google, Amazon, Meta Medium Feature Engineering 60 How to implement polynomial features? Scikit-Learn Docs Google, Amazon, Meta Easy Polynomial Features 61 What is learning curve and how to interpret it? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Learning Curves, Diagnostics 62 What is validation curve? Scikit-Learn Docs Google, Amazon, Meta Medium Validation Curves 63 How to save and load models with joblib? Scikit-Learn Docs Most Tech Companies Easy Model Persistence 64 What is calibration and why is it important? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Probability Calibration 65 How to use CalibratedClassifierCV? Scikit-Learn Docs Google, Amazon, Meta Medium Calibration 66 What is VotingClassifier? Scikit-Learn Docs Google, Amazon, Meta Medium Ensemble, Voting 67 What is StackingClassifier? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Ensemble, Stacking 68 How to implement AdaBoost? Scikit-Learn Docs Google, Amazon, Meta Medium AdaBoost, Ensemble 69 What is BaggingClassifier? Scikit-Learn Docs Google, Amazon, Meta Medium Bagging, Ensemble 70 How to extract feature importances? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Feature Importance 71 What is permutation importance? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Permutation Importance 72 How to implement multi-class classification? Scikit-Learn Docs Most Tech Companies Medium Multi-class Classification 73 What is One-vs-Rest (OvR) strategy? Scikit-Learn Docs Google, Amazon, Meta Medium Multiclass Strategies 74 What is One-vs-One (OvO) strategy? Scikit-Learn Docs Google, Amazon, Meta Medium Multiclass Strategies 75 How to implement multi-label classification? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Multi-label Classification 76 What is MultiOutputClassifier? Scikit-Learn Docs Google, Amazon, Meta Medium Multi-output 77 How to implement Gaussian Mixture Models (GMM)? Scikit-Learn Docs Google, Amazon, Meta Medium GMM, Clustering 78 What is Isolation Forest? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Anomaly Detection 79 How to implement One-Class SVM for anomaly detection? Scikit-Learn Docs Google, Amazon, Meta Medium Anomaly Detection 80 What is Local Outlier Factor (LOF)? Scikit-Learn Docs Google, Amazon, Meta Medium Anomaly Detection 81 How to implement text classification with TF-IDF? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Text Classification, TF-IDF 82 What is CountVectorizer vs TfidfVectorizer? Scikit-Learn Docs Google, Amazon, Meta, Netflix Easy Text Vectorization 83 How to use HashingVectorizer for large datasets? Scikit-Learn Docs Google, Amazon, Meta Hard Large-scale Text 84 What is SGDClassifier and when to use it? Scikit-Learn Docs Google, Amazon, Meta, Netflix Medium Online Learning, SGD 85 How to implement partial_fit for online learning? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Online Learning 86 What is MLPClassifier for neural networks? Scikit-Learn Docs Google, Amazon, Meta Medium Neural Networks 87 How to set random_state for reproducibility? Scikit-Learn Docs Most Tech Companies Easy Reproducibility 88 What is make_pipeline vs Pipeline? Scikit-Learn Docs Google, Amazon, Meta Easy Pipeline 89 How to get prediction probabilities? Scikit-Learn Docs Most Tech Companies Easy Probabilities 90 What is decision_function vs predict_proba? Scikit-Learn Docs Google, Amazon, Meta Medium Prediction Methods 91 [HARD] How to implement custom scoring functions for GridSearchCV? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Custom Metrics 92 [HARD] How to implement time series cross-validation (TimeSeriesSplit)? Scikit-Learn Docs Google, Amazon, Netflix, Apple Hard Time Series CV 93 [HARD] How to implement nested cross-validation? Scikit-Learn Docs Google, Amazon, Meta Hard Nested CV, Model Selection 94 [HARD] How to optimize memory with sparse matrices? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Sparse Matrices, Memory 95 [HARD] How to implement custom transformers with TransformerMixin? Scikit-Learn Docs Google, Amazon, Meta, Microsoft Hard Custom Transformers 96 [HARD] How to implement custom estimators with BaseEstimator? Scikit-Learn Docs Google, Amazon, Meta Hard Custom Estimators 97 [HARD] How to optimize hyperparameters with Bayesian optimization? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Hyperparameter Optimization 98 [HARD] How to implement stratified sampling for imbalanced regression? Scikit-Learn Docs Google, Amazon, Meta Hard Stratified Sampling 99 [HARD] How to implement target encoding without data leakage? Category Encoders Google, Amazon, Meta, Netflix Hard Target Encoding, Leakage 100 [HARD] How to implement cross-validation with grouped data? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard GroupKFold, Data Leakage 101 [HARD] How to implement feature selection with embedded methods? Scikit-Learn Docs Google, Amazon, Meta Hard Feature Selection 102 [HARD] How to handle high-cardinality categorical features? Stack Overflow Google, Amazon, Meta, Netflix Hard High Cardinality 103 [HARD] How to implement model interpretability with SHAP values? SHAP Docs Google, Amazon, Meta, Netflix, Apple Hard Model Interpretability, SHAP 104 [HARD] How to implement multivariate time series forecasting? Scikit-Learn Docs Google, Amazon, Netflix Hard Time Series, Multi-output 105 [HARD] How to handle concept drift in production models? Towards Data Science Google, Amazon, Meta, Netflix Hard Concept Drift, MLOps 106 [HARD] How to implement model monitoring for production? MLflow Docs Google, Amazon, Meta, Netflix, Apple Hard Model Monitoring, MLOps 107 [HARD] How to optimize inference latency for real-time predictions? Scikit-Learn Docs Google, Amazon, Meta, Netflix Hard Latency, Performance 108 [HARD] How to implement A/B testing for model comparison? Towards Data Science Google, Amazon, Meta, Netflix Hard A/B Testing, Experimentation 109 [HARD] How to handle data leakage in feature engineering? Kaggle Google, Amazon, Meta, Netflix, Apple Hard Data Leakage, Feature Engineering 110 [HARD] How to implement model versioning and tracking? MLflow Docs Google, Amazon, Meta, Netflix Hard Model Versioning, MLOps"},{"location":"Interview-Questions/Scikit-Learn/#code-examples","title":"Code Examples","text":""},{"location":"Interview-Questions/Scikit-Learn/#1-building-a-custom-transformer","title":"1. Building a Custom Transformer","text":"<pre><code>from sklearn.base import BaseEstimator, TransformerMixin\n\nclass OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self, factor=1.5):\n        self.factor = factor\n\n    def fit(self, X, y=None):\n        self.Q1 = X.quantile(0.25)\n        self.Q3 = X.quantile(0.75)\n        self.IQR = self.Q3 - self.Q1\n        return self\n\n    def transform(self, X):\n        return X[~((X &lt; (self.Q1 - self.factor * self.IQR)) | \n                   (X &gt; (self.Q3 + self.factor * self.IQR))).any(axis=1)]\n</code></pre>"},{"location":"Interview-Questions/Scikit-Learn/#2-nested-cross-validation","title":"2. Nested Cross-Validation","text":"<pre><code>from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nfrom sklearn.svm import SVC\nimport numpy as np\n\n# Inner loop for hyperparameter tuning\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\nsvm = SVC(kernel=\"rbf\")\ninner_cv = KFold(n_splits=4, shuffle=True, random_state=1)\nclf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n\n# Outer loop for model evaluation\nouter_cv = KFold(n_splits=4, shuffle=True, random_state=1)\nnested_score = cross_val_score(clf, X_iris, y_iris, cv=outer_cv)\n\nprint(f\"Nested CV Score: {nested_score.mean():.3f} +/- {nested_score.std():.3f}\")\n</code></pre>"},{"location":"Interview-Questions/Scikit-Learn/#3-pipeline-with-columntransformer","title":"3. Pipeline with ColumnTransformer","text":"<pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnumeric_features = ['age', 'fare']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['embarked', 'sex', 'pclass']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n</code></pre>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>How would you implement a custom loss function in Scikit-Learn?</li> <li>Explain how to handle data leakage in cross-validation</li> <li>Write code to implement nested cross-validation with hyperparameter tuning</li> <li>How would you optimize a model for minimal inference latency?</li> <li>Explain the bias-variance tradeoff with specific examples</li> <li>How would you implement model calibration for probability estimates?</li> <li>Write code to implement stratified sampling for imbalanced multi-class</li> <li>How would you handle concept drift in production ML systems?</li> <li>Explain how to implement feature importance with SHAP values</li> <li>How would you optimize memory for large sparse datasets?</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Write code to implement a complete ML pipeline for customer churn</li> <li>How would you handle high-cardinality categorical features?</li> <li>Explain the difference between different cross-validation strategies</li> <li>Write code to implement time series cross-validation</li> <li>How would you implement model monitoring in production?</li> <li>Explain how to handle missing data in production systems</li> <li>Write code to implement custom scoring functions</li> <li>How would you implement A/B testing for model comparison?</li> <li>Explain how to optimize hyperparameters efficiently</li> <li>How would you handle data leakage in feature engineering?</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-meta-interview","title":"Questions asked in Meta interview","text":"<ul> <li>Write code to implement user engagement prediction pipeline</li> <li>How would you implement multi-label classification for content tagging?</li> <li>Explain how to handle extremely imbalanced datasets</li> <li>Write code to implement custom transformers for text features</li> <li>How would you implement feature selection for high-dimensional data?</li> <li>Explain how to implement model interpretability</li> <li>Write code to implement online learning with partial_fit</li> <li>How would you implement model calibration?</li> <li>Explain how to prevent overfitting in ensemble models</li> <li>How would you implement multivariate predictions?</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Explain the Scikit-Learn estimator API design principles</li> <li>Write code to implement custom estimators extending BaseEstimator</li> <li>How would you implement regularization selection?</li> <li>Explain the differences between solver options in LogisticRegression</li> <li>Write code to implement feature engineering pipelines</li> <li>How would you optimize model training time?</li> <li>Explain how to implement model persistence correctly</li> <li>Write code to implement cross-validation with custom folds</li> <li>How would you handle numerical stability issues?</li> <li>Explain how to implement reproducible ML experiments</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-netflix-interview","title":"Questions asked in Netflix interview","text":"<ul> <li>Write code to implement recommendation feature engineering</li> <li>How would you implement content classification at scale?</li> <li>Explain how to handle user behavior data for ML</li> <li>Write code to implement streaming quality prediction</li> <li>How would you implement real-time inference optimization?</li> <li>Explain how to implement model monitoring and retraining</li> <li>Write code to implement cohort-based model evaluation</li> <li>How would you handle seasonality in user data?</li> <li>Explain how to implement A/B testing for ML models</li> <li>How would you implement customer lifetime value prediction?</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#questions-asked-in-apple-interview","title":"Questions asked in Apple interview","text":"<ul> <li>Write code to implement privacy-preserving ML pipelines</li> <li>How would you implement on-device ML model optimization?</li> <li>Explain how to handle sensor data for ML</li> <li>Write code to implement quality control classification</li> <li>How would you implement model quantization for deployment?</li> <li>Explain best practices for production ML systems</li> <li>Write code to implement automated model retraining</li> <li>How would you handle data versioning?</li> <li>Explain how to implement cross-platform model deployment</li> <li>How would you implement model security?</li> </ul>"},{"location":"Interview-Questions/Scikit-Learn/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Scikit-Learn Documentation</li> <li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</li> <li>Python Data Science Handbook</li> <li>Introduction to Machine Learning with Python</li> <li>Scikit-Learn Course by Andreas Mueller</li> </ul>"},{"location":"Interview-Questions/System-design/","title":"System Design Interview Questions (DS &amp; ML)","text":"<p>This document provides a curated list of system design questions tailored for Data Science and Machine Learning interviews. The questions focus on designing scalable, robust, and maintainable systems\u2014from end-to-end ML pipelines and data ingestion frameworks to model serving, monitoring, and MLOps architectures. Use the practice links provided to dive deeper into each topic.</p> Sno Question Title Practice Links Companies Asking Difficulty Topics 1 Design an End-to-End Machine Learning Pipeline Towards Data Science Google, Amazon, Facebook Medium ML Pipeline, MLOps 2 Design a Scalable Data Ingestion &amp; Processing System for ML Medium Amazon, Google, Microsoft Hard Data Engineering, Scalability 3 Design a Recommendation System Towards Data Science Google, Amazon, Facebook Medium Recommender Systems, Personalization 4 Design a Fraud Detection System Medium Amazon, Facebook, PayPal Hard Real-Time Analytics, Anomaly Detection 5 Design a Feature Store for Machine Learning Towards Data Science Google, Amazon, Microsoft Medium Data Preprocessing, Feature Engineering 6 Design an Online ML Model Serving Architecture Towards Data Science Google, Amazon, Facebook Hard Model Deployment, Real-Time Serving 7 Design a Continuous Model Retraining and Monitoring System Medium Google, Microsoft, Amazon Hard MLOps, Automation 8 Design an A/B Testing Framework for ML Models Towards Data Science Google, Facebook, Amazon Medium Experimentation, Evaluation 9 Design a Distributed ML Training System Towards Data Science Google, Amazon, Microsoft Hard Distributed Systems, Deep Learning 10 Design a Real-Time Prediction Serving System Towards Data Science Amazon, Google, Facebook Hard Model Serving, Real-Time Processing 11 Design a System for Anomaly Detection in Streaming Data Medium Amazon, Google, Facebook Hard Streaming Data, Anomaly Detection 12 Design a Real-Time Personalization System for E-Commerce Medium Amazon, Facebook, Uber Medium Personalization, Real-Time Analytics 13 Design a Data Versioning and Model Versioning System Towards Data Science Google, Amazon, Microsoft Medium MLOps, Version Control 14 Design a System to Ensure Fairness and Transparency in ML Predictions Medium Google, Facebook, Amazon Hard Ethics, Model Interpretability 15 Design a Data Governance and Compliance System for ML Towards Data Science Microsoft, Google, Amazon Hard Data Governance, Compliance 16 Design an MLOps Pipeline for End-to-End Automation Towards Data Science Google, Amazon, Facebook Hard MLOps, Automation 17 Design a System for Real-Time Prediction Serving with Low Latency Medium Google, Amazon, Microsoft Hard Model Serving, Scalability 18 Design a Scalable Data Warehouse for ML-Driven Analytics Towards Data Science Google, Amazon, Facebook Medium Data Warehousing, Analytics 19 Design a System for Hyperparameter Tuning at Scale Medium Google, Amazon, Microsoft Hard Optimization, Automation 20 Design an Event-Driven Architecture for ML Pipelines Towards Data Science Amazon, Google, Facebook Medium Event-Driven, Real-Time Processing 21 Design a System for Multimodal Data Processing in Machine Learning Towards Data Science Google, Amazon, Facebook Hard Data Integration, Deep Learning 22 Design a System to Handle High-Volume Streaming Data for ML Towards Data Science Amazon, Google, Microsoft Hard Streaming, Scalability 23 Design a Secure and Scalable ML Infrastructure Towards Data Science Google, Amazon, Facebook Hard Security, Scalability 24 Design a Scalable Feature Engineering Pipeline Towards Data Science Google, Amazon, Microsoft Medium Feature Engineering, Scalability 25 Design a System for Experimentation and A/B Testing in Data Science Towards Data Science Google, Amazon, Facebook Medium Experimentation, Analytics 26 Design an Architecture for a Data Lake Tailored for ML Applications Towards Data Science Amazon, Google, Microsoft Medium Data Lakes, Data Engineering 27 Design a Fault-Tolerant Machine Learning System Medium Google, Amazon, Facebook Hard Reliability, Distributed Systems 28 Design a System for Scalable Deep Learning Inference Towards Data Science Google, Amazon, Microsoft Hard Deep Learning, Inference 29 Design a Collaborative Platform for Data Science Projects Towards Data Science Google, Amazon, Facebook Medium Collaboration, Platform Design 30 Design a System for Model Monitoring and Logging Towards Data Science Google, Amazon, Microsoft Medium MLOps, Monitoring"},{"location":"Interview-Questions/System-design/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Design an End-to-End Machine Learning Pipeline  </li> <li>Design a Real-Time Prediction Serving System  </li> <li>Design a Continuous Model Retraining and Monitoring System  </li> <li>Design a System for Hyperparameter Tuning at Scale  </li> <li>Design a Secure and Scalable ML Infrastructure  </li> </ul>"},{"location":"Interview-Questions/System-design/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Design a Scalable Data Ingestion &amp; Processing System for ML  </li> <li>Design a Recommendation System  </li> <li>Design a Fraud Detection System  </li> <li>Design an MLOps Pipeline for End-to-End Automation  </li> <li>Design a System to Handle High-Volume Streaming Data for ML  </li> </ul>"},{"location":"Interview-Questions/System-design/#questions-asked-in-facebook-interview","title":"Questions asked in Facebook interview","text":"<ul> <li>Design an End-to-End Machine Learning Pipeline  </li> <li>Design an Online ML Model Serving Architecture  </li> <li>Design a Real-Time Personalization System for E-Commerce  </li> <li>Design a System for Model Monitoring and Logging  </li> <li>Design a System for Multimodal Data Processing in ML  </li> </ul>"},{"location":"Interview-Questions/System-design/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Design a Data Versioning and Model Versioning System  </li> <li>Design a Scalable Data Warehouse for ML-Driven Analytics  </li> <li>Design a Distributed ML Training System  </li> <li>Design a System for Real-Time Prediction Serving with Low Latency  </li> <li>Design a System for Secure and Scalable ML Infrastructure  </li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/","title":"Data Structures and Algorithms (DSA)","text":"<p>This document provides a curated list of Data Structures and Algorithms (DSA) questions commonly asked in technical interviews.  It covers a wide range of difficulty levels and topics.</p> <p>This is updated frequently but right now this is the most exhaustive list of type of questions being asked.</p> Sno Problem Title Practice Links Companies Asking Difficulty Topics 1 Two Number Sum LeetCode Two Sum Google, Facebook, Amazon Easy Array, Hashing 2 Reverse Linked List LeetCode Reverse Linked List Amazon, Facebook, Microsoft Easy Linked List 3 Valid Parentheses LeetCode Valid Parentheses Amazon, Facebook, Google Easy Stack, String 4 Binary Search LeetCode Binary Search Google, Facebook, Amazon Easy Array, Binary Search 5 Merge Two Sorted Arrays LeetCode Merge Sorted Array Google, Microsoft, Amazon Easy Array, Two Pointers 6 Meeting Rooms LeetCode Meeting Rooms II Microsoft, Google Medium Array, Sorting, Interval Scheduling 7 Climbing Stairs LeetCode Climbing Stairs Amazon, Facebook, Google Easy Dynamic Programming 8 Valid Anagram LeetCode Valid Anagram Google, Amazon Easy String, Hashing 9 Longest Substring Without Repeating Characters LeetCode Longest Substring Without Repeating Characters Amazon, Facebook, Google Medium String, Hashing, Sliding Window 10 Maximum Subarray (Kadane's Algorithm) LeetCode Maximum Subarray Google, Amazon, Facebook Medium Array, Dynamic Programming 11 Word Ladder LeetCode Word Ladder Google, Amazon, Facebook Very Hard Graph, BFS, String Transformation 12 4Sum (Four Number Sum) LeetCode 4Sum Amazon, Facebook, Google Hard Array, Hashing, Two Pointers 13 Median of Two Sorted Arrays LeetCode Median of Two Sorted Arrays Google, Amazon, Microsoft Hard Array, Binary Search 14 Longest Increasing Subsequence LeetCode Longest Increasing Subsequence Google, Facebook, Amazon Hard Array, Dynamic Programming 15 Longest Palindromic Substring LeetCode Longest Palindromic Substring Amazon, Google Hard String, Dynamic Programming 16 Design LRU Cache LeetCode LRU Cache Amazon, Facebook, Google, Microsoft Hard Design, Hashing, Linked List 17 Top K Frequent Elements LeetCode Top K Frequent Elements Google, Facebook, Amazon Medium Array, Hashing, Heap 18 Find Peak Element LeetCode Find Peak Element Google, Facebook, Amazon Medium Array, Binary Search 19 Candy (Min Rewards) LeetCode Candy Amazon, Facebook, Google Hard Array, Greedy 20 Array of Products LeetCode Product of Array Except Self Amazon, Google Medium Array, Prefix/Suffix Products 21 First Duplicate Value LeetCode Find the Duplicate Number Google, Facebook Medium Array, Hashing 22 Validate Subsequence GFG Validate Subsequence Amazon, Google, Microsoft Easy Array, Two Pointers 23 Nth Fibonacci LeetCode Fibonacci Number Google, Facebook, Microsoft Easy Recursion, Dynamic Programming 24 Spiral Traverse LeetCode Spiral Matrix Facebook, Amazon, Google Medium Matrix, Simulation 25 Subarray Sort GFG Minimum Unsorted Subarray Google, Uber Hard Array, Two Pointers 26 Largest Range GFG Largest Range Google, Amazon Hard Array, Hashing 27 Diagonal Traverse LeetCode Diagonal Traverse Google, Facebook Medium Array, Simulation 28 Longest Peak GFG Longest Peak Google, Uber Medium Array, Dynamic Programming 29 Product Sum GFG Product Sum Amazon, Facebook, Google Easy Array, Recursion 30 Merge Two Sorted Lists LeetCode Merge Two Sorted Lists Google, Amazon, Facebook Medium Linked List, Recursion 31 Binary Tree Level Order Traversal LeetCode Level Order Traversal Amazon, Google, Microsoft Easy Tree, BFS 32 Longest Valid Parentheses LeetCode Longest Valid Parentheses Facebook, Google, Amazon Medium String, Stack, Dynamic Programming 33 Word Break LeetCode Word Break Amazon, Google, Facebook Hard Dynamic Programming, String 34 Find Median from Data Stream LeetCode Find Median from Data Stream Facebook, Amazon, Google Hard Heap, Data Structures 35 Longest Repeating Character Replacement LeetCode Longest Repeating Character Replacement Google, Amazon, Facebook Hard String, Sliding Window, Greedy 36 Kth Largest Element in an Array LeetCode Kth Largest Element Google, Amazon, Facebook Medium Heap, Sorting 37 River Sizes GFG River Sizes Facebook, Google Very Hard Graph, DFS/BFS, Matrix 38 Youngest Common Ancestor LeetCode Lowest Common Ancestor Google, Microsoft Very Hard Tree, Ancestor Tracking 39 BST Construction LeetCode Validate BST Facebook, Amazon, Google Very Hard Tree, Binary Search Tree 40 Invert Binary Tree LeetCode Invert Binary Tree Amazon, Facebook, Google Very Hard Tree, Recursion 41 Validate BST LeetCode Validate BST Google, Amazon Very Hard Tree, Binary Search Tree 42 Node Depths GFG Sum of Node Depths Google, Facebook Very Hard Tree, Recursion 43 Branch Sums GFG Branch Sums Amazon, Facebook, Google Very Hard Tree, Recursion 44 Find Successor LeetCode Inorder Successor Facebook, Amazon, Google Very Hard Tree, BST, Inorder Traversal 45 Binary Tree Diameter GFG Diameter of Binary Tree Google, Uber Very Hard Tree, Recursion 46 Lowest Common Ancestor LeetCode Lowest Common Ancestor Amazon, Facebook, Google Very Hard Tree, Recursion 47 Dijkstra's Algorithm LeetCode Network Delay Time Google, Amazon Very Hard Graph, Shortest Paths, Greedy 48 Topological Sort GFG Topological Sort Google, Microsoft, Amazon Very Hard Graph, DFS/BFS, Sorting 49 Knapsack Problem LeetCode Coin Change 2 Facebook, Amazon, Google Very Hard Dynamic Programming, Knapsack 50 Disk Stacking GFG Disk Stacking Google, Facebook Very Hard Dynamic Programming, Sorting 51 Numbers In Pi N/A Google, Facebook Very Hard Dynamic Programming, String Processing 52 Longest Common Subsequence LeetCode Longest Common Subsequence Amazon, Google, Microsoft Very Hard Dynamic Programming, Strings 53 Min Number of Jumps LeetCode Min Number of Jumps Google, Facebook, Amazon Very Hard Dynamic Programming, Greedy 54 Water Area (Trapping Rain Water) LeetCode Trapping Rain Water Google, Amazon, Facebook Very Hard Array, Two Pointers, Greedy 55 Minimum Characters For Palindrome GFG Minimum Characters For Palindrome Amazon, Google Very Hard String, Dynamic Programming, KMP 56 Regular Expression Matching LeetCode Regular Expression Matching Google, Amazon, Facebook Very Hard Dynamic Programming, Strings, Recursion 57 Wildcard Matching LeetCode Wildcard Matching Amazon, Google Very Hard Dynamic Programming, Strings 58 Group Anagrams LeetCode Group Anagrams Google, Amazon, Facebook Medium Array, Hashing 59 Longest Consecutive Sequence LeetCode Longest Consecutive Sequence Facebook, Google, Amazon Hard Array, Hashing 60 Maximum Product Subarray LeetCode Maximum Product Subarray Amazon, Google, Facebook Medium Array, Dynamic Programming 61 Sum of Two Integers (Bit Manipulation) LeetCode Sum of Two Integers Google, Amazon, Facebook Medium Bit Manipulation 62 Course Schedule LeetCode Course Schedule Amazon, Facebook, Google Medium Graph, DFS/BFS 63 Add Two Numbers (Linked List) LeetCode Add Two Numbers Google, Facebook, Amazon Medium Linked List, Math 64 Reverse Words in a String LeetCode Reverse Words in a String Google, Amazon, Facebook Medium String, Two Pointers 65 Intersection of Two Arrays LeetCode Intersection of Two Arrays Amazon, Google, Facebook Easy Array, Hashing 66 Find All Duplicates in an Array LeetCode Find All Duplicates Facebook, Google, Amazon Medium Array, Hashing 67 Majority Element LeetCode Majority Element Google, Amazon Easy Array, Hashing, Boyer-Moore 68 Rotate Array LeetCode Rotate Array Amazon, Google, Facebook Medium Array, Two Pointers 69 Spiral Matrix II LeetCode Spiral Matrix II Google, Facebook, Amazon Medium Matrix, Simulation 70 Search in Rotated Sorted Array LeetCode Search in Rotated Sorted Array Google, Amazon, Facebook Medium Array, Binary Search 71 Design a URL Shortener LeetCode Design TinyURL Uber, Airbnb, Flipkart Medium Design, Hashing, Strings 72 Implement Autocomplete System GFG Autocomplete System Amazon, Google, Swiggy Hard Trie, Design, Strings 73 Design Twitter Feed LeetCode Design Twitter Twitter, Flipkart, Ola Medium Design, Heap, Linked List 74 Implement LFU Cache GFG LFU Cache Amazon, Paytm, Flipkart Hard Design, Hashing 75 Design a Rate Limiter N/A Uber, Ola, Swiggy Medium Design, Algorithms 76 Serialize and Deserialize Binary Tree LeetCode Serialize and Deserialize Binary Tree Amazon, Microsoft, Swiggy Hard Tree, DFS, Design 77 Design a File System LeetCode Design File System Google, Flipkart, Amazon Hard Design, Trie 78 Implement Magic Dictionary LeetCode Implement Magic Dictionary Facebook, Microsoft, Paytm Medium Trie, Design 79 Longest Substring with At Most K Distinct Characters LeetCode Longest Substring with At Most K Distinct Characters Amazon, Google Medium String, Sliding Window 80 Subarray Sum Equals K LeetCode Subarray Sum Equals K Microsoft, Amazon, Flipkart Medium Array, Hashing, Prefix Sum 81 Merge k Sorted Lists LeetCode Merge k Sorted Lists Google, Facebook, Amazon Hard Heap, Linked List 82 Longest Increasing Path in a Matrix LeetCode Longest Increasing Path Google, Microsoft Hard DFS, DP, Matrix 83 Design a Stock Price Fluctuation Tracker LeetCode Stock Price Fluctuation Amazon, Flipkart, Paytm Medium Design, Heap 84 Implement a Trie LeetCode Implement Trie Amazon, Google, Microsoft Medium Trie, Design 85 Design a Chat System Medium: Chat System Design (free article) WhatsApp, Slack, Swiggy Hard Design, Messaging 86 Design an Elevator System N/A OYO, Ola, Flipkart Hard Design, System Design 87 Implement a Sudoku Solver LeetCode Sudoku Solver Google, Microsoft, Amazon Hard Backtracking, Recursion 88 Find All Anagrams in a String LeetCode Find All Anagrams in a String Facebook, Google Medium String, Sliding Window, Hashing 89 Design Twitter-like Feed LeetCode Design Twitter Twitter, Facebook, Uber Medium Design, Heap, Linked List 90 Longest Palindromic Subsequence LeetCode Longest Palindromic Subsequence Amazon, Google Medium DP, String 91 Clone Graph LeetCode Clone Graph Amazon, Google Medium Graph, DFS/BFS 92 Design a Data Structure for the Stock Span Problem LeetCode Online Stock Span Amazon, Microsoft, Paytm Medium Stack, Array, Design 93 Design a Stack That Supports getMin() LeetCode Min Stack Facebook, Amazon, Google Easy Stack, Design 94 Convert Sorted Array to Binary Search Tree LeetCode Sorted Array to BST Facebook, Google Easy Tree, Recursion 95 Meeting Rooms II LeetCode Meeting Rooms II Microsoft, Google Medium Array, Heap, Sorting 96 Search in Rotated Sorted Array LeetCode Search in Rotated Sorted Array Google, Amazon, Facebook Medium Array, Binary Search 97 Design a URL Shortener LeetCode Design TinyURL Uber, Airbnb, Flipkart Medium Design, Hashing, Strings 98 Implement Autocomplete System GFG Autocomplete System Amazon, Google, Swiggy Hard Trie, Design, Strings 99 Design Twitter Feed LeetCode Design Twitter Twitter, Flipkart, Ola Medium Design, Heap, Linked List 100 Implement LFU Cache GFG LFU Cache Amazon, Paytm, Flipkart Hard Design, Hashing"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-google-interview","title":"Questions asked in Google interview","text":"<ul> <li>Two Number Sum</li> <li>Valid Parentheses</li> <li>Binary Search</li> <li>Merge Two Sorted Arrays</li> <li>Meeting Rooms</li> <li>Climbing Stairs</li> <li>Valid Anagram</li> <li>Longest Substring Without Repeating Characters</li> <li>Maximum Subarray (Kadane's Algorithm)</li> <li>Word Ladder</li> <li>4Sum (Four Number Sum)</li> <li>Median of Two Sorted Arrays</li> <li>Longest Increasing Subsequence</li> <li>Longest Palindromic Substring</li> <li>Design LRU Cache</li> <li>Top K Frequent Elements</li> <li>Find Peak Element</li> <li>Candy (Min Rewards)</li> <li>Array of Products</li> <li>First Duplicate Value</li> <li>Validate Subsequence</li> <li>Nth Fibonacci</li> <li>Spiral Traverse</li> <li>Largest Range</li> <li>Diagonal Traverse</li> <li>Longest Peak</li> <li>Product Sum</li> <li>Merge Two Sorted Lists</li> <li>Binary Tree Level Order Traversal</li> <li>Longest Valid Parentheses</li> <li>Word Break</li> <li>Find Median from Data Stream</li> <li>Longest Repeating Character Replacement</li> <li>Kth Largest Element in an Array</li> <li>River Sizes</li> <li>Youngest Common Ancestor</li> <li>BST Construction</li> <li>Invert Binary Tree</li> <li>Validate BST</li> <li>Node Depths</li> <li>Branch Sums</li> <li>Find Successor</li> <li>Binary Tree Diameter</li> <li>Lowest Common Ancestor</li> <li>Dijkstra's Algorithm</li> <li>Topological Sort</li> <li>Knapsack Problem</li> <li>Disk Stacking</li> <li>Numbers In Pi</li> <li>Longest Common Subsequence</li> <li>Min Number of Jumps</li> <li>Water Area (Trapping Rain Water)</li> <li>Minimum Characters For Palindrome</li> <li>Regular Expression Matching</li> <li>Wildcard Matching</li> <li>Group Anagrams</li> <li>Longest Consecutive Sequence</li> <li>Maximum Product Subarray</li> <li>Sum of Two Integers (Bit Manipulation)</li> <li>Course Schedule</li> <li>Add Two Numbers (Linked List)</li> <li>Reverse Words in a String</li> <li>Intersection of Two Arrays</li> <li>Find All Duplicates in an Array</li> <li>Majority Element</li> <li>Rotate Array</li> <li>Spiral Matrix II</li> <li>Search in Rotated Sorted Array</li> <li>Implement Autocomplete System</li> <li>Design a File System</li> <li>Longest Substring with At Most K Distinct Characters</li> <li>Merge k Sorted Lists</li> <li>Longest Increasing Path in a Matrix</li> <li>Implement a Trie</li> <li>Implement a Sudoku Solver</li> <li>Find All Anagrams in a String</li> <li>Longest Palindromic Subsequence</li> <li>Clone Graph</li> <li>Design a Stack That Supports getMin()</li> <li>Convert Sorted Array to Binary Search Tree</li> <li>Meeting Rooms II</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-facebook-interview","title":"Questions asked in Facebook interview","text":"<ul> <li>Two Number Sum</li> <li>Reverse Linked List</li> <li>Valid Parentheses</li> <li>Binary Search</li> <li>Merge Two Sorted Arrays</li> <li>Climbing Stairs</li> <li>Longest Substring Without Repeating Characters</li> <li>Maximum Subarray (Kadane's Algorithm)</li> <li>Word Ladder</li> <li>4Sum (Four Number Sum)</li> <li>Longest Increasing Subsequence</li> <li>Design LRU Cache</li> <li>Top K Frequent Elements</li> <li>Find Peak Element</li> <li>Candy (Min Rewards)</li> <li>Array of Products</li> <li>First Duplicate Value</li> <li>Word Break</li> <li>Spiral Traverse</li> <li>Diagonal Traverse</li> <li>Product Sum</li> <li>Merge Two Sorted Lists</li> <li>Binary Tree Level Order Traversal</li> <li>Longest Valid Parentheses</li> <li>Find Median from Data Stream</li> <li>Longest Repeating Character Replacement</li> <li>Kth Largest Element in an Array</li> <li>River Sizes</li> <li>BST Construction</li> <li>Invert Binary Tree</li> <li>Node Depths</li> <li>Branch Sums</li> <li>Find Successor</li> <li>Lowest Common Ancestor</li> <li>Dijkstra's Algorithm</li> <li>Knapsack Problem</li> <li>Disk Stacking</li> <li>Numbers In Pi</li> <li>Longest Common Subsequence</li> <li>Min Number of Jumps</li> <li>Water Area (Trapping Rain Water)</li> <li>Regular Expression Matching</li> <li>Wildcard Matching</li> <li>Group Anagrams</li> <li>Longest Consecutive Sequence</li> <li>Maximum Product Subarray</li> <li>Sum of Two Integers (Bit Manipulation)</li> <li>Add Two Numbers (Linked List)</li> <li>Reverse Words in a String</li> <li>Intersection of Two Arrays</li> <li>Find All Duplicates in an Array</li> <li>Rotate Array</li> <li>Spiral Matrix II</li> <li>Search in Rotated Sorted Array</li> <li>Design a Stack That Supports getMin()</li> <li>Convert Sorted Array to Binary Search Tree</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-amazon-interview","title":"Questions asked in Amazon interview","text":"<ul> <li>Two Number Sum</li> <li>Valid Parentheses</li> <li>Binary Search</li> <li>Merge Two Sorted Arrays</li> <li>Climbing Stairs</li> <li>Valid Anagram</li> <li>Longest Substring Without Repeating Characters</li> <li>Maximum Subarray (Kadane's Algorithm)</li> <li>Word Ladder</li> <li>4Sum (Four Number Sum)</li> <li>Median of Two Sorted Arrays</li> <li>Longest Increasing Subsequence</li> <li>Longest Palindromic Substring</li> <li>Design LRU Cache</li> <li>Top K Frequent Elements</li> <li>Find Peak Element</li> <li>Candy (Min Rewards)</li> <li>Array of Products</li> <li>First Duplicate Value</li> <li>Validate Subsequence</li> <li>Nth Fibonacci</li> <li>Spiral Traverse</li> <li>Largest Range</li> <li>Diagonal Traverse</li> <li>Longest Peak</li> <li>Product Sum</li> <li>Merge Two Sorted Lists</li> <li>Binary Tree Level Order Traversal</li> <li>Longest Valid Parentheses</li> <li>Word Break</li> <li>Find Median from Data Stream</li> <li>Longest Repeating Character Replacement</li> <li>Kth Largest Element in an Array</li> <li>River Sizes</li> <li>BST Construction</li> <li>Invert Binary Tree</li> <li>Validate BST</li> <li>Branch Sums</li> <li>Find Successor</li> <li>Lowest Common Ancestor</li> <li>Dijkstra's Algorithm</li> <li>Topological Sort</li> <li>Knapsack Problem</li> <li>Disk Stacking</li> <li>Numbers In Pi</li> <li>Longest Common Subsequence</li> <li>Min Number of Jumps</li> <li>Water Area (Trapping Rain Water)</li> <li>Minimum Characters For Palindrome</li> <li>Regular Expression Matching</li> <li>Wildcard Matching</li> <li>Group Anagrams</li> <li>Longest Consecutive Sequence</li> <li>Maximum Product Subarray</li> <li>Sum of Two Integers (Bit Manipulation)</li> <li>Course Schedule</li> <li>Add Two Numbers (Linked List)</li> <li>Reverse Words in a String</li> <li>Intersection of Two Arrays</li> <li>Find All Duplicates in an Array</li> <li>Majority Element</li> <li>Rotate Array</li> <li>Spiral Matrix II</li> <li>Search in Rotated Sorted Array</li> <li>Design a URL Shortener</li> <li>Implement Autocomplete System</li> <li>Design a File System</li> <li>Longest Substring with At Most K Distinct Characters</li> <li>Merge k Sorted Lists</li> <li>Implement a Trie</li> <li>Implement a Sudoku Solver</li> <li>Find All Anagrams in a String</li> <li>Longest Palindromic Subsequence</li> <li>Clone Graph</li> <li>Design a Stack That Supports getMin()</li> <li>Meeting Rooms II</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-microsoft-interview","title":"Questions asked in Microsoft interview","text":"<ul> <li>Reverse Linked List</li> <li>Merge Two Sorted Arrays</li> <li>Meeting Rooms</li> <li>Median of Two Sorted Arrays</li> <li>Nth Fibonacci</li> <li>Binary Tree Level Order Traversal</li> <li>Find Median from Data Stream</li> <li>Topological Sort</li> <li>Youngest Common Ancestor</li> <li>Longest Increasing Path in a Matrix</li> <li>Implement a Trie</li> <li>Implement a Sudoku Solver</li> <li>Convert Sorted Array to Binary Search Tree</li> <li>Meeting Rooms II</li> <li>Course Schedule</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-uber-interview","title":"Questions asked in Uber interview","text":"<ul> <li>Subarray Sort</li> <li>Longest Peak</li> <li>Binary Tree Diameter</li> <li>Design Twitter-like Feed</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-swiggy-interview","title":"Questions asked in Swiggy interview","text":"<ul> <li>Implement Autocomplete System</li> <li>Design a Rate Limiter</li> <li>Serialize and Deserialize Binary Tree</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-flipkart-interview","title":"Questions asked in Flipkart interview","text":"<ul> <li>Design a URL Shortener</li> <li>Design Twitter Feed</li> <li>Design a File System</li> <li>Subarray Sum Equals K</li> <li>Design an Elevator System</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-ola-interview","title":"Questions asked in Ola interview","text":"<ul> <li>Design Twitter Feed</li> <li>Design an Elevator System</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-paytm-interview","title":"Questions asked in Paytm interview","text":"<ul> <li>Implement LFU Cache</li> <li>Design a Data Structure for the Stock Span Problem</li> <li>Implement Magic Dictionary</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-oyo-interview","title":"Questions asked in OYO interview","text":"<ul> <li>Design an Elevator System</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-whatsapp-interview","title":"Questions asked in WhatsApp interview","text":"<ul> <li>Design a Chat System</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-slack-interview","title":"Questions asked in Slack interview","text":"<ul> <li>Design a Chat System</li> </ul>"},{"location":"Interview-Questions/data-structures-algorithms/#questions-asked-in-airbnb-interview","title":"Questions asked in Airbnb interview","text":"<ul> <li>Design a URL Shortener</li> </ul>"},{"location":"Machine-Learning/ARIMA/","title":"\ud83d\udcd8 ARIMA (AutoRegressive Integrated Moving Average)","text":"<p>ARIMA is a powerful time series forecasting method that combines autoregression, differencing, and moving averages to model and predict sequential data patterns.</p> <p>Resources: Statsmodels ARIMA | Time Series Analysis Book</p>"},{"location":"Machine-Learning/ARIMA/#summary","title":"\u270d\ufe0f Summary","text":"<p>ARIMA (AutoRegressive Integrated Moving Average) is a statistical model used for analyzing and forecasting time series data. It's particularly effective for data that shows patterns over time but may not be stationary. ARIMA combines three components:</p> <ul> <li>AR (AutoRegressive): Uses the relationship between an observation and lagged observations</li> <li>I (Integrated): Uses differencing to make the time series stationary</li> <li>MA (Moving Average): Uses the dependency between an observation and residual errors from lagged observations</li> </ul> <p>ARIMA is widely used in: - Stock price forecasting - Sales prediction - Economic indicators analysis - Weather forecasting - Demand planning</p> <p>The model is denoted as ARIMA(p,d,q) where: - p: number of lag observations (AR terms) - d: degree of differencing (I terms) - q: size of moving average window (MA terms)</p>"},{"location":"Machine-Learning/ARIMA/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/ARIMA/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The ARIMA model can be understood through its three components:</p>"},{"location":"Machine-Learning/ARIMA/#1-autoregressive-ar-component","title":"1. AutoRegressive (AR) Component","text":"<p>The AR(p) model predicts future values based on past values:</p> \\[X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\phi_p X_{t-p} + \\epsilon_t\\] <p>Where: - \\(X_t\\) is the value at time t - \\(c\\) is a constant - \\(\\phi_i\\) are the autoregressive parameters - \\(\\epsilon_t\\) is white noise</p>"},{"location":"Machine-Learning/ARIMA/#2-integrated-i-component","title":"2. Integrated (I) Component","text":"<p>The I(d) component makes the series stationary by differencing:</p> \\[\\nabla^d X_t = (1-L)^d X_t\\] <p>Where: - \\(L\\) is the lag operator - \\(d\\) is the degree of differencing - First difference: \\(\\nabla X_t = X_t - X_{t-1}\\) - Second difference: \\(\\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1}\\)</p>"},{"location":"Machine-Learning/ARIMA/#3-moving-average-ma-component","title":"3. Moving Average (MA) Component","text":"<p>The MA(q) model uses past forecast errors:</p> \\[X_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q}\\] <p>Where: - \\(\\mu\\) is the mean - \\(\\theta_i\\) are the moving average parameters - \\(\\epsilon_t\\) are error terms</p>"},{"location":"Machine-Learning/ARIMA/#complete-arima-model","title":"Complete ARIMA Model","text":"<p>Combining all components, ARIMA(p,d,q) is:</p> \\[(1 - \\phi_1 L - ... - \\phi_p L^p)(1-L)^d X_t = (1 + \\theta_1 L + ... + \\theta_q L^q)\\epsilon_t\\]"},{"location":"Machine-Learning/ARIMA/#intuitive-understanding","title":"Intuitive Understanding","text":"<p>Think of ARIMA as answering three questions: 1. AR: How much do past values influence future values? 2. I: How many times do we need to difference the data to remove trends? 3. MA: How much do past prediction errors affect current predictions?</p>"},{"location":"Machine-Learning/ARIMA/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/ARIMA/#using-statsmodels","title":"Using Statsmodels","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate sample time series data\nnp.random.seed(42)\ndates = pd.date_range('2020-01-01', periods=200, freq='D')\ntrend = np.linspace(100, 120, 200)\nseasonal = 10 * np.sin(2 * np.pi * np.arange(200) / 30)\nnoise = np.random.normal(0, 2, 200)\nts = trend + seasonal + noise\n\n# Create time series\ndata = pd.Series(ts, index=dates)\n\n# Step 1: Check stationarity\ndef check_stationarity(timeseries, title):\n    # Perform Augmented Dickey-Fuller test\n    result = adfuller(timeseries)\n    print(f'Results of Dickey-Fuller Test for {title}:')\n    print(f'ADF Statistic: {result[0]:.6f}')\n    print(f'p-value: {result[1]:.6f}')\n    print(f'Critical Values:')\n    for key, value in result[4].items():\n        print(f'\\t{key}: {value:.3f}')\n\n    if result[1] &lt;= 0.05:\n        print(\"Data is stationary\")\n    else:\n        print(\"Data is non-stationary\")\n    print(\"-\" * 50)\n\ncheck_stationarity(data, \"Original Series\")\n\n# Step 2: Make series stationary if needed\ndata_diff = data.diff().dropna()\ncheck_stationarity(data_diff, \"First Differenced Series\")\n\n# Step 3: Determine ARIMA parameters using ACF and PACF plots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Original series\naxes[0,0].plot(data)\naxes[0,0].set_title('Original Time Series')\naxes[0,0].set_xlabel('Date')\naxes[0,0].set_ylabel('Value')\n\n# Differenced series\naxes[0,1].plot(data_diff)\naxes[0,1].set_title('First Differenced Series')\naxes[0,1].set_xlabel('Date')\naxes[0,1].set_ylabel('Differenced Value')\n\n# ACF and PACF plots\nplot_acf(data_diff, ax=axes[1,0], lags=20, title='ACF of Differenced Series')\nplot_pacf(data_diff, ax=axes[1,1], lags=20, title='PACF of Differenced Series')\n\nplt.tight_layout()\nplt.show()\n\n# Step 4: Fit ARIMA model\n# Let's try ARIMA(2,1,2) based on the plots\nmodel = ARIMA(data, order=(2, 1, 2))\nfitted_model = model.fit()\n\n# Print model summary\nprint(fitted_model.summary())\n\n# Step 5: Make predictions\nn_periods = 30\nforecast = fitted_model.forecast(steps=n_periods)\nforecast_index = pd.date_range(start=data.index[-1] + pd.Timedelta(days=1), \n                               periods=n_periods, freq='D')\n\n# Get confidence intervals\nforecast_ci = fitted_model.get_forecast(steps=n_periods).conf_int()\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data, label='Original Data', color='blue')\nplt.plot(forecast_index, forecast, label='Forecast', color='red', linestyle='--')\nplt.fill_between(forecast_index, \n                 forecast_ci.iloc[:, 0], \n                 forecast_ci.iloc[:, 1], \n                 color='red', alpha=0.2, label='Confidence Interval')\nplt.legend()\nplt.title('ARIMA Forecast')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.grid(True)\nplt.show()\n\n# Step 6: Model diagnostics\nfitted_model.plot_diagnostics(figsize=(15, 8))\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/ARIMA/#auto-arima-for-parameter-selection","title":"Auto ARIMA for Parameter Selection","text":"<pre><code>from pmdarima import auto_arima\n\n# Automatically find best ARIMA parameters\nauto_model = auto_arima(data, \n                        start_p=0, start_q=0,\n                        max_p=5, max_q=5,\n                        seasonal=False,\n                        stepwise=True,\n                        suppress_warnings=True,\n                        error_action='ignore')\n\nprint(f\"Best ARIMA model: {auto_model.order}\")\nprint(auto_model.summary())\n\n# Forecast with auto ARIMA\nauto_forecast = auto_model.predict(n_periods=30)\nprint(f\"Next 30 predictions: {auto_forecast}\")\n</code></pre>"},{"location":"Machine-Learning/ARIMA/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\nclass ARIMAFromScratch:\n    def __init__(self, p=1, d=1, q=1):\n        \"\"\"\n        ARIMA model implementation from scratch\n\n        Parameters:\n        p (int): Order of autoregression\n        d (int): Degree of differencing\n        q (int): Order of moving average\n        \"\"\"\n        self.p = p\n        self.d = d\n        self.q = q\n        self.params = None\n        self.fitted_values = None\n        self.residuals = None\n\n    def difference(self, series, d):\n        \"\"\"Apply differencing to make series stationary\"\"\"\n        diff_series = series.copy()\n        for _ in range(d):\n            diff_series = np.diff(diff_series)\n        return diff_series\n\n    def inverse_difference(self, diff_series, original_series, d):\n        \"\"\"Reverse the differencing operation\"\"\"\n        result = diff_series.copy()\n        for _ in range(d):\n            # Add back the last value from previous level\n            cumsum_result = np.cumsum(result)\n            # Add the last original value before differencing\n            result = cumsum_result + original_series[-(d-_)]\n        return result\n\n    def ar_component(self, data, ar_params):\n        \"\"\"Calculate AR component\"\"\"\n        ar_component = np.zeros(len(data))\n        for i in range(self.p, len(data)):\n            for j in range(self.p):\n                ar_component[i] += ar_params[j] * data[i-j-1]\n        return ar_component\n\n    def ma_component(self, residuals, ma_params):\n        \"\"\"Calculate MA component\"\"\"\n        ma_component = np.zeros(len(residuals))\n        for i in range(self.q, len(residuals)):\n            for j in range(self.q):\n                ma_component[i] += ma_params[j] * residuals[i-j-1]\n        return ma_component\n\n    def likelihood(self, params, data):\n        \"\"\"Calculate negative log-likelihood for optimization\"\"\"\n        try:\n            # Split parameters\n            ar_params = params[:self.p] if self.p &gt; 0 else []\n            ma_params = params[self.p:self.p + self.q] if self.q &gt; 0 else []\n            sigma = params[-1] if len(params) &gt; self.p + self.q else 1.0\n\n            n = len(data)\n            errors = np.zeros(n)\n            predictions = np.zeros(n)\n\n            # Initialize predictions and errors\n            for i in range(max(self.p, self.q), n):\n                # AR component\n                ar_pred = 0\n                if self.p &gt; 0:\n                    for j in range(self.p):\n                        ar_pred += ar_params[j] * data[i-j-1]\n\n                # MA component\n                ma_pred = 0\n                if self.q &gt; 0:\n                    for j in range(self.q):\n                        ma_pred += ma_params[j] * errors[i-j-1]\n\n                predictions[i] = ar_pred + ma_pred\n                errors[i] = data[i] - predictions[i]\n\n            # Calculate log-likelihood\n            valid_errors = errors[max(self.p, self.q):]\n            log_likelihood = -0.5 * len(valid_errors) * np.log(2 * np.pi * sigma**2)\n            log_likelihood -= 0.5 * np.sum(valid_errors**2) / (sigma**2)\n\n            return -log_likelihood  # Return negative for minimization\n\n        except:\n            return np.inf\n\n    def fit(self, data):\n        \"\"\"Fit ARIMA model to data\"\"\"\n        # Apply differencing\n        if self.d &gt; 0:\n            diff_data = self.difference(data, self.d)\n        else:\n            diff_data = data\n\n        # Initial parameter guesses\n        initial_params = []\n\n        # AR parameters (between -1 and 1)\n        initial_params.extend([0.1] * self.p)\n\n        # MA parameters (between -1 and 1)\n        initial_params.extend([0.1] * self.q)\n\n        # Sigma (positive)\n        initial_params.append(1.0)\n\n        # Bounds for parameters\n        bounds = []\n        bounds.extend([(-0.99, 0.99)] * self.p)  # AR params\n        bounds.extend([(-0.99, 0.99)] * self.q)  # MA params\n        bounds.append((0.01, None))  # Sigma\n\n        # Optimize parameters\n        try:\n            result = minimize(self.likelihood, initial_params, args=(diff_data,),\n                            method='L-BFGS-B', bounds=bounds)\n\n            if result.success:\n                self.params = result.x\n\n                # Calculate fitted values and residuals\n                self._calculate_fitted_values(data)\n\n                return self\n            else:\n                raise ValueError(\"Optimization failed\")\n\n        except Exception as e:\n            print(f\"Error during fitting: {e}\")\n            return None\n\n    def _calculate_fitted_values(self, data):\n        \"\"\"Calculate fitted values and residuals\"\"\"\n        if self.params is None:\n            return\n\n        # Apply differencing\n        if self.d &gt; 0:\n            diff_data = self.difference(data, self.d)\n        else:\n            diff_data = data\n\n        # Get parameters\n        ar_params = self.params[:self.p] if self.p &gt; 0 else []\n        ma_params = self.params[self.p:self.p + self.q] if self.q &gt; 0 else []\n\n        n = len(diff_data)\n        fitted = np.zeros(n)\n        errors = np.zeros(n)\n\n        # Calculate fitted values\n        for i in range(max(self.p, self.q), n):\n            # AR component\n            ar_pred = 0\n            if self.p &gt; 0:\n                for j in range(self.p):\n                    ar_pred += ar_params[j] * diff_data[i-j-1]\n\n            # MA component\n            ma_pred = 0\n            if self.q &gt; 0:\n                for j in range(self.q):\n                    ma_pred += ma_params[j] * errors[i-j-1]\n\n            fitted[i] = ar_pred + ma_pred\n            errors[i] = diff_data[i] - fitted[i]\n\n        self.fitted_values = fitted\n        self.residuals = errors\n\n    def predict(self, steps):\n        \"\"\"Make predictions for future time steps\"\"\"\n        if self.params is None:\n            raise ValueError(\"Model must be fitted before prediction\")\n\n        ar_params = self.params[:self.p] if self.p &gt; 0 else []\n        ma_params = self.params[self.p:self.p + self.q] if self.q &gt; 0 else []\n\n        predictions = []\n\n        # Use last values from fitted data for prediction\n        last_values = self.fitted_values[-self.p:] if self.p &gt; 0 else []\n        last_errors = self.residuals[-self.q:] if self.q &gt; 0 else []\n\n        for step in range(steps):\n            # AR component\n            ar_pred = 0\n            if self.p &gt; 0 and len(last_values) &gt;= self.p:\n                for j in range(self.p):\n                    ar_pred += ar_params[j] * last_values[-(j+1)]\n\n            # MA component (assumes future errors are 0)\n            ma_pred = 0\n            if self.q &gt; 0 and len(last_errors) &gt;= self.q and step == 0:\n                for j in range(self.q):\n                    ma_pred += ma_params[j] * last_errors[-(j+1)]\n\n            pred = ar_pred + ma_pred\n            predictions.append(pred)\n\n            # Update last values for next prediction\n            if self.p &gt; 0:\n                last_values = np.append(last_values[1:], pred)\n            if self.q &gt; 0:\n                last_errors = np.append(last_errors[1:], 0)  # Assume future errors are 0\n\n        return np.array(predictions)\n\n    def summary(self):\n        \"\"\"Print model summary\"\"\"\n        if self.params is None:\n            print(\"Model not fitted\")\n            return\n\n        print(f\"ARIMA({self.p}, {self.d}, {self.q}) Model Summary\")\n        print(\"=\" * 50)\n\n        if self.p &gt; 0:\n            print(\"AR Parameters:\")\n            for i, param in enumerate(self.params[:self.p]):\n                print(f\"  AR({i+1}): {param:.6f}\")\n\n        if self.q &gt; 0:\n            print(\"MA Parameters:\")\n            for i, param in enumerate(self.params[self.p:self.p + self.q]):\n                print(f\"  MA({i+1}): {param:.6f}\")\n\n        print(f\"Sigma: {self.params[-1]:.6f}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample data\n    np.random.seed(42)\n    n = 100\n    true_ar = [0.7, -0.2]\n    true_ma = [0.3]\n\n    # Generate ARIMA(2,1,1) data\n    data = []\n    errors = np.random.normal(0, 1, n + 10)\n\n    for t in range(2, n):\n        if t &lt; 3:\n            value = errors[t]\n        else:\n            ar_component = true_ar[0] * (data[t-1] - data[t-2]) + true_ar[1] * (data[t-2] - data[t-3]) if t &gt; 2 else 0\n            ma_component = true_ma[0] * errors[t-1] if t &gt; 0 else 0\n            value = ar_component + ma_component + errors[t]\n            if t &gt; 0:\n                value += data[t-1]  # Add integration\n        data.append(value)\n\n    data = np.array(data)\n\n    # Fit custom ARIMA model\n    model = ARIMAFromScratch(p=2, d=1, q=1)\n    fitted_model = model.fit(data)\n\n    if fitted_model:\n        fitted_model.summary()\n\n        # Make predictions\n        predictions = fitted_model.predict(10)\n        print(f\"\\nNext 10 predictions: {predictions}\")\n</code></pre>"},{"location":"Machine-Learning/ARIMA/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/ARIMA/#assumptions","title":"Assumptions","text":"<ol> <li>Stationarity: After differencing, the series should be stationary (constant mean and variance)</li> <li>Linear relationships: ARIMA assumes linear dependencies between observations</li> <li>Normal residuals: Error terms should be normally distributed with zero mean</li> <li>Homoscedasticity: Constant variance of residuals over time</li> <li>No autocorrelation in residuals: Residuals should be independent</li> </ol>"},{"location":"Machine-Learning/ARIMA/#limitations","title":"Limitations","text":"<ol> <li>Linear models only: Cannot capture non-linear patterns</li> <li>Requires sufficient data: Needs adequate historical data for reliable forecasting</li> <li>Parameter selection complexity: Choosing optimal (p,d,q) can be challenging</li> <li>Poor with structural breaks: Struggles when underlying data patterns change</li> <li>No exogenous variables: Basic ARIMA doesn't include external predictors</li> <li>Computational intensity: Parameter estimation can be slow for large datasets</li> </ol>"},{"location":"Machine-Learning/ARIMA/#comparison-with-other-models","title":"Comparison with Other Models","text":"Model Strengths Weaknesses ARIMA Good for linear trends, well-established theory Limited to linear relationships LSTM Captures complex patterns, handles non-linearity Requires large datasets, black box Prophet Handles seasonality well, robust to outliers Less flexible than ARIMA Exponential Smoothing Simple, fast computation Limited complexity modeling"},{"location":"Machine-Learning/ARIMA/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. What does each parameter in ARIMA(p,d,q) represent? <p>Answer:  - p (AR order): Number of lagged observations used in the autoregressive component. Determines how many past values influence the current prediction. - d (Differencing degree): Number of times the series is differenced to achieve stationarity. Usually 0, 1, or 2. - q (MA order): Size of the moving average window, representing how many past forecast errors are used in the prediction.</p> <p>Example: ARIMA(2,1,1) uses 2 lagged values, applies 1 level of differencing, and includes 1 past error term.</p> 2. How do you determine the optimal ARIMA parameters? <p>Answer: Several methods can be used:</p> <p>Manual approach: - Use ACF and PACF plots to identify parameters - ACF helps determine q (cuts off after q lags for MA processes) - PACF helps determine p (cuts off after p lags for AR processes) - Use ADF test to determine d (degree of differencing needed for stationarity)</p> <p>Automatic approach: - Information criteria (AIC, BIC) - lower values indicate better fit - Grid search over parameter combinations - Auto ARIMA algorithms (like <code>pmdarima.auto_arima()</code>) - Cross-validation for out-of-sample performance</p> 3. What is the difference between MA and AR components? <p>Answer:</p> <p>Autoregressive (AR): - Uses past values of the series itself for prediction - Formula: \\(X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + ... + \\epsilon_t\\) - Captures the \"memory\" of the time series</p> <p>Moving Average (MA): - Uses past forecast errors (residuals) for prediction - Formula: \\(X_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ...\\) - Captures short-term irregularities and shocks</p> 4. How do you check if your ARIMA model is good? <p>Answer:</p> <p>Residual Analysis: - Residuals should be white noise (no patterns) - Ljung-Box test for autocorrelation in residuals - Jarque-Bera test for normality of residuals - Plot ACF/PACF of residuals (should show no significant lags)</p> <p>Performance Metrics: - AIC/BIC for model comparison - MAPE, RMSE, MAE for forecast accuracy - Out-of-sample validation</p> <p>Visual Inspection: - Q-Q plots for normality - Residual plots over time - Fitted vs actual values plot</p> 5. What makes a time series stationary and why is it important for ARIMA? <p>Answer:</p> <p>Stationary Series Properties: - Constant mean over time - Constant variance over time - Covariance between periods depends only on lag, not time</p> <p>Importance for ARIMA: - ARIMA models assume stationarity for reliable parameter estimation - Non-stationary data can lead to spurious relationships - Differencing (I component) is used to achieve stationarity</p> <p>Tests for Stationarity: - Augmented Dickey-Fuller (ADF) test - KPSS test - Visual inspection of plots</p> 6. Explain the concept of differencing in ARIMA <p>Answer:</p> <p>Differencing transforms a non-stationary series into stationary by computing differences between consecutive observations.</p> <p>Types: - First differencing: \\(\\nabla X_t = X_t - X_{t-1}\\) - Second differencing: \\(\\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1}\\) - Seasonal differencing: \\(\\nabla_s X_t = X_t - X_{t-s}\\)</p> <p>Effects: - Removes trends (first differencing) - Removes curvature (second differencing) - Usually d=1 is sufficient, rarely need d&gt;2 - Over-differencing can introduce unnecessary complexity</p> 7. How would you handle seasonality in ARIMA? <p>Answer:</p> <p>SARIMA (Seasonal ARIMA): - Extends ARIMA to SARIMA(p,d,q)(P,D,Q)s - Additional seasonal parameters P, D, Q for period s - Formula includes both non-seasonal and seasonal components</p> <p>Implementation: <pre><code>from statsmodels.tsa.statespace.sarimax import SARIMAX\nmodel = SARIMAX(data, order=(p,d,q), seasonal_order=(P,D,Q,s))\n</code></pre></p> <p>Alternative approaches: - Seasonal decomposition before applying ARIMA - Use of external regressors for seasonal patterns - Prophet or other specialized seasonal models</p> 8. What are the limitations of ARIMA and when would you choose alternatives? <p>Answer:</p> <p>ARIMA Limitations: - Only captures linear relationships - Requires stationary data - Sensitive to outliers - Cannot handle multiple seasonal patterns - No external variables incorporation</p> <p>When to choose alternatives: - Non-linear patterns: Use LSTM, Neural Networks - Multiple seasonalities: Use Prophet, TBATS - External predictors: Use ARIMAX, VAR models - Regime changes: Use structural break models - High-frequency data: Use GARCH for volatility modeling - Small datasets: Use Exponential Smoothing</p>"},{"location":"Machine-Learning/ARIMA/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/ARIMA/#example-1-stock-price-forecasting","title":"Example 1: Stock Price Forecasting","text":"<pre><code>import yfinance as yf\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Download stock data\nstock = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2023-01-01\")\nprices = stock['Close']\n\n# Fit ARIMA model\nmodel = ARIMA(prices, order=(1,1,1))\nfitted_model = model.fit()\n\n# Forecast next 30 days\nforecast = fitted_model.forecast(steps=30)\nconf_int = fitted_model.get_forecast(steps=30).conf_int()\n\n# Plot results\nplt.figure(figsize=(12, 6))\nplt.plot(prices.index[-60:], prices.values[-60:], label='Actual', color='blue')\nforecast_dates = pd.date_range(start=prices.index[-1], periods=31, freq='D')[1:]\nplt.plot(forecast_dates, forecast, label='Forecast', color='red', linestyle='--')\nplt.fill_between(forecast_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1], \n                 color='red', alpha=0.2)\nplt.legend()\nplt.title('AAPL Stock Price Forecast using ARIMA')\nplt.show()\n\nprint(\"Forecast Summary:\")\nprint(f\"Last actual price: ${prices.iloc[-1]:.2f}\")\nprint(f\"30-day forecast mean: ${forecast.mean():.2f}\")\nprint(f\"Expected return: {((forecast.mean()/prices.iloc[-1])-1)*100:.2f}%\")\n</code></pre>"},{"location":"Machine-Learning/ARIMA/#example-2-sales-forecasting-with-seasonality","title":"Example 2: Sales Forecasting with Seasonality","text":"<pre><code># Generate seasonal sales data\nnp.random.seed(42)\ndates = pd.date_range('2018-01-01', periods=365*3, freq='D')\ntrend = np.linspace(1000, 1500, len(dates))\nseasonal = 200 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\nweekly = 50 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)\nnoise = np.random.normal(0, 50, len(dates))\nsales = trend + seasonal + weekly + noise\n\nsales_ts = pd.Series(sales, index=dates)\n\n# Apply seasonal ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Fit SARIMA model\nsarima_model = SARIMAX(sales_ts, \n                       order=(1, 1, 1), \n                       seasonal_order=(1, 1, 1, 365))\nsarima_fitted = sarima_model.fit(disp=False)\n\n# Forecast next quarter\nforecast_days = 90\nforecast = sarima_fitted.forecast(steps=forecast_days)\nconf_int = sarima_fitted.get_forecast(steps=forecast_days).conf_int()\n\n# Performance metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# In-sample predictions for evaluation\nin_sample_pred = sarima_fitted.fittedvalues\nmae = mean_absolute_error(sales_ts, in_sample_pred)\nrmse = np.sqrt(mean_squared_error(sales_ts, in_sample_pred))\n\nprint(f\"Model Performance:\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"Mean Sales: {sales_ts.mean():.2f}\")\nprint(f\"MAPE: {(mae/sales_ts.mean()*100):.2f}%\")\n\n# Plot seasonal forecast\nplt.figure(figsize=(15, 8))\nplt.plot(sales_ts.index[-180:], sales_ts.values[-180:], \n         label='Historical Sales', color='blue')\nforecast_dates = pd.date_range(start=sales_ts.index[-1], \n                               periods=forecast_days+1, freq='D')[1:]\nplt.plot(forecast_dates, forecast, \n         label='SARIMA Forecast', color='red', linestyle='--')\nplt.fill_between(forecast_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1],\n                 color='red', alpha=0.2, label='Confidence Interval')\nplt.legend()\nplt.title('Seasonal Sales Forecasting with SARIMA')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/ARIMA/#references","title":"\ud83d\udcda References","text":"<ul> <li>Books:</li> <li>Forecasting: Principles and Practice by Rob Hyndman &amp; George Athanasopoulos</li> <li>Time Series Analysis by Box, Jenkins, Reinsel &amp; Ljung</li> <li> <p>Introduction to Time Series and Forecasting by Brockwell &amp; Davis</p> </li> <li> <p>Documentation:</p> </li> <li>Statsmodels ARIMA</li> <li>pmdarima (Auto ARIMA)</li> <li> <p>Scikit-learn Time Series</p> </li> <li> <p>Tutorials:</p> </li> <li>ARIMA Model Complete Guide</li> <li> <p>Time Series Forecasting Guide</p> </li> <li> <p>Research Papers:</p> </li> <li>Box, G. E. P., &amp; Jenkins, G. M. (1970). Time series analysis: Forecasting and control</li> <li> <p>Akaike, H. (1974). A new look at the statistical model identification</p> </li> <li> <p>Online Courses:</p> </li> <li>Time Series Analysis on Coursera</li> <li>Forecasting Using R on DataCamp</li> </ul>"},{"location":"Machine-Learning/Activation%20functions/","title":"\ud83d\udcd8 Activation Functions in Neural Networks","text":"<p>Activation functions are mathematical functions that determine the output of neural network nodes, introducing non-linearity to enable networks to learn complex patterns and relationships in data.</p> <p>Resources: Deep Learning Book - Chapter 6 | CS231n Activation Functions</p>"},{"location":"Machine-Learning/Activation%20functions/#summary","title":"\u270d\ufe0f Summary","text":"<p>Activation functions are crucial components of neural networks that determine whether a neuron should be activated (fired) based on the weighted sum of its inputs. They introduce non-linearity into the network, allowing it to learn and represent complex patterns that linear models cannot capture.</p> <p>Key purposes of activation functions: - Non-linearity: Enable networks to learn complex, non-linear relationships - Gradient flow: Control how gradients flow during backpropagation - Output range: Normalize outputs to specific ranges (e.g., 0-1, -1-1) - Decision boundaries: Help create complex decision boundaries for classification</p> <p>Common applications: - Hidden layers in deep neural networks - Output layers for classification and regression - Convolutional neural networks (CNNs) - Recurrent neural networks (RNNs) - Transformer models</p> <p>Without activation functions, neural networks would be equivalent to linear regression, regardless of depth.</p>"},{"location":"Machine-Learning/Activation%20functions/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Activation%20functions/#why-activation-functions-are-necessary","title":"Why Activation Functions are Necessary","text":"<p>Consider a simple neural network without activation functions: \\(\\(h_1 = W_1 x + b_1\\)\\) \\(\\(h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + W_2 b_1 + b_2\\)\\)</p> <p>This reduces to a linear transformation, equivalent to: \\(h_2 = W x + b\\) where \\(W = W_2 W_1\\) and \\(b = W_2 b_1 + b_2\\).</p>"},{"location":"Machine-Learning/Activation%20functions/#mathematical-properties","title":"Mathematical Properties","text":"<p>A good activation function should have:</p> <ol> <li>Non-linearity: \\(f(ax + by) \\neq af(x) + bf(y)\\)</li> <li>Differentiability: Must be differentiable for gradient-based optimization</li> <li>Monotonicity: Often preferred to preserve input ordering</li> <li>Bounded range: Helps prevent exploding gradients</li> <li>Zero-centered: Helps with gradient flow</li> </ol>"},{"location":"Machine-Learning/Activation%20functions/#common-activation-functions","title":"Common Activation Functions","text":""},{"location":"Machine-Learning/Activation%20functions/#1-sigmoid-logistic","title":"1. Sigmoid (Logistic)","text":"\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <p>Properties: - Range: (0, 1) - S-shaped curve - Smooth and differentiable - Derivative: \\(\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\\)</p>"},{"location":"Machine-Learning/Activation%20functions/#2-hyperbolic-tangent-tanh","title":"2. Hyperbolic Tangent (tanh)","text":"\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{2}{1 + e^{-2x}} - 1\\] <p>Properties: - Range: (-1, 1) - Zero-centered (unlike sigmoid) - Derivative: \\(\\tanh'(x) = 1 - \\tanh^2(x)\\)</p>"},{"location":"Machine-Learning/Activation%20functions/#3-relu-rectified-linear-unit","title":"3. ReLU (Rectified Linear Unit)","text":"\\[\\text{ReLU}(x) = \\max(0, x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0  \\end{cases}\\] <p>Properties: - Range: [0, \u221e) - Computationally efficient - Helps mitigate vanishing gradient problem - Derivative: \\(\\text{ReLU}'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p>"},{"location":"Machine-Learning/Activation%20functions/#4-leaky-relu","title":"4. Leaky ReLU","text":"\\[\\text{LeakyReLU}(x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0  \\end{cases}\\] <p>Where \\(\\alpha\\) is a small positive constant (typically 0.01).</p>"},{"location":"Machine-Learning/Activation%20functions/#5-elu-exponential-linear-unit","title":"5. ELU (Exponential Linear Unit)","text":"\\[\\text{ELU}(x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ \\alpha(e^x - 1) &amp; \\text{if } x \\leq 0  \\end{cases}\\]"},{"location":"Machine-Learning/Activation%20functions/#6-swishsilu","title":"6. Swish/SiLU","text":"\\[\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\\]"},{"location":"Machine-Learning/Activation%20functions/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Activation%20functions/#using-tensorflowkeras","title":"Using TensorFlow/Keras","text":"<pre><code>import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define input range\nx = np.linspace(-5, 5, 1000)\n\n# TensorFlow activation functions\nactivations = {\n    'sigmoid': tf.nn.sigmoid,\n    'tanh': tf.nn.tanh,\n    'relu': tf.nn.relu,\n    'leaky_relu': lambda x: tf.nn.leaky_relu(x, alpha=0.01),\n    'elu': tf.nn.elu,\n    'swish': tf.nn.swish,\n    'gelu': tf.nn.gelu,\n    'softplus': tf.nn.softplus\n}\n\n# Plot activation functions\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.ravel()\n\nfor i, (name, func) in enumerate(activations.items()):\n    y = func(x).numpy()\n    axes[i].plot(x, y, linewidth=2)\n    axes[i].set_title(f'{name.upper()}')\n    axes[i].grid(True, alpha=0.3)\n    axes[i].axhline(y=0, color='k', linewidth=0.5)\n    axes[i].axvline(x=0, color='k', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# Example neural network with different activations\ndef create_model(activation):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=activation, input_shape=(10,)),\n        tf.keras.layers.Dense(32, activation=activation),\n        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n    ])\n    return model\n\n# Compare training with different activations\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Test different activations\nactivation_results = {}\nactivations_to_test = ['relu', 'tanh', 'sigmoid', 'elu']\n\nfor activation in activations_to_test:\n    print(f\"Training with {activation} activation...\")\n\n    model = create_model(activation)\n    model.compile(optimizer='adam', \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n\n    history = model.fit(X_train, y_train, \n                       epochs=50, \n                       batch_size=32, \n                       validation_data=(X_test, y_test),\n                       verbose=0)\n\n    # Store results\n    activation_results[activation] = {\n        'history': history,\n        'final_accuracy': history.history['val_accuracy'][-1]\n    }\n\n# Plot training curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nfor activation, results in activation_results.items():\n    history = results['history']\n    ax1.plot(history.history['loss'], label=f'{activation} - train')\n    ax1.plot(history.history['val_loss'], label=f'{activation} - val', linestyle='--')\n\n    ax2.plot(history.history['accuracy'], label=f'{activation} - train')\n    ax2.plot(history.history['val_accuracy'], label=f'{activation} - val', linestyle='--')\n\nax1.set_title('Loss Curves')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True)\n\nax2.set_title('Accuracy Curves')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print final accuracies\nprint(\"\\nFinal Validation Accuracies:\")\nfor activation, results in activation_results.items():\n    print(f\"{activation}: {results['final_accuracy']:.4f}\")\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#using-pytorch","title":"Using PyTorch","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# Define activation functions in PyTorch\nclass ActivationShowcase(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, activation_type):\n        if activation_type == 'sigmoid':\n            return torch.sigmoid(x)\n        elif activation_type == 'tanh':\n            return torch.tanh(x)\n        elif activation_type == 'relu':\n            return F.relu(x)\n        elif activation_type == 'leaky_relu':\n            return F.leaky_relu(x, negative_slope=0.01)\n        elif activation_type == 'elu':\n            return F.elu(x)\n        elif activation_type == 'gelu':\n            return F.gelu(x)\n        elif activation_type == 'swish':\n            return x * torch.sigmoid(x)\n        else:\n            return x\n\n# Visualize derivatives\ndef compute_gradients():\n    x = torch.linspace(-5, 5, 1000, requires_grad=True)\n    showcase = ActivationShowcase()\n\n    activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu', 'elu', 'gelu']\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n\n    for i, activation in enumerate(activations):\n        # Forward pass\n        y = showcase(x, activation)\n\n        # Compute gradients\n        y.sum().backward(retain_graph=True)\n        gradients = x.grad.clone()\n        x.grad.zero_()\n\n        # Plot function and its derivative\n        axes[i].plot(x.detach().numpy(), y.detach().numpy(), \n                     label=f'{activation}', linewidth=2)\n        axes[i].plot(x.detach().numpy(), gradients.numpy(), \n                     label=f'{activation} derivative', linewidth=2, linestyle='--')\n        axes[i].set_title(f'{activation.upper()} and its derivative')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n        axes[i].axhline(y=0, color='k', linewidth=0.5)\n        axes[i].axvline(x=0, color='k', linewidth=0.5)\n\n    plt.tight_layout()\n    plt.show()\n\ncompute_gradients()\n\n# Neural network with custom activation\nclass CustomNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        self.activation = activation\n        self.showcase = ActivationShowcase()\n\n    def forward(self, x):\n        x = self.showcase(self.fc1(x), self.activation)\n        x = self.showcase(self.fc2(x), self.activation)\n        x = torch.sigmoid(self.fc3(x))  # Output activation\n        return x\n\n# Test gradient flow with different activations\ndef test_gradient_flow():\n    # Create deep network\n    input_size, hidden_size, output_size = 10, 128, 1\n    activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n\n    results = {}\n\n    for activation in activations:\n        print(f\"Testing gradient flow with {activation}...\")\n\n        # Create model\n        model = CustomNN(input_size, hidden_size, output_size, activation)\n\n        # Create dummy data\n        x = torch.randn(32, input_size)\n        y = torch.randint(0, 2, (32, 1)).float()\n\n        # Forward pass\n        output = model(x)\n        loss = F.binary_cross_entropy(output, y)\n\n        # Backward pass\n        loss.backward()\n\n        # Collect gradient statistics\n        gradients = []\n        for param in model.parameters():\n            if param.grad is not None:\n                gradients.extend(param.grad.flatten().tolist())\n\n        results[activation] = {\n            'mean_grad': np.mean(np.abs(gradients)),\n            'std_grad': np.std(gradients),\n            'max_grad': np.max(np.abs(gradients))\n        }\n\n        # Clear gradients\n        model.zero_grad()\n\n    # Print results\n    print(\"\\nGradient Flow Analysis:\")\n    print(\"Activation | Mean |Grad| | Std Grad | Max |Grad|\")\n    print(\"-\" * 50)\n    for activation, stats in results.items():\n        print(f\"{activation:10} | {stats['mean_grad']:.6f} | {stats['std_grad']:.6f} | {stats['max_grad']:.6f}\")\n\ntest_gradient_flow()\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass ActivationFunctions:\n    \"\"\"Complete implementation of activation functions from scratch\"\"\"\n\n    @staticmethod\n    def sigmoid(x):\n        \"\"\"Sigmoid activation function\"\"\"\n        # Clip x to prevent overflow\n        x = np.clip(x, -500, 500)\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def sigmoid_derivative(x):\n        \"\"\"Derivative of sigmoid function\"\"\"\n        s = ActivationFunctions.sigmoid(x)\n        return s * (1 - s)\n\n    @staticmethod\n    def tanh(x):\n        \"\"\"Hyperbolic tangent activation function\"\"\"\n        return np.tanh(x)\n\n    @staticmethod\n    def tanh_derivative(x):\n        \"\"\"Derivative of tanh function\"\"\"\n        return 1 - np.tanh(x) ** 2\n\n    @staticmethod\n    def relu(x):\n        \"\"\"ReLU activation function\"\"\"\n        return np.maximum(0, x)\n\n    @staticmethod\n    def relu_derivative(x):\n        \"\"\"Derivative of ReLU function\"\"\"\n        return (x &gt; 0).astype(float)\n\n    @staticmethod\n    def leaky_relu(x, alpha=0.01):\n        \"\"\"Leaky ReLU activation function\"\"\"\n        return np.where(x &gt; 0, x, alpha * x)\n\n    @staticmethod\n    def leaky_relu_derivative(x, alpha=0.01):\n        \"\"\"Derivative of Leaky ReLU function\"\"\"\n        return np.where(x &gt; 0, 1, alpha)\n\n    @staticmethod\n    def elu(x, alpha=1.0):\n        \"\"\"ELU activation function\"\"\"\n        return np.where(x &gt; 0, x, alpha * (np.exp(x) - 1))\n\n    @staticmethod\n    def elu_derivative(x, alpha=1.0):\n        \"\"\"Derivative of ELU function\"\"\"\n        return np.where(x &gt; 0, 1, alpha * np.exp(x))\n\n    @staticmethod\n    def swish(x):\n        \"\"\"Swish activation function\"\"\"\n        return x * ActivationFunctions.sigmoid(x)\n\n    @staticmethod\n    def swish_derivative(x):\n        \"\"\"Derivative of Swish function\"\"\"\n        sigmoid_x = ActivationFunctions.sigmoid(x)\n        return sigmoid_x + x * sigmoid_x * (1 - sigmoid_x)\n\n    @staticmethod\n    def softplus(x):\n        \"\"\"Softplus activation function\"\"\"\n        # Use log(1 + exp(x)) but handle large values to prevent overflow\n        return np.where(x &gt; 20, x, np.log(1 + np.exp(x)))\n\n    @staticmethod\n    def softplus_derivative(x):\n        \"\"\"Derivative of Softplus function\"\"\"\n        return ActivationFunctions.sigmoid(x)\n\n    @staticmethod\n    def gelu(x):\n        \"\"\"GELU activation function (approximation)\"\"\"\n        return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n\n    @staticmethod\n    def gelu_derivative(x):\n        \"\"\"Derivative of GELU function (approximation)\"\"\"\n        tanh_term = np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3))\n        sech_term = 1 - tanh_term**2\n        return 0.5 * (1 + tanh_term) + 0.5 * x * sech_term * np.sqrt(2/np.pi) * (1 + 3 * 0.044715 * x**2)\n\nclass NeuralNetwork:\n    \"\"\"Simple neural network implementation with custom activation functions\"\"\"\n\n    def __init__(self, layers, activation='relu'):\n        \"\"\"\n        Initialize neural network\n\n        Parameters:\n        layers: list of layer sizes [input, hidden1, hidden2, ..., output]\n        activation: activation function name\n        \"\"\"\n        self.layers = layers\n        self.activation = activation\n        self.act_func = ActivationFunctions()\n\n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n\n        for i in range(len(layers) - 1):\n            # Xavier initialization\n            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n\n    def get_activation_function(self, name):\n        \"\"\"Get activation function and its derivative\"\"\"\n        functions = {\n            'sigmoid': (self.act_func.sigmoid, self.act_func.sigmoid_derivative),\n            'tanh': (self.act_func.tanh, self.act_func.tanh_derivative),\n            'relu': (self.act_func.relu, self.act_func.relu_derivative),\n            'leaky_relu': (self.act_func.leaky_relu, self.act_func.leaky_relu_derivative),\n            'elu': (self.act_func.elu, self.act_func.elu_derivative),\n            'swish': (self.act_func.swish, self.act_func.swish_derivative)\n        }\n        return functions.get(name, (self.act_func.relu, self.act_func.relu_derivative))\n\n    def forward(self, X):\n        \"\"\"Forward propagation\"\"\"\n        self.activations = [X]\n        self.z_values = []\n\n        activation_func, _ = self.get_activation_function(self.activation)\n\n        for i in range(len(self.weights)):\n            # Linear transformation\n            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n\n            # Apply activation function (except for output layer)\n            if i &lt; len(self.weights) - 1:\n                a = activation_func(z)\n            else:\n                # Output layer - use sigmoid for binary classification\n                a = self.act_func.sigmoid(z)\n\n            self.activations.append(a)\n\n        return self.activations[-1]\n\n    def backward(self, X, y, learning_rate=0.01):\n        \"\"\"Backward propagation\"\"\"\n        m = X.shape[0]\n\n        _, activation_derivative = self.get_activation_function(self.activation)\n\n        # Start from output layer\n        dz = self.activations[-1] - y  # For sigmoid + BCE loss\n\n        # Backpropagate through all layers\n        for i in reversed(range(len(self.weights))):\n            # Compute gradients\n            dW = (1/m) * np.dot(self.activations[i].T, dz)\n            db = (1/m) * np.sum(dz, axis=0, keepdims=True)\n\n            # Update weights and biases\n            self.weights[i] -= learning_rate * dW\n            self.biases[i] -= learning_rate * db\n\n            # Compute dz for previous layer (if not input layer)\n            if i &gt; 0:\n                da_prev = np.dot(dz, self.weights[i].T)\n                dz = da_prev * activation_derivative(self.z_values[i-1])\n\n    def train(self, X, y, epochs=1000, learning_rate=0.01):\n        \"\"\"Train the neural network\"\"\"\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward propagation\n            output = self.forward(X)\n\n            # Compute loss (Binary Cross Entropy)\n            loss = -np.mean(y * np.log(output + 1e-15) + (1 - y) * np.log(1 - output + 1e-15))\n            losses.append(loss)\n\n            # Backward propagation\n            self.backward(X, y, learning_rate)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n\n        return losses\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        return self.forward(X)\n\n# Example usage and comparison\ndef compare_activations():\n    \"\"\"Compare different activation functions on a classification task\"\"\"\n\n    # Generate sample data\n    np.random.seed(42)\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                             n_informative=2, n_clusters_per_class=1, random_state=42)\n    y = y.reshape(-1, 1)\n\n    # Normalize features\n    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n\n    # Test different activation functions\n    activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu', 'elu', 'swish']\n    results = {}\n\n    for activation in activations:\n        print(f\"\\nTraining with {activation} activation...\")\n\n        # Create and train network\n        nn = NeuralNetwork([2, 10, 10, 1], activation=activation)\n        losses = nn.train(X, y, epochs=500, learning_rate=0.1)\n\n        # Final predictions\n        predictions = nn.predict(X)\n        accuracy = np.mean((predictions &gt; 0.5) == y)\n\n        results[activation] = {\n            'losses': losses,\n            'accuracy': accuracy,\n            'final_loss': losses[-1]\n        }\n\n        print(f\"Final accuracy: {accuracy:.4f}\")\n\n    # Plot training curves\n    plt.figure(figsize=(15, 10))\n\n    # Loss curves\n    plt.subplot(2, 2, 1)\n    for activation, result in results.items():\n        plt.plot(result['losses'], label=activation)\n    plt.title('Training Loss Curves')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Final accuracies\n    plt.subplot(2, 2, 2)\n    activations_list = list(results.keys())\n    accuracies = [results[act]['accuracy'] for act in activations_list]\n    plt.bar(activations_list, accuracies)\n    plt.title('Final Accuracies')\n    plt.ylabel('Accuracy')\n    plt.xticks(rotation=45)\n    plt.grid(True, alpha=0.3)\n\n    # Decision boundaries for best performing activation\n    best_activation = max(results.keys(), key=lambda x: results[x]['accuracy'])\n    print(f\"\\nBest performing activation: {best_activation}\")\n\n    # Plot decision boundary\n    plt.subplot(2, 1, 2)\n\n    # Create mesh\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Train best model\n    best_nn = NeuralNetwork([2, 10, 10, 1], activation=best_activation)\n    best_nn.train(X, y, epochs=500, learning_rate=0.1)\n\n    # Predict on mesh\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = best_nn.predict(mesh_points)\n    Z = Z.reshape(xx.shape)\n\n    # Plot\n    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='RdYlBu', edgecolors='black')\n    plt.colorbar(scatter)\n    plt.title(f'Decision Boundary ({best_activation} activation)')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n\n    plt.tight_layout()\n    plt.show()\n\n    return results\n\n# Run comparison\nif __name__ == \"__main__\":\n    results = compare_activations()\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Activation%20functions/#assumptions","title":"Assumptions","text":"<ol> <li>Differentiability: Most activation functions assume smooth, differentiable curves for gradient-based optimization</li> <li>Input range: Some functions work better with specific input ranges (e.g., sigmoid works well with inputs around 0)</li> <li>Output interpretation: The choice of activation function assumes certain output interpretations (probabilities, raw scores, etc.)</li> <li>Computational resources: Some activations (like GELU) require more computation than others</li> </ol>"},{"location":"Machine-Learning/Activation%20functions/#limitations-by-function-type","title":"Limitations by Function Type","text":""},{"location":"Machine-Learning/Activation%20functions/#sigmoid-function","title":"Sigmoid Function","text":"<ul> <li>Vanishing gradients: Gradients become very small for large |x|, slowing learning</li> <li>Not zero-centered: Outputs are always positive, leading to inefficient gradient updates</li> <li>Computational cost: Exponential operation is expensive</li> </ul>"},{"location":"Machine-Learning/Activation%20functions/#tanh-function","title":"Tanh Function","text":"<ul> <li>Vanishing gradients: Similar to sigmoid but less severe</li> <li>Computational cost: Exponential operations required</li> </ul>"},{"location":"Machine-Learning/Activation%20functions/#relu-function","title":"ReLU Function","text":"<ul> <li>Dying ReLU problem: Neurons can become inactive and never recover</li> <li>Not differentiable at x=0: Can cause optimization issues</li> <li>Unbounded: No upper limit on activations</li> </ul>"},{"location":"Machine-Learning/Activation%20functions/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li>Hyperparameter tuning: Requires tuning of the alpha parameter</li> <li>Still unbounded: Same issue as ReLU for positive inputs</li> </ul>"},{"location":"Machine-Learning/Activation%20functions/#comparison-table","title":"Comparison Table","text":"Activation Range Zero-centered Monotonic Vanishing Gradient Computational Cost Sigmoid (0,1) \u274c \u2705 \u274c High High Tanh (-1,1) \u2705 \u2705 \u274c Medium High ReLU [0,\u221e) \u274c \u2705 \u2705 Low Low Leaky ReLU (-\u221e,\u221e) \u274c \u2705 \u2705 Low Low ELU (-\u03b1,\u221e) \u274c \u2705 \u2705 Medium Medium Swish (-\u221e,\u221e) \u274c \u274c \u2705 Low Medium"},{"location":"Machine-Learning/Activation%20functions/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. Why do we need activation functions in neural networks? <p>Answer:</p> <p>Activation functions are essential because:</p> <p>Without activation functions: - Neural networks become linear transformations regardless of depth - Multiple layers collapse into a single linear layer: \\(f(W_2(W_1x + b_1) + b_2) = W_{combined}x + b_{combined}\\) - Cannot learn complex, non-linear patterns</p> <p>With activation functions: - Introduce non-linearity enabling complex pattern learning - Allow networks to approximate any continuous function (Universal Approximation Theorem) - Enable deep networks to learn hierarchical representations - Create complex decision boundaries for classification</p> <p>Example: Without activations, a 100-layer network is equivalent to logistic regression for classification tasks.</p> 2. What is the vanishing gradient problem and which activation functions suffer from it? <p>Answer:</p> <p>Vanishing Gradient Problem: - Gradients become exponentially small as they propagate backward through deep networks - Causes early layers to learn very slowly or not at all - Network training becomes ineffective for deep architectures</p> <p>Mathematical cause: During backpropagation: \\(\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial a_n} \\prod_{i=1}^{n-1} \\frac{\\partial a_{i+1}}{\\partial a_i}\\)</p> <p>If derivatives are small (&lt; 1), the product becomes exponentially small.</p> <p>Affected functions: - Sigmoid: Derivative max is 0.25, causing exponential decay - Tanh: Derivative max is 1, but typically much smaller</p> <p>Solutions: - Use ReLU and variants (derivative is 0 or 1) - Skip connections (ResNet) - Proper weight initialization - Batch normalization</p> 3. Compare ReLU with Sigmoid and Tanh. What are the advantages and disadvantages? <p>Answer:</p> Aspect Sigmoid Tanh ReLU Range (0,1) (-1,1) [0,\u221e) Zero-centered \u274c \u2705 \u274c Computation Expensive (exp) Expensive (exp) Very cheap Vanishing gradients Severe Moderate Minimal Sparsity No No Yes (50% neurons inactive) Dying neurons No No Yes <p>ReLU Advantages: - Computationally efficient: \\(\\max(0,x)\\) - Mitigates vanishing gradient problem - Induces sparsity (biological plausibility) - Faster convergence in practice</p> <p>ReLU Disadvantages: - Dying ReLU problem (neurons become permanently inactive) - Not differentiable at x=0 - Unbounded activations can cause exploding gradients - Not zero-centered</p> 4. What is the dying ReLU problem and how can it be solved? <p>Answer:</p> <p>Dying ReLU Problem: - Occurs when neurons get stuck in inactive state (output always 0) - Happens when weights become such that input is always negative - These neurons never contribute to learning again - Can affect 10-40% of neurons in a network</p> <p>Causes: - High learning rates pushing weights to negative values - Poor weight initialization - Large negative bias terms</p> <p>Solutions:</p> <ol> <li>Leaky ReLU: \\(f(x) = \\max(\\alpha x, x)\\) where \\(\\alpha = 0.01\\)</li> <li>ELU: \\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha(e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases}\\)</li> <li>Proper initialization: Xavier/He initialization</li> <li>Lower learning rates: Prevent drastic weight updates</li> <li>Batch normalization: Keeps inputs in reasonable range</li> </ol> 5. Explain the Swish activation function and why it might be better than ReLU <p>Answer:</p> <p>Swish Function: \\(\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\\)</p> <p>Properties: - Smooth and differentiable everywhere (unlike ReLU) - Self-gated: uses its own value to control the gate - Non-monotonic: can decrease for negative values then increase - Bounded below, unbounded above</p> <p>Advantages over ReLU: - No dying neuron problem: Always has non-zero gradient for negative inputs - Smooth function: Better optimization properties - Better empirical performance: Often outperforms ReLU in deep networks - Self-regularizing: The gating mechanism acts as implicit regularization</p> <p>Disadvantages: - More computationally expensive than ReLU - Requires tuning in some variants (Swish-\u03b2)</p> <p>When to use: - Deep networks where ReLU shows dying neuron issues - Tasks requiring smooth activation functions - When computational cost is not a primary concern</p> 6. How do you choose the right activation function for different layers? <p>Answer:</p> <p>Hidden Layers:</p> <p>For most cases: ReLU or variants (Leaky ReLU, ELU) - Fast computation, good gradient flow - Use Leaky ReLU if dying ReLU is observed</p> <p>For deep networks: Swish, GELU, or ELU - Better gradient flow in very deep networks - Smoother functions help optimization</p> <p>For RNNs: Tanh or LSTM gates - Zero-centered helps with recurrent connections - Bounded range prevents exploding gradients</p> <p>Output Layers:</p> <p>Binary classification: Sigmoid - Outputs probabilities [0,1]</p> <p>Multi-class classification: Softmax - Outputs probability distribution</p> <p>Regression: Linear (no activation) or ReLU - Linear for unrestricted output - ReLU for positive outputs only</p> <p>Considerations: - Network depth: Deeper networks benefit from ReLU variants - Task type: Classification vs regression affects output choice - Computational budget: ReLU is fastest - Gradient flow: Critical for very deep networks</p> 7. What are the mathematical properties that make a good activation function? <p>Answer:</p> <p>Essential Properties:</p> <ol> <li>Non-linearity: \\(f(\\alpha x + \\beta y) \\neq \\alpha f(x) + \\beta f(y)\\)</li> <li>Enables complex pattern learning</li> <li> <p>Without this, networks collapse to linear models</p> </li> <li> <p>Differentiability: Function should be differentiable almost everywhere</p> </li> <li>Required for gradient-based optimization</li> <li> <p>Allows backpropagation to work</p> </li> <li> <p>Computational efficiency: Should be fast to compute</p> </li> <li>Networks use millions of activations</li> <li>Speed directly impacts training time</li> </ol> <p>Desirable Properties:</p> <ol> <li>Zero-centered: Mean output should be near zero</li> <li>Helps with gradient flow and convergence</li> <li> <p>Prevents bias in weight updates</p> </li> <li> <p>Bounded range: Prevents exploding activations</p> </li> <li>Helps with numerical stability</li> <li> <p>Easier to normalize and regularize</p> </li> <li> <p>Monotonic: Preserves input ordering</p> </li> <li>Simplifies optimization landscape</li> <li> <p>More predictable behavior</p> </li> <li> <p>Good gradient properties: Derivatives should not vanish or explode</p> </li> <li>Enables effective learning in deep networks</li> <li>Critical for gradient-based optimization</li> </ol> 8. Explain GELU and why it's becoming popular in transformer models <p>Answer:</p> <p>GELU (Gaussian Error Linear Unit):</p> <p>Exact formula: \\(\\text{GELU}(x) = x \\cdot P(X \\leq x) = x \\cdot \\Phi(x)\\) where \\(\\Phi\\) is the CDF of standard normal distribution.</p> <p>Approximation: \\(\\text{GELU}(x) \\approx 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\\)</p> <p>Key Properties: - Smooth, non-monotonic activation - Stochastic interpretation: gates inputs based on their magnitude - Zero-centered with bounded derivatives</p> <p>Why popular in Transformers:</p> <ol> <li>Better gradient flow: Smooth function helps optimization</li> <li>Probabilistic interpretation: Aligns with attention mechanisms</li> <li>Empirical performance: Consistently outperforms ReLU in NLP tasks</li> <li>Self-regularization: The probabilistic gating acts as implicit regularization</li> <li>Scale invariance: Works well with layer normalization</li> </ol> <p>Comparison with others: - More expensive than ReLU but cheaper than Swish - Better than ReLU for language modeling - Smoother than ReLU, helping with fine-tuning</p> <p>Usage: <pre><code># PyTorch\nimport torch.nn.functional as F\noutput = F.gelu(input)\n\n# TensorFlow\nimport tensorflow as tf\noutput = tf.nn.gelu(input)\n</code></pre></p>"},{"location":"Machine-Learning/Activation%20functions/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Activation%20functions/#example-1-visualizing-activation-functions-and-their-gradients","title":"Example 1: Visualizing Activation Functions and Their Gradients","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create comprehensive visualization\ndef plot_activations_and_gradients():\n    x = np.linspace(-5, 5, 1000)\n\n    # Define activation functions\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\n    def tanh(x):\n        return np.tanh(x)\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def leaky_relu(x, alpha=0.01):\n        return np.where(x &gt; 0, x, alpha * x)\n\n    def elu(x, alpha=1.0):\n        return np.where(x &gt; 0, x, alpha * (np.exp(np.clip(x, -500, 500)) - 1))\n\n    def swish(x):\n        return x * sigmoid(x)\n\n    # Define derivatives\n    def sigmoid_grad(x):\n        s = sigmoid(x)\n        return s * (1 - s)\n\n    def tanh_grad(x):\n        return 1 - np.tanh(x)**2\n\n    def relu_grad(x):\n        return (x &gt; 0).astype(float)\n\n    def leaky_relu_grad(x, alpha=0.01):\n        return np.where(x &gt; 0, 1, alpha)\n\n    def elu_grad(x, alpha=1.0):\n        return np.where(x &gt; 0, 1, alpha * np.exp(np.clip(x, -500, 500)))\n\n    def swish_grad(x):\n        s = sigmoid(x)\n        return s + x * s * (1 - s)\n\n    activations = [\n        ('Sigmoid', sigmoid, sigmoid_grad, 'blue'),\n        ('Tanh', tanh, tanh_grad, 'red'),\n        ('ReLU', relu, relu_grad, 'green'),\n        ('Leaky ReLU', leaky_relu, leaky_relu_grad, 'orange'),\n        ('ELU', elu, elu_grad, 'purple'),\n        ('Swish', swish, swish_grad, 'brown')\n    ]\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.ravel()\n\n    for i, (name, func, grad_func, color) in enumerate(activations):\n        y = func(x)\n        dy = grad_func(x)\n\n        ax = axes[i]\n        ax.plot(x, y, label=f'{name}', color=color, linewidth=2)\n        ax.plot(x, dy, label=f'{name} derivative', color=color, linewidth=2, linestyle='--', alpha=0.7)\n\n        ax.set_title(f'{name} Activation Function')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.axhline(y=0, color='black', linewidth=0.5)\n        ax.axvline(x=0, color='black', linewidth=0.5)\n        ax.set_xlabel('Input (x)')\n        ax.set_ylabel('Output')\n\n    plt.tight_layout()\n    plt.suptitle('Activation Functions and Their Derivatives', fontsize=16, y=1.02)\n    plt.show()\n\nplot_activations_and_gradients()\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#example-2-comparing-activation-functions-on-real-dataset","title":"Example 2: Comparing Activation Functions on Real Dataset","text":"<pre><code>from sklearn.datasets import load_breast_cancer, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\ndef comprehensive_activation_comparison():\n    \"\"\"Compare activation functions on real datasets\"\"\"\n\n    # Load datasets\n    datasets = {\n        'Breast Cancer (Binary)': load_breast_cancer(),\n        'Iris (Multi-class)': load_iris()\n    }\n\n    activations = ['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu', 'swish']\n    results = {}\n\n    for dataset_name, dataset in datasets.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Dataset: {dataset_name}\")\n        print(f\"{'='*50}\")\n\n        X, y = dataset.data, dataset.target\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, stratify=y\n        )\n\n        # Standardize features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n        dataset_results = {}\n\n        for activation in activations:\n            print(f\"\\nTesting {activation}...\")\n\n            # Create model architecture based on dataset\n            if 'Binary' in dataset_name:\n                # Binary classification\n                model = tf.keras.Sequential([\n                    tf.keras.layers.Dense(64, activation=activation, input_shape=(X_train.shape[1],)),\n                    tf.keras.layers.Dropout(0.3),\n                    tf.keras.layers.Dense(32, activation=activation),\n                    tf.keras.layers.Dropout(0.3),\n                    tf.keras.layers.Dense(1, activation='sigmoid')\n                ])\n                model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n                y_train_model, y_test_model = y_train, y_test\n            else:\n                # Multi-class classification\n                model = tf.keras.Sequential([\n                    tf.keras.layers.Dense(64, activation=activation, input_shape=(X_train.shape[1],)),\n                    tf.keras.layers.Dropout(0.3),\n                    tf.keras.layers.Dense(32, activation=activation),\n                    tf.keras.layers.Dropout(0.3),\n                    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n                ])\n                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n                y_train_model, y_test_model = y_train, y_test\n\n            # Train model\n            history = model.fit(\n                X_train_scaled, y_train_model,\n                validation_data=(X_test_scaled, y_test_model),\n                epochs=100,\n                batch_size=32,\n                verbose=0\n            )\n\n            # Evaluate\n            test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_model, verbose=0)\n\n            # Store results\n            dataset_results[activation] = {\n                'test_accuracy': test_accuracy,\n                'test_loss': test_loss,\n                'train_history': history.history,\n                'convergence_epoch': np.argmin(history.history['val_loss']) + 1\n            }\n\n            print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n            print(f\"  Test Loss: {test_loss:.4f}\")\n            print(f\"  Convergence Epoch: {dataset_results[activation]['convergence_epoch']}\")\n\n        results[dataset_name] = dataset_results\n\n        # Plot results for this dataset\n        plot_dataset_results(dataset_name, dataset_results)\n\n    # Summary comparison\n    print_summary_results(results)\n\n    return results\n\ndef plot_dataset_results(dataset_name, results):\n    \"\"\"Plot training curves and final metrics for a dataset\"\"\"\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle(f'Results for {dataset_name}', fontsize=16)\n\n    # Training curves\n    for activation, result in results.items():\n        history = result['train_history']\n\n        # Training loss\n        axes[0, 0].plot(history['loss'], label=f'{activation}')\n        axes[0, 1].plot(history['val_loss'], label=f'{activation}')\n        axes[1, 0].plot(history['accuracy'], label=f'{activation}')\n        axes[1, 1].plot(history['val_accuracy'], label=f'{activation}')\n\n    axes[0, 0].set_title('Training Loss')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n\n    axes[0, 1].set_title('Validation Loss')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n\n    axes[1, 0].set_title('Training Accuracy')\n    axes[1, 0].set_ylabel('Accuracy')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n\n    axes[1, 1].set_title('Validation Accuracy')\n    axes[1, 1].set_ylabel('Accuracy')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\ndef print_summary_results(results):\n    \"\"\"Print summary comparison across all datasets\"\"\"\n\n    print(f\"\\n{'='*80}\")\n    print(\"SUMMARY COMPARISON\")\n    print(f\"{'='*80}\")\n\n    for dataset_name, dataset_results in results.items():\n        print(f\"\\n{dataset_name}:\")\n        print(\"-\" * (len(dataset_name) + 1))\n\n        # Sort by test accuracy\n        sorted_results = sorted(dataset_results.items(), \n                              key=lambda x: x[1]['test_accuracy'], \n                              reverse=True)\n\n        print(f\"{'Activation':&lt;15} {'Test Acc':&lt;10} {'Test Loss':&lt;10} {'Convergence':&lt;12}\")\n        print(\"-\" * 55)\n\n        for activation, result in sorted_results:\n            print(f\"{activation:&lt;15} {result['test_accuracy']:&lt;10.4f} \"\n                  f\"{result['test_loss']:&lt;10.4f} {result['convergence_epoch']:&lt;12}\")\n\n# Run comprehensive comparison\n# results = comprehensive_activation_comparison()\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#example-3-gradient-flow-analysis","title":"Example 3: Gradient Flow Analysis","text":"<pre><code>def analyze_gradient_flow():\n    \"\"\"Analyze how gradients flow through deep networks with different activations\"\"\"\n\n    def create_deep_network(activation, depth=10):\n        \"\"\"Create a deep network for gradient flow analysis\"\"\"\n        layers = []\n\n        # Input layer\n        layers.append(tf.keras.layers.Dense(64, activation=activation, input_shape=(100,)))\n\n        # Hidden layers\n        for _ in range(depth - 2):\n            layers.append(tf.keras.layers.Dense(64, activation=activation))\n\n        # Output layer\n        layers.append(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n        return tf.keras.Sequential(layers)\n\n    # Test different depths and activations\n    activations = ['sigmoid', 'tanh', 'relu', 'leaky_relu', 'elu', 'swish']\n    depths = [3, 5, 10, 15, 20]\n\n    results = {}\n\n    # Generate dummy data\n    X = np.random.randn(1000, 100)\n    y = np.random.randint(0, 2, 1000)\n\n    for activation in activations:\n        results[activation] = {}\n\n        for depth in depths:\n            print(f\"Testing {activation} with depth {depth}\")\n\n            # Create model\n            model = create_deep_network(activation, depth)\n            model.compile(optimizer='adam', loss='binary_crossentropy')\n\n            # Single forward-backward pass to analyze gradients\n            with tf.GradientTape() as tape:\n                predictions = model(X[:32])  # Small batch for analysis\n                loss = tf.keras.losses.binary_crossentropy(y[:32], predictions)\n                loss = tf.reduce_mean(loss)\n\n            # Compute gradients\n            gradients = tape.gradient(loss, model.trainable_variables)\n\n            # Analyze gradient statistics\n            gradient_norms = []\n            layer_names = []\n\n            for i, grad in enumerate(gradients):\n                if grad is not None:\n                    norm = tf.norm(grad).numpy()\n                    gradient_norms.append(norm)\n                    layer_names.append(f\"Layer_{i//2 + 1}\")  # Account for weights and biases\n\n            # Store results\n            results[activation][depth] = {\n                'gradient_norms': gradient_norms,\n                'mean_gradient_norm': np.mean(gradient_norms),\n                'std_gradient_norm': np.std(gradient_norms),\n                'min_gradient_norm': np.min(gradient_norms),\n                'max_gradient_norm': np.max(gradient_norms)\n            }\n\n    # Plot results\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Mean gradient norm vs depth\n    for activation in activations:\n        mean_norms = [results[activation][depth]['mean_gradient_norm'] for depth in depths]\n        axes[0, 0].plot(depths, mean_norms, marker='o', label=activation)\n\n    axes[0, 0].set_title('Mean Gradient Norm vs Network Depth')\n    axes[0, 0].set_xlabel('Network Depth')\n    axes[0, 0].set_ylabel('Mean Gradient Norm')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n    axes[0, 0].set_yscale('log')\n\n    # Gradient norm variance vs depth\n    for activation in activations:\n        std_norms = [results[activation][depth]['std_gradient_norm'] for depth in depths]\n        axes[0, 1].plot(depths, std_norms, marker='o', label=activation)\n\n    axes[0, 1].set_title('Gradient Norm Std vs Network Depth')\n    axes[0, 1].set_xlabel('Network Depth')\n    axes[0, 1].set_ylabel('Gradient Norm Std')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n    axes[0, 1].set_yscale('log')\n\n    # Min gradient norm (vanishing gradient indicator)\n    for activation in activations:\n        min_norms = [results[activation][depth]['min_gradient_norm'] for depth in depths]\n        axes[1, 0].plot(depths, min_norms, marker='o', label=activation)\n\n    axes[1, 0].set_title('Min Gradient Norm vs Network Depth')\n    axes[1, 0].set_xlabel('Network Depth')\n    axes[1, 0].set_ylabel('Min Gradient Norm')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n    axes[1, 0].set_yscale('log')\n\n    # Max gradient norm (exploding gradient indicator)\n    for activation in activations:\n        max_norms = [results[activation][depth]['max_gradient_norm'] for depth in depths]\n        axes[1, 1].plot(depths, max_norms, marker='o', label=activation)\n\n    axes[1, 1].set_title('Max Gradient Norm vs Network Depth')\n    axes[1, 1].set_xlabel('Network Depth')\n    axes[1, 1].set_ylabel('Max Gradient Norm')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True)\n    axes[1, 1].set_yscale('log')\n\n    plt.tight_layout()\n    plt.show()\n\n    return results\n\n# Run gradient flow analysis\n# gradient_results = analyze_gradient_flow()\n</code></pre>"},{"location":"Machine-Learning/Activation%20functions/#references","title":"\ud83d\udcda References","text":"<ul> <li>Books:</li> <li>Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</li> <li>Neural Networks and Deep Learning by Michael Nielsen</li> <li> <p>Hands-On Machine Learning by Aur\u00e9lien G\u00e9ron</p> </li> <li> <p>Research Papers:</p> </li> <li>ReLU Networks - Deep Sparse Rectifier Neural Networks</li> <li>ELU Paper - Fast and Accurate Deep Network Learning by Exponential Linear Units</li> <li>Swish Paper - Searching for Activation Functions</li> <li> <p>GELU Paper - Gaussian Error Linear Units</p> </li> <li> <p>Online Resources:</p> </li> <li>CS231n Convolutional Neural Networks</li> <li>Activation Functions Explained</li> <li>TensorFlow Activation Functions</li> <li> <p>PyTorch Activation Functions</p> </li> <li> <p>Tutorials:</p> </li> <li>Understanding Activation Functions</li> <li>Activation Functions in Neural Networks</li> <li> <p>A Practical Guide to ReLU</p> </li> <li> <p>Interactive Resources:</p> </li> <li>TensorFlow Playground - Visualize how different activations affect learning</li> <li>Neural Network Playground - Interactive neural network visualization</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/","title":"\ud83d\udcd8 Collaborative Filtering","text":"<p>Collaborative filtering is a recommendation technique that predicts user preferences by analyzing the behavior and preferences of similar users or items, leveraging the collective intelligence of the user community.</p> <p>Resources: Surprise Documentation | Netflix Prize Paper | Recommender Systems Handbook</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#summary","title":"\u270d\ufe0f Summary","text":"<p>Collaborative Filtering (CF) is a method used in recommendation systems that makes automatic predictions about user preferences by collecting preferences from many users. The underlying assumption is that users who agreed in the past will agree in the future, and they will like similar kinds of items.</p> <p>Key concepts: - User-based CF: Find similar users and recommend items they liked - Item-based CF: Find similar items to those the user has liked - Matrix Factorization: Decompose user-item interaction matrix into latent factors</p> <p>Applications: - Movie recommendations (Netflix, IMDb) - Product recommendations (Amazon, eBay) - Music recommendations (Spotify, Pandora) - Social media content (Facebook, Twitter) - News recommendations (Google News) - Book recommendations (Goodreads)</p> <p>Collaborative filtering works well when you have sufficient user interaction data but doesn't require knowledge about item content.</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Collaborative%20Filtering/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Collaborative filtering can be formulated as a matrix completion problem. Given a user-item rating matrix \\(R\\) where \\(R_{ui}\\) represents the rating user \\(u\\) gave to item \\(i\\):</p> \\[R = \\begin{bmatrix} r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n} \\\\ r_{21} &amp; r_{22} &amp; \\cdots &amp; r_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{m1} &amp; r_{m2} &amp; \\cdots &amp; r_{mn} \\end{bmatrix}\\] <p>Where many entries are missing (unobserved ratings).</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#user-based-collaborative-filtering","title":"User-Based Collaborative Filtering","text":"<p>The similarity between users \\(u\\) and \\(v\\) can be measured using:</p> <p>Cosine Similarity: \\(\\(\\text{sim}(u,v) = \\frac{\\sum_{i \\in I} r_{ui} \\cdot r_{vi}}{\\sqrt{\\sum_{i \\in I} r_{ui}^2} \\cdot \\sqrt{\\sum_{i \\in I} r_{vi}^2}}\\)\\)</p> <p>Pearson Correlation: \\(\\(\\text{sim}(u,v) = \\frac{\\sum_{i \\in I} (r_{ui} - \\bar{r}_u)(r_{vi} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I} (r_{ui} - \\bar{r}_u)^2} \\cdot \\sqrt{\\sum_{i \\in I} (r_{vi} - \\bar{r}_v)^2}}\\)\\)</p> <p>Prediction formula: \\(\\(\\hat{r}_{ui} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} \\text{sim}(u,v) \\cdot (r_{vi} - \\bar{r}_v)}{\\sum_{v \\in N(u)} |\\text{sim}(u,v)|}\\)\\)</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#item-based-collaborative-filtering","title":"Item-Based Collaborative Filtering","text":"<p>Similar to user-based, but focuses on item similarities:</p> \\[\\hat{r}_{ui} = \\frac{\\sum_{j \\in N(i)} \\text{sim}(i,j) \\cdot r_{uj}}{\\sum_{j \\in N(i)} |\\text{sim}(i,j)|}\\]"},{"location":"Machine-Learning/Collaborative%20Filtering/#matrix-factorization","title":"Matrix Factorization","text":"<p>Decompose the rating matrix \\(R\\) into two lower-dimensional matrices: \\(\\(R \\approx P \\times Q^T\\)\\)</p> <p>Where: - \\(P \\in \\mathbb{R}^{m \\times k}\\) represents user latent factors - \\(Q \\in \\mathbb{R}^{n \\times k}\\) represents item latent factors - \\(k\\) is the number of latent factors</p> <p>Objective function: \\(\\(\\min_{P,Q} \\sum_{(u,i) \\in \\text{observed}} (r_{ui} - p_u^T q_i)^2 + \\lambda(||P||^2 + ||Q||^2)\\)\\)</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Collaborative%20Filtering/#using-surprise-library","title":"Using Surprise Library","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom surprise import Dataset, Reader, SVD, KNNBasic, accuracy\nfrom surprise.model_selection import train_test_split, cross_validate\nfrom collections import defaultdict\n\n# Sample dataset creation\ndef create_sample_data():\n    \"\"\"Create sample movie ratings dataset\"\"\"\n    np.random.seed(42)\n\n    users = [f'User_{i}' for i in range(1, 101)]\n    movies = [f'Movie_{i}' for i in range(1, 51)]\n\n    # Generate ratings with some pattern\n    ratings = []\n    for user in users:\n        # Each user rates 10-30 movies\n        n_ratings = np.random.randint(10, 31)\n        user_movies = np.random.choice(movies, n_ratings, replace=False)\n\n        for movie in user_movies:\n            # Add some user bias and item bias\n            user_bias = np.random.normal(0, 0.5)\n            movie_bias = np.random.normal(0, 0.3)\n            rating = np.clip(3 + user_bias + movie_bias + np.random.normal(0, 0.8), 1, 5)\n            ratings.append([user, movie, round(rating, 1)])\n\n    return pd.DataFrame(ratings, columns=['user', 'item', 'rating'])\n\n# Create and prepare data\ndf = create_sample_data()\nprint(\"Sample data:\")\nprint(df.head())\n\n# Surprise dataset format\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\ntrainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n\n# 1. Matrix Factorization (SVD)\nprint(\"\\n1. Matrix Factorization (SVD)\")\nsvd = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\nsvd.fit(trainset)\n\n# Make predictions\npredictions = svd.test(testset)\nprint(f\"RMSE: {accuracy.rmse(predictions):.4f}\")\n\n# 2. User-based Collaborative Filtering\nprint(\"\\n2. User-based Collaborative Filtering\")\nuser_based = KNNBasic(sim_options={'name': 'cosine', 'user_based': True}, k=20)\nuser_based.fit(trainset)\n\npredictions_user = user_based.test(testset)\nprint(f\"RMSE: {accuracy.rmse(predictions_user):.4f}\")\n\n# 3. Item-based Collaborative Filtering  \nprint(\"\\n3. Item-based Collaborative Filtering\")\nitem_based = KNNBasic(sim_options={'name': 'cosine', 'user_based': False}, k=20)\nitem_based.fit(trainset)\n\npredictions_item = item_based.test(testset)\nprint(f\"RMSE: {accuracy.rmse(predictions_item):.4f}\")\n\n# Get recommendations for a user\ndef get_recommendations(model, user_id, trainset, n_recommendations=5):\n    \"\"\"Get top N recommendations for a user\"\"\"\n    # Get list of all items\n    all_items = set([item for (_, item, _) in trainset.all_ratings()])\n\n    # Get items the user has already rated\n    user_items = set([item for (user, item, _) in trainset.all_ratings() if user == user_id])\n\n    # Get items the user hasn't rated\n    unrated_items = all_items - user_items\n\n    # Predict ratings for unrated items\n    predictions = []\n    for item in unrated_items:\n        pred = model.predict(user_id, item)\n        predictions.append((item, pred.est))\n\n    # Sort by predicted rating\n    predictions.sort(key=lambda x: x[1], reverse=True)\n\n    return predictions[:n_recommendations]\n\n# Example recommendations\nuser_id = trainset.to_raw_uid(0)  # First user in trainset\nrecommendations = get_recommendations(svd, user_id, trainset)\nprint(f\"\\nTop 5 recommendations for {user_id}:\")\nfor item, rating in recommendations:\n    print(f\"  {item}: {rating:.2f}\")\n</code></pre>"},{"location":"Machine-Learning/Collaborative%20Filtering/#using-scikit-learn","title":"Using scikit-learn","text":"<pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import NMF\nimport pandas as pd\n\nclass CollaborativeFilteringScratch:\n    def __init__(self, method='user_based'):\n        self.method = method\n        self.user_similarity = None\n        self.item_similarity = None\n        self.user_mean = None\n\n    def fit(self, ratings_matrix):\n        \"\"\"\n        Fit collaborative filtering model\n        ratings_matrix: pandas DataFrame with users as rows, items as columns\n        \"\"\"\n        self.ratings_matrix = ratings_matrix.fillna(0)\n        self.user_mean = ratings_matrix.mean(axis=1)\n\n        if self.method == 'user_based':\n            # Calculate user similarity matrix\n            self.user_similarity = cosine_similarity(self.ratings_matrix)\n            np.fill_diagonal(self.user_similarity, 0)  # Remove self-similarity\n\n        elif self.method == 'item_based':\n            # Calculate item similarity matrix\n            self.item_similarity = cosine_similarity(self.ratings_matrix.T)\n            np.fill_diagonal(self.item_similarity, 0)\n\n    def predict_user_based(self, user_idx, item_idx, k=20):\n        \"\"\"Predict rating using user-based collaborative filtering\"\"\"\n        if self.ratings_matrix.iloc[user_idx, item_idx] &gt; 0:\n            return self.ratings_matrix.iloc[user_idx, item_idx]\n\n        # Find k most similar users\n        similarities = self.user_similarity[user_idx]\n        similar_users = np.argsort(similarities)[::-1][:k]\n\n        # Remove users who haven't rated this item\n        similar_users = [u for u in similar_users \n                        if self.ratings_matrix.iloc[u, item_idx] &gt; 0]\n\n        if not similar_users:\n            return self.user_mean.iloc[user_idx]\n\n        # Calculate weighted average\n        numerator = sum(similarities[u] * \n                       (self.ratings_matrix.iloc[u, item_idx] - self.user_mean.iloc[u])\n                       for u in similar_users)\n        denominator = sum(abs(similarities[u]) for u in similar_users)\n\n        if denominator == 0:\n            return self.user_mean.iloc[user_idx]\n\n        return self.user_mean.iloc[user_idx] + numerator / denominator\n\n    def predict_item_based(self, user_idx, item_idx, k=20):\n        \"\"\"Predict rating using item-based collaborative filtering\"\"\"\n        if self.ratings_matrix.iloc[user_idx, item_idx] &gt; 0:\n            return self.ratings_matrix.iloc[user_idx, item_idx]\n\n        # Find k most similar items that the user has rated\n        similarities = self.item_similarity[item_idx]\n        user_rated_items = [i for i in range(len(similarities))\n                           if self.ratings_matrix.iloc[user_idx, i] &gt; 0]\n\n        if not user_rated_items:\n            return self.user_mean.iloc[user_idx]\n\n        # Sort by similarity and take top k\n        similar_items = sorted(user_rated_items, \n                             key=lambda x: similarities[x], reverse=True)[:k]\n\n        # Calculate weighted average\n        numerator = sum(similarities[i] * self.ratings_matrix.iloc[user_idx, i]\n                       for i in similar_items)\n        denominator = sum(abs(similarities[i]) for i in similar_items)\n\n        if denominator == 0:\n            return self.user_mean.iloc[user_idx]\n\n        return numerator / denominator\n\n# Example usage with sample data\nnp.random.seed(42)\nn_users, n_items = 20, 15\n\n# Create sample ratings matrix (sparse)\nratings = np.random.choice([0, 1, 2, 3, 4, 5], \n                          size=(n_users, n_items), \n                          p=[0.7, 0.05, 0.05, 0.1, 0.05, 0.05])\nratings_df = pd.DataFrame(ratings, \n                         index=[f'User_{i}' for i in range(n_users)],\n                         columns=[f'Item_{i}' for i in range(n_items)])\n\n# Replace 0s with NaN to represent missing ratings\nratings_df = ratings_df.replace(0, np.nan)\n\nprint(\"Sample ratings matrix:\")\nprint(ratings_df.head())\n\n# Fit models\ncf_user = CollaborativeFilteringScratch(method='user_based')\ncf_user.fit(ratings_df)\n\ncf_item = CollaborativeFilteringScratch(method='item_based')  \ncf_item.fit(ratings_df)\n\n# Make predictions\nuser_idx, item_idx = 0, 5\npred_user = cf_user.predict_user_based(user_idx, item_idx)\npred_item = cf_item.predict_item_based(user_idx, item_idx)\n\nprint(f\"\\nPredictions for User_0, Item_5:\")\nprint(f\"User-based CF: {pred_user:.2f}\")\nprint(f\"Item-based CF: {pred_item:.2f}\")\n</code></pre>"},{"location":"Machine-Learning/Collaborative%20Filtering/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Tuple\nimport math\n\nclass CollaborativeFilteringFromScratch:\n    \"\"\"\n    Complete implementation of Collaborative Filtering from scratch\n    Includes User-based, Item-based, and Matrix Factorization approaches\n    \"\"\"\n\n    def __init__(self, approach='user_based', n_factors=10, learning_rate=0.01, \n                 regularization=0.01, n_epochs=100):\n        self.approach = approach\n        self.n_factors = n_factors\n        self.learning_rate = learning_rate\n        self.regularization = regularization\n        self.n_epochs = n_epochs\n\n        # Will be populated during training\n        self.ratings_matrix = None\n        self.user_mean = None\n        self.item_mean = None\n        self.global_mean = None\n        self.user_factors = None\n        self.item_factors = None\n        self.user_bias = None\n        self.item_bias = None\n\n    def pearson_correlation(self, x, y):\n        \"\"\"Calculate Pearson correlation coefficient\"\"\"\n        # Remove NaN values\n        mask = ~(np.isnan(x) | np.isnan(y))\n        if np.sum(mask) &lt; 2:\n            return 0\n\n        x_clean, y_clean = x[mask], y[mask]\n\n        if len(x_clean) == 0 or np.std(x_clean) == 0 or np.std(y_clean) == 0:\n            return 0\n\n        return np.corrcoef(x_clean, y_clean)[0, 1] if len(x_clean) &gt; 1 else 0\n\n    def cosine_similarity(self, x, y):\n        \"\"\"Calculate cosine similarity\"\"\"\n        # Replace NaN with 0 for cosine similarity\n        x_clean = np.nan_to_num(x)\n        y_clean = np.nan_to_num(y)\n\n        dot_product = np.dot(x_clean, y_clean)\n        norm_x = np.linalg.norm(x_clean)\n        norm_y = np.linalg.norm(y_clean)\n\n        if norm_x == 0 or norm_y == 0:\n            return 0\n\n        return dot_product / (norm_x * norm_y)\n\n    def fit(self, ratings_df):\n        \"\"\"\n        Fit the collaborative filtering model\n        ratings_df: DataFrame with users as index, items as columns\n        \"\"\"\n        self.ratings_matrix = ratings_df.copy()\n        self.users = list(ratings_df.index)\n        self.items = list(ratings_df.columns)\n        self.n_users = len(self.users)\n        self.n_items = len(self.items)\n\n        # Calculate means\n        self.user_mean = ratings_df.mean(axis=1, skipna=True)\n        self.item_mean = ratings_df.mean(axis=0, skipna=True)\n        self.global_mean = ratings_df.stack().mean()\n\n        if self.approach == 'matrix_factorization':\n            self._fit_matrix_factorization()\n\n    def _fit_matrix_factorization(self):\n        \"\"\"Fit matrix factorization using gradient descent\"\"\"\n        # Initialize factors and biases\n        np.random.seed(42)\n        self.user_factors = np.random.normal(0, 0.1, (self.n_users, self.n_factors))\n        self.item_factors = np.random.normal(0, 0.1, (self.n_items, self.n_factors))\n        self.user_bias = np.zeros(self.n_users)\n        self.item_bias = np.zeros(self.n_items)\n\n        # Get all known ratings\n        known_ratings = []\n        for i, user in enumerate(self.users):\n            for j, item in enumerate(self.items):\n                rating = self.ratings_matrix.loc[user, item]\n                if not np.isnan(rating):\n                    known_ratings.append((i, j, rating))\n\n        # Gradient descent\n        for epoch in range(self.n_epochs):\n            total_error = 0\n\n            for user_idx, item_idx, rating in known_ratings:\n                # Predict rating\n                prediction = (self.global_mean + \n                             self.user_bias[user_idx] + \n                             self.item_bias[item_idx] +\n                             np.dot(self.user_factors[user_idx], \n                                   self.item_factors[item_idx]))\n\n                # Calculate error\n                error = rating - prediction\n                total_error += error ** 2\n\n                # Update biases\n                self.user_bias[user_idx] += self.learning_rate * (\n                    error - self.regularization * self.user_bias[user_idx])\n                self.item_bias[item_idx] += self.learning_rate * (\n                    error - self.regularization * self.item_bias[item_idx])\n\n                # Update factors\n                user_factors_old = self.user_factors[user_idx].copy()\n                self.user_factors[user_idx] += self.learning_rate * (\n                    error * self.item_factors[item_idx] - \n                    self.regularization * self.user_factors[user_idx])\n                self.item_factors[item_idx] += self.learning_rate * (\n                    error * user_factors_old - \n                    self.regularization * self.item_factors[item_idx])\n\n            # Print progress\n            if (epoch + 1) % 20 == 0:\n                rmse = np.sqrt(total_error / len(known_ratings))\n                print(f\"Epoch {epoch + 1}/{self.n_epochs}, RMSE: {rmse:.4f}\")\n\n    def predict(self, user, item, k=20):\n        \"\"\"Predict rating for user-item pair\"\"\"\n        if user not in self.users or item not in self.items:\n            return self.global_mean\n\n        user_idx = self.users.index(user)\n        item_idx = self.items.index(item)\n\n        # If rating already exists, return it\n        existing_rating = self.ratings_matrix.loc[user, item]\n        if not np.isnan(existing_rating):\n            return existing_rating\n\n        if self.approach == 'user_based':\n            return self._predict_user_based(user_idx, item_idx, k)\n        elif self.approach == 'item_based':\n            return self._predict_item_based(user_idx, item_idx, k)\n        elif self.approach == 'matrix_factorization':\n            return self._predict_matrix_factorization(user_idx, item_idx)\n        else:\n            return self.global_mean\n\n    def _predict_user_based(self, user_idx, item_idx, k):\n        \"\"\"User-based collaborative filtering prediction\"\"\"\n        target_user_ratings = self.ratings_matrix.iloc[user_idx].values\n        similarities = []\n\n        # Calculate similarities with all other users\n        for i, other_user in enumerate(self.users):\n            if i == user_idx:\n                continue\n\n            other_user_ratings = self.ratings_matrix.iloc[i].values\n            similarity = self.pearson_correlation(target_user_ratings, other_user_ratings)\n\n            # Only consider users who have rated this item\n            if not np.isnan(self.ratings_matrix.iloc[i, item_idx]) and similarity &gt; 0:\n                similarities.append((i, similarity))\n\n        # Sort by similarity and take top k\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        top_similar = similarities[:k]\n\n        if not top_similar:\n            return self.user_mean.iloc[user_idx]\n\n        # Calculate weighted average\n        numerator = sum(sim * (self.ratings_matrix.iloc[user_i, item_idx] - \n                              self.user_mean.iloc[user_i])\n                       for user_i, sim in top_similar)\n        denominator = sum(abs(sim) for _, sim in top_similar)\n\n        if denominator == 0:\n            return self.user_mean.iloc[user_idx]\n\n        return self.user_mean.iloc[user_idx] + numerator / denominator\n\n    def _predict_item_based(self, user_idx, item_idx, k):\n        \"\"\"Item-based collaborative filtering prediction\"\"\"\n        target_item_ratings = self.ratings_matrix.iloc[:, item_idx].values\n        similarities = []\n\n        # Calculate similarities with all other items\n        for j, other_item in enumerate(self.items):\n            if j == item_idx:\n                continue\n\n            other_item_ratings = self.ratings_matrix.iloc[:, j].values\n            similarity = self.pearson_correlation(target_item_ratings, other_item_ratings)\n\n            # Only consider items that this user has rated\n            if not np.isnan(self.ratings_matrix.iloc[user_idx, j]) and similarity &gt; 0:\n                similarities.append((j, similarity))\n\n        # Sort by similarity and take top k\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        top_similar = similarities[:k]\n\n        if not top_similar:\n            return self.item_mean.iloc[item_idx]\n\n        # Calculate weighted average\n        numerator = sum(sim * self.ratings_matrix.iloc[user_idx, item_j]\n                       for item_j, sim in top_similar)\n        denominator = sum(abs(sim) for _, sim in top_similar)\n\n        if denominator == 0:\n            return self.item_mean.iloc[item_idx]\n\n        return numerator / denominator\n\n    def _predict_matrix_factorization(self, user_idx, item_idx):\n        \"\"\"Matrix factorization prediction\"\"\"\n        prediction = (self.global_mean + \n                     self.user_bias[user_idx] + \n                     self.item_bias[item_idx] +\n                     np.dot(self.user_factors[user_idx], \n                           self.item_factors[item_idx]))\n        return prediction\n\n    def get_recommendations(self, user, n_recommendations=5):\n        \"\"\"Get top N recommendations for a user\"\"\"\n        if user not in self.users:\n            return []\n\n        user_idx = self.users.index(user)\n        user_ratings = self.ratings_matrix.loc[user]\n\n        # Find items the user hasn't rated\n        unrated_items = user_ratings[user_ratings.isna()].index.tolist()\n\n        # Predict ratings for unrated items\n        predictions = []\n        for item in unrated_items:\n            pred_rating = self.predict(user, item)\n            predictions.append((item, pred_rating))\n\n        # Sort by predicted rating\n        predictions.sort(key=lambda x: x[1], reverse=True)\n\n        return predictions[:n_recommendations]\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    # Create sample data\n    np.random.seed(42)\n    users = [f'User_{i}' for i in range(15)]\n    items = [f'Movie_{i}' for i in range(10)]\n\n    # Create ratings matrix with missing values\n    ratings_data = {}\n    for user in users:\n        ratings_data[user] = {}\n        for item in items:\n            # 60% chance of having a rating\n            if np.random.random() &gt; 0.6:\n                ratings_data[user][item] = np.random.randint(1, 6)\n            else:\n                ratings_data[user][item] = np.nan\n\n    ratings_df = pd.DataFrame(ratings_data).T\n    print(\"Sample ratings matrix:\")\n    print(ratings_df.head())\n\n    # Test different approaches\n    approaches = ['user_based', 'item_based', 'matrix_factorization']\n\n    for approach in approaches:\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {approach.replace('_', ' ').title()}\")\n        print('='*50)\n\n        cf_model = CollaborativeFilteringFromScratch(approach=approach, n_epochs=50)\n        cf_model.fit(ratings_df)\n\n        # Test predictions\n        test_user = 'User_0'\n        test_item = 'Movie_5'\n\n        prediction = cf_model.predict(test_user, test_item)\n        print(f\"Prediction for {test_user} -&gt; {test_item}: {prediction:.2f}\")\n\n        # Get recommendations\n        recommendations = cf_model.get_recommendations(test_user, n_recommendations=3)\n        print(f\"\\nTop 3 recommendations for {test_user}:\")\n        for item, rating in recommendations:\n            print(f\"  {item}: {rating:.2f}\")\n</code></pre>"},{"location":"Machine-Learning/Collaborative%20Filtering/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Collaborative%20Filtering/#assumptions","title":"Assumptions","text":"<ol> <li>User Consistency: Users have consistent preferences over time</li> <li>Transitivity: If user A is similar to user B, and user B likes item X, then user A will also like item X</li> <li>Sufficient Data: Enough user-item interactions exist for meaningful patterns</li> <li>Rating Reliability: User ratings accurately reflect their true preferences</li> </ol>"},{"location":"Machine-Learning/Collaborative%20Filtering/#limitations","title":"Limitations","text":""},{"location":"Machine-Learning/Collaborative%20Filtering/#1-cold-start-problems","title":"1. Cold Start Problems","text":"<ul> <li>New Users: Cannot make recommendations for users with no rating history</li> <li>New Items: Cannot recommend items with no ratings</li> <li>Solution: Use hybrid approaches combining content-based filtering</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/#2-data-sparsity","title":"2. Data Sparsity","text":"<ul> <li>Most user-item matrices are extremely sparse (95%+ missing values)</li> <li>Few overlapping ratings between users make similarity calculations unreliable</li> <li>Solution: Matrix factorization, dimensionality reduction</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/#3-scalability-issues","title":"3. Scalability Issues","text":"<ul> <li>User-based CF: O(mn) for m users, n items per prediction</li> <li>Similarity calculations become expensive with large datasets</li> <li>Solution: Approximate algorithms, sampling, clustering</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/#4-gray-sheep-problem","title":"4. Gray Sheep Problem","text":"<ul> <li>Users with unique tastes don't match well with any group</li> <li>Hard to find similar users for recommendations</li> <li>Solution: Content-based or demographic filtering</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/#5-filter-bubble","title":"5. Filter Bubble","text":"<ul> <li>Recommends similar items to what user already likes</li> <li>Reduces serendipity and diversity</li> <li>Solution: Add randomness, diversity metrics</li> </ul>"},{"location":"Machine-Learning/Collaborative%20Filtering/#comparison-with-other-approaches","title":"Comparison with Other Approaches","text":"Aspect User-Based CF Item-Based CF Matrix Factorization Interpretability High High Low Scalability Poor Better Good Accuracy Medium Medium High Cold Start Poor Poor Better Sparsity Handling Poor Better Good"},{"location":"Machine-Learning/Collaborative%20Filtering/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"Q1: What is collaborative filtering and how does it differ from content-based filtering? <p>Answer:</p> <p>Collaborative filtering predicts user preferences based on behavior of similar users, while content-based filtering uses item features.</p> <p>Key differences: - Data Required: CF needs user behavior data; content-based needs item features - Recommendations: CF can recommend items dissimilar in content but liked by similar users - Cold Start: CF struggles with new users/items; content-based can handle new items - Serendipity: CF provides more surprising recommendations - Domain Knowledge: CF doesn't require domain expertise; content-based does</p> Q2: Explain the cold start problem in collaborative filtering and potential solutions. <p>Answer:</p> <p>Cold start occurs when there's insufficient data for new users or items.</p> <p>Types: 1. New User: No rating history \u2192 Cannot find similar users 2. New Item: No ratings \u2192 Cannot recommend to anyone 3. New System: Few users/items overall</p> <p>Solutions: - Hybrid Systems: Combine with content-based filtering - Demographic Filtering: Use age, gender, location for new users - Popular Items: Recommend trending/popular items to new users - Active Learning: Ask new users to rate popular items - Side Information: Use implicit feedback (views, clicks, time spent)</p> Q3: What are the advantages and disadvantages of user-based vs item-based collaborative filtering? <p>Answer:</p> <p>User-Based CF: - Advantages: Intuitive, good for diverse recommendations, works well with user communities - Disadvantages: Poor scalability (users grow faster than items), unstable (user preferences change)</p> <p>Item-Based CF: - Advantages: Better scalability, more stable (item relationships don't change often), pre-computable - Disadvantages: Less diverse recommendations, may create filter bubbles</p> <p>When to use: - User-based: Small user base, community-driven platforms, need for diversity - Item-based: Large user base, stable item catalog, need for stability</p> Q4: How does matrix factorization work in collaborative filtering? What are its benefits? <p>Answer:</p> <p>Matrix factorization decomposes the user-item rating matrix R into two lower-dimensional matrices P (user factors) and Q (item factors):</p> \\[R \\approx P \\times Q^T\\] <p>How it works: 1. Initialize P and Q with random values 2. For each known rating, predict: \\(\\hat{r}_{ui} = p_u^T q_i\\) 3. Minimize error: \\(\\min \\sum (r_{ui} - p_u^T q_i)^2 + \\lambda(||P||^2 + ||Q||^2)\\) 4. Update factors using gradient descent</p> <p>Benefits: - Handles sparsity better than neighborhood methods - More scalable than user/item-based approaches - Can incorporate biases and side information - Discovers latent factors automatically - Better accuracy on sparse datasets</p> Q5: What evaluation metrics would you use for a recommendation system? <p>Answer:</p> <p>Accuracy Metrics: - RMSE/MAE: For rating prediction tasks - Precision/Recall: For top-N recommendations - F1-Score: Harmonic mean of precision and recall - AUC: Area under ROC curve for binary relevance</p> <p>Ranking Metrics: - NDCG: Normalized Discounted Cumulative Gain - MAP: Mean Average Precision - MRR: Mean Reciprocal Rank</p> <p>Beyond Accuracy: - Coverage: Percentage of items that can be recommended - Diversity: Variety in recommendations - Novelty: How unknown recommended items are - Serendipity: Surprising but relevant recommendations - Business Metrics: Click-through rate, conversion rate, user engagement</p> Q6: How would you handle the scalability challenges in collaborative filtering? <p>Answer:</p> <p>Techniques for scalability:</p> <ol> <li>Dimensionality Reduction:</li> <li>SVD, NMF for matrix factorization</li> <li> <p>Clustering users/items to reduce computation</p> </li> <li> <p>Sampling Strategies:</p> </li> <li>Sample subset of similar users/items</li> <li> <p>Negative sampling for implicit feedback</p> </li> <li> <p>Approximate Algorithms:</p> </li> <li>Locality Sensitive Hashing (LSH) for similarity</li> <li> <p>Randomized algorithms</p> </li> <li> <p>Distributed Computing:</p> </li> <li>MapReduce implementations</li> <li> <p>Spark MLlib for large-scale CF</p> </li> <li> <p>Preprocessing:</p> </li> <li>Pre-compute item-item similarities (more stable)</li> <li>Use incremental learning algorithms</li> </ol> Q7: What is the difference between explicit and implicit feedback? How do you handle each? <p>Answer:</p> <p>Explicit Feedback: - Direct ratings (1-5 stars, thumbs up/down) - Advantages: Clear preference signal - Disadvantages: Sparse, biased (only engaged users rate)</p> <p>Implicit Feedback: - Indirect behavior (views, clicks, purchases, time spent) - Advantages: Abundant, all users generate it - Disadvantages: Noisy, positive-only (no explicit negatives)</p> <p>Handling Strategies: - Explicit: Standard CF algorithms, handle missing as unknown - Implicit: Treat confidence as rating strength, generate negative samples, use specialized algorithms (BPR, WARP)</p> <p>Example transformation for implicit: - View time \u2192 confidence score - Multiple purchases \u2192 higher preference - Recent activity \u2192 higher weight</p> Q8: How would you detect and prevent data quality issues in collaborative filtering? <p>Answer:</p> <p>Common Issues: 1. Fake Reviews/Ratings: Artificially inflate/deflate ratings 2. Rating Bias: Users with extreme rating patterns 3. Data Sparsity: Very few ratings per user/item 4. Temporal Effects: Preferences change over time</p> <p>Detection Methods: - Statistical analysis (rating distributions, user patterns) - Anomaly detection algorithms - Graph-based analysis (unusual rating patterns) - Temporal analysis (sudden rating spikes)</p> <p>Prevention/Mitigation: - User verification and reputation systems - Rate limiting and CAPTCHA - Weighted ratings by user trustworthiness - Temporal weighting (recent ratings more important) - Robust algorithms less sensitive to outliers</p> Q9: How would you design a recommendation system for a new e-commerce platform? <p>Answer:</p> <p>Initial Phase (Cold Start): 1. Popular Items: Show trending/bestselling products 2. Content-Based: Use product features, categories, descriptions 3. Demographic: Age, gender, location-based recommendations</p> <p>Growth Phase: 1. Simple CF: User-based or item-based with sufficient data 2. Hybrid Approach: Combine content-based and collaborative 3. Implicit Feedback: Views, cart additions, purchases</p> <p>Mature Phase: 1. Matrix Factorization: Handle large sparse matrices 2. Deep Learning: Neural collaborative filtering, autoencoders 3. Real-time: Online learning, session-based recommendations</p> <p>System Design Considerations: - A/B testing framework for algorithm comparison - Real-time vs batch processing - Scalable infrastructure (distributed computing) - Business metrics alignment</p> Q10: What are some advanced techniques in modern collaborative filtering? <p>Answer:</p> <p>Deep Learning Approaches: 1. Neural Collaborative Filtering: Replace dot product with neural network 2. Autoencoders: Learn user/item representations 3. RNNs/LSTMs: Model sequential behavior 4. Graph Neural Networks: Leverage user-item graph structure</p> <p>Advanced Matrix Factorization: - Non-negative Matrix Factorization: Interpretable factors - Bayesian Matrix Factorization: Uncertainty quantification - Tensor Factorization: Multi-dimensional data (user-item-context)</p> <p>Multi-Armed Bandits: - Exploration vs exploitation in recommendations - Contextual bandits for personalization</p> <p>Reinforcement Learning: - Long-term user satisfaction optimization - Dynamic recommendation strategies</p> <p>Fairness and Bias Mitigation: - Demographic parity in recommendations - Bias-aware collaborative filtering</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Collaborative%20Filtering/#example-1-movie-recommendation-system","title":"Example 1: Movie Recommendation System","text":"<pre><code># Real-world example using MovieLens dataset structure\nimport pandas as pd\nimport numpy as np\n\n# Sample movie data (simplified MovieLens format)\nmovies_data = {\n    'movie_id': [1, 2, 3, 4, 5],\n    'title': ['Toy Story', 'Jumanji', 'Heat', 'Casino', 'Sabrina'],\n    'genres': ['Animation|Children|Comedy', 'Adventure|Children|Fantasy', \n               'Action|Crime|Thriller', 'Crime|Drama', 'Comedy|Romance']\n}\n\nratings_data = {\n    'user_id': [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5],\n    'movie_id': [1, 2, 3, 1, 4, 2, 3, 5, 1, 5, 3, 4],\n    'rating': [5, 4, 3, 4, 5, 3, 4, 5, 5, 4, 3, 4]\n}\n\nmovies_df = pd.DataFrame(movies_data)\nratings_df = pd.DataFrame(ratings_data)\n\n# Create user-item matrix\nuser_item_matrix = ratings_df.pivot(index='user_id', \n                                   columns='movie_id', \n                                   values='rating')\n\nprint(\"User-Item Rating Matrix:\")\nprint(user_item_matrix)\n\n# Apply collaborative filtering\ncf_model = CollaborativeFilteringFromScratch(approach='item_based')\ncf_model.fit(user_item_matrix)\n\n# Get recommendations for User 1\nrecommendations = cf_model.get_recommendations(1, n_recommendations=2)\nprint(f\"\\nRecommendations for User 1:\")\nfor movie_id, predicted_rating in recommendations:\n    movie_title = movies_df[movies_df['movie_id'] == movie_id]['title'].values[0]\n    print(f\"  {movie_title}: {predicted_rating:.2f}\")\n</code></pre> <p>Output: <pre><code>User-Item Rating Matrix:\nmovie_id    1    2    3    4    5\nuser_id                         \n1         5.0  4.0  3.0  NaN  NaN\n2         4.0  NaN  NaN  5.0  NaN\n3         NaN  3.0  4.0  NaN  5.0\n4         5.0  NaN  NaN  NaN  4.0\n5         NaN  NaN  3.0  4.0  NaN\n\nRecommendations for User 1:\n  Casino: 4.21\n  Sabrina: 3.87\n</code></pre></p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#example-2-performance-comparison","title":"Example 2: Performance Comparison","text":"<pre><code># Compare different approaches on synthetic data\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n# Generate larger synthetic dataset\nnp.random.seed(42)\nn_users, n_items = 100, 50\nsparsity = 0.1  # 10% of entries are filled\n\n# Create synthetic ratings with latent factors\ntrue_user_factors = np.random.normal(0, 1, (n_users, 5))\ntrue_item_factors = np.random.normal(0, 1, (n_items, 5))\ntrue_ratings = np.dot(true_user_factors, true_item_factors.T)\n\n# Add noise and sparsity\nmask = np.random.random((n_users, n_items)) &lt; sparsity\nobserved_ratings = true_ratings + np.random.normal(0, 0.5, (n_users, n_items))\nobserved_ratings = np.clip(observed_ratings, 1, 5)  # Clip to rating scale\nobserved_ratings[~mask] = np.nan\n\n# Convert to DataFrame\nratings_df = pd.DataFrame(observed_ratings, \n                         index=[f'User_{i}' for i in range(n_users)],\n                         columns=[f'Item_{i}' for i in range(n_items)])\n\n# Split train/test\ntrain_mask = np.random.random((n_users, n_items)) &lt; 0.8\ntest_mask = mask &amp; ~train_mask\n\ntrain_df = ratings_df.copy()\ntrain_df[~train_mask] = np.nan\n\n# Test different approaches\napproaches = ['user_based', 'item_based', 'matrix_factorization']\nresults = {}\n\nfor approach in approaches:\n    print(f\"\\nTesting {approach}...\")\n    start_time = time.time()\n\n    model = CollaborativeFilteringFromScratch(approach=approach, n_epochs=50)\n    model.fit(train_df)\n\n    # Make predictions on test set\n    predictions = []\n    actuals = []\n\n    for i in range(n_users):\n        for j in range(n_items):\n            if test_mask[i, j]:\n                user = f'User_{i}'\n                item = f'Item_{j}'\n                pred = model.predict(user, item)\n                actual = ratings_df.iloc[i, j]\n\n                predictions.append(pred)\n                actuals.append(actual)\n\n    # Calculate metrics\n    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n    training_time = time.time() - start_time\n\n    results[approach] = {\n        'RMSE': rmse,\n        'Training Time': training_time,\n        'Predictions': len(predictions)\n    }\n\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"Training Time: {training_time:.2f}s\")\n\n# Display results\nprint(f\"\\n{'='*60}\")\nprint(\"PERFORMANCE COMPARISON\")\nprint('='*60)\nprint(f\"{'Approach':&lt;20} {'RMSE':&lt;10} {'Time (s)':&lt;10}\")\nprint('-'*40)\nfor approach, metrics in results.items():\n    print(f\"{approach.replace('_', ' ').title():&lt;20} {metrics['RMSE']:&lt;10.4f} {metrics['Training Time']:&lt;10.2f}\")\n</code></pre> <p>This comprehensive implementation demonstrates how collaborative filtering works in practice, handles real-world challenges, and provides a foundation for building production recommendation systems.</p>"},{"location":"Machine-Learning/Collaborative%20Filtering/#references","title":"\ud83d\udcda References","text":"<ol> <li> <p>Ricci, F., Rokach, L., &amp; Shapira, B. (2015). Recommender Systems Handbook. Springer.</p> </li> <li> <p>Koren, Y., Bell, R., &amp; Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 30-37.</p> </li> <li> <p>Su, X., &amp; Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009.</p> </li> <li> <p>Sarwar, B., et al. (2001). Item-based collaborative filtering recommendation algorithms. Proceedings of the 10<sup>th</sup> international conference on World Wide Web.</p> </li> <li> <p>Netflix Prize Documentation: Netflix Prize</p> </li> <li> <p>Surprise Library Documentation: Surprise</p> </li> <li> <p>MovieLens Datasets: GroupLens Research</p> </li> <li> <p>Collaborative Filtering Tutorial: Towards Data Science</p> </li> <li> <p>Matrix Factorization: Netflix Tech Blog</p> </li> <li> <p>Modern Recommender Systems: RecSys Conference Proceedings</p> </li> </ol>"},{"location":"Machine-Learning/Confusion%20Matrix/","title":"\ud83d\udcd8 Confusion Matrix","text":"<p>A confusion matrix is a table used to evaluate the performance of classification models by showing the actual vs predicted classifications in a structured format.</p> <p>Resources: Scikit-learn Confusion Matrix | Wikipedia Confusion Matrix</p>"},{"location":"Machine-Learning/Confusion%20Matrix/#summary","title":"\u270d\ufe0f Summary","text":"<p>A confusion matrix is a fundamental tool in machine learning for evaluating the performance of classification algorithms. It provides a detailed breakdown of correct and incorrect predictions for each class, enabling comprehensive analysis of model performance.</p> <p>Key characteristics: - Visual representation: Clear tabular format showing prediction accuracy - Multi-class support: Works with binary and multi-class classification - Metric foundation: Basis for calculating precision, recall, F1-score, etc. - Error analysis: Helps identify which classes are being confused</p> <p>Applications: - Model evaluation and comparison - Error analysis and debugging - Performance reporting - Threshold optimization - Medical diagnosis validation - Quality control systems</p> <p>The matrix is typically organized with: - Rows: Actual (true) class labels - Columns: Predicted class labels - Diagonal: Correct predictions - Off-diagonal: Misclassifications</p>"},{"location":"Machine-Learning/Confusion%20Matrix/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Confusion%20Matrix/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>For a binary classification problem, the confusion matrix is a 2\u00d72 table:</p> <pre><code>                Predicted\n                0    1\nActual    0    TN   FP\n          1    FN   TP\n</code></pre> <p>Where: - TP (True Positive): Correctly predicted positive cases - TN (True Negative): Correctly predicted negative cases - FP (False Positive): Incorrectly predicted as positive (Type I error) - FN (False Negative): Incorrectly predicted as negative (Type II error)</p>"},{"location":"Machine-Learning/Confusion%20Matrix/#derived-metrics","title":"Derived Metrics","text":"<p>From the confusion matrix, we can calculate several important metrics:</p> <p>Accuracy: Overall correctness \\(\\(\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\)\\)</p> <p>Precision: How many selected items are relevant \\(\\(\\text{Precision} = \\frac{TP}{TP + FP}\\)\\)</p> <p>Recall (Sensitivity): How many relevant items are selected \\(\\(\\text{Recall} = \\frac{TP}{TP + FN}\\)\\)</p> <p>Specificity: True negative rate \\(\\(\\text{Specificity} = \\frac{TN}{TN + FP}\\)\\)</p> <p>F1-Score: Harmonic mean of precision and recall \\(\\(\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\\)</p>"},{"location":"Machine-Learning/Confusion%20Matrix/#multi-class-extension","title":"Multi-class Extension","text":"<p>For multi-class problems with \\(n\\) classes, the matrix becomes \\(n \\times n\\):</p> \\[C_{i,j} = \\text{number of observations known to be in group } i \\text{ and predicted to be in group } j\\]"},{"location":"Machine-Learning/Confusion%20Matrix/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Confusion%20Matrix/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Generate sample dataset\nX, y = make_classification(n_samples=1000, n_features=4, n_classes=3, \n                          n_redundant=0, n_informative=4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"\\nMetrics:\")\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1-Score: {f1:.3f}\")\n\n# Detailed classification report\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#visualization-with-seaborn","title":"Visualization with Seaborn","text":"<pre><code># Create a more detailed visualization\ndef plot_confusion_matrix(cm, class_names=None, title='Confusion Matrix'):\n    \"\"\"\n    Plot confusion matrix with annotations and percentages\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n\n    # Calculate percentages\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n\n    # Create annotations with both counts and percentages\n    annotations = []\n    for i in range(cm.shape[0]):\n        row_annotations = []\n        for j in range(cm.shape[1]):\n            row_annotations.append(f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)')\n        annotations.append(row_annotations)\n\n    # Plot heatmap\n    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Count'})\n\n    plt.title(title)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    plt.show()\n\n# Plot the confusion matrix\nclass_names = ['Class 0', 'Class 1', 'Class 2']\nplot_confusion_matrix(cm, class_names, 'Random Forest Confusion Matrix')\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#binary-classification-example","title":"Binary Classification Example","text":"<pre><code># Binary classification example\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\n\n# Generate binary classification data\nX_binary, y_binary = make_classification(n_samples=500, n_features=2, \n                                        n_redundant=0, n_informative=2,\n                                        n_classes=2, random_state=42)\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n    X_binary, y_binary, test_size=0.3, random_state=42)\n\n# Train logistic regression\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train_b, y_train_b)\ny_pred_b = log_reg.predict(X_test_b)\n\n# Binary confusion matrix\ncm_binary = confusion_matrix(y_test_b, y_pred_b)\nprint(\"Binary Confusion Matrix:\")\nprint(cm_binary)\n\n# Extract values\ntn, fp, fn, tp = cm_binary.ravel()\nprint(f\"\\nTrue Negatives: {tn}\")\nprint(f\"False Positives: {fp}\")\nprint(f\"False Negatives: {fn}\")\nprint(f\"True Positives: {tp}\")\n\n# Calculate metrics manually\naccuracy = (tp + tn) / (tp + tn + fp + fn)\nprecision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\nrecall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\nspecificity = tn / (tn + fp) if (tn + fp) &gt; 0 else 0\nf1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\nprint(f\"\\nManually Calculated Metrics:\")\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"Specificity: {specificity:.3f}\")\nprint(f\"F1-Score: {f1:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nfrom collections import Counter\n\nclass ConfusionMatrix:\n    \"\"\"\n    From-scratch implementation of Confusion Matrix with metric calculations\n    \"\"\"\n\n    def __init__(self):\n        self.matrix = None\n        self.classes = None\n        self.n_classes = None\n\n    def fit(self, y_true, y_pred):\n        \"\"\"\n        Create confusion matrix from true and predicted labels\n\n        Parameters:\n        y_true: array-like, true class labels\n        y_pred: array-like, predicted class labels\n        \"\"\"\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Get unique classes\n        self.classes = np.unique(np.concatenate([y_true, y_pred]))\n        self.n_classes = len(self.classes)\n\n        # Create mapping from class to index\n        class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n\n        # Initialize matrix\n        self.matrix = np.zeros((self.n_classes, self.n_classes), dtype=int)\n\n        # Fill matrix\n        for true_label, pred_label in zip(y_true, y_pred):\n            true_idx = class_to_idx[true_label]\n            pred_idx = class_to_idx[pred_label]\n            self.matrix[true_idx, pred_idx] += 1\n\n        return self\n\n    def get_matrix(self):\n        \"\"\"Return the confusion matrix\"\"\"\n        if self.matrix is None:\n            raise ValueError(\"Matrix not computed. Call fit() first.\")\n        return self.matrix\n\n    def accuracy(self):\n        \"\"\"Calculate overall accuracy\"\"\"\n        if self.matrix is None:\n            raise ValueError(\"Matrix not computed. Call fit() first.\")\n\n        correct = np.trace(self.matrix)  # Sum of diagonal\n        total = np.sum(self.matrix)\n        return correct / total if total &gt; 0 else 0\n\n    def precision(self, average='macro'):\n        \"\"\"\n        Calculate precision for each class or average\n\n        Parameters:\n        average: str, 'macro', 'micro', 'weighted', or None\n        \"\"\"\n        if self.matrix is None:\n            raise ValueError(\"Matrix not computed. Call fit() first.\")\n\n        # Per-class precision\n        precisions = []\n        for i in range(self.n_classes):\n            true_positives = self.matrix[i, i]\n            predicted_positives = np.sum(self.matrix[:, i])\n\n            if predicted_positives == 0:\n                precision = 0.0\n            else:\n                precision = true_positives / predicted_positives\n\n            precisions.append(precision)\n\n        precisions = np.array(precisions)\n\n        if average is None:\n            return precisions\n        elif average == 'macro':\n            return np.mean(precisions)\n        elif average == 'micro':\n            total_tp = np.trace(self.matrix)\n            total_pred_pos = np.sum(self.matrix)\n            return total_tp / total_pred_pos if total_pred_pos &gt; 0 else 0\n        elif average == 'weighted':\n            support = np.sum(self.matrix, axis=1)\n            return np.average(precisions, weights=support)\n        else:\n            raise ValueError(\"Invalid average type\")\n\n    def recall(self, average='macro'):\n        \"\"\"\n        Calculate recall for each class or average\n\n        Parameters:\n        average: str, 'macro', 'micro', 'weighted', or None\n        \"\"\"\n        if self.matrix is None:\n            raise ValueError(\"Matrix not computed. Call fit() first.\")\n\n        # Per-class recall\n        recalls = []\n        for i in range(self.n_classes):\n            true_positives = self.matrix[i, i]\n            actual_positives = np.sum(self.matrix[i, :])\n\n            if actual_positives == 0:\n                recall = 0.0\n            else:\n                recall = true_positives / actual_positives\n\n            recalls.append(recall)\n\n        recalls = np.array(recalls)\n\n        if average is None:\n            return recalls\n        elif average == 'macro':\n            return np.mean(recalls)\n        elif average == 'micro':\n            total_tp = np.trace(self.matrix)\n            total_actual_pos = np.sum(self.matrix)\n            return total_tp / total_actual_pos if total_actual_pos &gt; 0 else 0\n        elif average == 'weighted':\n            support = np.sum(self.matrix, axis=1)\n            return np.average(recalls, weights=support)\n        else:\n            raise ValueError(\"Invalid average type\")\n\n    def f1_score(self, average='macro'):\n        \"\"\"Calculate F1-score\"\"\"\n        precision = self.precision(average=average)\n        recall = self.recall(average=average)\n\n        if isinstance(precision, np.ndarray):\n            # Per-class F1 scores\n            f1_scores = 2 * (precision * recall) / (precision + recall)\n            f1_scores = np.nan_to_num(f1_scores)  # Handle division by zero\n            return f1_scores\n        else:\n            # Average F1 score\n            if (precision + recall) == 0:\n                return 0.0\n            return 2 * (precision * recall) / (precision + recall)\n\n    def classification_report(self):\n        \"\"\"Generate a detailed classification report\"\"\"\n        if self.matrix is None:\n            raise ValueError(\"Matrix not computed. Call fit() first.\")\n\n        precisions = self.precision(average=None)\n        recalls = self.recall(average=None)\n        f1_scores = self.f1_score(average=None)\n        support = np.sum(self.matrix, axis=1)\n\n        print(\"Classification Report:\")\n        print(\"-\" * 60)\n        print(f\"{'Class':&lt;10} {'Precision':&lt;12} {'Recall':&lt;12} {'F1-Score':&lt;12} {'Support':&lt;10}\")\n        print(\"-\" * 60)\n\n        for i, cls in enumerate(self.classes):\n            print(f\"{cls:&lt;10} {precisions[i]:&lt;12.3f} {recalls[i]:&lt;12.3f} \"\n                  f\"{f1_scores[i]:&lt;12.3f} {support[i]:&lt;10}\")\n\n        print(\"-\" * 60)\n        print(f\"{'Accuracy':&lt;10} {'':&lt;12} {'':&lt;12} {self.accuracy():&lt;12.3f} {np.sum(support):&lt;10}\")\n        print(f\"{'Macro Avg':&lt;10} {self.precision('macro'):&lt;12.3f} \"\n              f\"{self.recall('macro'):&lt;12.3f} {self.f1_score('macro'):&lt;12.3f} {np.sum(support):&lt;10}\")\n        print(f\"{'Weighted':&lt;10} {self.precision('weighted'):&lt;12.3f} \"\n              f\"{self.recall('weighted'):&lt;12.3f} {self.f1_score('weighted'):&lt;12.3f} {np.sum(support):&lt;10}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample data\n    np.random.seed(42)\n    n_samples = 300\n\n    # Create synthetic predictions vs true labels\n    y_true = np.random.choice([0, 1, 2], size=n_samples, p=[0.4, 0.35, 0.25])\n\n    # Create predictions with some errors\n    y_pred = y_true.copy()\n    error_indices = np.random.choice(n_samples, size=int(0.2 * n_samples), replace=False)\n    y_pred[error_indices] = np.random.choice([0, 1, 2], size=len(error_indices))\n\n    # Create confusion matrix\n    cm = ConfusionMatrix()\n    cm.fit(y_true, y_pred)\n\n    print(\"Confusion Matrix:\")\n    print(cm.get_matrix())\n    print(f\"\\nAccuracy: {cm.accuracy():.3f}\")\n    print(f\"Macro Precision: {cm.precision('macro'):.3f}\")\n    print(f\"Macro Recall: {cm.recall('macro'):.3f}\")\n    print(f\"Macro F1-Score: {cm.f1_score('macro'):.3f}\")\n\n    print(\"\\n\" + \"=\"*60)\n    cm.classification_report()\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Confusion%20Matrix/#assumptions","title":"Assumptions","text":"<ol> <li>Ground Truth Availability: Requires true labels for evaluation</li> <li>Consistent Labeling: True and predicted labels must use the same class encoding</li> <li>Complete Predictions: Every sample must have both true and predicted labels</li> <li>Class Balance Consideration: Some metrics are sensitive to class imbalance</li> </ol>"},{"location":"Machine-Learning/Confusion%20Matrix/#limitations","title":"Limitations","text":"<ol> <li>Information Loss: </li> <li>Doesn't show prediction confidence/probability</li> <li> <p>No information about feature importance</p> </li> <li> <p>Class Imbalance Sensitivity:</p> </li> <li>Accuracy can be misleading with imbalanced datasets</li> <li> <p>May need to focus on per-class metrics</p> </li> <li> <p>Multi-label Limitations:</p> </li> <li>Standard confusion matrix doesn't handle multi-label classification well</li> <li> <p>Each label needs separate evaluation</p> </li> <li> <p>Threshold Independence:</p> </li> <li>Doesn't show how performance varies with different classification thresholds</li> <li>May need ROC curves for threshold analysis</li> </ol>"},{"location":"Machine-Learning/Confusion%20Matrix/#comparison-with-other-evaluation-methods","title":"Comparison with Other Evaluation Methods","text":"Method Pros Cons Confusion Matrix Detailed breakdown, interpretable Static, no confidence info ROC Curve Threshold analysis, AUC metric Only for binary/one-vs-rest PR Curve Better for imbalanced data More complex to interpret Cross-validation Robust performance estimate Computationally expensive"},{"location":"Machine-Learning/Confusion%20Matrix/#when-to-use-alternatives","title":"When to Use Alternatives","text":"<ul> <li>Highly Imbalanced Data: Use precision-recall curves</li> <li>Probability Calibration: Use reliability diagrams</li> <li>Cost-Sensitive Applications: Use cost matrices</li> <li>Ranking Problems: Use ranking metrics (NDCG, MAP)</li> </ul>"},{"location":"Machine-Learning/Confusion%20Matrix/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"Q1: What is a confusion matrix and what does each cell represent? <p>Answer: A confusion matrix is a table used to evaluate classification model performance. For binary classification: - True Positives (TP): Correctly predicted positive cases - True Negatives (TN): Correctly predicted negative cases - False Positives (FP): Incorrectly predicted as positive (Type I error) - False Negatives (FN): Incorrectly predicted as negative (Type II error)</p> <p>The diagonal represents correct predictions, while off-diagonal elements represent errors.</p> Q2: How do you calculate precision and recall from a confusion matrix? <p>Answer: From a binary confusion matrix: - Precision = TP / (TP + FP) - \"Of all positive predictions, how many were correct?\" - Recall = TP / (TP + FN) - \"Of all actual positives, how many did we find?\"</p> <p>For multi-class: Calculate per-class metrics and then average (macro, micro, or weighted).</p> Q3: What's the difference between macro, micro, and weighted averaging? <p>Answer: - Macro Average: Simple average of per-class metrics (treats all classes equally) - Micro Average: Calculate metrics globally by counting total TP, FP, FN - Weighted Average: Average of per-class metrics weighted by class support</p> <p>Micro average is better for imbalanced datasets, macro average for balanced datasets.</p> Q4: When would accuracy be a poor metric to use? <p>Answer: Accuracy is poor when: - Class Imbalance: 95% accuracy on a 95%-5% dataset might just predict majority class - Cost-Sensitive Applications: False negatives in medical diagnosis are more costly - Multi-label Problems: Partial correctness isn't captured - Different Error Costs: When different types of errors have different consequences</p> Q5: How do you interpret a confusion matrix for multi-class classification? <p>Answer: In an n\u00d7n matrix for n classes: - Diagonal elements: Correct predictions for each class - Row sums: Total actual instances of each class - Column sums: Total predicted instances of each class - Off-diagonal: Shows which classes are confused with each other</p> <p>Look for patterns: Are specific classes consistently confused?</p> Q6: What is the relationship between specificity and false positive rate? <p>Answer:  - Specificity = TN / (TN + FP) (True Negative Rate) - False Positive Rate = FP / (TN + FP) - Relationship: Specificity + FPR = 1</p> <p>High specificity means low false positive rate. This is important in applications where false alarms are costly.</p> Q7: How would you handle a confusion matrix with very small numbers? <p>Answer: When dealing with small sample sizes: - Use confidence intervals for metrics - Consider bootstrapping for robust estimates - Be cautious of overfitting to small test sets - Use cross-validation for better estimates - Consider Bayesian approaches with priors</p> Q8: Can you explain the trade-off between precision and recall? <p>Answer: There's typically an inverse relationship: - Higher Precision: Fewer false positives, but might miss true positives (lower recall) - Higher Recall: Catch more true positives, but might include false positives (lower precision)</p> <p>F1-score balances both. The optimal balance depends on the application's cost of false positives vs false negatives.</p> Q9: How do you create a normalized confusion matrix and why is it useful? <p>Answer: Normalize by dividing each row by its sum: <pre><code>normalized_cm = cm / cm.sum(axis=1)[:, np.newaxis]\n</code></pre></p> <p>Benefits: - Shows proportions instead of absolute counts - Better for comparing across different datasets - Easier to identify per-class performance patterns - Less affected by class imbalance in visualization</p> Q10: What additional information would you want beyond a confusion matrix? <p>Answer: - Prediction probabilities: For threshold tuning - Feature importance: To understand model decisions - ROC/PR curves: For threshold-dependent analysis - Cost matrix: For business-specific error costs - Learning curves: To check for overfitting - Per-sample analysis: To identify difficult cases</p>"},{"location":"Machine-Learning/Confusion%20Matrix/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Confusion%20Matrix/#medical-diagnosis-example","title":"Medical Diagnosis Example","text":"<pre><code># Simulate medical diagnosis scenario\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simulate a medical test for disease diagnosis\n# True condition: 0 = Healthy, 1 = Disease\n# Test result: 0 = Negative, 1 = Positive\n\nnp.random.seed(42)\n\n# Create realistic medical scenario\n# Disease prevalence: 5% (realistic for many conditions)\nn_patients = 1000\ndisease_prevalence = 0.05\n\n# Generate true conditions\ny_true = np.random.choice([0, 1], size=n_patients, \n                         p=[1-disease_prevalence, disease_prevalence])\n\n# Simulate test with known sensitivity and specificity\nsensitivity = 0.95  # True positive rate\nspecificity = 0.90  # True negative rate\n\ny_pred = []\nfor true_condition in y_true:\n    if true_condition == 1:  # Patient has disease\n        # Test positive with probability = sensitivity\n        prediction = np.random.choice([0, 1], p=[1-sensitivity, sensitivity])\n    else:  # Patient is healthy\n        # Test negative with probability = specificity\n        prediction = np.random.choice([0, 1], p=[specificity, 1-specificity])\n    y_pred.append(prediction)\n\ny_pred = np.array(y_pred)\n\n# Create confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Medical Test Confusion Matrix:\")\nprint(\"                Predicted\")\nprint(\"               Neg  Pos\")\nprint(f\"Actual   Neg   {cm[0,0]:3d}  {cm[0,1]:3d}\")\nprint(f\"         Pos   {cm[1,0]:3d}  {cm[1,1]:3d}\")\n\n# Calculate important medical metrics\ntn, fp, fn, tp = cm.ravel()\n\nsensitivity_calc = tp / (tp + fn)\nspecificity_calc = tn / (tn + fp)\nppv = tp / (tp + fp) if (tp + fp) &gt; 0 else 0  # Positive Predictive Value\nnpv = tn / (tn + fn) if (tn + fn) &gt; 0 else 0  # Negative Predictive Value\n\nprint(f\"\\nMedical Test Performance:\")\nprint(f\"Sensitivity (True Positive Rate): {sensitivity_calc:.3f}\")\nprint(f\"Specificity (True Negative Rate): {specificity_calc:.3f}\")\nprint(f\"Positive Predictive Value (Precision): {ppv:.3f}\")\nprint(f\"Negative Predictive Value: {npv:.3f}\")\n\n# Visualize with medical terminology\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Raw confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\nax1.set_title('Medical Test Confusion Matrix')\nax1.set_xlabel('Predicted')\nax1.set_ylabel('Actual')\nax1.set_xticklabels(['Negative', 'Positive'])\nax1.set_yticklabels(['Healthy', 'Disease'])\n\n# Normalized confusion matrix\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues', ax=ax2)\nax2.set_title('Normalized Confusion Matrix (Percentages)')\nax2.set_xlabel('Predicted')\nax2.set_ylabel('Actual')\nax2.set_xticklabels(['Negative', 'Positive'])\nax2.set_yticklabels(['Healthy', 'Disease'])\n\nplt.tight_layout()\nplt.show()\n\n# Interpretation\nprint(f\"\\nInterpretation:\")\nprint(f\"\u2022 Out of {tn + fp} healthy patients, {tn} were correctly identified (Specificity: {specificity_calc:.1%})\")\nprint(f\"\u2022 Out of {tp + fn} disease patients, {tp} were correctly identified (Sensitivity: {sensitivity_calc:.1%})\")\nprint(f\"\u2022 Out of {tp + fp} positive tests, {tp} were true positives (PPV: {ppv:.1%})\")\nprint(f\"\u2022 Out of {tn + fn} negative tests, {tn} were true negatives (NPV: {npv:.1%})\")\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#e-commerce-recommendation-example","title":"E-commerce Recommendation Example","text":"<pre><code># E-commerce recommendation system evaluation\n# Predict whether user will purchase recommended items\n\n# Simulate user behavior data\nnp.random.seed(123)\nn_recommendations = 2000\n\n# Features that might affect purchase (simplified)\nuser_engagement = np.random.beta(2, 5, n_recommendations)  # 0-1 engagement score\nitem_popularity = np.random.beta(1.5, 3, n_recommendations)  # 0-1 popularity score\nprice_sensitivity = np.random.normal(0.5, 0.2, n_recommendations)  # Price factor\n\n# True purchase probability (complex relationship)\npurchase_prob = (0.4 * user_engagement + \n                0.3 * item_popularity + \n                0.3 * (1 - price_sensitivity))\npurchase_prob = np.clip(purchase_prob, 0.1, 0.9)\n\n# Generate true purchases\ny_true_ecommerce = np.random.binomial(1, purchase_prob)\n\n# Simulate recommendation algorithm predictions (with some errors)\npred_prob = purchase_prob + np.random.normal(0, 0.15, n_recommendations)\npred_prob = np.clip(pred_prob, 0, 1)\n\n# Convert probabilities to binary predictions using threshold\nthreshold = 0.5\ny_pred_ecommerce = (pred_prob &gt; threshold).astype(int)\n\n# Create confusion matrix\ncm_ecommerce = confusion_matrix(y_true_ecommerce, y_pred_ecommerce)\n\nprint(\"E-commerce Recommendation Confusion Matrix:\")\nprint(\"                    Predicted\")\nprint(\"                No Purchase  Purchase\")\nprint(f\"Actual No Purchase    {cm_ecommerce[0,0]:4d}      {cm_ecommerce[0,1]:4d}\")\nprint(f\"       Purchase       {cm_ecommerce[1,0]:4d}      {cm_ecommerce[1,1]:4d}\")\n\n# Business metrics\ntn, fp, fn, tp = cm_ecommerce.ravel()\n\n# Business interpretation\nconversion_rate = (tp + fn) / (tn + fp + fn + tp)\npredicted_conversion = (tp + fp) / (tn + fp + fn + tp)\nprecision_purchase = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\nrecall_purchase = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n\nprint(f\"\\nBusiness Metrics:\")\nprint(f\"Overall Conversion Rate: {conversion_rate:.1%}\")\nprint(f\"Predicted Conversion Rate: {predicted_conversion:.1%}\")\nprint(f\"Recommendation Precision: {precision_purchase:.1%} (of recommended items, how many were purchased)\")\nprint(f\"Purchase Recall: {recall_purchase:.1%} (of actual purchases, how many were recommended)\")\n\n# Cost analysis (hypothetical)\nrevenue_per_purchase = 50  # $50 average order value\ncost_per_recommendation = 0.1  # $0.10 cost to show recommendation\n\ntotal_revenue = tp * revenue_per_purchase\ntotal_cost = (tp + fp) * cost_per_recommendation\nnet_profit = total_revenue - total_cost\nroi = (net_profit / total_cost) * 100 if total_cost &gt; 0 else 0\n\nprint(f\"\\nCost Analysis:\")\nprint(f\"Total Revenue from TP: ${total_revenue:.2f}\")\nprint(f\"Total Recommendation Cost: ${total_cost:.2f}\")\nprint(f\"Net Profit: ${net_profit:.2f}\")\nprint(f\"ROI: {roi:.1f}%\")\n\n# Show impact of different thresholds\nthresholds = np.arange(0.1, 0.9, 0.1)\nresults = []\n\nfor thresh in thresholds:\n    y_pred_thresh = (pred_prob &gt; thresh).astype(int)\n    cm_thresh = confusion_matrix(y_true_ecommerce, y_pred_thresh)\n    tn_t, fp_t, fn_t, tp_t = cm_thresh.ravel()\n\n    precision_t = tp_t / (tp_t + fp_t) if (tp_t + fp_t) &gt; 0 else 0\n    recall_t = tp_t / (tp_t + fn_t) if (tp_t + fn_t) &gt; 0 else 0\n\n    revenue_t = tp_t * revenue_per_purchase\n    cost_t = (tp_t + fp_t) * cost_per_recommendation\n    profit_t = revenue_t - cost_t\n\n    results.append({\n        'threshold': thresh,\n        'precision': precision_t,\n        'recall': recall_t,\n        'profit': profit_t,\n        'recommendations': tp_t + fp_t\n    })\n\n# Find optimal threshold\noptimal_thresh = max(results, key=lambda x: x['profit'])\nprint(f\"\\nOptimal Threshold Analysis:\")\nprint(f\"Best threshold for profit: {optimal_thresh['threshold']:.1f}\")\nprint(f\"Precision at optimal: {optimal_thresh['precision']:.1%}\")\nprint(f\"Recall at optimal: {optimal_thresh['recall']:.1%}\")\nprint(f\"Profit at optimal: ${optimal_thresh['profit']:.2f}\")\nprint(f\"Total recommendations: {optimal_thresh['recommendations']}\")\n</code></pre>"},{"location":"Machine-Learning/Confusion%20Matrix/#references","title":"\ud83d\udcda References","text":"<ol> <li>Documentation:</li> <li>Scikit-learn Confusion Matrix</li> <li> <p>Scikit-learn Classification Metrics</p> </li> <li> <p>Books:</p> </li> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman</li> <li> <p>\"Hands-On Machine Learning\" by Aur\u00e9lien G\u00e9ron</p> </li> <li> <p>Research Papers:</p> </li> <li>\"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\" - Kohavi (1995)</li> <li> <p>\"The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets\" - Saito &amp; Rehmsmeier (2015)</p> </li> <li> <p>Online Resources:</p> </li> <li>Wikipedia: Confusion Matrix</li> <li>Google ML Crash Course: Classification</li> <li> <p>Towards Data Science: Confusion Matrix Articles</p> </li> <li> <p>Video Tutorials:</p> </li> <li>StatQuest: Confusion Matrix</li> <li>3Blue1Brown: Neural Networks Series</li> </ol>"},{"location":"Machine-Learning/DBSCAN/","title":"\ud83d\udcd8 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)","text":"<p>DBSCAN is a density-based clustering algorithm that groups together points that are closely packed while marking points in low-density regions as outliers.</p> <p>Resources: Scikit-learn DBSCAN | Original DBSCAN Paper</p>"},{"location":"Machine-Learning/DBSCAN/#summary","title":"\u270d\ufe0f Summary","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a data clustering algorithm that finds clusters of varying shapes and sizes from a large amount of data containing noise and outliers. Unlike centroid-based algorithms like K-means, DBSCAN doesn't require specifying the number of clusters beforehand.</p> <p>Key characteristics: - Density-based: Groups points that are closely packed together - Noise handling: Identifies outliers as noise points - Arbitrary shapes: Can find clusters of any shape - Parameter-driven: Requires two parameters: <code>eps</code> and <code>min_samples</code> - No cluster count: Automatically determines the number of clusters</p> <p>Applications: - Customer segmentation - Image processing and computer vision - Fraud detection - Anomaly detection in networks - Gene sequencing analysis - Social network analysis</p> <p>Advantages: - Finds clusters of arbitrary shapes - Robust to outliers - Doesn't require prior knowledge of cluster count - Can identify noise points</p> <p>Disadvantages: - Sensitive to hyperparameters (<code>eps</code> and <code>min_samples</code>) - Struggles with varying densities - High-dimensional data challenges - Memory intensive for large datasets</p>"},{"location":"Machine-Learning/DBSCAN/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/DBSCAN/#core-concepts","title":"Core Concepts","text":"<p>DBSCAN groups together points that are closely packed and marks as outliers points that lie alone in low-density regions. The algorithm uses two key parameters:</p> <ol> <li>\u03b5 (epsilon): Maximum distance between two points to be considered neighbors</li> <li>MinPts (min_samples): Minimum number of points required to form a dense region</li> </ol>"},{"location":"Machine-Learning/DBSCAN/#point-classifications","title":"Point Classifications","text":"<p>DBSCAN classifies points into three categories:</p>"},{"location":"Machine-Learning/DBSCAN/#1-core-points","title":"1. Core Points","text":"<p>A point \\(p\\) is a core point if at least <code>MinPts</code> points lie within distance <code>\u03b5</code> of it (including \\(p\\) itself).</p> \\[|N_\u03b5(p)| \u2265 MinPts\\] <p>Where \\(N_\u03b5(p)\\) is the \u03b5-neighborhood of point \\(p\\).</p>"},{"location":"Machine-Learning/DBSCAN/#2-border-points","title":"2. Border Points","text":"<p>A point is a border point if it has fewer than <code>MinPts</code> within distance <code>\u03b5</code>, but lies within the \u03b5-neighborhood of a core point.</p>"},{"location":"Machine-Learning/DBSCAN/#3-noise-points","title":"3. Noise Points","text":"<p>A point is noise if it's neither a core point nor a border point.</p>"},{"location":"Machine-Learning/DBSCAN/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Distance Calculation:  For points \\(p = (x_1, y_1)\\) and \\(q = (x_2, y_2)\\), Euclidean distance:</p> \\[d(p,q) = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}\\] <p>Density Reachability: A point \\(p\\) is directly density-reachable from point \\(q\\) if: 1. \\(p \u2208 N_\u03b5(q)\\) (p is in \u03b5-neighborhood of q) 2. \\(q\\) is a core point</p> <p>Density Connectivity: Points \\(p\\) and \\(q\\) are density-connected if there exists a point \\(o\\) such that both \\(p\\) and \\(q\\) are density-reachable from \\(o\\).</p>"},{"location":"Machine-Learning/DBSCAN/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>For each unvisited point:</li> <li>Mark as visited</li> <li> <p>Find all points within \u03b5 distance</p> </li> <li> <p>If point has &lt; MinPts neighbors:</p> </li> <li> <p>Mark as noise (may change later)</p> </li> <li> <p>If point has \u2265 MinPts neighbors:</p> </li> <li>Start new cluster</li> <li>Add point to cluster</li> <li>For each neighbor:<ul> <li>If unvisited, mark as visited and find its neighbors</li> <li>If neighbor has \u2265 MinPts neighbors, add them to seed set</li> <li>If neighbor not in any cluster, add to current cluster</li> </ul> </li> </ol>"},{"location":"Machine-Learning/DBSCAN/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/DBSCAN/#basic-dbscan-with-scikit-learn","title":"Basic DBSCAN with Scikit-learn","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs, make_moons\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\nimport seaborn as sns\n\n# Generate sample data\nnp.random.seed(42)\n\n# Create datasets with different characteristics\n# Dataset 1: Circular blobs\nX_blobs, y_true_blobs = make_blobs(n_samples=300, centers=4, \n                                   n_features=2, cluster_std=0.5, \n                                   random_state=42)\n\n# Dataset 2: Non-linear shapes (moons)\nX_moons, y_true_moons = make_moons(n_samples=200, noise=0.1, \n                                   random_state=42)\n\n# Dataset 3: Varying densities\nX_varied = np.random.rand(250, 2) * 10\n# Add dense regions\ndense_region1 = np.random.multivariate_normal([2, 2], [[0.1, 0], [0, 0.1]], 50)\ndense_region2 = np.random.multivariate_normal([7, 7], [[0.2, 0], [0, 0.2]], 30)\nX_varied = np.vstack([X_varied, dense_region1, dense_region2])\n\ndatasets = [\n    (X_blobs, \"Circular Blobs\"),\n    (X_moons, \"Non-linear Shapes\"),\n    (X_varied, \"Varying Densities\")\n]\n\n# Apply DBSCAN to each dataset\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\nfor idx, (X, title) in enumerate(datasets):\n    # Standardize data\n    X_scaled = StandardScaler().fit_transform(X)\n\n    # Apply DBSCAN\n    dbscan = DBSCAN(eps=0.3, min_samples=5)\n    cluster_labels = dbscan.fit_predict(X_scaled)\n\n    # Number of clusters (excluding noise)\n    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n    n_noise = list(cluster_labels).count(-1)\n\n    # Plot original data\n    axes[0, idx].scatter(X[:, 0], X[:, 1], c='blue', alpha=0.6)\n    axes[0, idx].set_title(f'Original: {title}')\n    axes[0, idx].set_xlabel('Feature 1')\n    axes[0, idx].set_ylabel('Feature 2')\n\n    # Plot clustered data\n    unique_labels = set(cluster_labels)\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            # Noise points in black\n            col = 'black'\n            marker = 'x'\n            label = 'Noise'\n        else:\n            marker = 'o'\n            label = f'Cluster {k}'\n\n        class_member_mask = (cluster_labels == k)\n        xy = X[class_member_mask]\n        axes[1, idx].scatter(xy[:, 0], xy[:, 1], c=[col], \n                           marker=marker, alpha=0.6, s=50, label=label)\n\n    axes[1, idx].set_title(f'DBSCAN: {n_clusters} clusters, {n_noise} noise points')\n    axes[1, idx].set_xlabel('Feature 1')\n    axes[1, idx].set_ylabel('Feature 2')\n    axes[1, idx].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Performance metrics example\nX_sample, y_true = make_blobs(n_samples=200, centers=3, \n                              n_features=2, random_state=42)\nX_sample = StandardScaler().fit_transform(X_sample)\n\ndbscan_sample = DBSCAN(eps=0.5, min_samples=5)\ny_pred = dbscan_sample.fit_predict(X_sample)\n\n# Remove noise points for silhouette score calculation\nmask = y_pred != -1\nif np.sum(mask) &gt; 1:\n    silhouette = silhouette_score(X_sample[mask], y_pred[mask])\n    print(f\"Silhouette Score: {silhouette:.3f}\")\n\n# If we have true labels\nari = adjusted_rand_score(y_true, y_pred)\nprint(f\"Adjusted Rand Index: {ari:.3f}\")\nprint(f\"Number of clusters found: {len(set(y_pred)) - (1 if -1 in y_pred else 0)}\")\nprint(f\"Number of noise points: {list(y_pred).count(-1)}\")\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#parameter-tuning-and-analysis","title":"Parameter Tuning and Analysis","text":"<pre><code>from sklearn.neighbors import NearestNeighbors\nfrom kneed import KneeLocator\n\ndef find_optimal_eps(X, min_samples=5, plot=True):\n    \"\"\"\n    Find optimal eps parameter using k-distance graph\n    \"\"\"\n    # Calculate distances to k-th nearest neighbor\n    neighbors = NearestNeighbors(n_neighbors=min_samples)\n    neighbors_fit = neighbors.fit(X)\n    distances, indices = neighbors_fit.kneighbors(X)\n\n    # Sort distances\n    distances = np.sort(distances[:, min_samples-1], axis=0)\n\n    if plot:\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(len(distances)), distances)\n        plt.xlabel('Data Points sorted by distance')\n        plt.ylabel(f'{min_samples}-NN Distance')\n        plt.title('K-Distance Graph for Optimal Eps Selection')\n        plt.grid(True)\n\n        # Find knee point\n        kneedle = KneeLocator(range(len(distances)), distances, \n                             curve=\"convex\", direction=\"increasing\")\n        if kneedle.knee:\n            optimal_eps = distances[kneedle.knee]\n            plt.axhline(y=optimal_eps, color='red', linestyle='--', \n                       label=f'Optimal eps \u2248 {optimal_eps:.3f}')\n            plt.legend()\n            plt.show()\n            return optimal_eps\n        else:\n            plt.show()\n            return None\n    else:\n        kneedle = KneeLocator(range(len(distances)), distances, \n                             curve=\"convex\", direction=\"increasing\")\n        return distances[kneedle.knee] if kneedle.knee else None\n\n# Parameter sensitivity analysis\ndef analyze_parameter_sensitivity(X, eps_range, min_samples_range):\n    \"\"\"\n    Analyze how different parameter combinations affect clustering\n    \"\"\"\n    results = []\n\n    for eps in eps_range:\n        for min_samples in min_samples_range:\n            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n            labels = dbscan.fit_predict(X)\n\n            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n            n_noise = list(labels).count(-1)\n\n            results.append({\n                'eps': eps,\n                'min_samples': min_samples,\n                'n_clusters': n_clusters,\n                'n_noise': n_noise,\n                'noise_ratio': n_noise / len(X)\n            })\n\n    return results\n\n# Example usage\nX_analysis, _ = make_blobs(n_samples=300, centers=4, random_state=42)\nX_analysis = StandardScaler().fit_transform(X_analysis)\n\n# Find optimal eps\noptimal_eps = find_optimal_eps(X_analysis, min_samples=5)\n\n# Parameter sensitivity analysis\neps_range = np.arange(0.1, 1.0, 0.1)\nmin_samples_range = range(3, 15, 2)\n\nresults = analyze_parameter_sensitivity(X_analysis, eps_range, min_samples_range)\n\n# Visualize parameter sensitivity\nimport pandas as pd\n\ndf_results = pd.DataFrame(results)\npivot_clusters = df_results.pivot(index='min_samples', columns='eps', values='n_clusters')\npivot_noise = df_results.pivot(index='min_samples', columns='eps', values='noise_ratio')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\nsns.heatmap(pivot_clusters, annot=True, fmt='d', cmap='viridis', ax=ax1)\nax1.set_title('Number of Clusters')\nax1.set_xlabel('Eps')\nax1.set_ylabel('Min Samples')\n\nsns.heatmap(pivot_noise, annot=True, fmt='.2f', cmap='Reds', ax=ax2)\nax2.set_title('Noise Ratio')\nax2.set_xlabel('Eps')\nax2.set_ylabel('Min Samples')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#real-world-application-customer-segmentation","title":"Real-world Application: Customer Segmentation","text":"<pre><code># Simulate customer data for segmentation\nnp.random.seed(42)\n\n# Create synthetic customer data\nn_customers = 1000\n\n# Customer features\nage = np.random.normal(35, 12, n_customers)\nage = np.clip(age, 18, 80)\n\nincome = np.random.lognormal(10.5, 0.5, n_customers)\nincome = np.clip(income, 20000, 200000)\n\nspending_score = np.random.beta(2, 5, n_customers) * 100\n\n# Add some correlation\nspending_score += (income / 2000) + np.random.normal(0, 5, n_customers)\nspending_score = np.clip(spending_score, 0, 100)\n\n# Create customer dataset\ncustomer_data = np.column_stack([age, income/1000, spending_score])\nfeature_names = ['Age', 'Income (k$)', 'Spending Score']\n\n# Standardize the data\nscaler = StandardScaler()\ncustomer_data_scaled = scaler.fit_transform(customer_data)\n\n# Apply DBSCAN\ndbscan_customers = DBSCAN(eps=0.5, min_samples=20)\ncustomer_clusters = dbscan_customers.fit_predict(customer_data_scaled)\n\n# Analyze results\nn_clusters = len(set(customer_clusters)) - (1 if -1 in customer_clusters else 0)\nn_noise = list(customer_clusters).count(-1)\n\nprint(f\"Customer Segmentation Results:\")\nprint(f\"Number of customer segments: {n_clusters}\")\nprint(f\"Number of outlier customers: {n_noise}\")\nprint(f\"Percentage of outliers: {n_noise/len(customer_data)*100:.1f}%\")\n\n# Visualize customer segments\nfig = plt.figure(figsize=(15, 5))\n\n# 2D projections\nfeature_pairs = [(0, 1), (0, 2), (1, 2)]\npair_names = [('Age', 'Income'), ('Age', 'Spending'), ('Income', 'Spending')]\n\nfor i, ((f1, f2), (name1, name2)) in enumerate(zip(feature_pairs, pair_names)):\n    ax = plt.subplot(1, 3, i+1)\n\n    unique_labels = set(customer_clusters)\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            col = 'black'\n            marker = 'x'\n            label = 'Outliers'\n            alpha = 0.3\n        else:\n            marker = 'o'\n            label = f'Segment {k}'\n            alpha = 0.7\n\n        class_member_mask = (customer_clusters == k)\n        xy = customer_data[class_member_mask]\n        plt.scatter(xy[:, f1], xy[:, f2], c=[col], marker=marker, \n                   alpha=alpha, s=30, label=label)\n\n    plt.xlabel(name1)\n    plt.ylabel(name2)\n    plt.title(f'{name1} vs {name2}')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Segment analysis\nprint(\"\\nCustomer Segment Analysis:\")\nfor cluster_id in sorted(set(customer_clusters)):\n    if cluster_id == -1:\n        continue\n\n    mask = customer_clusters == cluster_id\n    segment_data = customer_data[mask]\n\n    print(f\"\\nSegment {cluster_id} (n={np.sum(mask)}):\")\n    print(f\"  Average Age: {np.mean(segment_data[:, 0]):.1f} years\")\n    print(f\"  Average Income: ${np.mean(segment_data[:, 1]*1000):,.0f}\")\n    print(f\"  Average Spending Score: {np.mean(segment_data[:, 2]):.1f}\")\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\nclass DBSCAN_FromScratch:\n    \"\"\"\n    From-scratch implementation of DBSCAN clustering algorithm\n    \"\"\"\n\n    def __init__(self, eps=0.5, min_samples=5, metric='euclidean'):\n        \"\"\"\n        Initialize DBSCAN parameters\n\n        Parameters:\n        eps: float, maximum distance between two samples for one to be \n             considered as in the neighborhood of the other\n        min_samples: int, number of samples in a neighborhood for a point\n                    to be considered as a core point\n        metric: str, distance metric to use\n        \"\"\"\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.labels_ = None\n        self.core_sample_indices_ = None\n\n    def _get_neighbors(self, X, point_idx):\n        \"\"\"\n        Find all neighbors within eps distance of a point\n\n        Parameters:\n        X: array-like, shape (n_samples, n_features)\n        point_idx: int, index of the point to find neighbors for\n\n        Returns:\n        neighbors: list of indices of neighboring points\n        \"\"\"\n        neighbors = []\n        point = X[point_idx]\n\n        for i in range(len(X)):\n            if i != point_idx:\n                distance = np.linalg.norm(X[i] - point)\n                if distance &lt;= self.eps:\n                    neighbors.append(i)\n\n        # Include the point itself\n        neighbors.append(point_idx)\n        return neighbors\n\n    def _expand_cluster(self, X, point_idx, neighbors, cluster_id, labels, visited):\n        \"\"\"\n        Expand cluster by adding density-reachable points\n\n        Parameters:\n        X: array-like, input data\n        point_idx: int, index of core point\n        neighbors: list, indices of neighbors\n        cluster_id: int, current cluster identifier\n        labels: array, cluster labels for all points\n        visited: set, set of visited points\n\n        Returns:\n        bool: True if cluster was expanded successfully\n        \"\"\"\n        labels[point_idx] = cluster_id\n\n        i = 0\n        while i &lt; len(neighbors):\n            neighbor_idx = neighbors[i]\n\n            if neighbor_idx not in visited:\n                visited.add(neighbor_idx)\n                neighbor_neighbors = self._get_neighbors(X, neighbor_idx)\n\n                # If neighbor is also a core point, add its neighbors\n                if len(neighbor_neighbors) &gt;= self.min_samples:\n                    # Add new neighbors to the list\n                    for new_neighbor in neighbor_neighbors:\n                        if new_neighbor not in neighbors:\n                            neighbors.append(new_neighbor)\n\n            # If neighbor is not assigned to any cluster, assign to current cluster\n            if labels[neighbor_idx] == -2:  # -2 means unassigned\n                labels[neighbor_idx] = cluster_id\n\n            i += 1\n\n        return True\n\n    def fit_predict(self, X):\n        \"\"\"\n        Perform DBSCAN clustering\n\n        Parameters:\n        X: array-like, shape (n_samples, n_features)\n\n        Returns:\n        labels: array, cluster labels for each point (-1 for noise)\n        \"\"\"\n        X = np.array(X)\n        n_points = len(X)\n\n        # Initialize labels: -2 = unassigned, -1 = noise, \u22650 = cluster id\n        labels = np.full(n_points, -2, dtype=int)\n        visited = set()\n        cluster_id = 0\n        core_samples = []\n\n        for point_idx in range(n_points):\n            if point_idx in visited:\n                continue\n\n            visited.add(point_idx)\n\n            # Find neighbors\n            neighbors = self._get_neighbors(X, point_idx)\n\n            # Check if point is a core point\n            if len(neighbors) &lt; self.min_samples:\n                # Mark as noise (may change later if it becomes border point)\n                labels[point_idx] = -1\n            else:\n                # Point is a core point\n                core_samples.append(point_idx)\n\n                # Expand cluster from this core point\n                self._expand_cluster(X, point_idx, neighbors, cluster_id, \n                                   labels, visited)\n                cluster_id += 1\n\n        self.labels_ = labels\n        self.core_sample_indices_ = np.array(core_samples)\n\n        return labels\n\n    def fit(self, X):\n        \"\"\"\n        Fit DBSCAN clustering\n\n        Parameters:\n        X: array-like, shape (n_samples, n_features)\n\n        Returns:\n        self: object\n        \"\"\"\n        self.fit_predict(X)\n        return self\n\n    def get_cluster_info(self):\n        \"\"\"\n        Get information about clustering results\n\n        Returns:\n        dict: clustering information\n        \"\"\"\n        if self.labels_ is None:\n            raise ValueError(\"Model has not been fitted yet.\")\n\n        unique_labels = set(self.labels_)\n        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n        n_noise = list(self.labels_).count(-1)\n\n        cluster_sizes = {}\n        for label in unique_labels:\n            if label != -1:\n                cluster_sizes[f'cluster_{label}'] = list(self.labels_).count(label)\n\n        return {\n            'n_clusters': n_clusters,\n            'n_noise_points': n_noise,\n            'n_core_points': len(self.core_sample_indices_),\n            'cluster_sizes': cluster_sizes\n        }\n\n# Example usage and comparison with sklearn\nif __name__ == \"__main__\":\n    # Generate test data\n    np.random.seed(42)\n    X_test, _ = make_blobs(n_samples=150, centers=3, \n                          n_features=2, cluster_std=0.8, \n                          random_state=42)\n\n    # Standardize data\n    X_test = StandardScaler().fit_transform(X_test)\n\n    # Our implementation\n    dbscan_custom = DBSCAN_FromScratch(eps=0.3, min_samples=5)\n    labels_custom = dbscan_custom.fit_predict(X_test)\n\n    # Sklearn implementation\n    dbscan_sklearn = DBSCAN(eps=0.3, min_samples=5)\n    labels_sklearn = dbscan_sklearn.fit_predict(X_test)\n\n    # Compare results\n    print(\"Comparison of implementations:\")\n    print(f\"Custom DBSCAN - Clusters: {len(set(labels_custom)) - (1 if -1 in labels_custom else 0)}, \"\n          f\"Noise: {list(labels_custom).count(-1)}\")\n    print(f\"Sklearn DBSCAN - Clusters: {len(set(labels_sklearn)) - (1 if -1 in labels_sklearn else 0)}, \"\n          f\"Noise: {list(labels_sklearn).count(-1)}\")\n\n    # Check if results are identical (may differ due to tie-breaking)\n    agreement = np.mean(labels_custom == labels_sklearn)\n    print(f\"Agreement between implementations: {agreement:.1%}\")\n\n    # Visualize both results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Plot custom implementation results\n    unique_labels = set(labels_custom)\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            col = 'black'\n            marker = 'x'\n        else:\n            marker = 'o'\n\n        class_member_mask = (labels_custom == k)\n        xy = X_test[class_member_mask]\n        ax1.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, alpha=0.7, s=50)\n\n    ax1.set_title('Custom DBSCAN Implementation')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n\n    # Plot sklearn results\n    unique_labels = set(labels_sklearn)\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            col = 'black'\n            marker = 'x'\n        else:\n            marker = 'o'\n\n        class_member_mask = (labels_sklearn == k)\n        xy = X_test[class_member_mask]\n        ax2.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, alpha=0.7, s=50)\n\n    ax2.set_title('Sklearn DBSCAN')\n    ax2.set_xlabel('Feature 1')\n    ax2.set_ylabel('Feature 2')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Show detailed cluster info\n    info = dbscan_custom.get_cluster_info()\n    print(\"\\nDetailed clustering information:\")\n    for key, value in info.items():\n        print(f\"{key}: {value}\")\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/DBSCAN/#assumptions","title":"Assumptions","text":"<ol> <li>Distance Metric: Assumes that the chosen distance metric (usually Euclidean) is appropriate for the data</li> <li>Density Definition: Assumes that clusters can be defined by regions of high density</li> <li>Parameter Stability: Assumes that optimal <code>eps</code> and <code>min_samples</code> parameters exist and are stable</li> <li>Global Density: Works best when clusters have similar densities</li> </ol>"},{"location":"Machine-Learning/DBSCAN/#limitations","title":"Limitations","text":"<ol> <li>Parameter Sensitivity:</li> <li>Very sensitive to <code>eps</code> parameter choice</li> <li><code>min_samples</code> affects the minimum cluster size</li> <li> <p>No systematic way to choose optimal parameters</p> </li> <li> <p>Varying Densities:</p> </li> <li>Struggles with clusters of very different densities</li> <li>May merge nearby clusters of different densities</li> <li> <p>May split single clusters with varying internal density</p> </li> <li> <p>High Dimensions:</p> </li> <li>Curse of dimensionality affects distance calculations</li> <li>All points may appear equidistant in high dimensions</li> <li> <p>Performance degrades significantly above ~10-15 dimensions</p> </li> <li> <p>Memory Usage:</p> </li> <li>Requires computing all pairwise distances</li> <li>Memory complexity: O(n\u00b2)</li> <li> <p>Can be prohibitive for very large datasets</p> </li> <li> <p>Border Point Assignment:</p> </li> <li>Border points may be assigned to different clusters depending on processing order</li> <li>Results may not be deterministic for border cases</li> </ol>"},{"location":"Machine-Learning/DBSCAN/#comparison-with-other-clustering-algorithms","title":"Comparison with Other Clustering Algorithms","text":"Algorithm Pros Cons Best For DBSCAN Handles noise, arbitrary shapes, no K needed Parameter sensitive, struggles with varying densities Non-linear shapes, outlier detection K-Means Fast, simple, works well with spherical clusters Need to specify K, assumes spherical clusters Well-separated, spherical clusters Hierarchical No K needed, creates hierarchy Slow (O(n\u00b3)), sensitive to noise Small datasets, understanding cluster structure Mean Shift No parameters, finds modes Slow, bandwidth selection challenging Image segmentation, mode detection Gaussian Mixture Probabilistic, handles overlapping clusters Assumes Gaussian distributions, need K Overlapping clusters, probabilistic assignments"},{"location":"Machine-Learning/DBSCAN/#when-to-use-dbscan","title":"When to Use DBSCAN","text":"<p>Good for: - Irregularly shaped clusters - Data with noise and outliers - When you don't know the number of clusters - Spatial data analysis - Anomaly detection</p> <p>Avoid when: - Clusters have very different densities - High-dimensional data (&gt;15 dimensions) - Very large datasets (memory constraints) - Need deterministic results for border points</p>"},{"location":"Machine-Learning/DBSCAN/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"Q1: What are the key differences between DBSCAN and K-means clustering? <p>Answer:  | Aspect | DBSCAN | K-means | |--------|---------|---------| | Cluster shape | Arbitrary shapes | Spherical clusters | | Number of clusters | Automatic | Must specify K | | Noise handling | Identifies outliers | Assigns all points to clusters | | Parameters | eps, min_samples | K, random initialization | | Scalability | O(n\u00b2) memory | O(nkd) time | | Deterministic | No (border points) | No (random initialization) |</p> Q2: How do you choose optimal parameters for DBSCAN? <p>Answer: Parameter selection strategies:</p> <p>For eps: - K-distance graph: Plot k-NN distances, look for \"elbow/knee\" point - Domain knowledge: Use meaningful distances for your data - Grid search: Try multiple values with validation metric</p> <p>For min_samples: - Rule of thumb: Start with dimensionality + 1 - Domain specific: Consider minimum meaningful cluster size - Data size: Larger for bigger datasets to avoid noise</p> <p>Example approach: <pre><code># K-distance method\nneighbors = NearestNeighbors(n_neighbors=min_samples)\ndistances = np.sort(neighbors.fit(X).kneighbors(X)[0][:, -1])\n# Plot and find elbow point\n</code></pre></p> Q3: Explain the three types of points in DBSCAN. <p>Answer: - Core Points: Have \u2265 min_samples neighbors within eps distance. Form the \"interior\" of clusters. - Border Points: Have &lt; min_samples neighbors but lie within eps of a core point. Form cluster \"boundaries.\" - Noise Points: Neither core nor border points. Considered outliers.</p> <p>Key insight: Border points can belong to multiple clusters but are assigned to the first one discovered during the algorithm's execution.</p> Q4: What happens when DBSCAN encounters clusters with different densities? <p>Answer: DBSCAN struggles with varying densities: - Low eps: Dense clusters split, sparse clusters become noise - High eps: Sparse clusters merge, may connect distant dense clusters - Result: No single eps value works well for all clusters</p> <p>Solutions: - HDBSCAN: Hierarchical extension that handles varying densities - Preprocessing: Normalize/transform data to similar densities - Local methods: Use locally adaptive parameters</p> Q5: How does DBSCAN handle high-dimensional data? <p>Answer: DBSCAN faces challenges in high dimensions:</p> <p>Problems: - Curse of dimensionality: All points appear equidistant - Concentration: Distances lose discriminative power - Sparsity: All points may become noise</p> <p>Solutions: - Dimensionality reduction: PCA, t-SNE before clustering - Feature selection: Keep only relevant dimensions - Alternative metrics: Use cosine similarity instead of Euclidean - Ensemble methods: Cluster in multiple subspaces</p> Q6: Is DBSCAN deterministic? Why or why not? <p>Answer: DBSCAN is not fully deterministic:</p> <p>Deterministic aspects: - Core point identification is deterministic - Noise point identification is deterministic</p> <p>Non-deterministic aspects: - Border point assignment: Can belong to multiple clusters - Processing order: Algorithm visits points in data order - Tie-breaking: When border point is reachable from multiple cores</p> <p>Making it more deterministic: - Sort data before processing - Use consistent tie-breaking rules - Post-process to resolve ambiguities</p> Q7: How would you evaluate DBSCAN clustering results? <p>Answer: Evaluation approaches depend on label availability:</p> <p>With ground truth labels: - Adjusted Rand Index (ARI): Measures agreement with true clusters - Normalized Mutual Information: Information-theoretic measure - Homogeneity &amp; Completeness: Cluster purity measures</p> <p>Without ground truth: - Silhouette Score: Average silhouette across all samples (excluding noise) - Davies-Bouldin Index: Ratio of within-cluster to between-cluster distances - Visual inspection: Plot clusters in 2D/3D projections - Domain expertise: Check if clusters make business sense</p> Q8: What is the time and space complexity of DBSCAN? <p>Answer: Time Complexity: - Worst case: O(n\u00b2) - when distance computation dominates - Best case: O(n log n) - with spatial indexing (k-d trees, R-trees) - Average: O(n log n) for low dimensions, O(n\u00b2) for high dimensions</p> <p>Space Complexity: - O(n) - storing labels and visited status - Additional O(n\u00b2) if distance matrix is precomputed</p> <p>Optimizations: - Spatial indexing: k-d trees, ball trees, LSH - Approximate methods: LSH for high dimensions - Parallel processing: Parallelize neighbor searches</p> Q9: How does DBSCAN compare to hierarchical clustering? <p>Answer: | Aspect | DBSCAN | Hierarchical | |--------|---------|--------------| | Output | Flat clustering + noise | Dendrogram/hierarchy | | Parameters | eps, min_samples | Linkage criteria, distance metric | | Complexity | O(n\u00b2) to O(n log n) | O(n\u00b3) for agglomerative | | Noise handling | Explicit noise detection | All points clustered | | Shape flexibility | Any shape | Depends on linkage | | Interpretability | Less interpretable | Hierarchy is interpretable |</p> <p>When to choose each: - DBSCAN: Noise detection needed, arbitrary shapes - Hierarchical: Need cluster hierarchy, small datasets</p> Q10: Can you implement a simplified version of the DBSCAN algorithm? <p>Answer: Core algorithm structure: <pre><code>def simple_dbscan(X, eps, min_samples):\n    labels = [-2] * len(X)  # -2: unvisited, -1: noise, \u22650: cluster\n    visited = set()\n    cluster_id = 0\n\n    for i in range(len(X)):\n        if i in visited:\n            continue\n        visited.add(i)\n\n        # Find neighbors\n        neighbors = find_neighbors(X, i, eps)\n\n        if len(neighbors) &lt; min_samples:\n            labels[i] = -1  # Noise\n        else:\n            # Expand cluster\n            expand_cluster(X, i, neighbors, cluster_id, \n                         labels, visited, eps, min_samples)\n            cluster_id += 1\n\n    return labels\n</code></pre> Key steps: Visit points, find dense regions, expand clusters through density-connectivity.</p>"},{"location":"Machine-Learning/DBSCAN/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/DBSCAN/#anomaly-detection-in-network-traffic","title":"Anomaly Detection in Network Traffic","text":"<pre><code># Simulate network traffic data for anomaly detection\nnp.random.seed(42)\n\n# Generate normal network traffic patterns\nn_normal = 800\nnormal_packet_size = np.random.normal(1500, 300, n_normal)  # Bytes\nnormal_frequency = np.random.exponential(0.1, n_normal)     # Packets/sec\nnormal_duration = np.random.gamma(2, 2, n_normal)          # Connection duration\n\n# Generate anomalous patterns\nn_anomalies = 50\n\n# DDoS attack - high frequency, small packets\nddos_packet_size = np.random.normal(64, 10, 20)\nddos_frequency = np.random.normal(100, 20, 20)\nddos_duration = np.random.normal(1, 0.2, 20)\n\n# Port scanning - many short connections\nscan_packet_size = np.random.normal(40, 5, 15)\nscan_frequency = np.random.normal(50, 10, 15)\nscan_duration = np.random.normal(0.1, 0.05, 15)\n\n# Data exfiltration - large packets, sustained\nexfil_packet_size = np.random.normal(5000, 500, 15)\nexfil_frequency = np.random.normal(0.5, 0.1, 15)\nexfil_duration = np.random.normal(300, 50, 15)\n\n# Combine all data\npacket_sizes = np.concatenate([normal_packet_size, ddos_packet_size, \n                              scan_packet_size, exfil_packet_size])\nfrequencies = np.concatenate([normal_frequency, ddos_frequency, \n                             scan_frequency, exfil_frequency])\ndurations = np.concatenate([normal_duration, ddos_duration, \n                           scan_duration, exfil_duration])\n\n# Create feature matrix\nnetwork_data = np.column_stack([packet_sizes, frequencies, durations])\n\n# Standardize features\nscaler = StandardScaler()\nnetwork_data_scaled = scaler.fit_transform(network_data)\n\n# Apply DBSCAN for anomaly detection\ndbscan_network = DBSCAN(eps=0.6, min_samples=10)\nnetwork_labels = dbscan_network.fit_predict(network_data_scaled)\n\n# Analyze results\nn_clusters = len(set(network_labels)) - (1 if -1 in network_labels else 0)\nn_anomalies_detected = list(network_labels).count(-1)\n\nprint(f\"Network Anomaly Detection Results:\")\nprint(f\"Total connections analyzed: {len(network_data)}\")\nprint(f\"Normal behavior clusters found: {n_clusters}\")\nprint(f\"Anomalies detected: {n_anomalies_detected}\")\nprint(f\"Anomaly detection rate: {n_anomalies_detected/len(network_data)*100:.1f}%\")\n\n# Visualize results\nfig = plt.figure(figsize=(15, 10))\n\n# 3D visualization\nax1 = fig.add_subplot(221, projection='3d')\n\nunique_labels = set(network_labels)\ncolors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        col = 'red'\n        marker = '^'\n        label = f'Anomalies (n={list(network_labels).count(k)})'\n        alpha = 0.8\n        size = 60\n    else:\n        marker = 'o'\n        label = f'Normal Cluster {k} (n={list(network_labels).count(k)})'\n        alpha = 0.6\n        size = 30\n\n    class_member_mask = (network_labels == k)\n    data_subset = network_data[class_member_mask]\n\n    ax1.scatter(data_subset[:, 0], data_subset[:, 1], data_subset[:, 2],\n               c=[col], marker=marker, alpha=alpha, s=size, label=label)\n\nax1.set_xlabel('Packet Size (bytes)')\nax1.set_ylabel('Frequency (packets/sec)')\nax1.set_zlabel('Duration (seconds)')\nax1.set_title('3D Network Traffic Analysis')\nax1.legend()\n\n# 2D projections\nprojections = [(0, 1, 'Packet Size', 'Frequency'),\n               (0, 2, 'Packet Size', 'Duration'),\n               (1, 2, 'Frequency', 'Duration')]\n\nfor i, (f1, f2, name1, name2) in enumerate(projections):\n    ax = fig.add_subplot(2, 2, i+2)\n\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            col = 'red'\n            marker = '^'\n            alpha = 0.8\n            size = 60\n        else:\n            marker = 'o'\n            alpha = 0.6\n            size = 30\n\n        class_member_mask = (network_labels == k)\n        data_subset = network_data[class_member_mask]\n\n        if len(data_subset) &gt; 0:\n            ax.scatter(data_subset[:, f1], data_subset[:, f2],\n                      c=[col], marker=marker, alpha=alpha, s=size)\n\n    ax.set_xlabel(name1)\n    ax.set_ylabel(name2)\n    ax.set_title(f'{name1} vs {name2}')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Detailed anomaly analysis\nanomaly_indices = np.where(network_labels == -1)[0]\nnormal_indices = np.where(network_labels != -1)[0]\n\nif len(anomaly_indices) &gt; 0:\n    print(f\"\\nAnomaly Characteristics:\")\n    anomaly_data = network_data[anomaly_indices]\n    normal_data = network_data[normal_indices]\n\n    features = ['Packet Size', 'Frequency', 'Duration']\n\n    for i, feature in enumerate(features):\n        anomaly_mean = np.mean(anomaly_data[:, i])\n        normal_mean = np.mean(normal_data[:, i])\n\n        print(f\"{feature}:\")\n        print(f\"  Anomalies - Mean: {anomaly_mean:.2f}, Std: {np.std(anomaly_data[:, i]):.2f}\")\n        print(f\"  Normal - Mean: {normal_mean:.2f}, Std: {np.std(normal_data[:, i]):.2f}\")\n        print(f\"  Difference: {(anomaly_mean - normal_mean)/normal_mean*100:+.1f}%\")\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#image-segmentation-application","title":"Image Segmentation Application","text":"<pre><code>from sklearn.datasets import load_sample_image\nfrom skimage import segmentation, color\n\ndef image_segmentation_dbscan(image_path=None, n_segments=100):\n    \"\"\"\n    Perform image segmentation using DBSCAN on SLIC superpixels\n    \"\"\"\n    # Load sample image (or use sklearn's sample)\n    if image_path is None:\n        image = load_sample_image(\"flower.jpg\")\n    else:\n        from PIL import Image\n        image = np.array(Image.open(image_path))\n\n    # Resize for faster processing\n    if image.shape[0] &gt; 300:\n        from skimage.transform import resize\n        image = resize(image, (300, 400), anti_aliasing=True)\n        image = (image * 255).astype(np.uint8)\n\n    print(f\"Image shape: {image.shape}\")\n\n    # Convert to LAB color space for better segmentation\n    image_lab = color.rgb2lab(image)\n\n    # Generate superpixels using SLIC\n    segments = segmentation.slic(image, n_segments=n_segments, compactness=10, \n                                sigma=1, start_label=1)\n\n    # Extract features for each superpixel\n    n_superpixels = np.max(segments)\n    features = []\n\n    for segment_id in range(1, n_superpixels + 1):\n        mask = segments == segment_id\n        if np.sum(mask) == 0:\n            continue\n\n        # Color features (mean LAB values)\n        l_mean = np.mean(image_lab[mask, 0])\n        a_mean = np.mean(image_lab[mask, 1])\n        b_mean = np.mean(image_lab[mask, 2])\n\n        # Texture features (standard deviation)\n        l_std = np.std(image_lab[mask, 0])\n        a_std = np.std(image_lab[mask, 1])\n        b_std = np.std(image_lab[mask, 2])\n\n        # Spatial features (centroid)\n        y_coords, x_coords = np.where(mask)\n        centroid_y = np.mean(y_coords)\n        centroid_x = np.mean(x_coords)\n\n        # Size feature\n        size = np.sum(mask)\n\n        features.append([l_mean, a_mean, b_mean, l_std, a_std, b_std,\n                        centroid_y/image.shape[0], centroid_x/image.shape[1], \n                        np.log(size)])\n\n    features = np.array(features)\n\n    # Standardize features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n\n    # Apply DBSCAN clustering\n    dbscan_img = DBSCAN(eps=0.5, min_samples=3)\n    cluster_labels = dbscan_img.fit_predict(features_scaled)\n\n    # Create segmented image\n    segmented_image = np.zeros_like(image)\n    unique_labels = set(cluster_labels)\n    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n\n    # Assign colors to clusters\n    colors_palette = plt.cm.Set3(np.linspace(0, 1, n_clusters + 1))\n\n    color_map = {}\n    color_idx = 0\n    for label in unique_labels:\n        if label == -1:\n            color_map[label] = [0, 0, 0]  # Black for noise\n        else:\n            color_map[label] = (colors_palette[color_idx][:3] * 255).astype(int)\n            color_idx += 1\n\n    # Apply colors to segments\n    for i, (segment_id, cluster_label) in enumerate(zip(range(1, n_superpixels + 1), \n                                                        cluster_labels)):\n        if i &gt;= len(cluster_labels):\n            break\n        mask = segments == segment_id\n        segmented_image[mask] = color_map[cluster_label]\n\n    # Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # Original image\n    axes[0, 0].imshow(image)\n    axes[0, 0].set_title('Original Image')\n    axes[0, 0].axis('off')\n\n    # SLIC superpixels\n    axes[0, 1].imshow(segmentation.mark_boundaries(image, segments))\n    axes[0, 1].set_title(f'SLIC Superpixels (n={n_superpixels})')\n    axes[0, 1].axis('off')\n\n    # DBSCAN segmentation\n    axes[1, 0].imshow(segmented_image)\n    axes[1, 0].set_title(f'DBSCAN Segmentation (n={n_clusters} regions)')\n    axes[1, 0].axis('off')\n\n    # Combined overlay\n    overlay = image.copy()\n    boundaries = segmentation.find_boundaries(segments, mode='thick')\n    overlay[boundaries] = [255, 0, 0]  # Red boundaries\n    axes[1, 1].imshow(overlay)\n    axes[1, 1].set_title('Superpixel Boundaries')\n    axes[1, 1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Segmentation Results:\")\n    print(f\"Number of superpixels: {n_superpixels}\")\n    print(f\"Number of regions found: {n_clusters}\")\n    print(f\"Number of noise superpixels: {list(cluster_labels).count(-1)}\")\n\n    return segmented_image, cluster_labels, features\n\n# Run image segmentation\ntry:\n    segmented_img, labels, features = image_segmentation_dbscan()\nexcept ImportError:\n    print(\"Skipping image segmentation example - requires additional dependencies\")\n    print(\"Install with: pip install scikit-image pillow\")\n</code></pre>"},{"location":"Machine-Learning/DBSCAN/#references","title":"\ud83d\udcda References","text":"<ol> <li>Original Paper:</li> <li>\"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\" - Ester et al. (1996)</li> <li> <p>DBSCAN Paper</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn DBSCAN</li> <li> <p>Scikit-learn Clustering Guide</p> </li> <li> <p>Books:</p> </li> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>\"Data Mining: Concepts and Techniques\" by Han, Kamber, and Pei</li> <li> <p>\"Introduction to Data Mining\" by Tan, Steinbach, and Kumar</p> </li> <li> <p>Extensions and Variations:</p> </li> <li>\"HDBSCAN: Hierarchical Density-Based Spatial Clustering of Applications with Noise\" - Campello et al. (2013)</li> <li> <p>\"OPTICS: Ordering Points To Identify the Clustering Structure\" - Ankerst et al. (1999)</p> </li> <li> <p>Online Resources:</p> </li> <li>DBSCAN Visualization</li> <li>Scikit-learn Clustering Comparison</li> <li> <p>Towards Data Science: DBSCAN Articles</p> </li> <li> <p>Video Tutorials:</p> </li> <li>StatQuest: DBSCAN</li> <li> <p>Machine Learning Explained: DBSCAN</p> </li> <li> <p>Implementations:</p> </li> <li>HDBSCAN Library</li> <li>Fast DBSCAN Implementation</li> </ol>"},{"location":"Machine-Learning/Decision%20Trees/","title":"\ud83d\udcd8 Decision Trees","text":"<p>Decision Trees are versatile, interpretable machine learning algorithms that make predictions by learning simple decision rules inferred from data features, creating a tree-like model of decisions.</p> <p>Resources: Scikit-learn Decision Trees | Elements of Statistical Learning - Chapter 9</p>"},{"location":"Machine-Learning/Decision%20Trees/#summary","title":"\u270d\ufe0f Summary","text":"<p>Decision Trees are supervised learning algorithms that can be used for both classification and regression tasks. They work by recursively splitting the data into subsets based on feature values that best separate the target classes or minimize prediction error.</p> <p>Key characteristics: - Interpretability: Easy to understand and visualize decision paths - Non-parametric: No assumptions about data distribution - Feature selection: Automatically identifies important features - Handles mixed data: Works with both numerical and categorical features - Non-linear relationships: Can capture complex patterns</p> <p>Applications: - Medical diagnosis systems - Credit approval decisions - Customer segmentation - Feature selection - Rule extraction - Fraud detection</p> <p>Types: - Classification Trees: Predict discrete class labels - Regression Trees: Predict continuous values</p>"},{"location":"Machine-Learning/Decision%20Trees/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Decision%20Trees/#how-decision-trees-work","title":"How Decision Trees Work","text":"<p>A Decision Tree learns by asking a series of binary questions about the features. Each internal node represents a test on a feature, each branch represents an outcome of the test, and each leaf node represents a class prediction or numerical value.</p>"},{"location":"Machine-Learning/Decision%20Trees/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Decision%20Trees/#1-splitting-criteria","title":"1. Splitting Criteria","text":"<p>For Classification (Gini Impurity): \\(\\(\\text{Gini}(S) = 1 - \\sum_{i=1}^{c} p_i^2\\)\\)</p> <p>Where: - \\(S\\) is the set of examples - \\(c\\) is the number of classes - \\(p_i\\) is the proportion of examples belonging to class \\(i\\)</p> <p>For Classification (Entropy): \\(\\(\\text{Entropy}(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\\)</p> <p>For Regression (Mean Squared Error): \\(\\(\\text{MSE}(S) = \\frac{1}{|S|} \\sum_{i=1}^{|S|} (y_i - \\bar{y})^2\\)\\)</p> <p>Where \\(\\bar{y}\\) is the mean of target values in set \\(S\\).</p>"},{"location":"Machine-Learning/Decision%20Trees/#2-information-gain","title":"2. Information Gain","text":"<p>The algorithm selects the feature that maximizes information gain:</p> \\[\\text{InfoGain}(S, A) = \\text{Impurity}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Impurity}(S_v)\\] <p>Where: - \\(A\\) is the attribute (feature) - \\(S_v\\) is the subset of \\(S\\) where attribute \\(A\\) has value \\(v\\)</p>"},{"location":"Machine-Learning/Decision%20Trees/#3-stopping-criteria","title":"3. Stopping Criteria","text":"<p>The tree stops growing when: - All examples have the same class (pure node) - No more features to split on - Maximum depth reached - Minimum samples per leaf reached - Information gain below threshold</p>"},{"location":"Machine-Learning/Decision%20Trees/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Start with root node containing all training data</li> <li>For each node:</li> <li>Calculate impurity measure</li> <li>Find best feature and threshold to split</li> <li>Create child nodes</li> <li>Recursively repeat for child nodes</li> <li>Stop when stopping criteria met</li> <li>Assign prediction to leaf nodes</li> </ol>"},{"location":"Machine-Learning/Decision%20Trees/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Decision%20Trees/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification, load_iris, load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.tree import export_text, plot_tree\nfrom sklearn.metrics import accuracy_score, classification_report, mean_squared_error\nimport seaborn as sns\n\n# Classification Example\n# Generate sample data\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_informative=3,\n    n_redundant=1,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train classifier\nclf = DecisionTreeClassifier(\n    criterion='gini',           # or 'entropy'\n    max_depth=5,               # prevent overfitting\n    min_samples_split=20,      # minimum samples to split\n    min_samples_leaf=10,       # minimum samples in leaf\n    random_state=42\n)\n\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Visualize tree structure\nplt.figure(figsize=(20, 10))\nplot_tree(clf, \n          feature_names=[f'Feature_{i}' for i in range(X.shape[1])],\n          class_names=['Class_0', 'Class_1'],\n          filled=True,\n          rounded=True,\n          fontsize=10)\nplt.title(\"Decision Tree Structure\")\nplt.show()\n\n# Feature importance\nfeature_importance = clf.feature_importances_\nfeatures = [f'Feature_{i}' for i in range(X.shape[1])]\n\nplt.figure(figsize=(10, 6))\nplt.bar(features, feature_importance)\nplt.title('Feature Importance')\nplt.ylabel('Importance')\nplt.xticks(rotation=45)\nplt.show()\n\nprint(\"\\nFeature Importances:\")\nfor feature, importance in zip(features, feature_importance):\n    print(f\"{feature}: {importance:.3f}\")\n\n# Real-world example with Iris dataset\niris = load_iris()\nX_iris, y_iris = iris.data, iris.target\n\nX_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n    X_iris, y_iris, test_size=0.3, random_state=42\n)\n\n# Train classifier\niris_clf = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=3,\n    random_state=42\n)\niris_clf.fit(X_train_iris, y_train_iris)\n\n# Predictions\ny_pred_iris = iris_clf.predict(X_test_iris)\niris_accuracy = accuracy_score(y_test_iris, y_pred_iris)\nprint(f\"\\nIris Dataset Accuracy: {iris_accuracy:.3f}\")\n\n# Print decision tree rules\ntree_rules = export_text(iris_clf, \n                        feature_names=iris.feature_names)\nprint(\"\\nDecision Tree Rules:\")\nprint(tree_rules)\n\n# Regression Example\nfrom sklearn.datasets import make_regression\n\n# Generate regression data\nX_reg, y_reg = make_regression(\n    n_samples=500,\n    n_features=1,\n    noise=10,\n    random_state=42\n)\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\n# Train regression tree\nreg_tree = DecisionTreeRegressor(\n    max_depth=3,\n    min_samples_split=20,\n    random_state=42\n)\nreg_tree.fit(X_train_reg, y_train_reg)\n\n# Predictions\ny_pred_reg = reg_tree.predict(X_test_reg)\nmse = mean_squared_error(y_test_reg, y_pred_reg)\nprint(f\"\\nRegression MSE: {mse:.3f}\")\n\n# Visualize regression tree predictions\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Actual')\nplt.scatter(X_test_reg, y_pred_reg, alpha=0.6, label='Predicted')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.title('Decision Tree Regression')\nplt.legend()\n\n# Show step-wise predictions\nX_plot = np.linspace(X_reg.min(), X_reg.max(), 300).reshape(-1, 1)\ny_plot = reg_tree.predict(X_plot)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_train_reg, y_train_reg, alpha=0.6, label='Training Data')\nplt.plot(X_plot, y_plot, color='red', linewidth=2, label='Tree Prediction')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.title('Decision Tree Regression Function')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Decision%20Trees/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 5, 7, None],\n    'min_samples_split': [2, 10, 20],\n    'min_samples_leaf': [1, 5, 10]\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best cross-validation score:\", grid_search.best_score_)\n\n# Use best model\nbest_clf = grid_search.best_estimator_\nbest_pred = best_clf.predict(X_test)\nprint(\"Best model accuracy:\", accuracy_score(y_test, best_pred))\n</code></pre>"},{"location":"Machine-Learning/Decision%20Trees/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nfrom collections import Counter\n\nclass Node:\n    \"\"\"Represents a node in the decision tree\"\"\"\n\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature      # Feature index to split on\n        self.threshold = threshold  # Threshold value for splitting\n        self.left = left           # Left child node\n        self.right = right         # Right child node\n        self.value = value         # Value if leaf node (class or regression value)\n\nclass DecisionTreeFromScratch:\n    \"\"\"Decision Tree implementation from scratch\"\"\"\n\n    def __init__(self, max_depth=3, min_samples_split=2, criterion='gini', tree_type='classification'):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.criterion = criterion\n        self.tree_type = tree_type\n        self.root = None\n\n    def fit(self, X, y):\n        \"\"\"Train the decision tree\"\"\"\n        self.n_features = X.shape[1]\n        self.root = self._grow_tree(X, y, depth=0)\n\n    def predict(self, X):\n        \"\"\"Make predictions on test data\"\"\"\n        return np.array([self._predict_single(sample, self.root) for sample in X])\n\n    def _grow_tree(self, X, y, depth):\n        \"\"\"Recursively grow the tree\"\"\"\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        # Stopping criteria\n        if (depth &gt;= self.max_depth or \n            n_samples &lt; self.min_samples_split or \n            n_classes == 1):\n            leaf_value = self._most_common_class(y) if self.tree_type == 'classification' else np.mean(y)\n            return Node(value=leaf_value)\n\n        # Find best split\n        best_feature, best_threshold = self._best_split(X, y)\n\n        # Create child splits\n        left_indices, right_indices = self._split_data(X[:, best_feature], best_threshold)\n        left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n        right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n\n        return Node(feature=best_feature, threshold=best_threshold, \n                   left=left_child, right=right_child)\n\n    def _best_split(self, X, y):\n        \"\"\"Find the best feature and threshold to split on\"\"\"\n        best_gain = -1\n        best_feature, best_threshold = None, None\n\n        for feature_idx in range(self.n_features):\n            feature_values = X[:, feature_idx]\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                gain = self._information_gain(y, feature_values, threshold)\n\n                if gain &gt; best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        return best_feature, best_threshold\n\n    def _information_gain(self, y, feature_values, threshold):\n        \"\"\"Calculate information gain for a split\"\"\"\n        # Parent impurity\n        parent_impurity = self._calculate_impurity(y)\n\n        # Create splits\n        left_indices, right_indices = self._split_data(feature_values, threshold)\n\n        if len(left_indices) == 0 or len(right_indices) == 0:\n            return 0\n\n        # Calculate weighted impurity of children\n        n = len(y)\n        n_left, n_right = len(left_indices), len(right_indices)\n        impurity_left = self._calculate_impurity(y[left_indices])\n        impurity_right = self._calculate_impurity(y[right_indices])\n\n        child_impurity = (n_left / n) * impurity_left + (n_right / n) * impurity_right\n\n        # Information gain\n        return parent_impurity - child_impurity\n\n    def _calculate_impurity(self, y):\n        \"\"\"Calculate impurity based on criterion\"\"\"\n        if self.tree_type == 'classification':\n            if self.criterion == 'gini':\n                return self._gini_impurity(y)\n            elif self.criterion == 'entropy':\n                return self._entropy(y)\n        else:  # regression\n            return self._mse(y)\n\n    def _gini_impurity(self, y):\n        \"\"\"Calculate Gini impurity\"\"\"\n        if len(y) == 0:\n            return 0\n\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / len(y)\n        return 1 - np.sum(probabilities ** 2)\n\n    def _entropy(self, y):\n        \"\"\"Calculate entropy\"\"\"\n        if len(y) == 0:\n            return 0\n\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / len(y)\n        return -np.sum(probabilities * np.log2(probabilities + 1e-8))  # Add small value to avoid log(0)\n\n    def _mse(self, y):\n        \"\"\"Calculate Mean Squared Error\"\"\"\n        if len(y) == 0:\n            return 0\n        return np.mean((y - np.mean(y)) ** 2)\n\n    def _split_data(self, feature_values, threshold):\n        \"\"\"Split data based on threshold\"\"\"\n        left_indices = np.where(feature_values &lt;= threshold)[0]\n        right_indices = np.where(feature_values &gt; threshold)[0]\n        return left_indices, right_indices\n\n    def _most_common_class(self, y):\n        \"\"\"Return the most common class\"\"\"\n        counter = Counter(y)\n        return counter.most_common(1)[0][0]\n\n    def _predict_single(self, sample, node):\n        \"\"\"Predict a single sample\"\"\"\n        if node.value is not None:  # Leaf node\n            return node.value\n\n        if sample[node.feature] &lt;= node.threshold:\n            return self._predict_single(sample, node.left)\n        else:\n            return self._predict_single(sample, node.right)\n\n    def print_tree(self, node=None, depth=0, side='root'):\n        \"\"\"Print tree structure\"\"\"\n        if node is None:\n            node = self.root\n\n        if node.value is not None:\n            print(f\"{'  ' * depth}{side}: Predict {node.value}\")\n        else:\n            print(f\"{'  ' * depth}{side}: Feature_{node.feature} &lt;= {node.threshold:.2f}\")\n            self.print_tree(node.left, depth + 1, 'left')\n            self.print_tree(node.right, depth + 1, 'right')\n\n# Example usage of from-scratch implementation\nprint(\"=\" * 50)\nprint(\"FROM SCRATCH IMPLEMENTATION\")\nprint(\"=\" * 50)\n\n# Generate sample data\nnp.random.seed(42)\nX_sample = np.random.randn(200, 4)\ny_sample = (X_sample[:, 0] + X_sample[:, 1] &gt; 0).astype(int)\n\n# Split data\nX_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = train_test_split(\n    X_sample, y_sample, test_size=0.2, random_state=42\n)\n\n# Train custom decision tree\ncustom_tree = DecisionTreeFromScratch(\n    max_depth=3,\n    min_samples_split=10,\n    criterion='gini',\n    tree_type='classification'\n)\n\ncustom_tree.fit(X_train_scratch, y_train_scratch)\n\n# Make predictions\ny_pred_scratch = custom_tree.predict(X_test_scratch)\ncustom_accuracy = np.mean(y_pred_scratch == y_test_scratch)\n\nprint(f\"Custom Decision Tree Accuracy: {custom_accuracy:.3f}\")\nprint(\"\\nTree Structure:\")\ncustom_tree.print_tree()\n\n# Compare with sklearn\nsklearn_tree = DecisionTreeClassifier(max_depth=3, min_samples_split=10, random_state=42)\nsklearn_tree.fit(X_train_scratch, y_train_scratch)\nsklearn_pred = sklearn_tree.predict(X_test_scratch)\nsklearn_accuracy = accuracy_score(y_test_scratch, sklearn_pred)\n\nprint(f\"Scikit-learn Decision Tree Accuracy: {sklearn_accuracy:.3f}\")\nprint(f\"Difference: {abs(custom_accuracy - sklearn_accuracy):.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Decision%20Trees/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Decision%20Trees/#assumptions","title":"Assumptions","text":"<ol> <li>Feature relevance: Assumes that features contain information relevant to the target</li> <li>Finite feature space: Works with discrete or discretized continuous features</li> <li>Independent samples: Training samples should be independent</li> <li>Consistent labeling: No contradictory examples (same features, different labels)</li> </ol>"},{"location":"Machine-Learning/Decision%20Trees/#limitations","title":"Limitations","text":"<ol> <li>Overfitting: </li> <li>Prone to creating overly complex trees that memorize training data</li> <li> <p>Solution: Pruning, max_depth, min_samples constraints</p> </li> <li> <p>Instability: </p> </li> <li>Small changes in data can result in very different trees</li> <li> <p>Solution: Ensemble methods (Random Forest, Gradient Boosting)</p> </li> <li> <p>Bias towards features with many levels: </p> </li> <li>Favor features with more possible split points</li> <li> <p>Solution: Use conditional inference trees or random feature selection</p> </li> <li> <p>Difficulty with linear relationships: </p> </li> <li>Inefficient at modeling linear relationships</li> <li> <p>Solution: Combine with linear models or use ensemble methods</p> </li> <li> <p>Imbalanced data issues: </p> </li> <li>May be biased towards majority class</li> <li>Solution: Class weighting, resampling techniques</li> </ol>"},{"location":"Machine-Learning/Decision%20Trees/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Algorithm Interpretability Overfitting Risk Performance Training Speed Decision Trees Very High High Medium Fast Random Forest Medium Low High Medium SVM Low Medium High Slow Logistic Regression High Low Medium Fast Neural Networks Very Low High Very High Very Slow <p>When to use Decision Trees: - \u2705 When interpretability is crucial - \u2705 Mixed data types (numerical + categorical) - \u2705 Feature selection is needed - \u2705 Non-linear relationships exist - \u2705 Quick prototyping needed</p> <p>When to avoid: - \u274c When accuracy is paramount (use ensembles instead) - \u274c With very noisy data - \u274c When dataset is very small - \u274c Linear relationships dominate</p>"},{"location":"Machine-Learning/Decision%20Trees/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. What is the difference between Gini impurity and Entropy? When would you use each? <p>Gini Impurity:</p> <ul> <li>Formula: \\(\\text{Gini} = 1 - \\sum_{i=1}^{c} p_i^2\\)</li> <li>Range: [0, 0.5] for binary classification</li> <li>Computationally faster (no logarithms)</li> <li>Tends to isolate most frequent class</li> <li>Default in scikit-learn</li> </ul> <p>Entropy:</p> <ul> <li>Formula: \\(\\text{Entropy} = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)</li> <li>Range: [0, 1] for binary classification  </li> <li>More computationally expensive</li> <li>Tends to create more balanced splits</li> <li>Better theoretical foundation in information theory</li> </ul> <p>When to use:</p> <ul> <li>Gini: When computational speed is important, when you want to isolate the most frequent class</li> <li>Entropy: When you need more balanced trees, when theoretical interpretability matters</li> </ul> <p>Both typically produce similar trees in practice.</p> 2. How do you prevent overfitting in Decision Trees? <p>Pre-pruning (Early Stopping):</p> <ol> <li>max_depth: Limit tree depth</li> <li>min_samples_split: Minimum samples required to split a node</li> <li>min_samples_leaf: Minimum samples required in a leaf node</li> <li>max_features: Limit features considered for splitting</li> <li>min_impurity_decrease: Minimum impurity decrease required for split</li> </ol> <p>Post-pruning:</p> <ol> <li>Cost Complexity Pruning: Remove branches that don't improve performance</li> <li>Reduced Error Pruning: Use validation set to prune nodes</li> <li>Rule Post-pruning: Convert tree to rules, then prune rules</li> </ol> <p>Other techniques:</p> <ul> <li>Cross-validation: Use CV to select hyperparameters</li> <li>Ensemble methods: Random Forest, Gradient Boosting</li> <li>Feature selection: Remove irrelevant features</li> <li>Data augmentation: Increase training data size</li> </ul> <p>Example code: <pre><code># Pre-pruning\ntree = DecisionTreeClassifier(\n    max_depth=5,\n    min_samples_split=20,\n    min_samples_leaf=10,\n    max_features='sqrt'\n)\n\n# Post-pruning (cost complexity)\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas\n</code></pre></p> 3. Explain the algorithm for building a Decision Tree step by step. <p>Decision Tree Construction Algorithm:</p> <ol> <li> <p>Initialize: Start with root node containing all training samples</p> </li> <li> <p>For each node:</p> </li> <li> <p>Check stopping criteria:</p> <ul> <li>All samples have same class (pure node)</li> <li>Maximum depth reached</li> <li>Minimum samples threshold reached</li> <li>No information gain possible</li> </ul> </li> <li> <p>Find best split:</p> </li> <li>For each feature:<ul> <li>For each possible threshold:</li> <li>Calculate information gain</li> </ul> </li> <li> <p>Select feature and threshold with highest gain</p> </li> <li> <p>Split data: Create left and right child nodes based on best split</p> </li> <li> <p>Recursive expansion: Repeat process for each child node</p> </li> <li> <p>Assign predictions: For leaf nodes, assign most common class (classification) or mean value (regression)</p> </li> </ol> <p>Pseudocode: <pre><code>def build_tree(data, labels, depth):\n    if stopping_criteria_met:\n        return create_leaf_node(labels)\n\n    best_feature, best_threshold = find_best_split(data, labels)\n\n    left_data, left_labels = split_left(data, labels, best_feature, best_threshold)\n    right_data, right_labels = split_right(data, labels, best_feature, best_threshold)\n\n    left_child = build_tree(left_data, left_labels, depth+1)\n    right_child = build_tree(right_data, right_labels, depth+1)\n\n    return create_internal_node(best_feature, best_threshold, left_child, right_child)\n</code></pre></p> 4. How do Decision Trees handle categorical features? <p>Methods for handling categorical features:</p> <p>1. Binary encoding for each category: <pre><code># For feature \"Color\" with values [Red, Blue, Green]\n# Create binary splits: \"Is Red?\", \"Is Blue?\", \"Is Green?\"\n</code></pre></p> <p>2. Subset-based splits: - Consider all possible subsets of categories - Computationally expensive: 2^(k-1) - 1 possible splits for k categories - Used in algorithms like C4.5</p> <p>3. Ordinal encoding: - Assign numerical values to categories - Only appropriate if natural ordering exists - Example: [Low, Medium, High] \u2192 [1, 2, 3]</p> <p>4. One-hot encoding (preprocessing): - Convert each category to binary feature - Most common approach in scikit-learn</p> <p>Implementation considerations: - Scikit-learn: Requires preprocessing (one-hot encoding) - XGBoost: Native support for categorical features - LightGBM: Native categorical support</p> <p>Example: <pre><code>from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('cat', OneHotEncoder(), categorical_features),\n    ('num', 'passthrough', numerical_features)\n])\n\nX_processed = preprocessor.fit_transform(X)\ntree.fit(X_processed, y)\n</code></pre></p> 5. What are the advantages and disadvantages of Decision Trees compared to other algorithms? <p>Advantages:</p> <ol> <li>High Interpretability: Easy to understand and explain decisions</li> <li>No assumptions: No statistical assumptions about data distribution</li> <li>Handles mixed data: Both numerical and categorical features</li> <li>Automatic feature selection: Identifies important features</li> <li>Non-linear relationships: Captures complex patterns</li> <li>Fast prediction: O(log n) prediction time</li> <li>Robust to outliers: Splits are based on order, not actual values</li> <li>No preprocessing: No need for feature scaling or normalization</li> </ol> <p>Disadvantages:</p> <ol> <li>Overfitting: Tends to create overly complex models</li> <li>Instability: Small data changes cause different trees</li> <li>Bias: Favors features with more levels</li> <li>Poor extrapolation: Cannot predict beyond training data range</li> <li>Limited expressiveness: Axis-parallel splits only</li> <li>Imbalanced data: Biased towards majority class</li> <li>Greedy algorithm: May not find globally optimal tree</li> </ol> <p>Comparison table:</p> Aspect Decision Trees Random Forest SVM Logistic Regression Interpretability \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50\u2b50 Accuracy \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Speed \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Overfitting Risk \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 6. How do you handle missing values in Decision Trees? <p>Approaches for missing values:</p> <p>1. Preprocessing approaches: <pre><code># Remove rows with missing values\nX_clean = X.dropna()\n\n# Imputation\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')  # or 'mean', 'most_frequent'\nX_imputed = imputer.fit_transform(X)\n</code></pre></p> <p>2. Algorithm-level handling: - Surrogate splits: Use correlated features when primary feature is missing - Probabilistic splits: Send sample down both branches with appropriate probabilities - Missing as category: Treat missing as separate category</p> <p>3. Advanced techniques: <pre><code># XGBoost handles missing values natively\nimport xgboost as xgb\nmodel = xgb.XGBClassifier()\nmodel.fit(X_with_missing, y)  # No preprocessing needed\n\n# Custom handling in decision tree\nclass MissingValueTree:\n    def split_with_missing(self, X, feature, threshold):\n        # Handle missing values by going to majority direction\n        mask = ~np.isnan(X[:, feature])\n        left_indices = np.where((X[:, feature] &lt;= threshold) &amp; mask)[0]\n        right_indices = np.where((X[:, feature] &gt; threshold) &amp; mask)[0]\n        missing_indices = np.where(~mask)[0]\n\n        # Assign missing to majority branch\n        if len(left_indices) &gt; len(right_indices):\n            left_indices = np.concatenate([left_indices, missing_indices])\n        else:\n            right_indices = np.concatenate([right_indices, missing_indices])\n\n        return left_indices, right_indices\n</code></pre></p> <p>Best practices: - Understand the mechanism of missingness - Consider domain knowledge - Evaluate impact of different strategies - Monitor performance with validation data</p> 7. Explain information gain and how it's calculated. <p>Information Gain measures the reduction in impurity achieved by splitting on a particular feature.</p> <p>Formula: \\(\\(\\text{Information Gain}(S, A) = \\text{Impurity}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Impurity}(S_v)\\)\\)</p> <p>Where: - \\(S\\) = current set of examples - \\(A\\) = attribute/feature to split on - \\(S_v\\) = subset where attribute \\(A\\) has value \\(v\\)</p> <p>Step-by-step calculation:</p> <ol> <li> <p>Calculate parent impurity:    <pre><code>def gini_impurity(y):\n    _, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return 1 - np.sum(probabilities ** 2)\n\nparent_gini = gini_impurity(y_parent)\n</code></pre></p> </li> <li> <p>For each possible split:    <pre><code>def information_gain(y_parent, y_left, y_right):\n    n = len(y_parent)\n    n_left, n_right = len(y_left), len(y_right)\n\n    parent_impurity = gini_impurity(y_parent)\n\n    weighted_child_impurity = (\n        (n_left / n) * gini_impurity(y_left) + \n        (n_right / n) * gini_impurity(y_right)\n    )\n\n    return parent_impurity - weighted_child_impurity\n</code></pre></p> </li> </ol> <p>Example: <pre><code>Dataset: [Yes, Yes, No, Yes, No, No, Yes, No]\nParent Gini = 1 - (4/8)\u00b2 - (4/8)\u00b2 = 0.5\n\nSplit on Feature X &lt;= 0.5:\nLeft:  [Yes, Yes, Yes, Yes] \u2192 Gini = 0\nRight: [No, No, No, No]     \u2192 Gini = 0\n\nInformation Gain = 0.5 - (4/8 * 0 + 4/8 * 0) = 0.5\n</code></pre></p> <p>This split perfectly separates classes, achieving maximum information gain.</p> 8. What is the difference between pre-pruning and post-pruning? <p>Pre-pruning (Early Stopping): - Stops tree growth during construction - Prevents overfitting by limiting growth - More efficient (less computation) - Risk of under-fitting</p> <p>Common pre-pruning parameters: <pre><code>DecisionTreeClassifier(\n    max_depth=5,                    # Maximum tree depth\n    min_samples_split=20,           # Min samples to split node\n    min_samples_leaf=10,            # Min samples in leaf\n    max_features='sqrt',            # Features to consider\n    min_impurity_decrease=0.01      # Min impurity decrease\n)\n</code></pre></p> <p>Post-pruning: - Builds full tree first, then removes branches - More thorough exploration of tree space - Better performance but more computationally expensive - Lower risk of under-fitting</p> <p>Post-pruning techniques:</p> <ol> <li> <p>Cost Complexity Pruning: <pre><code># Find optimal alpha using cross-validation\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas[:-1]  # Exclude max alpha\n\ntrees = []\nfor alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(ccp_alpha=alpha)\n    clf.fit(X_train, y_train)\n    trees.append(clf)\n\n# Select best alpha using validation\nscores = [accuracy_score(y_val, tree.predict(X_val)) for tree in trees]\nbest_alpha = ccp_alphas[np.argmax(scores)]\n</code></pre></p> </li> <li> <p>Reduced Error Pruning:</p> </li> <li>Use validation set to evaluate node removal</li> <li>Remove nodes that improve validation performance</li> </ol> <p>Comparison:</p> Aspect Pre-pruning Post-pruning Computation Faster Slower Memory Less More Risk Under-fitting Over-fitting Performance Good Better Implementation Simpler Complex <p>Recommendation: Start with pre-pruning for quick results, use post-pruning for optimal performance.</p> 9. How do you evaluate the performance of a Decision Tree? <p>Classification Metrics:</p> <ol> <li> <p>Accuracy: Overall correctness    <pre><code>from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_true, y_pred)\n</code></pre></p> </li> <li> <p>Precision, Recall, F1-score: Class-specific performance    <pre><code>from sklearn.metrics import classification_report, precision_recall_fscore_support\nprint(classification_report(y_true, y_pred))\n</code></pre></p> </li> <li> <p>Confusion Matrix: Detailed error analysis    <pre><code>from sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\n</code></pre></p> </li> <li> <p>ROC Curve and AUC: Threshold-independent evaluation    <pre><code>from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(y_true, y_proba)\nauc_score = auc(fpr, tpr)\n</code></pre></p> </li> </ol> <p>Regression Metrics:</p> <ol> <li> <p>Mean Squared Error (MSE):    <pre><code>from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_true, y_pred)\n</code></pre></p> </li> <li> <p>R\u00b2 Score: Explained variance    <pre><code>from sklearn.metrics import r2_score\nr2 = r2_score(y_true, y_pred)\n</code></pre></p> </li> </ol> <p>Cross-validation: <pre><code>from sklearn.model_selection import cross_val_score, StratifiedKFold\n\n# Stratified K-fold for classification\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(tree, X, y, cv=skf, scoring='accuracy')\nprint(f\"CV Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n\n# Multiple metrics\nfrom sklearn.model_selection import cross_validate\nscoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\nscores = cross_validate(tree, X, y, cv=skf, scoring=scoring)\n</code></pre></p> <p>Validation Curves: <pre><code>from sklearn.model_selection import validation_curve\n\n# Evaluate effect of max_depth\nparam_range = range(1, 11)\ntrain_scores, val_scores = validation_curve(\n    DecisionTreeClassifier(random_state=42),\n    X, y, param_name='max_depth', param_range=param_range,\n    cv=5, scoring='accuracy'\n)\n\nplt.plot(param_range, train_scores.mean(axis=1), label='Training')\nplt.plot(param_range, val_scores.mean(axis=1), label='Validation')\nplt.xlabel('Max Depth')\nplt.ylabel('Accuracy')\nplt.legend()\n</code></pre></p> <p>Business metrics: - Consider domain-specific metrics - Cost of false positives vs false negatives - Interpretability requirements - Prediction speed requirements</p> 10. When would you choose Decision Trees over other algorithms, and when would you avoid them? <p>Choose Decision Trees when:</p> <ol> <li>Interpretability is crucial:</li> <li>Medical diagnosis</li> <li>Legal decisions  </li> <li>Regulatory compliance</li> <li> <p>Business rule extraction</p> </li> <li> <p>Mixed data types:</p> </li> <li>Combination of numerical and categorical features</li> <li> <p>No need for extensive preprocessing</p> </li> <li> <p>Feature selection needed:</p> </li> <li>High-dimensional data</li> <li>Unknown feature importance</li> <li> <p>Automatic relevance detection</p> </li> <li> <p>Non-linear relationships:</p> </li> <li>Complex interaction patterns</li> <li>Threshold-based decisions</li> <li> <p>Rule-based logic</p> </li> <li> <p>Quick prototyping:</p> </li> <li>Fast training and prediction</li> <li>Baseline model development</li> <li> <p>Proof of concept</p> </li> <li> <p>Robust to outliers:</p> </li> <li>Noisy data with extreme values</li> <li>No assumptions about distribution</li> </ol> <p>Avoid Decision Trees when:</p> <ol> <li>Maximum accuracy required:</li> <li>Use ensemble methods (Random Forest, XGBoost)</li> <li>Deep learning for complex patterns</li> <li> <p>SVMs for high-dimensional data</p> </li> <li> <p>Linear relationships dominate:</p> </li> <li>Use linear/logistic regression</li> <li> <p>More efficient and interpretable for linear patterns</p> </li> <li> <p>Very small datasets:</p> </li> <li>High variance with limited data</li> <li>Risk of overfitting</li> <li> <p>Simple models preferred</p> </li> <li> <p>Stable predictions needed:</p> </li> <li>High variance to data changes</li> <li> <p>Use ensemble methods for stability</p> </li> <li> <p>Extrapolation required:</p> </li> <li>Cannot predict outside training range</li> <li>Use regression models for extrapolation</li> </ol> <p>Decision Matrix: <pre><code>Data Size:      Small \u2192 Avoid, Large \u2192 Consider\nInterpretability: High need \u2192 Choose, Low need \u2192 Consider alternatives  \nAccuracy req:   High \u2192 Ensemble, Medium \u2192 Consider\nData type:      Mixed \u2192 Choose, Numerical only \u2192 Consider alternatives\nStability:      Required \u2192 Avoid, Not critical \u2192 Consider\nLinear patterns: Dominant \u2192 Avoid, Minimal \u2192 Choose\n</code></pre></p> <p>Best practice: Start with Decision Trees for understanding, then move to ensemble methods for performance.</p>"},{"location":"Machine-Learning/Decision%20Trees/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Decision%20Trees/#example-1-credit-approval-system","title":"Example 1: Credit Approval System","text":"<pre><code># Simulate credit approval data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Generate synthetic credit data\nnp.random.seed(42)\nn_samples = 1000\n\ndata = {\n    'income': np.random.normal(50000, 20000, n_samples),\n    'credit_score': np.random.normal(650, 100, n_samples),\n    'age': np.random.normal(35, 10, n_samples),\n    'debt_ratio': np.random.uniform(0, 0.8, n_samples),\n    'employment_years': np.random.exponential(5, n_samples)\n}\n\n# Create approval logic (simplified)\napproval = []\nfor i in range(n_samples):\n    score = 0\n    if data['income'][i] &gt; 40000: score += 2\n    if data['credit_score'][i] &gt; 600: score += 3\n    if data['age'][i] &gt; 25: score += 1\n    if data['debt_ratio'][i] &lt; 0.4: score += 2\n    if data['employment_years'][i] &gt; 2: score += 1\n\n    # Add some noise\n    score += np.random.normal(0, 1)\n    approval.append(1 if score &gt; 5 else 0)\n\n# Create DataFrame\ndf = pd.DataFrame(data)\ndf['approved'] = approval\n\nprint(\"Credit Approval Dataset:\")\nprint(df.head())\nprint(f\"\\nApproval rate: {df['approved'].mean():.2%}\")\n\n# Train Decision Tree\nX = df.drop('approved', axis=1)\ny = df['approved']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Create interpretable model\ncredit_tree = DecisionTreeClassifier(\n    max_depth=4,\n    min_samples_split=50,\n    min_samples_leaf=20,\n    random_state=42\n)\n\ncredit_tree.fit(X_train, y_train)\n\n# Evaluate\ny_pred = credit_tree.predict(X_test)\nprint(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Rejected', 'Approved']))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': credit_tree.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Extract decision rules\nprint(\"\\nDecision Rules:\")\ntree_rules = export_text(credit_tree, feature_names=list(X.columns))\nprint(tree_rules)\n\n# Visualize tree\nplt.figure(figsize=(15, 10))\nplot_tree(credit_tree, \n          feature_names=X.columns,\n          class_names=['Rejected', 'Approved'],\n          filled=True,\n          rounded=True,\n          fontsize=10)\nplt.title(\"Credit Approval Decision Tree\")\nplt.show()\n\n# Test specific cases\ntest_cases = pd.DataFrame({\n    'income': [30000, 60000, 45000],\n    'credit_score': [550, 750, 650],\n    'age': [22, 40, 30],\n    'debt_ratio': [0.6, 0.2, 0.3],\n    'employment_years': [1, 8, 3]\n})\n\npredictions = credit_tree.predict(test_cases)\nprobabilities = credit_tree.predict_proba(test_cases)\n\nprint(\"\\nTest Cases:\")\nfor i, (idx, row) in enumerate(test_cases.iterrows()):\n    result = \"Approved\" if predictions[i] == 1 else \"Rejected\"\n    confidence = probabilities[i].max()\n    print(f\"Case {i+1}: {result} (Confidence: {confidence:.2%})\")\n    print(f\"  Income: ${row['income']:,.0f}, Credit Score: {row['credit_score']:.0f}\")\n</code></pre>"},{"location":"Machine-Learning/Decision%20Trees/#example-2-medical-diagnosis","title":"Example 2: Medical Diagnosis","text":"<pre><code># Medical diagnosis example using Decision Tree\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\n# Load breast cancer dataset\ncancer = load_breast_cancer()\nX_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ny_cancer = cancer.target\n\nprint(\"Medical Diagnosis Dataset:\")\nprint(f\"Features: {len(cancer.feature_names)}\")\nprint(f\"Samples: {len(X_cancer)}\")\nprint(f\"Classes: {cancer.target_names}\")\n\n# Focus on most interpretable features\nimportant_features = [\n    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n    'mean smoothness', 'mean compactness', 'mean concavity',\n    'mean concave points', 'mean symmetry'\n]\n\nX_medical = X_cancer[important_features]\n\n# Train interpretable model\nX_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n    X_medical, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n)\n\nmedical_tree = DecisionTreeClassifier(\n    max_depth=3,  # Keep simple for medical interpretability\n    min_samples_split=20,\n    min_samples_leaf=10,\n    random_state=42\n)\n\nmedical_tree.fit(X_train_med, y_train_med)\n\n# Evaluate\ny_pred_med = medical_tree.predict(X_test_med)\nprint(f\"\\nDiagnostic Accuracy: {accuracy_score(y_test_med, y_pred_med):.3f}\")\n\n# Medical decision rules\nprint(\"\\nMedical Decision Rules:\")\nmed_rules = export_text(medical_tree, \n                       feature_names=important_features)\nprint(med_rules)\n\n# Feature importance for medical interpretation\nmed_importance = pd.DataFrame({\n    'Medical Feature': important_features,\n    'Clinical Importance': medical_tree.feature_importances_\n}).sort_values('Clinical Importance', ascending=False)\n\nprint(\"\\nClinical Feature Importance:\")\nfor _, row in med_importance.iterrows():\n    print(f\"{row['Medical Feature']}: {row['Clinical Importance']:.3f}\")\n\n# Confusion matrix for medical evaluation\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test_med, y_pred_med)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', \n            xticklabels=['Malignant', 'Benign'],\n            yticklabels=['Malignant', 'Benign'])\nplt.title('Medical Diagnosis Confusion Matrix')\nplt.ylabel('Actual Diagnosis')\nplt.xlabel('Predicted Diagnosis')\nplt.show()\n\n# Calculate medical metrics\ntn, fp, fn, tp = cm.ravel()\nsensitivity = tp / (tp + fn)  # True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nppv = tp / (tp + fp)         # Positive Predictive Value\nnpv = tn / (tn + fn)         # Negative Predictive Value\n\nprint(f\"\\nMedical Performance Metrics:\")\nprint(f\"Sensitivity (Recall): {sensitivity:.3f}\")\nprint(f\"Specificity: {specificity:.3f}\")\nprint(f\"Positive Predictive Value: {ppv:.3f}\")\nprint(f\"Negative Predictive Value: {npv:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Decision%20Trees/#references","title":"\ud83d\udcda References","text":"<ol> <li>Books:</li> <li>The Elements of Statistical Learning - Hastie, Tibshirani, Friedman</li> <li>Pattern Recognition and Machine Learning - Christopher Bishop</li> <li> <p>Hands-On Machine Learning - Aur\u00e9lien G\u00e9ron</p> </li> <li> <p>Academic Papers:</p> </li> <li>Induction of Decision Trees - J.R. Quinlan (1986)</li> <li> <p>C4.5: Programs for Machine Learning - J.R. Quinlan (1993)</p> </li> <li> <p>Online Resources:</p> </li> <li>Scikit-learn Decision Trees</li> <li>CS229 Stanford - Decision Trees</li> <li> <p>Towards Data Science - Decision Trees Explained</p> </li> <li> <p>Interactive Tools:</p> </li> <li>Decision Tree Visualizer</li> <li> <p>Teachable Machine</p> </li> <li> <p>Video Lectures:</p> </li> <li>MIT 6.034 Artificial Intelligence - Decision Trees</li> <li>StatQuest - Decision Trees</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/","title":"\ud83d\udcd8 Gradient Boosting","text":"<p>Gradient Boosting is an ensemble machine learning technique that builds models sequentially, where each new model corrects the errors made by the previous models, creating a strong predictor from many weak learners.</p> <p>Resources: Scikit-learn Gradient Boosting | XGBoost Documentation | Original Paper by Friedman</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#summary","title":"\u270d\ufe0f Summary","text":"<p>Gradient Boosting is a machine learning ensemble method that combines multiple weak predictors (typically decision trees) to create a strong predictor. It works by iteratively adding models that predict the residual errors of the previous models, gradually improving the overall prediction.</p> <p>Key characteristics: - Sequential learning: Models are built one after another - Error correction: Each model focuses on correcting previous mistakes - Gradient descent: Uses gradient descent to minimize loss function - Flexible: Can handle different loss functions and data types - High accuracy: Often achieves state-of-the-art performance</p> <p>Applications: - Ranking problems (web search, recommendation systems) - Regression tasks with complex patterns - Classification with high accuracy requirements - Feature importance analysis - Kaggle competitions (very popular) - Financial modeling and risk assessment</p> <p>Popular implementations: - Gradient Boosting Machines (GBM): Original implementation - XGBoost: Extreme Gradient Boosting (optimized) - LightGBM: Microsoft's fast implementation - CatBoost: Handles categorical features well</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#how-gradient-boosting-works","title":"How Gradient Boosting Works","text":"<p>Imagine you're trying to hit a target with arrows. After your first shot, you see where you missed and adjust your aim for the second shot. Gradient Boosting works similarly - each new model tries to correct the \"mistakes\" (residuals) of the combined previous models.</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#1-general-algorithm","title":"1. General Algorithm","text":"<p>Given training data \\((x_i, y_i)\\) for \\(i = 1, ..., n\\), Gradient Boosting learns a function \\(F(x)\\) that minimizes a loss function \\(L(y, F(x))\\).</p> <p>Algorithm steps: 1. Initialize with a constant: \\(F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^n L(y_i, \\gamma)\\) 2. For \\(m = 1\\) to \\(M\\) (number of iterations):    - Compute negative gradients: \\(r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}\\)    - Fit weak learner \\(h_m(x)\\) to targets \\(r_{im}\\)    - Find optimal step size: \\(\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)    - Update: \\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#2-loss-functions","title":"2. Loss Functions","text":"<p>For Regression: - Squared Loss: \\(L(y, F) = \\frac{1}{2}(y - F)^2\\), negative gradient: \\(r = y - F\\) - Absolute Loss: \\(L(y, F) = |y - F|\\), negative gradient: \\(r = \\text{sign}(y - F)\\) - Huber Loss: Combines squared and absolute loss</p> <p>For Classification: - Logistic Loss: \\(L(y, F) = \\log(1 + e^{-yF})\\) - Exponential Loss: \\(L(y, F) = e^{-yF}\\) (AdaBoost)</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#3-regularization","title":"3. Regularization","text":"<p>To prevent overfitting: - Learning rate \\(\\nu\\): \\(F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)\\) - Tree depth: Limit complexity of weak learners - Subsampling: Use random subset of data for each iteration - Feature subsampling: Use random subset of features</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#key-insights","title":"Key Insights","text":"<ol> <li>Residual fitting: Each model predicts what previous models missed</li> <li>Gradient descent: Follows gradient to minimize loss function</li> <li>Bias-variance tradeoff: Reduces bias while controlling variance</li> <li>Sequential dependency: Cannot be parallelized easily (unlike Random Forest)</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression, make_classification, load_boston\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error, accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeRegressor\nimport seaborn as sns\n\n# Regression Example\n# Generate sample data\nX, y = make_regression(\n    n_samples=1000,\n    n_features=10,\n    n_informative=5,\n    noise=0.1,\n    random_state=42\n)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train Gradient Boosting Regressor\ngb_regressor = GradientBoostingRegressor(\n    n_estimators=100,          # number of boosting stages\n    learning_rate=0.1,         # shrinkage parameter\n    max_depth=3,              # max depth of individual trees\n    min_samples_split=20,      # min samples to split\n    min_samples_leaf=10,       # min samples in leaf\n    subsample=0.8,            # fraction of samples for each tree\n    random_state=42\n)\n\ngb_regressor.fit(X_train, y_train)\n\n# Make predictions\ny_pred = gb_regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.3f}\")\n\n# Plot feature importance\nfeature_importance = gb_regressor.feature_importances_\nindices = np.argsort(feature_importance)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(X.shape[1]), feature_importance[indices])\nplt.title(\"Feature Importance - Gradient Boosting\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.show()\n\n# Classification Example\nX_class, y_class = make_classification(\n    n_samples=1000,\n    n_features=10,\n    n_informative=5,\n    n_redundant=2,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\nX_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42\n)\n\n# Gradient Boosting Classifier\ngb_classifier = GradientBoostingClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\n\ngb_classifier.fit(X_train_c, y_train_c)\n\n# Predictions and evaluation\ny_pred_c = gb_classifier.predict(X_test_c)\naccuracy = accuracy_score(y_test_c, y_pred_c)\nprint(f\"Classification Accuracy: {accuracy:.3f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_c, y_pred_c))\n\n# Plot learning curve\ntest_scores = []\ntrain_scores = []\n\nfor i, pred in enumerate(gb_regressor.staged_predict(X_test)):\n    test_scores.append(mean_squared_error(y_test, pred))\n\nfor i, pred in enumerate(gb_regressor.staged_predict(X_train)):\n    train_scores.append(mean_squared_error(y_train, pred))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(test_scores) + 1), test_scores, label='Test Error')\nplt.plot(range(1, len(train_scores) + 1), train_scores, label='Train Error')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Mean Squared Error')\nplt.title('Gradient Boosting Learning Curve')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Gradient%20Boosting/#using-xgboost-advanced-implementation","title":"Using XGBoost (Advanced Implementation)","text":"<pre><code>import xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\nX, y = load_boston(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create XGBoost regressor\nxgb_regressor = xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\n# Fit model\nxgb_regressor.fit(X_train, y_train)\n\n# Predictions\ny_pred_xgb = xgb_regressor.predict(X_test)\nmse_xgb = mean_squared_error(y_test, y_pred_xgb)\nprint(f\"XGBoost MSE: {mse_xgb:.3f}\")\n\n# Plot feature importance\nxgb.plot_importance(xgb_regressor, max_num_features=10)\nplt.title(\"XGBoost Feature Importance\")\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Gradient%20Boosting/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\nimport matplotlib.pyplot as plt\n\nclass GradientBoostingRegressor:\n    \"\"\"\n    Gradient Boosting Regressor implementation from scratch.\n    \"\"\"\n\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        \"\"\"\n        Initialize Gradient Boosting Regressor.\n\n        Parameters:\n        -----------\n        n_estimators : int, number of boosting stages\n        learning_rate : float, shrinkage parameter\n        max_depth : int, maximum depth of individual trees\n        \"\"\"\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.models = []\n        self.initial_prediction = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit gradient boosting model.\n\n        Parameters:\n        -----------\n        X : array-like, shape = [n_samples, n_features]\n        y : array-like, shape = [n_samples]\n        \"\"\"\n        # Initialize with mean of target values\n        self.initial_prediction = np.mean(y)\n\n        # Initialize predictions with constant\n        predictions = np.full_like(y, self.initial_prediction, dtype=float)\n\n        for i in range(self.n_estimators):\n            # Compute negative gradients (residuals for squared loss)\n            residuals = y - predictions\n\n            # Fit weak learner to residuals\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residuals)\n\n            # Make predictions with current tree\n            tree_predictions = tree.predict(X)\n\n            # Update overall predictions\n            predictions += self.learning_rate * tree_predictions\n\n            # Store the model\n            self.models.append(tree)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Parameters:\n        -----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns:\n        --------\n        predictions : array-like, shape = [n_samples]\n        \"\"\"\n        # Start with initial prediction\n        predictions = np.full(X.shape[0], self.initial_prediction, dtype=float)\n\n        # Add predictions from each tree\n        for model in self.models:\n            predictions += self.learning_rate * model.predict(X)\n\n        return predictions\n\n    def staged_predict(self, X):\n        \"\"\"\n        Predict at each stage for plotting learning curves.\n\n        Parameters:\n        -----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Yields:\n        -------\n        predictions : array-like, shape = [n_samples]\n        \"\"\"\n        predictions = np.full(X.shape[0], self.initial_prediction, dtype=float)\n        yield predictions.copy()\n\n        for model in self.models:\n            predictions += self.learning_rate * model.predict(X)\n            yield predictions.copy()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample data\n    X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n\n    # Split data\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create and train our model\n    gb_scratch = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3)\n    gb_scratch.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred_scratch = gb_scratch.predict(X_test)\n\n    # Calculate MSE\n    mse_scratch = np.mean((y_test - y_pred_scratch) ** 2)\n    print(f\"From-scratch MSE: {mse_scratch:.3f}\")\n\n    # Compare with sklearn\n    from sklearn.ensemble import GradientBoostingRegressor as SklearnGB\n    sklearn_gb = SklearnGB(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n    sklearn_gb.fit(X_train, y_train)\n    y_pred_sklearn = sklearn_gb.predict(X_test)\n    mse_sklearn = np.mean((y_test - y_pred_sklearn) ** 2)\n    print(f\"Sklearn MSE: {mse_sklearn:.3f}\")\n\n    # Plot comparison\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.scatter(y_test, y_pred_scratch, alpha=0.6)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('From Scratch Implementation')\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(y_test, y_pred_sklearn, alpha=0.6)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('Sklearn Implementation')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"Machine-Learning/Gradient%20Boosting/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#assumptions","title":"Assumptions","text":"<ol> <li>Weak learners are better than random: Each base model should perform slightly better than chance</li> <li>Sequential dependency: Models are built sequentially, not independently</li> <li>Gradient computability: Loss function must be differentiable</li> <li>Sufficient data: Needs adequate training data to avoid overfitting</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#limitations","title":"Limitations","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#1-overfitting-risk","title":"1. Overfitting Risk","text":"<ul> <li>Can easily overfit with too many iterations</li> <li>Requires careful tuning of hyperparameters</li> <li>Solution: Use validation set, early stopping, and regularization</li> </ul>"},{"location":"Machine-Learning/Gradient%20Boosting/#2-sequential-training","title":"2. Sequential Training","text":"<ul> <li>Cannot parallelize training (unlike Random Forest)</li> <li>Slower to train on large datasets</li> <li>Alternative: Use parallelized versions like LightGBM</li> </ul>"},{"location":"Machine-Learning/Gradient%20Boosting/#3-hyperparameter-sensitivity","title":"3. Hyperparameter Sensitivity","text":"<ul> <li>Performance highly dependent on hyperparameter tuning</li> <li>Many parameters to optimize (learning rate, depth, iterations)</li> <li>Solution: Use automated hyperparameter tuning</li> </ul>"},{"location":"Machine-Learning/Gradient%20Boosting/#4-memory-usage","title":"4. Memory Usage","text":"<ul> <li>Stores all weak learners</li> <li>Can become memory-intensive with many iterations</li> <li>Solution: Limit number of estimators</li> </ul>"},{"location":"Machine-Learning/Gradient%20Boosting/#5-prediction-time","title":"5. Prediction Time","text":"<ul> <li>Slower prediction than single models</li> <li>Each prediction requires all weak learners</li> <li>Trade-off: Accuracy vs speed</li> </ul>"},{"location":"Machine-Learning/Gradient%20Boosting/#comparison-with-other-methods","title":"Comparison with Other Methods","text":"Method Accuracy Speed Interpretability Parallelization Gradient Boosting High Slow Low No (training) Random Forest High Fast Medium Yes Single Decision Tree Medium Fast High N/A Linear Models Low-Medium Very Fast High Yes Neural Networks High Variable Low Yes"},{"location":"Machine-Learning/Gradient%20Boosting/#when-to-use-vs-avoid","title":"When to Use vs Avoid","text":"<p>Use Gradient Boosting when: - High accuracy is crucial - You have sufficient computational resources - Data is not extremely noisy - You can invest time in hyperparameter tuning</p> <p>Avoid Gradient Boosting when: - Real-time predictions are critical - Interpretability is most important - Training time is heavily constrained - Data is very noisy or has many outliers</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. How does Gradient Boosting differ from Random Forest? <p>Answer:  - Training: Gradient Boosting builds trees sequentially where each tree corrects errors of previous ones, while Random Forest builds trees independently in parallel - Overfitting: Gradient Boosting is more prone to overfitting due to sequential error correction, Random Forest reduces overfitting through averaging - Speed: Random Forest is faster to train due to parallelization, Gradient Boosting is sequential - Bias-Variance: Gradient Boosting reduces bias primarily, Random Forest reduces variance - Hyperparameters: Gradient Boosting has more critical hyperparameters (learning rate, n_estimators) that need careful tuning</p> 2. Explain the mathematical intuition behind gradient boosting. How does it use gradients? <p>Answer: Gradient Boosting minimizes a loss function using gradient descent in function space: - At each iteration, it computes negative gradients of the loss function with respect to current predictions - These gradients represent the direction of steepest decrease in the loss - A new weak learner is trained to predict these gradients (residuals for squared loss) - The predictions are updated by adding the new model's output scaled by a learning rate - This process is repeated until convergence or max iterations reached</p> <p>For squared loss: gradient = y - F(x) (actual residual) For logistic loss: gradient = y - p(x) (probability residual)</p> 3. What role does the learning rate play in Gradient Boosting? How do you choose it? <p>Answer: The learning rate (\u03b7) controls how much each weak learner contributes to the final prediction: - Small \u03b7 (0.01-0.1): More conservative updates, requires more iterations but often better generalization - Large \u03b7 (0.3-1.0): Faster learning but higher overfitting risk - Trade-off: Lower learning rate with more estimators often yields better results - Selection: Use validation curves or cross-validation to find optimal value - Common practice: Start with \u03b7=0.1, then try \u03b7=0.05 with 2x estimators or \u03b7=0.2 with 0.5x estimators</p> 4. How do you prevent overfitting in Gradient Boosting? <p>Answer: Multiple regularization techniques: - Learning rate: Lower values (0.01-0.1) prevent overfitting - Tree depth: Limit max_depth (3-8) to keep weak learners simple - Subsampling: Use fraction of data for each tree (0.5-0.8) - Feature subsampling: Use random subset of features per split - Early stopping: Monitor validation error and stop when it starts increasing - Minimum samples: Set min_samples_split and min_samples_leaf - Cross-validation: Use CV to select optimal number of estimators</p> 5. What are the advantages of XGBoost over traditional Gradient Boosting? <p>Answer: XGBoost improvements: - Regularization: Built-in L1 and L2 regularization in objective function - Missing values: Handles missing values automatically by learning best direction - Parallelization: Parallel tree construction (not sequential like boosting stages) - Speed: Optimized implementation with caching and approximation algorithms - Memory efficiency: Block structure for out-of-core computation - Cross-validation: Built-in cross-validation during training - Flexibility: More loss functions and evaluation metrics - Pruning: Bottom-up tree pruning removes splits with negative gain</p> 6. How would you tune hyperparameters for a Gradient Boosting model? <p>Answer: Systematic approach: 1. Start with defaults: n_estimators=100, learning_rate=0.1, max_depth=3 2. Tune tree parameters: max_depth (3-10), min_samples_split (10-50) 3. Optimize learning rate and estimators: Lower learning_rate, increase n_estimators 4. Add regularization: subsample (0.6-0.9), max_features 5. Use techniques: Grid search, random search, or Bayesian optimization 6. Validation: Use time-series split for temporal data, stratified CV for classification 7. Monitor: Plot validation curves to detect overfitting</p> <p>Example order: max_depth \u2192 n_estimators &amp; learning_rate \u2192 subsampling \u2192 feature selection</p> 7. Explain the difference between AdaBoost and Gradient Boosting. <p>Answer: Key differences: - Error focus: AdaBoost reweights misclassified samples, Gradient Boosting fits residuals - Loss function: AdaBoost uses exponential loss, Gradient Boosting can use various losses - Weight updates: AdaBoost changes sample weights, Gradient Boosting changes predictions - Flexibility: Gradient Boosting works with any differentiable loss, AdaBoost is more restrictive - Outlier sensitivity: AdaBoost very sensitive to outliers, Gradient Boosting less so - Base learners: AdaBoost typically uses stumps, Gradient Boosting uses deeper trees - Applications: AdaBoost mainly classification, Gradient Boosting both regression and classification</p> 8. How do you interpret feature importance in Gradient Boosting? <p>Answer: Feature importance calculation: - Frequency-based: How often a feature is used for splits across all trees - Gain-based: Average improvement in objective function when feature is used - Permutation importance: Decrease in model performance when feature values are randomly permuted - SHAP values: Game-theoretic approach showing contribution of each feature to predictions</p> <p>Interpretation tips: - Higher values indicate more important features - Consider feature interactions and multicollinearity - Use multiple importance measures for robustness - Validate importance with domain knowledge</p> 9. When would you choose Gradient Boosting over Deep Learning? <p>Answer: Choose Gradient Boosting when: - Tabular data: Works exceptionally well on structured data - Small to medium datasets: Less prone to overfitting than deep learning - Interpretability needed: Feature importance and decision paths are clearer - No image/text data: Deep learning excels with unstructured data - Quick deployment: Faster to train and tune than neural networks - Limited computational resources: Less GPU dependency - Heterogeneous features: Mix of numerical and categorical features - Proven track record: Dominates many Kaggle tabular competitions</p> 10. How does the choice of loss function affect Gradient Boosting performance? <p>Answer: Loss function impacts: - Squared loss: Sensitive to outliers, good for normal residuals - Absolute loss: Robust to outliers, good for heavy-tailed distributions - Huber loss: Combines benefits of both, balanced approach - Logistic loss: For classification, provides probability estimates - Custom loss: Can optimize specific business metrics</p> <p>Selection guidelines: - Analyze residual distribution - Consider outlier presence - Match business objective (e.g., quantile loss for different percentiles) - Use validation to compare different loss functions</p>"},{"location":"Machine-Learning/Gradient%20Boosting/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#real-world-example-house-price-prediction","title":"Real-world Example: House Price Prediction","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load Boston housing dataset\nboston = load_boston()\nX, y = boston.data, boston.target\nfeature_names = boston.feature_names\n\n# Create DataFrame for better visualization\ndf = pd.DataFrame(X, columns=feature_names)\ndf['PRICE'] = y\n\nprint(\"Dataset Info:\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"Features: {list(feature_names)}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train Gradient Boosting model\ngb_model = GradientBoostingRegressor(\n    n_estimators=200,\n    learning_rate=0.1,\n    max_depth=4,\n    min_samples_split=20,\n    min_samples_leaf=10,\n    subsample=0.8,\n    random_state=42\n)\n\ngb_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred = gb_model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nModel Performance:\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R\u00b2 Score: {r2:.3f}\")\n\n# Feature importance analysis\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': gb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(15, 5))\n\n# Plot 1: Predictions vs Actual\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.title(f'Predictions vs Actual (R\u00b2 = {r2:.3f})')\n\n# Plot 2: Feature Importance\nplt.subplot(1, 3, 2)\nplt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\nplt.xlabel('Feature Importance')\nplt.title('Top 10 Most Important Features')\nplt.gca().invert_yaxis()\n\n# Plot 3: Learning Curve\nplt.subplot(1, 3, 3)\ntrain_scores, test_scores = [], []\nfor pred_train, pred_test in zip(gb_model.staged_predict(X_train), \n                                gb_model.staged_predict(X_test)):\n    train_scores.append(mean_squared_error(y_train, pred_train))\n    test_scores.append(mean_squared_error(y_test, pred_test))\n\nplt.plot(range(1, len(train_scores) + 1), train_scores, label='Train MSE')\nplt.plot(range(1, len(test_scores) + 1), test_scores, label='Test MSE')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Mean Squared Error')\nplt.title('Learning Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Residual analysis\nresiduals = y_test - y_pred\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\nplt.subplot(1, 2, 2)\nplt.hist(residuals, bins=20, alpha=0.7)\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Residual Distribution')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop 5 Most Important Features:\")\nprint(feature_importance.head())\n\n# Business insights\nprint(\"\\nBusiness Insights:\")\nprint(\"1. LSTAT (% lower status population) is the most important predictor\")\nprint(\"2. RM (average rooms per dwelling) significantly affects price\")\nprint(\"3. DIS (distance to employment centers) impacts housing values\")\nprint(\"4. The model explains {:.1f}% of price variation\".format(r2 * 100))\n</code></pre>"},{"location":"Machine-Learning/Gradient%20Boosting/#example-multi-class-classification","title":"Example: Multi-class Classification","text":"<pre><code>from sklearn.datasets import load_wine\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Load wine dataset\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train gradient boosting classifier\ngb_clf = GradientBoostingClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\n\ngb_clf.fit(X_train, y_train)\n\n# Predictions\ny_pred = gb_clf.predict(X_test)\ny_pred_proba = gb_clf.predict_proba(X_test)\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=wine.target_names))\n\n# Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=wine.target_names, \n            yticklabels=wine.target_names)\nplt.title('Confusion Matrix - Wine Classification')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n# Feature importance for classification\nfeature_importance_clf = pd.DataFrame({\n    'feature': wine.feature_names,\n    'importance': gb_clf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 5 Most Important Features for Wine Classification:\")\nprint(feature_importance_clf.head())\n</code></pre>"},{"location":"Machine-Learning/Gradient%20Boosting/#references","title":"\ud83d\udcda References","text":""},{"location":"Machine-Learning/Gradient%20Boosting/#books","title":"Books","text":"<ol> <li>\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman - Chapter 10</li> <li>\"Hands-On Machine Learning\" by Aur\u00e9lien G\u00e9ron - Ensemble Methods chapter</li> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li> <li>\"Machine Learning: A Probabilistic Perspective\" by Kevin Murphy</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#papers","title":"Papers","text":"<ol> <li>Greedy Function Approximation: A Gradient Boosting Machine - Jerome Friedman (2001)</li> <li>XGBoost: A Scalable Tree Boosting System - Chen &amp; Guestrin (2016)</li> <li>LightGBM: A Highly Efficient Gradient Boosting Decision Tree - Microsoft (2017)</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#online-resources","title":"Online Resources","text":"<ol> <li>Scikit-learn Gradient Boosting Guide</li> <li>XGBoost Documentation</li> <li>LightGBM Documentation</li> <li>CatBoost Documentation</li> <li>Gradient Boosting Explained - Video by StatQuest</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#tutorials-and-blogs","title":"Tutorials and Blogs","text":"<ol> <li>Complete Guide to Parameter Tuning in Gradient Boosting</li> <li>Understanding Gradient Boosting - Interactive explanation</li> <li>Kaggle Learn: Gradient Boosting</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#implementation-references","title":"Implementation References","text":"<ol> <li>Scikit-learn Source Code</li> <li>XGBoost GitHub</li> <li>From Scratch Implementations</li> </ol>"},{"location":"Machine-Learning/Gradient%20Boosting/#competitions-and-case-studies","title":"Competitions and Case Studies","text":"<ol> <li>Kaggle Competitions using Gradient Boosting</li> <li>Netflix Prize - Gradient Boosting Application</li> <li>Real-world Applications in Industry</li> </ol>"},{"location":"Machine-Learning/K-means%20clustering/","title":"\ud83d\udcd8 K-means Clustering","text":"<p>K-means is a popular unsupervised learning algorithm that partitions data into k clusters by grouping similar data points together and identifying underlying patterns in the data.</p> <p>Resources: Scikit-learn K-means | Pattern Recognition and Machine Learning - Chapter 9</p>"},{"location":"Machine-Learning/K-means%20clustering/#summary","title":"\u270d\ufe0f Summary","text":"<p>K-means clustering is an unsupervised machine learning algorithm that aims to partition n observations into k clusters where each observation belongs to the cluster with the nearest centroid (cluster center). It's one of the simplest and most popular clustering algorithms.</p> <p>Key characteristics: - Unsupervised: No labeled data required - Centroid-based: Uses cluster centers to define clusters - Hard clustering: Each point belongs to exactly one cluster - Iterative: Uses an expectation-maximization approach - Distance-based: Uses Euclidean distance (typically)</p> <p>Applications: - Customer segmentation in marketing - Image segmentation and compression - Market research and analysis - Data preprocessing for other ML algorithms - Gene sequencing analysis - Recommendation systems</p> <p>When to use K-means: - When you know the approximate number of clusters - When clusters are roughly spherical and similar sized - When you need interpretable results - When computational efficiency is important</p>"},{"location":"Machine-Learning/K-means%20clustering/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/K-means%20clustering/#how-k-means-works","title":"How K-means Works","text":"<p>K-means follows a simple iterative process:</p> <ol> <li>Initialize k cluster centroids randomly</li> <li>Assign each data point to the nearest centroid</li> <li>Update centroids by calculating the mean of assigned points</li> <li>Repeat steps 2-3 until centroids stop moving significantly</li> </ol>"},{"location":"Machine-Learning/K-means%20clustering/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/K-means%20clustering/#1-objective-function","title":"1. Objective Function","text":"<p>K-means minimizes the Within-Cluster Sum of Squares (WCSS):</p> \\[J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Where: - \\(J\\) is the objective function to minimize - \\(k\\) is the number of clusters - \\(C_i\\) is the set of points in cluster \\(i\\) - \\(\\mu_i\\) is the centroid of cluster \\(i\\) - \\(||x - \\mu_i||^2\\) is the squared Euclidean distance</p>"},{"location":"Machine-Learning/K-means%20clustering/#2-centroid-update-rule","title":"2. Centroid Update Rule","text":"<p>The centroid of cluster \\(i\\) is updated as:</p> \\[\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x\\] <p>Where \\(|C_i|\\) is the number of points in cluster \\(i\\).</p>"},{"location":"Machine-Learning/K-means%20clustering/#3-distance-calculation","title":"3. Distance Calculation","text":"<p>Euclidean distance between point \\(x\\) and centroid \\(\\mu_i\\):</p> \\[d(x, \\mu_i) = \\sqrt{\\sum_{j=1}^{d} (x_j - \\mu_{i,j})^2}\\] <p>Where \\(d\\) is the number of dimensions.</p>"},{"location":"Machine-Learning/K-means%20clustering/#4-convergence-criteria","title":"4. Convergence Criteria","text":"<p>The algorithm stops when: - Centroids don't move significantly: \\(||\\mu_i^{(t+1)} - \\mu_i^{(t)}|| &lt; \\epsilon\\) - Maximum number of iterations reached - No points change cluster assignments</p>"},{"location":"Machine-Learning/K-means%20clustering/#algorithm-complexity","title":"Algorithm Complexity","text":"<ul> <li>Time Complexity: \\(O(n \\cdot k \\cdot d \\cdot t)\\)</li> <li>\\(n\\): number of data points</li> <li>\\(k\\): number of clusters  </li> <li>\\(d\\): number of dimensions</li> <li> <p>\\(t\\): number of iterations</p> </li> <li> <p>Space Complexity: \\(O(n \\cdot d + k \\cdot d)\\)</p> </li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/K-means%20clustering/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs, load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\nfrom sklearn.decomposition import PCA\n\n# Generate sample data\nnp.random.seed(42)\nX, y_true = make_blobs(\n    n_samples=300, \n    centers=4, \n    n_features=2, \n    cluster_std=0.8,\n    random_state=42\n)\n\n# Basic K-means clustering\nkmeans = KMeans(\n    n_clusters=4,\n    init='k-means++',      # Smart initialization\n    n_init=10,             # Number of initializations\n    max_iter=300,          # Maximum iterations\n    tol=1e-4,              # Convergence tolerance\n    random_state=42\n)\n\n# Fit the model\nkmeans.fit(X)\n\n# Get predictions and centroids\ny_pred = kmeans.labels_\ncentroids = kmeans.cluster_centers_\ninertia = kmeans.inertia_  # WCSS value\n\nprint(f\"Inertia (WCSS): {inertia:.2f}\")\nprint(f\"Silhouette Score: {silhouette_score(X, y_pred):.3f}\")\n\n# Visualize results\nplt.figure(figsize=(15, 5))\n\n# Original data with true labels\nplt.subplot(1, 3, 1)\nplt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.7)\nplt.title('True Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\n# K-means results\nplt.subplot(1, 3, 2)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.7)\nplt.scatter(centroids[:, 0], centroids[:, 1], \n           marker='x', s=200, linewidths=3, color='red')\nplt.title('K-means Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\n# Comparison\nplt.subplot(1, 3, 3)\nplt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.5, label='True')\nplt.scatter(centroids[:, 0], centroids[:, 1], \n           marker='x', s=200, linewidths=3, color='red', label='Centroids')\nplt.title('Comparison')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Calculate accuracy (for labeled data comparison)\naccuracy = adjusted_rand_score(y_true, y_pred)\nprint(f\"Adjusted Rand Index: {accuracy:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#determining-optimal-number-of-clusters","title":"Determining Optimal Number of Clusters","text":""},{"location":"Machine-Learning/K-means%20clustering/#elbow-method","title":"Elbow Method","text":"<pre><code>def plot_elbow_method(X, max_k=10):\n    \"\"\"Plot elbow method to find optimal k\"\"\"\n    wcss = []\n    k_range = range(1, max_k + 1)\n\n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X)\n        wcss.append(kmeans.inertia_)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(k_range, wcss, 'bo-')\n    plt.title('Elbow Method for Optimal k')\n    plt.xlabel('Number of Clusters (k)')\n    plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n    plt.grid(True, alpha=0.3)\n\n    # Find elbow point using second derivative\n    second_derivative = np.diff(wcss, 2)\n    elbow_point = np.argmax(second_derivative) + 2\n    plt.axvline(x=elbow_point, color='red', linestyle='--', \n                label=f'Elbow at k={elbow_point}')\n    plt.legend()\n    plt.show()\n\n    return wcss\n\nwcss_values = plot_elbow_method(X, max_k=10)\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#silhouette-method","title":"Silhouette Method","text":"<pre><code>def plot_silhouette_method(X, max_k=10):\n    \"\"\"Plot silhouette scores for different k values\"\"\"\n    silhouette_scores = []\n    k_range = range(2, max_k + 1)\n\n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X)\n        score = silhouette_score(X, labels)\n        silhouette_scores.append(score)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(k_range, silhouette_scores, 'go-')\n    plt.title('Silhouette Method for Optimal k')\n    plt.xlabel('Number of Clusters (k)')\n    plt.ylabel('Silhouette Score')\n    plt.grid(True, alpha=0.3)\n\n    # Find optimal k\n    optimal_k = k_range[np.argmax(silhouette_scores)]\n    plt.axvline(x=optimal_k, color='red', linestyle='--', \n                label=f'Optimal k={optimal_k}')\n    plt.legend()\n    plt.show()\n\n    return silhouette_scores\n\nsilhouette_values = plot_silhouette_method(X, max_k=10)\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#real-world-example-iris-dataset","title":"Real-world Example: Iris Dataset","text":"<pre><code># Load Iris dataset\niris = load_iris()\nX_iris = iris.data\ny_true_iris = iris.target\n\n# Standardize features\nscaler = StandardScaler()\nX_iris_scaled = scaler.fit_transform(X_iris)\n\n# Apply K-means\nkmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\ny_pred_iris = kmeans_iris.fit_predict(X_iris_scaled)\n\n# Evaluate\nsilhouette_iris = silhouette_score(X_iris_scaled, y_pred_iris)\nari_iris = adjusted_rand_score(y_true_iris, y_pred_iris)\n\nprint(f\"Iris Dataset Results:\")\nprint(f\"Silhouette Score: {silhouette_iris:.3f}\")\nprint(f\"Adjusted Rand Index: {ari_iris:.3f}\")\n\n# Visualize using PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_iris_scaled)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true_iris, cmap='viridis', alpha=0.7)\nplt.title('True Species (PCA)')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n\nplt.subplot(1, 3, 2)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred_iris, cmap='viridis', alpha=0.7)\nplt.title('K-means Clusters (PCA)')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n\n# Feature importance\nfeature_names = iris.feature_names\ncentroids_original = scaler.inverse_transform(kmeans_iris.cluster_centers_)\n\nplt.subplot(1, 3, 3)\nfor i, centroid in enumerate(centroids_original):\n    plt.plot(feature_names, centroid, 'o-', label=f'Cluster {i}')\nplt.title('Cluster Centroids (Original Features)')\nplt.xlabel('Features')\nplt.ylabel('Values')\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Print cluster characteristics\nprint(\"\\nCluster Centroids (Original Scale):\")\nfor i, centroid in enumerate(centroids_original):\n    print(f\"Cluster {i}:\")\n    for feature, value in zip(feature_names, centroid):\n        print(f\"  {feature}: {value:.2f}\")\n    print()\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#mini-batch-k-means-for-large-datasets","title":"Mini-batch K-means for Large Datasets","text":"<pre><code>from sklearn.cluster import MiniBatchKMeans\nimport time\n\n# Generate larger dataset\nX_large, _ = make_blobs(n_samples=10000, centers=5, n_features=10, random_state=42)\n\n# Compare standard K-means vs Mini-batch K-means\nprint(\"Comparing K-means vs Mini-batch K-means:\")\n\n# Standard K-means\nstart_time = time.time()\nkmeans_standard = KMeans(n_clusters=5, random_state=42, n_init=10)\nlabels_standard = kmeans_standard.fit_predict(X_large)\ntime_standard = time.time() - start_time\n\n# Mini-batch K-means\nstart_time = time.time()\nkmeans_mini = MiniBatchKMeans(n_clusters=5, batch_size=100, random_state=42)\nlabels_mini = kmeans_mini.fit_predict(X_large)\ntime_mini = time.time() - start_time\n\nprint(f\"Standard K-means - Time: {time_standard:.3f}s, Inertia: {kmeans_standard.inertia_:.2f}\")\nprint(f\"Mini-batch K-means - Time: {time_mini:.3f}s, Inertia: {kmeans_mini.inertia_:.2f}\")\nprint(f\"Speedup: {time_standard/time_mini:.1f}x\")\n\n# Compare clustering quality\nari_comparison = adjusted_rand_score(labels_standard, labels_mini)\nprint(f\"Agreement between methods (ARI): {ari_comparison:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\n\nclass KMeansFromScratch:\n    \"\"\"K-means clustering implementation from scratch\"\"\"\n\n    def __init__(self, k=3, max_iters=100, tol=1e-4, init='k-means++', random_state=None):\n        \"\"\"\n        Initialize K-means clusterer\n\n        Parameters:\n        -----------\n        k : int, default=3\n            Number of clusters\n        max_iters : int, default=100\n            Maximum number of iterations\n        tol : float, default=1e-4\n            Tolerance for convergence\n        init : str, default='k-means++'\n            Initialization method ('random' or 'k-means++')\n        random_state : int, default=None\n            Random seed for reproducibility\n        \"\"\"\n        self.k = k\n        self.max_iters = max_iters\n        self.tol = tol\n        self.init = init\n        self.random_state = random_state\n\n        # Initialize attributes\n        self.centroids = None\n        self.labels = None\n        self.inertia_ = None\n        self.n_iter_ = 0\n\n    def _initialize_centroids(self, X):\n        \"\"\"Initialize centroids using specified method\"\"\"\n        if self.random_state:\n            np.random.seed(self.random_state)\n\n        n_samples, n_features = X.shape\n\n        if self.init == 'random':\n            # Random initialization\n            min_vals = np.min(X, axis=0)\n            max_vals = np.max(X, axis=0)\n            centroids = np.random.uniform(min_vals, max_vals, (self.k, n_features))\n\n        elif self.init == 'k-means++':\n            # K-means++ initialization for better initial centroids\n            centroids = np.zeros((self.k, n_features))\n\n            # Choose first centroid randomly\n            centroids[0] = X[np.random.randint(n_samples)]\n\n            for i in range(1, self.k):\n                # Calculate distances from each point to nearest centroid\n                distances = np.array([min([np.linalg.norm(x - c)**2 for c in centroids[:i]]) \n                                    for x in X])\n\n                # Choose next centroid with probability proportional to squared distance\n                probabilities = distances / distances.sum()\n                cumulative_probabilities = probabilities.cumsum()\n                r = np.random.rand()\n\n                for j, p in enumerate(cumulative_probabilities):\n                    if r &lt; p:\n                        centroids[i] = X[j]\n                        break\n        else:\n            raise ValueError(\"init must be 'random' or 'k-means++'\")\n\n        return centroids\n\n    def _assign_clusters(self, X, centroids):\n        \"\"\"Assign each point to the nearest centroid\"\"\"\n        # Calculate distances from each point to each centroid\n        distances = cdist(X, centroids, metric='euclidean')\n\n        # Assign each point to the nearest centroid\n        labels = np.argmin(distances, axis=1)\n\n        return labels\n\n    def _update_centroids(self, X, labels):\n        \"\"\"Update centroids based on current cluster assignments\"\"\"\n        centroids = np.zeros((self.k, X.shape[1]))\n\n        for i in range(self.k):\n            # Find points belonging to cluster i\n            cluster_points = X[labels == i]\n\n            if len(cluster_points) &gt; 0:\n                # Update centroid as mean of cluster points\n                centroids[i] = np.mean(cluster_points, axis=0)\n            else:\n                # Keep old centroid if no points assigned to cluster\n                centroids[i] = self.centroids[i]\n\n        return centroids\n\n    def _calculate_inertia(self, X, labels, centroids):\n        \"\"\"Calculate within-cluster sum of squares (inertia)\"\"\"\n        inertia = 0\n        for i in range(self.k):\n            cluster_points = X[labels == i]\n            if len(cluster_points) &gt; 0:\n                inertia += np.sum((cluster_points - centroids[i])**2)\n        return inertia\n\n    def fit(self, X):\n        \"\"\"Fit K-means clustering to data\"\"\"\n        # Initialize centroids\n        self.centroids = self._initialize_centroids(X)\n\n        # Store convergence history\n        self.centroid_history = [self.centroids.copy()]\n        self.inertia_history = []\n\n        for iteration in range(self.max_iters):\n            # Assign points to clusters\n            labels = self._assign_clusters(X, self.centroids)\n\n            # Update centroids\n            new_centroids = self._update_centroids(X, labels)\n\n            # Calculate inertia\n            inertia = self._calculate_inertia(X, labels, new_centroids)\n            self.inertia_history.append(inertia)\n\n            # Check for convergence\n            centroid_shift = np.linalg.norm(new_centroids - self.centroids)\n\n            if centroid_shift &lt; self.tol:\n                self.n_iter_ = iteration + 1\n                break\n\n            # Update centroids\n            self.centroids = new_centroids\n            self.centroid_history.append(self.centroids.copy())\n\n        # Final assignments\n        self.labels = self._assign_clusters(X, self.centroids)\n        self.inertia_ = self._calculate_inertia(X, self.labels, self.centroids)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict cluster labels for new data\"\"\"\n        if self.centroids is None:\n            raise ValueError(\"Model must be fitted before predicting\")\n\n        return self._assign_clusters(X, self.centroids)\n\n    def fit_predict(self, X):\n        \"\"\"Fit model and return cluster labels\"\"\"\n        self.fit(X)\n        return self.labels\n\n    def plot_convergence(self):\n        \"\"\"Plot convergence of inertia over iterations\"\"\"\n        if not hasattr(self, 'inertia_history'):\n            raise ValueError(\"Model must be fitted first\")\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(self.inertia_history) + 1), self.inertia_history, 'b-o')\n        plt.title('K-means Convergence')\n        plt.xlabel('Iteration')\n        plt.ylabel('Inertia (WCSS)')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n    def plot_clusters(self, X):\n        \"\"\"Visualize clusters (works for 2D data)\"\"\"\n        if X.shape[1] != 2:\n            raise ValueError(\"Visualization only works for 2D data\")\n\n        if self.labels is None:\n            raise ValueError(\"Model must be fitted first\")\n\n        plt.figure(figsize=(10, 8))\n\n        # Plot points colored by cluster\n        colors = plt.cm.viridis(np.linspace(0, 1, self.k))\n        for i in range(self.k):\n            cluster_points = X[self.labels == i]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n                       c=[colors[i]], alpha=0.7, label=f'Cluster {i}')\n\n        # Plot centroids\n        plt.scatter(self.centroids[:, 0], self.centroids[:, 1], \n                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n\n        plt.title(f'K-means Clustering (k={self.k})')\n        plt.xlabel('Feature 1')\n        plt.ylabel('Feature 2')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n# Example usage of custom implementation\nprint(\"Testing custom K-means implementation:\")\n\n# Generate sample data\nnp.random.seed(42)\nX_test, y_true_test = make_blobs(n_samples=300, centers=3, n_features=2, \n                                cluster_std=1.0, random_state=42)\n\n# Fit custom K-means\nkmeans_custom = KMeansFromScratch(k=3, max_iters=100, init='k-means++', random_state=42)\nlabels_custom = kmeans_custom.fit_predict(X_test)\n\nprint(f\"Custom K-means converged in {kmeans_custom.n_iter_} iterations\")\nprint(f\"Final inertia: {kmeans_custom.inertia_:.2f}\")\n\n# Compare with sklearn\nfrom sklearn.cluster import KMeans\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_sklearn = kmeans_sklearn.fit_predict(X_test)\n\nprint(f\"Sklearn K-means inertia: {kmeans_sklearn.inertia_:.2f}\")\nprint(f\"Agreement between implementations: {adjusted_rand_score(labels_custom, labels_sklearn):.3f}\")\n\n# Visualize results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_true_test, cmap='viridis', alpha=0.7)\nplt.title('True Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 3, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=labels_custom, cmap='viridis', alpha=0.7)\nplt.scatter(kmeans_custom.centroids[:, 0], kmeans_custom.centroids[:, 1], \n           marker='x', s=200, linewidths=3, color='red')\nplt.title('Custom K-means')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 3, 3)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=labels_sklearn, cmap='viridis', alpha=0.7)\nplt.scatter(kmeans_sklearn.cluster_centers_[:, 0], kmeans_sklearn.cluster_centers_[:, 1], \n           marker='x', s=200, linewidths=3, color='red')\nplt.title('Sklearn K-means')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n# Plot convergence\nkmeans_custom.plot_convergence()\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/K-means%20clustering/#assumptions","title":"Assumptions","text":"<ol> <li>Spherical Clusters: K-means assumes clusters are roughly spherical and have similar sizes</li> <li>Similar Variance: Clusters should have similar variance in all directions</li> <li>Isotropic Clusters: Equal variance in all dimensions</li> <li>Fixed Number of Clusters: You must specify k in advance</li> <li>Euclidean Distance: Uses Euclidean distance metric (sensitive to scale)</li> </ol>"},{"location":"Machine-Learning/K-means%20clustering/#limitations","title":"Limitations","text":"<ol> <li>Sensitive to Initialization: Can converge to local optima</li> <li>Requires Preprocessing: Sensitive to feature scaling and outliers</li> <li>Difficulty with Non-spherical Clusters: Performs poorly on elongated or irregular shapes</li> <li>Fixed k: Need to know or estimate the number of clusters</li> <li>Sensitive to Outliers: Outliers can significantly affect centroids</li> <li>Equal Cluster Size Assumption: Tends to create clusters of similar sizes</li> </ol>"},{"location":"Machine-Learning/K-means%20clustering/#comparison-with-other-clustering-algorithms","title":"Comparison with Other Clustering Algorithms","text":"Algorithm Advantages Disadvantages Best Use Cases K-means Fast, simple, works well with spherical clusters Requires k, sensitive to initialization, assumes spherical clusters Customer segmentation, image compression Hierarchical No need to specify k, creates dendrogram Slow O(n\u00b3), sensitive to noise Small datasets, understanding cluster hierarchy DBSCAN Finds arbitrary shaped clusters, robust to outliers Sensitive to parameters, struggles with varying densities Anomaly detection, irregular shaped clusters Gaussian Mixture Soft clustering, handles elliptical clusters More complex, requires knowing k When cluster overlap is expected"},{"location":"Machine-Learning/K-means%20clustering/#when-not-to-use-k-means","title":"When NOT to Use K-means","text":"<ul> <li>Non-spherical clusters: Use DBSCAN or spectral clustering</li> <li>Varying cluster densities: Use DBSCAN</li> <li>Unknown number of clusters: Use hierarchical clustering or DBSCAN</li> <li>Categorical data: Use K-modes or mixed-type clustering</li> <li>High-dimensional data: Consider dimensionality reduction first</li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. Explain the K-means algorithm step by step. <p>Answer:</p> <p>K-means follows these steps:</p> <ol> <li>Initialization: Choose k cluster centers (centroids) randomly or using k-means++</li> <li>Assignment: Assign each data point to the nearest centroid based on Euclidean distance</li> <li>Update: Recalculate centroids as the mean of all points assigned to each cluster</li> <li>Convergence Check: Repeat steps 2-3 until centroids stop moving significantly or max iterations reached</li> </ol> <p>Mathematical formulation: - Objective: Minimize \\(J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\) - Centroid update: \\(\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x\\)</p> <p>Time complexity: O(n\u00b7k\u00b7d\u00b7t) where n=samples, k=clusters, d=dimensions, t=iterations</p> 2. What are the main assumptions and limitations of K-means? <p>Answer:</p> <p>Assumptions: - Clusters are spherical and have similar sizes - Features have similar variances (isotropic) - Number of clusters k is known - Data is continuous and suitable for Euclidean distance</p> <p>Limitations: - Sensitive to initialization (can converge to local optima) - Requires specifying k in advance - Assumes spherical clusters of similar size - Sensitive to outliers and feature scaling - Poor performance on non-convex clusters - Hard clustering (each point belongs to exactly one cluster)</p> 3. How do you determine the optimal number of clusters (k)? <p>Answer:</p> <p>Methods to determine optimal k:</p> <ol> <li>Elbow Method: Plot WCSS vs k, look for the \"elbow\" point</li> <li>Silhouette Method: Choose k with highest average silhouette score</li> <li>Gap Statistic: Compare within-cluster dispersion with expected dispersion</li> <li>Information Criteria: Use AIC/BIC for model selection</li> <li>Domain Knowledge: Use business/domain expertise</li> </ol> <p>Elbow Method Example: <pre><code>wcss = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k)\n    wcss.append(kmeans.fit(X).inertia_)\n# Plot and look for elbow\n</code></pre></p> 4. What is the difference between K-means and K-means++? <p>Answer:</p> <p>K-means++ is an initialization method, not a different algorithm:</p> <p>Standard K-means initialization: - Randomly selects k points as initial centroids - Can lead to poor convergence and local optima - Results may vary significantly between runs</p> <p>K-means++ initialization: - First centroid chosen randomly - Subsequent centroids chosen with probability proportional to squared distance from nearest existing centroid - Provides better initial centroids, leading to better final clustering - More consistent results across multiple runs - Typically converges faster with better final objective value</p> 5. How does K-means handle outliers and what can you do about it? <p>Answer:</p> <p>How K-means handles outliers: - Outliers significantly affect centroid positions since centroids are calculated as means - Can cause centroids to shift away from main cluster mass - May create clusters around outliers - Reduces overall clustering quality</p> <p>Solutions:</p> <ol> <li>Preprocessing:</li> <li>Remove outliers using IQR, Z-score, or isolation forest</li> <li> <p>Use robust scaling instead of standard scaling</p> </li> <li> <p>Alternative algorithms:</p> </li> <li>Use K-medoids (uses medians instead of means)</li> <li> <p>Use DBSCAN (treats outliers as noise)</p> </li> <li> <p>Outlier-aware variants:</p> </li> <li>Trimmed K-means (removes certain percentage of farthest points)</li> <li>Robust K-means with M-estimators</li> </ol> 6. Compare K-means with Hierarchical clustering. <p>Answer:</p> Aspect K-means Hierarchical k specification Must specify k No need to specify k Time complexity O(nkdt) O(n\u00b3) for agglomerative Shape assumption Spherical clusters Any shape Scalability Good for large datasets Poor for large datasets Deterministic No (depends on initialization) Yes Output Flat partitioning Dendrogram hierarchy Interpretability Cluster centers Hierarchy of merges Memory usage Low High O(n\u00b2) 7. What is the objective function of K-means and how is it optimized? <p>Answer:</p> <p>Objective Function (Within-Cluster Sum of Squares): \\(\\(J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\)\\)</p> <p>Optimization: - K-means uses Lloyd's algorithm (Expectation-Maximization) - E-step: Assign points to nearest centroids (minimize J w.r.t. cluster assignments) - M-step: Update centroids as cluster means (minimize J w.r.t. centroids)</p> <p>Key properties: - Guaranteed to converge (objective function decreases monotonically) - May converge to local minimum - Convergence to global optimum not guaranteed - Each step reduces or maintains the objective value</p> 8. How do you evaluate the quality of K-means clustering? <p>Answer:</p> <p>Internal Metrics (no ground truth needed):</p> <ol> <li>Silhouette Score: Measures how similar points are to their own cluster vs other clusters</li> <li> <p>Range: [-1, 1], higher is better</p> </li> <li> <p>Within-Cluster Sum of Squares (WCSS): Lower is better</p> </li> <li> <p>Calinski-Harabasz Index: Ratio of between-cluster to within-cluster variance</p> </li> </ol> <p>External Metrics (with ground truth):</p> <ol> <li>Adjusted Rand Index (ARI): Measures similarity to true clustering</li> <li>Normalized Mutual Information (NMI): Information-theoretic measure</li> <li>Fowlkes-Mallows Index: Geometric mean of precision and recall</li> </ol> <p>Example: <pre><code>from sklearn.metrics import silhouette_score, calinski_harabasz_score\nsilhouette = silhouette_score(X, labels)\nch_score = calinski_harabasz_score(X, labels)\n</code></pre></p> 9. What is Mini-batch K-means and when would you use it? <p>Answer:</p> <p>Mini-batch K-means: - Variant that uses small random batches of data for updates - Updates centroids using only a subset of data points in each iteration - Significantly faster than standard K-means - Slightly lower quality but much more scalable</p> <p>When to use: - Large datasets where standard K-means is too slow - Online/streaming data scenarios - When approximate results are acceptable - Limited computational resources</p> <p>Trade-offs: - Pros: Much faster, memory efficient, good for large datasets - Cons: Slightly less accurate, may need more iterations for convergence</p> <p>Example: <pre><code>from sklearn.cluster import MiniBatchKMeans\nkmeans = MiniBatchKMeans(n_clusters=k, batch_size=100)\n</code></pre></p> 10. How does feature scaling affect K-means clustering? <p>Answer:</p> <p>Impact of feature scaling: - K-means uses Euclidean distance, which is sensitive to feature scales - Features with larger scales dominate the distance calculation - Can lead to poor clustering where high-scale features determine clusters</p> <p>Example: <pre><code># Age: 20-80, Income: 20000-100000\n# Income will dominate distance calculation\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>StandardScaler: Mean=0, Std=1    <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre></p> </li> <li> <p>MinMaxScaler: Scale to [0,1]    <pre><code>from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre></p> </li> <li> <p>RobustScaler: Uses median and IQR (robust to outliers)</p> </li> </ol> <p>Best practice: Always scale features before applying K-means</p>"},{"location":"Machine-Learning/K-means%20clustering/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/K-means%20clustering/#example-1-customer-segmentation","title":"Example 1: Customer Segmentation","text":"<pre><code># Generate customer data\nnp.random.seed(42)\nn_customers = 1000\n\n# Features: Age, Income, Spending Score\nages = np.random.normal(40, 12, n_customers)\nincomes = np.random.normal(60000, 20000, n_customers)\nspending_scores = np.random.normal(50, 25, n_customers)\n\n# Create DataFrame\ncustomer_data = pd.DataFrame({\n    'Age': ages,\n    'Annual_Income': incomes,\n    'Spending_Score': spending_scores\n})\n\n# Add some correlation (higher income -&gt; higher spending for some customers)\nmask = np.random.choice(n_customers, size=int(0.6 * n_customers), replace=False)\ncustomer_data.loc[mask, 'Spending_Score'] += (customer_data.loc[mask, 'Annual_Income'] - 60000) / 1000\n\nprint(\"Customer Data Summary:\")\nprint(customer_data.describe())\n\n# Prepare data for clustering\nX_customers = customer_data.values\nscaler = StandardScaler()\nX_customers_scaled = scaler.fit_transform(X_customers)\n\n# Determine optimal number of clusters\ndef analyze_optimal_k(X, max_k=10):\n    \"\"\"Analyze optimal k using multiple methods\"\"\"\n    wcss = []\n    silhouette_scores = []\n    k_range = range(2, max_k + 1)\n\n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X)\n\n        wcss.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X, labels))\n\n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Elbow method\n    ax1.plot(k_range, wcss, 'bo-')\n    ax1.set_title('Elbow Method')\n    ax1.set_xlabel('Number of Clusters (k)')\n    ax1.set_ylabel('WCSS')\n    ax1.grid(True, alpha=0.3)\n\n    # Silhouette method\n    ax2.plot(k_range, silhouette_scores, 'ro-')\n    ax2.set_title('Silhouette Analysis')\n    ax2.set_xlabel('Number of Clusters (k)')\n    ax2.set_ylabel('Silhouette Score')\n    ax2.grid(True, alpha=0.3)\n\n    # Find optimal k\n    optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n    ax2.axvline(x=optimal_k_silhouette, color='red', linestyle='--', \n                label=f'Optimal k={optimal_k_silhouette}')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return optimal_k_silhouette, silhouette_scores\n\noptimal_k, _ = analyze_optimal_k(X_customers_scaled, max_k=8)\nprint(f\"Optimal number of clusters: {optimal_k}\")\n\n# Apply K-means with optimal k\nkmeans_customers = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncustomer_segments = kmeans_customers.fit_predict(X_customers_scaled)\n\n# Add cluster labels to original data\ncustomer_data['Segment'] = customer_segments\n\n# Analyze segments\nprint(f\"\\nCustomer Segmentation Results (k={optimal_k}):\")\nprint(f\"Silhouette Score: {silhouette_score(X_customers_scaled, customer_segments):.3f}\")\n\nsegment_analysis = customer_data.groupby('Segment').agg({\n    'Age': ['mean', 'std'],\n    'Annual_Income': ['mean', 'std'],\n    'Spending_Score': ['mean', 'std'],\n    'Segment': 'count'\n}).round(2)\n\nsegment_analysis.columns = ['Age_Mean', 'Age_Std', 'Income_Mean', 'Income_Std', \n                           'Spending_Mean', 'Spending_Std', 'Count']\nprint(\"\\nSegment Characteristics:\")\nprint(segment_analysis)\n\n# Visualize segments\nfig = plt.figure(figsize=(18, 12))\n\n# 2D scatter plots\nfeature_pairs = [\n    ('Age', 'Annual_Income'),\n    ('Age', 'Spending_Score'),\n    ('Annual_Income', 'Spending_Score')\n]\n\nfor i, (feat1, feat2) in enumerate(feature_pairs):\n    ax = fig.add_subplot(2, 3, i+1)\n\n    for segment in range(optimal_k):\n        segment_data = customer_data[customer_data['Segment'] == segment]\n        ax.scatter(segment_data[feat1], segment_data[feat2], \n                  alpha=0.6, label=f'Segment {segment}')\n\n    ax.set_xlabel(feat1)\n    ax.set_ylabel(feat2)\n    ax.set_title(f'{feat1} vs {feat2}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n# 3D scatter plot\nax = fig.add_subplot(2, 3, 4, projection='3d')\ncolors = plt.cm.viridis(np.linspace(0, 1, optimal_k))\n\nfor segment in range(optimal_k):\n    segment_data = customer_data[customer_data['Segment'] == segment]\n    ax.scatter(segment_data['Age'], segment_data['Annual_Income'], \n              segment_data['Spending_Score'], c=[colors[segment]], \n              alpha=0.6, label=f'Segment {segment}')\n\nax.set_xlabel('Age')\nax.set_ylabel('Annual Income')\nax.set_zlabel('Spending Score')\nax.set_title('3D Customer Segments')\nax.legend()\n\n# Segment size distribution\nax = fig.add_subplot(2, 3, 5)\nsegment_counts = customer_data['Segment'].value_counts().sort_index()\nax.bar(range(optimal_k), segment_counts.values, color=colors[:optimal_k])\nax.set_xlabel('Segment')\nax.set_ylabel('Number of Customers')\nax.set_title('Segment Size Distribution')\nax.set_xticks(range(optimal_k))\n\n# Radar chart for segment characteristics\nax = fig.add_subplot(2, 3, 6, projection='polar')\n\n# Normalize features for radar chart\ncentroids_original = scaler.inverse_transform(kmeans_customers.cluster_centers_)\nfeatures = ['Age', 'Annual_Income', 'Spending_Score']\n\n# Normalize each feature to 0-1 scale for visualization\nnormalized_centroids = np.zeros_like(centroids_original)\nfor i, feature in enumerate(features):\n    min_val = customer_data[feature].min()\n    max_val = customer_data[feature].max()\n    normalized_centroids[:, i] = (centroids_original[:, i] - min_val) / (max_val - min_val)\n\nangles = np.linspace(0, 2*np.pi, len(features), endpoint=False).tolist()\nangles += angles[:1]  # Complete the circle\n\nfor segment in range(optimal_k):\n    values = normalized_centroids[segment].tolist()\n    values += values[:1]  # Complete the circle\n    ax.plot(angles, values, 'o-', linewidth=2, label=f'Segment {segment}')\n    ax.fill(angles, values, alpha=0.25)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(features)\nax.set_title('Segment Characteristics (Normalized)')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Business insights\nprint(\"\\nBusiness Insights:\")\nfor segment in range(optimal_k):\n    segment_data = segment_analysis.loc[segment]\n    print(f\"\\nSegment {segment} ({segment_data['Count']} customers):\")\n    print(f\"  Average Age: {segment_data['Age_Mean']:.1f} years\")\n    print(f\"  Average Income: ${segment_data['Income_Mean']:,.0f}\")\n    print(f\"  Average Spending Score: {segment_data['Spending_Mean']:.1f}\")\n\n    # Generate insights based on characteristics\n    if segment_data['Age_Mean'] &lt; 35 and segment_data['Spending_Mean'] &gt; 60:\n        print(f\"  \u2192 Young high spenders - target for premium products\")\n    elif segment_data['Income_Mean'] &gt; 70000 and segment_data['Spending_Mean'] &lt; 40:\n        print(f\"  \u2192 High income, low spending - potential for marketing campaigns\")\n    elif segment_data['Age_Mean'] &gt; 50 and segment_data['Spending_Mean'] &gt; 60:\n        print(f\"  \u2192 Mature high spenders - focus on quality and service\")\n    else:\n        print(f\"  \u2192 Standard customers - balanced approach\")\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#example-2-image-color-quantization","title":"Example 2: Image Color Quantization","text":"<pre><code>from sklearn.datasets import load_sample_image\nimport matplotlib.image as mpimg\n\ndef quantize_image_colors(image_path=None, n_colors=8):\n    \"\"\"Reduce number of colors in an image using K-means\"\"\"\n\n    # Load image (use sample image if path not provided)\n    if image_path is None:\n        # Use sklearn sample image\n        china = load_sample_image(\"china.jpg\")\n        image = china / 255.0  # Normalize to [0, 1]\n    else:\n        image = mpimg.imread(image_path)\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n    print(f\"Original image shape: {image.shape}\")\n\n    # Reshape image to be a list of pixels\n    original_shape = image.shape\n    image_2d = image.reshape(-1, 3)  # Flatten to (n_pixels, 3)\n\n    print(f\"Number of pixels: {image_2d.shape[0]:,}\")\n    print(f\"Original colors: {len(np.unique(image_2d, axis=0)):,} unique colors\")\n\n    # Apply K-means clustering\n    print(f\"Reducing to {n_colors} colors using K-means...\")\n\n    kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(image_2d)\n\n    # Replace each pixel with its cluster center\n    quantized_colors = kmeans.cluster_centers_[labels]\n    quantized_image = quantized_colors.reshape(original_shape)\n\n    # Calculate compression ratio\n    original_unique_colors = len(np.unique(image_2d, axis=0))\n    compression_ratio = original_unique_colors / n_colors\n\n    print(f\"Compression ratio: {compression_ratio:.1f}x\")\n    print(f\"Final colors: {n_colors}\")\n\n    # Visualize results\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n    # Original image\n    axes[0, 0].imshow(image)\n    axes[0, 0].set_title(f'Original Image\\n({original_unique_colors:,} colors)')\n    axes[0, 0].axis('off')\n\n    # Quantized image\n    axes[0, 1].imshow(quantized_image)\n    axes[0, 1].set_title(f'Quantized Image\\n({n_colors} colors)')\n    axes[0, 1].axis('off')\n\n    # Difference\n    difference = np.abs(image - quantized_image)\n    axes[0, 2].imshow(difference)\n    axes[0, 2].set_title('Absolute Difference')\n    axes[0, 2].axis('off')\n\n    # Color palette\n    palette = kmeans.cluster_centers_.reshape(1, n_colors, 3)\n    axes[1, 0].imshow(palette)\n    axes[1, 0].set_title('Color Palette')\n    axes[1, 0].axis('off')\n\n    # Color distribution\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    colors_rgb = kmeans.cluster_centers_\n\n    axes[1, 1].bar(range(n_colors), counts, color=colors_rgb, alpha=0.8)\n    axes[1, 1].set_title('Color Frequency')\n    axes[1, 1].set_xlabel('Color Index')\n    axes[1, 1].set_ylabel('Pixel Count')\n\n    # MSE plot for different number of colors\n    color_range = range(2, 17)\n    mse_values = []\n\n    for n_c in color_range:\n        temp_kmeans = KMeans(n_clusters=n_c, random_state=42, n_init=5)\n        temp_labels = temp_kmeans.fit_predict(image_2d)\n        temp_quantized = temp_kmeans.cluster_centers_[temp_labels]\n        mse = np.mean((image_2d - temp_quantized) ** 2)\n        mse_values.append(mse)\n\n    axes[1, 2].plot(color_range, mse_values, 'bo-')\n    axes[1, 2].axvline(x=n_colors, color='red', linestyle='--', \n                       label=f'Selected k={n_colors}')\n    axes[1, 2].set_title('MSE vs Number of Colors')\n    axes[1, 2].set_xlabel('Number of Colors')\n    axes[1, 2].set_ylabel('Mean Squared Error')\n    axes[1, 2].legend()\n    axes[1, 2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return quantized_image, kmeans\n\n# Apply color quantization\nquantized_img, color_kmeans = quantize_image_colors(n_colors=16)\n\nprint(\"\\nColor palette (RGB values):\")\nfor i, color in enumerate(color_kmeans.cluster_centers_):\n    print(f\"Color {i}: RGB({color[0]:.3f}, {color[1]:.3f}, {color[2]:.3f})\")\n</code></pre>"},{"location":"Machine-Learning/K-means%20clustering/#references","title":"\ud83d\udcda References","text":""},{"location":"Machine-Learning/K-means%20clustering/#academic-papers","title":"Academic Papers","text":"<ul> <li>Lloyd, S. (1982). \"Least squares quantization in PCM\". IEEE Transactions on Information Theory</li> <li>Arthur, D. &amp; Vassilvitskii, S. (2007). \"K-means++: The advantages of careful seeding\". SODA '07</li> <li>MacQueen, J. (1967). \"Some methods for classification and analysis of multivariate observations\"</li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#books","title":"Books","text":"<ul> <li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The Elements of Statistical Learning. Chapter 14.3</li> <li>Bishop, C. (2006). Pattern Recognition and Machine Learning. Chapter 9</li> <li>Murphy, K. (2012). Machine Learning: A Probabilistic Perspective. Chapter 25</li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#online-resources","title":"Online Resources","text":"<ul> <li>Scikit-learn K-means Documentation</li> <li>K-means Clustering: Algorithm, Applications, Evaluation Methods</li> <li>An Introduction to Statistical Learning with R - Chapter 10</li> <li>Stanford CS229 Machine Learning Course Notes</li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#tutorials-and-guides","title":"Tutorials and Guides","text":"<ul> <li>K-means Clustering in Python: A Practical Guide</li> <li>Clustering Algorithms Comparison</li> <li>How to Determine the Optimal Number of Clusters</li> </ul>"},{"location":"Machine-Learning/K-means%20clustering/#interactive-visualizations","title":"Interactive Visualizations","text":"<ul> <li>K-means Interactive Demo</li> <li>Clustering Visualization Tool</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/","title":"\ud83d\udcd8 Linear Regression","text":"<p>Linear Regression is a fundamental supervised learning algorithm that models the linear relationship between a dependent variable and one or more independent variables by finding the best-fitting straight line through the data points.</p> <p>Resources: Scikit-learn Linear Regression | Stanford CS229 Notes | ISL Chapter 3</p>"},{"location":"Machine-Learning/Linear%20Regression/#summary","title":"\u270d\ufe0f Summary","text":"<p>Linear Regression is the simplest and most widely used regression technique that assumes a linear relationship between input features and the target variable. It aims to find the best line (or hyperplane in higher dimensions) that minimizes the sum of squared differences between actual and predicted values.</p> <p>Key characteristics: - Simplicity: Easy to understand and implement - Interpretability: Coefficients have clear meaning - Fast: Quick to train and predict - Baseline: Often used as a starting point for regression problems - Probabilistic: Provides confidence intervals and statistical tests</p> <p>Applications: - Predicting house prices based on features - Sales forecasting from marketing spend - Risk assessment in finance - Medical diagnosis and treatment effects - Economics and business analytics - Scientific research and hypothesis testing</p> <p>Types: - Simple Linear Regression: One independent variable - Multiple Linear Regression: Multiple independent variables - Polynomial Regression: Non-linear relationships using polynomial features - Regularized Regression: Ridge, Lasso, and Elastic Net</p>"},{"location":"Machine-Learning/Linear%20Regression/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Linear%20Regression/#how-linear-regression-works","title":"How Linear Regression Works","text":"<p>Imagine you're trying to predict house prices based on their size. Linear regression finds the straight line that best fits through all the data points, minimizing the overall prediction error. This line can then be used to predict prices for new houses.</p>"},{"location":"Machine-Learning/Linear%20Regression/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Linear%20Regression/#1-simple-linear-regression","title":"1. Simple Linear Regression","text":"<p>For one feature, the model is: \\(\\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\\)</p> <p>Where: - \\(y\\) is the dependent variable (target) - \\(x\\) is the independent variable (feature) - \\(\\beta_0\\) is the intercept (y-intercept) - \\(\\beta_1\\) is the slope (coefficient) - \\(\\epsilon\\) is the error term</p>"},{"location":"Machine-Learning/Linear%20Regression/#2-multiple-linear-regression","title":"2. Multiple Linear Regression","text":"<p>For multiple features: \\(\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\\)\\)</p> <p>In matrix form: \\(\\(\\mathbf{y} = \\mathbf{X\\beta} + \\boldsymbol{\\epsilon}\\)\\)</p> <p>Where: - \\(\\mathbf{y}\\) is the target vector \\((n \\times 1)\\) - \\(\\mathbf{X}\\) is the feature matrix \\((n \\times p)\\) with bias column - \\(\\boldsymbol{\\beta}\\) is the coefficient vector \\((p \\times 1)\\) - \\(\\boldsymbol{\\epsilon}\\) is the error vector \\((n \\times 1)\\)</p>"},{"location":"Machine-Learning/Linear%20Regression/#3-cost-function-mean-squared-error","title":"3. Cost Function (Mean Squared Error)","text":"\\[J(\\boldsymbol{\\beta}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\boldsymbol{\\beta}}(\\mathbf{x}^{(i)}) - y^{(i)})^2\\] <p>Or in matrix form: \\(\\(J(\\boldsymbol{\\beta}) = \\frac{1}{2m} (\\mathbf{X\\beta} - \\mathbf{y})^T(\\mathbf{X\\beta} - \\mathbf{y})\\)\\)</p>"},{"location":"Machine-Learning/Linear%20Regression/#4-normal-equation-closed-form-solution","title":"4. Normal Equation (Closed-form Solution)","text":"<p>The optimal coefficients can be found analytically: \\(\\(\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\\)</p>"},{"location":"Machine-Learning/Linear%20Regression/#5-gradient-descent-iterative-solution","title":"5. Gradient Descent (Iterative Solution)","text":"<p>Update rule for coefficients: \\(\\(\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} J(\\boldsymbol{\\beta})\\)\\)</p> <p>The gradient is: \\(\\(\\frac{\\partial J}{\\partial \\boldsymbol{\\beta}} = \\frac{1}{m} \\mathbf{X}^T(\\mathbf{X\\beta} - \\mathbf{y})\\)\\)</p>"},{"location":"Machine-Learning/Linear%20Regression/#key-assumptions","title":"Key Assumptions","text":"<ol> <li>Linearity: Relationship between features and target is linear</li> <li>Independence: Observations are independent</li> <li>Homoscedasticity: Constant variance of errors</li> <li>Normality: Errors are normally distributed</li> <li>No multicollinearity: Features are not highly correlated</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/Linear%20Regression/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression, load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Generate sample data\nX, y = make_regression(\n    n_samples=1000,\n    n_features=1,\n    noise=20,\n    random_state=42\n)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred = lr_model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Model Performance:\")\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"R\u00b2 Score: {r2:.3f}\")\nprint(f\"Intercept: {lr_model.intercept_:.2f}\")\nprint(f\"Coefficient: {lr_model.coef_[0]:.2f}\")\n\n# Visualize results\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X_test, y_test, alpha=0.6, label='Actual')\nplt.scatter(X_test, y_pred, alpha=0.6, label='Predicted')\nplt.plot(X_test, y_pred, color='red', linewidth=2)\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.title('Linear Regression Fit')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title(f'Predictions vs Actual (R\u00b2 = {r2:.3f})')\n\nplt.subplot(1, 3, 3)\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\nplt.tight_layout()\nplt.show()\n\n# Multiple Linear Regression Example\n# Load Boston housing dataset\nboston = load_boston()\nX_multi, y_multi = boston.data, boston.target\nfeature_names = boston.feature_names\n\n# Split data\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.2, random_state=42\n)\n\n# Scale features for better interpretation\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_multi)\nX_test_scaled = scaler.transform(X_test_multi)\n\n# Train model\nlr_multi = LinearRegression()\nlr_multi.fit(X_train_scaled, y_train_multi)\n\n# Predictions\ny_pred_multi = lr_multi.predict(X_test_scaled)\n\n# Metrics\nr2_multi = r2_score(y_test_multi, y_pred_multi)\nrmse_multi = np.sqrt(mean_squared_error(y_test_multi, y_pred_multi))\n\nprint(f\"\\nMultiple Linear Regression Results:\")\nprint(f\"R\u00b2 Score: {r2_multi:.3f}\")\nprint(f\"RMSE: {rmse_multi:.2f}\")\n\n# Feature coefficients analysis\ncoef_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': lr_multi.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(\"\\nFeature Coefficients (scaled):\")\nprint(coef_df)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(coef_df['Feature'], coef_df['Coefficient'])\nplt.xlabel('Coefficient Value')\nplt.title('Linear Regression Coefficients')\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.5)\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Linear%20Regression/#using-statsmodels-for-statistical-analysis","title":"Using StatsModels for Statistical Analysis","text":"<pre><code>import statsmodels.api as sm\nfrom scipy import stats\n\n# Prepare data with intercept\nX_with_intercept = sm.add_constant(X_train_multi)\nX_test_with_intercept = sm.add_constant(X_test_multi)\n\n# Fit OLS model\nols_model = sm.OLS(y_train_multi, X_with_intercept).fit()\n\n# Print comprehensive summary\nprint(\"OLS Regression Results:\")\nprint(ols_model.summary())\n\n# Predictions with confidence intervals\npredictions = ols_model.get_prediction(X_test_with_intercept)\npred_summary = predictions.summary_frame(alpha=0.05)\n\nprint(\"\\nPredictions with Confidence Intervals (first 5):\")\nprint(pred_summary.head())\n\n# Statistical tests\nprint(f\"\\nModel Statistics:\")\nprint(f\"F-statistic: {ols_model.fvalue:.2f}\")\nprint(f\"F-statistic p-value: {ols_model.f_pvalue:.2e}\")\nprint(f\"AIC: {ols_model.aic:.2f}\")\nprint(f\"BIC: {ols_model.bic:.2f}\")\n\n# Residual analysis\nresiduals = ols_model.resid\nfitted_values = ols_model.fittedvalues\n\n# Diagnostic plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Q-Q plot\nstats.probplot(residuals, dist=\"norm\", plot=axes[0,0])\naxes[0,0].set_title(\"Q-Q Plot\")\n\n# Residuals vs Fitted\naxes[0,1].scatter(fitted_values, residuals, alpha=0.6)\naxes[0,1].axhline(y=0, color='r', linestyle='--')\naxes[0,1].set_xlabel('Fitted Values')\naxes[0,1].set_ylabel('Residuals')\naxes[0,1].set_title('Residuals vs Fitted')\n\n# Histogram of residuals\naxes[1,0].hist(residuals, bins=20, alpha=0.7)\naxes[1,0].set_xlabel('Residuals')\naxes[1,0].set_ylabel('Frequency')\naxes[1,0].set_title('Histogram of Residuals')\n\n# Scale-Location plot\nstandardized_residuals = np.sqrt(np.abs(residuals / np.std(residuals)))\naxes[1,1].scatter(fitted_values, standardized_residuals, alpha=0.6)\naxes[1,1].set_xlabel('Fitted Values')\naxes[1,1].set_ylabel('\u221a|Standardized Residuals|')\naxes[1,1].set_title('Scale-Location Plot')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Linear%20Regression/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\n\nclass LinearRegressionScratch:\n    \"\"\"\n    Linear Regression implementation from scratch using both\n    Normal Equation and Gradient Descent methods.\n    \"\"\"\n\n    def __init__(self, method='normal_equation', learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Initialize Linear Regression.\n\n        Parameters:\n        -----------\n        method : str, 'normal_equation' or 'gradient_descent'\n        learning_rate : float, learning rate for gradient descent\n        n_iterations : int, number of iterations for gradient descent\n        \"\"\"\n        self.method = method\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.coefficients = None\n        self.intercept = None\n        self.cost_history = []\n\n    def add_intercept(self, X):\n        \"\"\"Add bias column to the feature matrix.\"\"\"\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit linear regression model.\n\n        Parameters:\n        -----------\n        X : array-like, shape = [n_samples, n_features]\n        y : array-like, shape = [n_samples]\n        \"\"\"\n        # Ensure y is a column vector\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        # Add intercept term\n        X_with_intercept = self.add_intercept(X)\n\n        if self.method == 'normal_equation':\n            self._fit_normal_equation(X_with_intercept, y)\n        elif self.method == 'gradient_descent':\n            self._fit_gradient_descent(X_with_intercept, y)\n        else:\n            raise ValueError(\"Method must be 'normal_equation' or 'gradient_descent'\")\n\n    def _fit_normal_equation(self, X, y):\n        \"\"\"Fit using normal equation: \u03b2 = (X^T X)^(-1) X^T y\"\"\"\n        try:\n            # Normal equation\n            XtX = np.dot(X.T, X)\n            XtX_inv = np.linalg.inv(XtX)\n            Xty = np.dot(X.T, y)\n            theta = np.dot(XtX_inv, Xty)\n\n            self.intercept = theta[0, 0]\n            self.coefficients = theta[1:].flatten()\n\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse if matrix is singular\n            theta = np.dot(np.linalg.pinv(X), y)\n            self.intercept = theta[0, 0]\n            self.coefficients = theta[1:].flatten()\n\n    def _fit_gradient_descent(self, X, y):\n        \"\"\"Fit using gradient descent.\"\"\"\n        m, n = X.shape\n\n        # Initialize parameters\n        theta = np.zeros((n, 1))\n\n        for i in range(self.n_iterations):\n            # Forward pass\n            predictions = np.dot(X, theta)\n\n            # Compute cost\n            cost = self._compute_cost(predictions, y)\n            self.cost_history.append(cost)\n\n            # Compute gradients\n            gradients = (1/m) * np.dot(X.T, (predictions - y))\n\n            # Update parameters\n            theta = theta - self.learning_rate * gradients\n\n        self.intercept = theta[0, 0]\n        self.coefficients = theta[1:].flatten()\n\n    def _compute_cost(self, predictions, y):\n        \"\"\"Compute mean squared error cost.\"\"\"\n        m = y.shape[0]\n        cost = (1/(2*m)) * np.sum(np.power(predictions - y, 2))\n        return cost\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the fitted model.\n\n        Parameters:\n        -----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns:\n        --------\n        predictions : array-like, shape = [n_samples]\n        \"\"\"\n        return np.dot(X, self.coefficients) + self.intercept\n\n    def score(self, X, y):\n        \"\"\"Calculate R\u00b2 score.\"\"\"\n        y_pred = self.predict(X)\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        return 1 - (ss_res / ss_tot)\n\n    def get_params(self):\n        \"\"\"Get model parameters.\"\"\"\n        return {\n            'intercept': self.intercept,\n            'coefficients': self.coefficients,\n            'cost_history': self.cost_history\n        }\n\n# Example usage and comparison\nif __name__ == \"__main__\":\n    # Generate sample data\n    X, y = make_regression(n_samples=1000, n_features=3, noise=10, random_state=42)\n\n    # Split data\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Our implementation - Normal Equation\n    lr_normal = LinearRegressionScratch(method='normal_equation')\n    lr_normal.fit(X_train, y_train)\n    y_pred_normal = lr_normal.predict(X_test)\n    r2_normal = lr_normal.score(X_test, y_test)\n\n    # Our implementation - Gradient Descent\n    lr_gd = LinearRegressionScratch(method='gradient_descent', learning_rate=0.01, n_iterations=1000)\n    lr_gd.fit(X_train, y_train)\n    y_pred_gd = lr_gd.predict(X_test)\n    r2_gd = lr_gd.score(X_test, y_test)\n\n    # Sklearn for comparison\n    from sklearn.linear_model import LinearRegression\n    sklearn_lr = LinearRegression()\n    sklearn_lr.fit(X_train, y_train)\n    y_pred_sklearn = sklearn_lr.predict(X_test)\n    r2_sklearn = sklearn_lr.score(X_test, y_test)\n\n    # Compare results\n    print(\"Comparison of Implementations:\")\n    print(f\"Normal Equation R\u00b2: {r2_normal:.6f}\")\n    print(f\"Gradient Descent R\u00b2: {r2_gd:.6f}\")\n    print(f\"Sklearn R\u00b2: {r2_sklearn:.6f}\")\n\n    print(f\"\\nIntercept comparison:\")\n    print(f\"Normal Equation: {lr_normal.intercept:.6f}\")\n    print(f\"Gradient Descent: {lr_gd.intercept:.6f}\")\n    print(f\"Sklearn: {sklearn_lr.intercept_:.6f}\")\n\n    print(f\"\\nCoefficients comparison:\")\n    print(f\"Normal Equation: {lr_normal.coefficients}\")\n    print(f\"Gradient Descent: {lr_gd.coefficients}\")\n    print(f\"Sklearn: {sklearn_lr.coef_}\")\n\n    # Plot cost history for gradient descent\n    if lr_gd.cost_history:\n        plt.figure(figsize=(10, 4))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(lr_gd.cost_history)\n        plt.title('Cost Function During Training')\n        plt.xlabel('Iterations')\n        plt.ylabel('Cost (MSE)')\n\n        plt.subplot(1, 2, 2)\n        plt.scatter(y_test, y_pred_normal, alpha=0.6, label='Normal Equation')\n        plt.scatter(y_test, y_pred_gd, alpha=0.6, label='Gradient Descent')\n        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n        plt.xlabel('Actual Values')\n        plt.ylabel('Predicted Values')\n        plt.title('Predictions Comparison')\n        plt.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n# Polynomial Regression from scratch\nclass PolynomialRegressionScratch:\n    \"\"\"Polynomial Regression using our Linear Regression implementation.\"\"\"\n\n    def __init__(self, degree=2, method='normal_equation'):\n        self.degree = degree\n        self.linear_regression = LinearRegressionScratch(method=method)\n\n    def _create_polynomial_features(self, X):\n        \"\"\"Create polynomial features up to specified degree.\"\"\"\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n\n        n_samples, n_features = X.shape\n\n        # Start with original features\n        poly_features = X.copy()\n\n        # Add polynomial terms\n        for deg in range(2, self.degree + 1):\n            for feature_idx in range(n_features):\n                poly_feature = np.power(X[:, feature_idx], deg).reshape(-1, 1)\n                poly_features = np.concatenate([poly_features, poly_feature], axis=1)\n\n        return poly_features\n\n    def fit(self, X, y):\n        \"\"\"Fit polynomial regression.\"\"\"\n        X_poly = self._create_polynomial_features(X)\n        self.linear_regression.fit(X_poly, y)\n\n    def predict(self, X):\n        \"\"\"Make predictions.\"\"\"\n        X_poly = self._create_polynomial_features(X)\n        return self.linear_regression.predict(X_poly)\n\n    def score(self, X, y):\n        \"\"\"Calculate R\u00b2 score.\"\"\"\n        X_poly = self._create_polynomial_features(X)\n        return self.linear_regression.score(X_poly, y)\n\n# Test polynomial regression\nif __name__ == \"__main__\":\n    # Generate non-linear data\n    np.random.seed(42)\n    X_poly = np.linspace(-2, 2, 100).reshape(-1, 1)\n    y_poly = 0.5 * X_poly.ravel()**3 - 2 * X_poly.ravel()**2 + X_poly.ravel() + np.random.normal(0, 0.5, 100)\n\n    # Fit polynomial regression\n    poly_reg = PolynomialRegressionScratch(degree=3)\n    poly_reg.fit(X_poly, y_poly)\n\n    # Predictions\n    X_plot = np.linspace(-2, 2, 300).reshape(-1, 1)\n    y_pred_poly = poly_reg.predict(X_plot)\n\n    # Plot results\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X_poly, y_poly, alpha=0.6, label='Data')\n    plt.plot(X_plot, y_pred_poly, color='red', linewidth=2, label='Polynomial Fit (degree=3)')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Polynomial Regression from Scratch')\n    plt.legend()\n    plt.show()\n\n    r2_poly = poly_reg.score(X_poly, y_poly)\n    print(f\"Polynomial Regression R\u00b2: {r2_poly:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Linear%20Regression/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Linear%20Regression/#assumptions","title":"Assumptions","text":"<ol> <li>Linearity</li> <li>Relationship between features and target is linear</li> <li>Check: Scatter plots, residual plots</li> <li> <p>Violation: Use polynomial features or non-linear models</p> </li> <li> <p>Independence</p> </li> <li>Observations are independent of each other</li> <li>Check: Domain knowledge, autocorrelation tests</li> <li> <p>Violation: Use time series models or clustered standard errors</p> </li> <li> <p>Homoscedasticity</p> </li> <li>Constant variance of residuals across all levels of features</li> <li>Check: Residuals vs fitted values plot</li> <li> <p>Violation: Use weighted least squares or transform target variable</p> </li> <li> <p>Normality of Residuals</p> </li> <li>Residuals should be normally distributed</li> <li>Check: Q-Q plots, Shapiro-Wilk test</li> <li> <p>Violation: Transform variables or use robust regression</p> </li> <li> <p>No Multicollinearity</p> </li> <li>Features should not be highly correlated</li> <li>Check: Correlation matrix, VIF (Variance Inflation Factor)</li> <li>Violation: Remove features, use regularization (Ridge/Lasso)</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#limitations","title":"Limitations","text":""},{"location":"Machine-Learning/Linear%20Regression/#1-linear-relationship-only","title":"1. Linear Relationship Only","text":"<ul> <li>Cannot capture non-linear patterns without feature engineering</li> <li>Solution: Polynomial features, interaction terms, or non-linear models</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/#2-sensitive-to-outliers","title":"2. Sensitive to Outliers","text":"<ul> <li>Outliers can significantly affect the regression line</li> <li>Solution: Robust regression, outlier detection and removal</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/#3-multicollinearity-issues","title":"3. Multicollinearity Issues","text":"<ul> <li>High correlation between features causes unstable coefficients</li> <li>Solution: Feature selection, regularization techniques</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/#4-overfitting-with-many-features","title":"4. Overfitting with Many Features","text":"<ul> <li>Can overfit when number of features approaches number of samples</li> <li>Solution: Regularization (Ridge, Lasso), feature selection</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/#5-assumes-linear-relationship","title":"5. Assumes Linear Relationship","text":"<ul> <li>May perform poorly on complex, non-linear datasets</li> <li>Alternative: Polynomial regression, kernel methods, tree-based models</li> </ul>"},{"location":"Machine-Learning/Linear%20Regression/#when-to-use-vs-avoid","title":"When to Use vs Avoid","text":"<p>Use Linear Regression when: - Relationship appears linear - Interpretability is important - Need quick baseline model - Small to medium datasets - Features are not highly correlated - Statistical inference is needed</p> <p>Avoid Linear Regression when: - Clear non-linear relationships exist - Many irrelevant features present - High multicollinearity among features - Outliers are prevalent and cannot be removed - Need high prediction accuracy over interpretability</p>"},{"location":"Machine-Learning/Linear%20Regression/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. Explain the difference between correlation and causation in the context of linear regression. <p>Answer: - Correlation: Statistical relationship between variables; high correlation doesn't imply causation - Causation: One variable directly influences another - In regression: A significant coefficient shows correlation but not necessarily causation - Example: Ice cream sales and drowning deaths are correlated (both increase in summer) but ice cream doesn't cause drowning - Establishing causation: Requires randomized controlled experiments, domain expertise, and careful study design - Confounding variables: Can create spurious correlations that disappear when controlled for</p> 2. What is the difference between R\u00b2 and adjusted R\u00b2? When should you use each? <p>Answer: - R\u00b2: Proportion of variance in dependent variable explained by independent variables   - Formula: \\(R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\\)   - Always increases with more features - Adjusted R\u00b2: Penalizes for number of features   - Formula: \\(R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\\)   - Can decrease if adding irrelevant features - Use R\u00b2: When comparing models with same number of features - Use Adjusted R\u00b2: When comparing models with different numbers of features - Better metric: Adjusted R\u00b2 prevents overfitting by penalizing model complexity</p> 3. How do you handle multicollinearity in linear regression? <p>Answer: Detection methods: - Correlation matrix (threshold &gt; 0.8) - VIF (Variance Inflation Factor) &gt; 5 or 10 - Condition number &gt; 30</p> <p>Solutions: 1. Remove highly correlated features: Drop one from each correlated pair 2. Principal Component Analysis (PCA): Transform to orthogonal components 3. Ridge Regression: L2 regularization reduces impact of multicollinearity 4. Feature combination: Create new features by combining correlated ones 5. Domain knowledge: Remove features that don't make business sense 6. Regularization: Lasso can automatically select relevant features</p> 4. Explain the normal equation vs gradient descent for linear regression. When would you use each? <p>Answer: Normal Equation: \\(\\beta = (X^TX)^{-1}X^Ty\\) - Advantages: Exact solution, no hyperparameters, no iterations needed - Disadvantages: O(n\u00b3) complexity for matrix inversion, doesn't work if \\(X^TX\\) is singular - Use when: Small datasets (n &lt; 10,000), need exact solution</p> <p>Gradient Descent: - Advantages: Works with large datasets, O(kn\u00b2) per iteration, more memory efficient - Disadvantages: Requires hyperparameter tuning, may not converge, approximate solution - Use when: Large datasets (n &gt; 10,000), online learning needed</p> <p>Practical rule: Normal equation for small datasets, gradient descent for large ones</p> 5. What are the key assumptions of linear regression and how do you test them? <p>Answer: 1. Linearity:     - Test: Scatter plots of features vs target, residual plots    - Violation: Add polynomial terms or use non-linear models</p> <ol> <li>Independence:</li> <li>Test: Domain knowledge, Durbin-Watson test for autocorrelation</li> <li> <p>Violation: Use time series models or account for clustering</p> </li> <li> <p>Homoscedasticity:</p> </li> <li>Test: Residuals vs fitted plot, Breusch-Pagan test</li> <li> <p>Violation: Use weighted least squares or log transformation</p> </li> <li> <p>Normality of residuals:</p> </li> <li>Test: Q-Q plots, Shapiro-Wilk test, histogram of residuals</li> <li> <p>Violation: Transform variables or use robust regression</p> </li> <li> <p>No multicollinearity:</p> </li> <li>Test: VIF &gt; 5, correlation matrix</li> <li>Violation: Remove features, use regularization</li> </ol> 6. How do you interpret the coefficients in linear regression? <p>Answer: For continuous variables: - Coefficient represents change in target for one unit change in feature, holding other features constant - Example: If coefficient for 'years of experience' is 5000, each additional year increases salary by $5000</p> <p>For categorical variables (dummy coded): - Coefficient represents difference from reference category - Example: If 'gender_male' coefficient is 3000, males earn $3000 more than females (reference)</p> <p>Important considerations: - Scale matters: Larger-scale features have smaller coefficients - Standardization: Standardized coefficients allow comparison of feature importance - Interaction effects: Coefficients change meaning with interaction terms - Confidence intervals: Provide uncertainty estimates around coefficients</p> 7. What is the bias-variance tradeoff in linear regression? <p>Answer: Bias: Error from overly simplistic assumptions - High bias: Model consistently misses relevant patterns - In linear regression: Assuming linear relationship when it's non-linear</p> <p>Variance: Error from sensitivity to small fluctuations in training set - High variance: Model changes significantly with different training data - In linear regression: Overfitting with too many features relative to data</p> <p>Tradeoff:  - Simple models (fewer features): High bias, low variance - Complex models (many features): Low bias, high variance - Optimal point: Minimizes total error = bias\u00b2 + variance + irreducible error</p> <p>Solutions: - Cross-validation to find optimal complexity - Regularization (Ridge/Lasso) to balance bias-variance - More training data reduces variance</p> 8. Compare Ridge, Lasso, and Elastic Net regression. <p>Answer:</p> Aspect Ridge (L2) Lasso (L1) Elastic Net Penalty \\(\\lambda \\sum \\beta_i^2\\) $\\lambda \\sum \\beta_i Feature Selection No (shrinks to near 0) Yes (shrinks to exactly 0) Yes (selective) Multicollinearity Handles well Arbitrary selection Handles well Sparse Solutions No Yes Yes Groups of correlated features Includes all Picks one arbitrarily Tends to include/exclude together <p>When to use: - Ridge: Multicollinearity, want to keep all features - Lasso: Feature selection, want sparse model - Elastic Net: Best of both, handles grouped variables well</p> 9. How do you handle categorical variables in linear regression? <p>Answer: Encoding methods:</p> <ol> <li>One-Hot Encoding (Dummy Variables):</li> <li>Create binary columns for each category</li> <li>Drop one category to avoid multicollinearity (dummy variable trap)</li> <li> <p>Example: Color {Red, Blue, Green} \u2192 Color_Red, Color_Blue (Green is reference)</p> </li> <li> <p>Effect Coding:</p> </li> <li>Similar to dummy coding but reference category coded as -1</li> <li> <p>Coefficients represent deviation from overall mean</p> </li> <li> <p>Ordinal Encoding:</p> </li> <li>For ordered categories (Low, Medium, High \u2192 1, 2, 3)</li> <li>Assumes linear relationship between categories</li> </ol> <p>Considerations: - Reference category: Choose meaningful baseline for interpretation - High cardinality: Use target encoding or frequency encoding - Interaction effects: May need to include interactions with other features - Regularization: Helps when many categories create many dummy variables</p> 10. What evaluation metrics would you use for regression problems and why? <p>Answer: Common metrics:</p> <ol> <li>Mean Absolute Error (MAE):</li> <li>\\(MAE = \\frac{1}{n}\\sum|y_i - \\hat{y}_i|\\)</li> <li>Pros: Easy to interpret, robust to outliers</li> <li> <p>Cons: Not differentiable at zero</p> </li> <li> <p>Mean Squared Error (MSE):</p> </li> <li>\\(MSE = \\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2\\)</li> <li>Pros: Differentiable, penalizes large errors more</li> <li> <p>Cons: Sensitive to outliers, units are squared</p> </li> <li> <p>Root Mean Squared Error (RMSE):</p> </li> <li>\\(RMSE = \\sqrt{MSE}\\)</li> <li>Pros: Same units as target, interpretable</li> <li> <p>Cons: Still sensitive to outliers</p> </li> <li> <p>R\u00b2 Score:</p> </li> <li>\\(R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\\)</li> <li>Pros: Scale-independent, easy to interpret (% variance explained)</li> <li>Cons: Can be misleading with non-linear relationships</li> </ol> <p>Choose based on: - Business context: What type of errors are most costly? - Outliers: Use MAE if outliers present, RMSE if not - Interpretability: R\u00b2 for general performance, RMSE for same-unit comparison</p>"},{"location":"Machine-Learning/Linear%20Regression/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/Linear%20Regression/#real-world-example-sales-prediction","title":"Real-world Example: Sales Prediction","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport seaborn as sns\n\n# Create synthetic sales data\nnp.random.seed(42)\nn_samples = 500\n\n# Generate features\nadvertising_spend = np.random.normal(50, 20, n_samples)  # in thousands\ntemperature = np.random.normal(70, 15, n_samples)  # Fahrenheit\nis_weekend = np.random.binomial(1, 0.3, n_samples)  # 30% weekends\nseason = np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], n_samples)\ncompetitor_price = np.random.normal(25, 5, n_samples)\n\n# Create realistic sales relationship\nbase_sales = 100\nsales = (base_sales + \n         2.5 * advertising_spend +  # Strong positive effect\n         0.5 * temperature +        # Weather effect\n         15 * is_weekend +          # Weekend boost\n         -1.2 * competitor_price +  # Competition effect\n         np.random.normal(0, 10, n_samples))  # Random noise\n\n# Add seasonal effects\nseason_effects = {'Spring': 10, 'Summer': 20, 'Fall': 5, 'Winter': -15}\nsales += np.array([season_effects[s] for s in season])\n\n# Create DataFrame\nsales_data = pd.DataFrame({\n    'advertising_spend': advertising_spend,\n    'temperature': temperature,\n    'is_weekend': is_weekend,\n    'season': season,\n    'competitor_price': competitor_price,\n    'sales': sales\n})\n\nprint(\"Sales Dataset:\")\nprint(sales_data.head())\nprint(f\"\\nDataset shape: {sales_data.shape}\")\nprint(\"\\nBasic statistics:\")\nprint(sales_data.describe())\n\n# One-hot encode categorical variables\nsales_encoded = pd.get_dummies(sales_data, columns=['season'], prefix='season')\n\n# Prepare features and target\nX = sales_encoded.drop('sales', axis=1)\ny = sales_encoded['sales']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred = lr_model.predict(X_test)\n\n# Calculate metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nModel Performance:\")\nprint(f\"RMSE: ${rmse:.2f}\")\nprint(f\"R\u00b2 Score: {r2:.3f}\")\n\n# Analyze coefficients\ncoefficients = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': lr_model.coef_,\n    'Abs_Coefficient': np.abs(lr_model.coef_)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(f\"\\nFeature Importance (Coefficients):\")\nprint(coefficients)\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Actual vs Predicted\naxes[0,0].scatter(y_test, y_pred, alpha=0.6)\naxes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\naxes[0,0].set_xlabel('Actual Sales')\naxes[0,0].set_ylabel('Predicted Sales')\naxes[0,0].set_title(f'Predictions vs Actual (R\u00b2 = {r2:.3f})')\n\n# 2. Residual plot\nresiduals = y_test - y_pred\naxes[0,1].scatter(y_pred, residuals, alpha=0.6)\naxes[0,1].axhline(y=0, color='r', linestyle='--')\naxes[0,1].set_xlabel('Predicted Sales')\naxes[0,1].set_ylabel('Residuals')\naxes[0,1].set_title('Residual Plot')\n\n# 3. Feature importance\ntop_features = coefficients.head(8)\naxes[1,0].barh(range(len(top_features)), top_features['Coefficient'])\naxes[1,0].set_yticks(range(len(top_features)))\naxes[1,0].set_yticklabels(top_features['Feature'])\naxes[1,0].set_xlabel('Coefficient Value')\naxes[1,0].set_title('Feature Coefficients')\naxes[1,0].axvline(x=0, color='k', linestyle='--', alpha=0.5)\n\n# 4. Sales vs Advertising relationship\naxes[1,1].scatter(sales_data['advertising_spend'], sales_data['sales'], alpha=0.6)\naxes[1,1].set_xlabel('Advertising Spend ($000)')\naxes[1,1].set_ylabel('Sales')\naxes[1,1].set_title('Sales vs Advertising Spend')\n\n# Add trend line\nz = np.polyfit(sales_data['advertising_spend'], sales_data['sales'], 1)\np = np.poly1d(z)\naxes[1,1].plot(sales_data['advertising_spend'], p(sales_data['advertising_spend']), \"r--\", alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\n# Business insights\nprint(f\"\\n=== Business Insights ===\")\nprint(f\"1. Every $1K in advertising spend increases sales by ${lr_model.coef_[0]:.2f}\")\nprint(f\"2. Weekend sales are ${lr_model.coef_[2]:.2f} higher than weekdays\")\nprint(f\"3. Each degree temperature increase adds ${coefficients[coefficients['Feature']=='temperature']['Coefficient'].iloc[0]:.2f} to sales\")\n\n# Prediction example\nprint(f\"\\n=== Sales Prediction Example ===\")\nexample_data = np.array([[60, 75, 1, 20, 0, 0, 1, 0]])  # Summer weekend with high advertising\nexample_pred = lr_model.predict(example_data)[0]\nprint(f\"Predicted sales for summer weekend with $60K advertising: ${example_pred:.2f}\")\n\n# Feature correlation analysis\ncorrelation_matrix = X.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('Feature Correlation Matrix')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Linear%20Regression/#example-medical-diagnosis-drug-dosage-prediction","title":"Example: Medical Diagnosis - Drug Dosage Prediction","text":"<pre><code># Generate synthetic medical data for drug dosage prediction\nnp.random.seed(123)\nn_patients = 300\n\n# Patient characteristics\nage = np.random.normal(55, 15, n_patients)\nweight = np.random.normal(70, 12, n_patients)  # kg\nheight = np.random.normal(170, 10, n_patients)  # cm\ngender = np.random.binomial(1, 0.5, n_patients)  # 0=Female, 1=Male\nkidney_function = np.random.normal(90, 20, n_patients)  # GFR\nliver_function = np.random.normal(80, 15, n_patients)  # ALT levels\n\n# Calculate BMI\nbmi = weight / ((height/100)**2)\n\n# Generate optimal dosage based on medical relationships\noptimal_dosage = (\n    5 +                           # Base dosage\n    0.1 * age +                   # Age factor\n    0.3 * weight +                # Weight-based dosing\n    2 * gender +                  # Gender difference\n    0.05 * kidney_function +      # Kidney clearance\n    -0.02 * liver_function +      # Liver metabolism\n    0.2 * bmi +                   # Body mass effect\n    np.random.normal(0, 2, n_patients)  # Individual variation\n)\n\n# Ensure dosage is positive and reasonable\noptimal_dosage = np.clip(optimal_dosage, 1, 50)\n\n# Create medical dataset\nmedical_data = pd.DataFrame({\n    'age': age,\n    'weight': weight,\n    'height': height,\n    'gender': gender,\n    'kidney_function': kidney_function,\n    'liver_function': liver_function,\n    'bmi': bmi,\n    'optimal_dosage': optimal_dosage\n})\n\nprint(\"Medical Dataset for Drug Dosage Prediction:\")\nprint(medical_data.head())\nprint(f\"\\nDataset shape: {medical_data.shape}\")\n\n# Prepare data\nX_med = medical_data.drop('optimal_dosage', axis=1)\ny_med = medical_data['optimal_dosage']\n\n# Split data\nX_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n    X_med, y_med, test_size=0.2, random_state=42\n)\n\n# Scale features for better interpretation\nscaler_med = StandardScaler()\nX_train_scaled_med = scaler_med.fit_transform(X_train_med)\nX_test_scaled_med = scaler_med.transform(X_test_med)\n\n# Train model\nlr_med = LinearRegression()\nlr_med.fit(X_train_scaled_med, y_train_med)\n\n# Predictions\ny_pred_med = lr_med.predict(X_test_scaled_med)\n\n# Metrics\nr2_med = r2_score(y_test_med, y_pred_med)\nrmse_med = np.sqrt(mean_squared_error(y_test_med, y_pred_med))\n\nprint(f\"\\nMedical Model Performance:\")\nprint(f\"R\u00b2 Score: {r2_med:.3f}\")\nprint(f\"RMSE: {rmse_med:.2f} mg\")\n\n# Feature importance analysis\ncoef_med = pd.DataFrame({\n    'Feature': X_med.columns,\n    'Coefficient': lr_med.coef_,\n    'Abs_Coefficient': np.abs(lr_med.coef_)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(f\"\\nFeature Importance in Dosage Prediction:\")\nprint(coef_med)\n\n# Safety analysis - prediction intervals\nfrom scipy import stats\nresiduals_med = y_test_med - y_pred_med\nresidual_std = np.std(residuals_med)\n\n# 95% prediction intervals\nprediction_interval = 1.96 * residual_std\nprint(f\"\\n95% Prediction Interval: \u00b1{prediction_interval:.2f} mg\")\n\n# Clinical interpretation\nprint(f\"\\n=== Clinical Insights ===\")\nprint(f\"1. Model explains {r2_med*100:.1f}% of dosage variation\")\nprint(f\"2. Average prediction error: {rmse_med:.2f} mg\")\nprint(f\"3. Most important factors: {', '.join(coef_med.head(3)['Feature'].tolist())}\")\n\n# Visualize medical relationships\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Age vs Dosage\naxes[0,0].scatter(medical_data['age'], medical_data['optimal_dosage'], alpha=0.6)\naxes[0,0].set_xlabel('Age (years)')\naxes[0,0].set_ylabel('Optimal Dosage (mg)')\naxes[0,0].set_title('Dosage vs Age')\n\n# Weight vs Dosage\naxes[0,1].scatter(medical_data['weight'], medical_data['optimal_dosage'], alpha=0.6)\naxes[0,1].set_xlabel('Weight (kg)')\naxes[0,1].set_ylabel('Optimal Dosage (mg)')\naxes[0,1].set_title('Dosage vs Weight')\n\n# Gender differences\ngender_data = medical_data.groupby('gender')['optimal_dosage'].agg(['mean', 'std'])\naxes[0,2].bar(['Female', 'Male'], gender_data['mean'], \n              yerr=gender_data['std'], alpha=0.7, capsize=5)\naxes[0,2].set_ylabel('Average Dosage (mg)')\naxes[0,2].set_title('Dosage by Gender')\n\n# Kidney function vs Dosage\naxes[1,0].scatter(medical_data['kidney_function'], medical_data['optimal_dosage'], alpha=0.6)\naxes[1,0].set_xlabel('Kidney Function (GFR)')\naxes[1,0].set_ylabel('Optimal Dosage (mg)')\naxes[1,0].set_title('Dosage vs Kidney Function')\n\n# Predictions vs Actual\naxes[1,1].scatter(y_test_med, y_pred_med, alpha=0.6)\naxes[1,1].plot([y_test_med.min(), y_test_med.max()], \n               [y_test_med.min(), y_test_med.max()], 'r--', lw=2)\naxes[1,1].set_xlabel('Actual Dosage (mg)')\naxes[1,1].set_ylabel('Predicted Dosage (mg)')\naxes[1,1].set_title(f'Medical Model Predictions (R\u00b2 = {r2_med:.3f})')\n\n# Feature importance\naxes[1,2].barh(range(len(coef_med)), coef_med['Coefficient'])\naxes[1,2].set_yticks(range(len(coef_med)))\naxes[1,2].set_yticklabels(coef_med['Feature'])\naxes[1,2].set_xlabel('Coefficient (Standardized)')\naxes[1,2].set_title('Feature Importance')\naxes[1,2].axvline(x=0, color='k', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# Dosage recommendation system\ndef predict_dosage(age, weight, height, gender, kidney_func, liver_func):\n    \"\"\"Predict optimal drug dosage for a patient.\"\"\"\n    bmi = weight / ((height/100)**2)\n\n    patient_data = np.array([[age, weight, height, gender, kidney_func, liver_func, bmi]])\n    patient_scaled = scaler_med.transform(patient_data)\n    predicted_dosage = lr_med.predict(patient_scaled)[0]\n\n    # Add safety bounds\n    predicted_dosage = np.clip(predicted_dosage, 1, 50)\n\n    return predicted_dosage, prediction_interval\n\n# Example patient\nexample_age, example_weight, example_height = 65, 75, 175\nexample_gender, example_kidney, example_liver = 1, 85, 75\n\npredicted_dose, interval = predict_dosage(\n    example_age, example_weight, example_height,\n    example_gender, example_kidney, example_liver\n)\n\nprint(f\"\\n=== Dosage Recommendation ===\")\nprint(f\"Patient: {example_age}yr old, {example_weight}kg, {'Male' if example_gender else 'Female'}\")\nprint(f\"Recommended dosage: {predicted_dose:.1f} mg\")\nprint(f\"95% confidence interval: {predicted_dose-interval:.1f} - {predicted_dose+interval:.1f} mg\")\n</code></pre>"},{"location":"Machine-Learning/Linear%20Regression/#references","title":"\ud83d\udcda References","text":""},{"location":"Machine-Learning/Linear%20Regression/#books","title":"Books","text":"<ol> <li>\"An Introduction to Statistical Learning\" by James, Witten, Hastie, and Tibshirani - Chapter 3</li> <li>\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman - Chapter 3</li> <li>\"Hands-On Machine Learning\" by Aur\u00e9lien G\u00e9ron - Chapter 4</li> <li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop - Chapter 3</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#papers-and-articles","title":"Papers and Articles","text":"<ol> <li>Linear Regression (Wikipedia) - Comprehensive overview</li> <li>Ordinary Least Squares - Mathematical foundation</li> <li>The Gauss-Markov Theorem - Theoretical properties</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#online-resources","title":"Online Resources","text":"<ol> <li>Scikit-learn Linear Regression Documentation</li> <li>StatsModels OLS Documentation</li> <li>Khan Academy: Linear Regression</li> <li>Coursera Machine Learning Course by Andrew Ng</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#interactive-tutorials","title":"Interactive Tutorials","text":"<ol> <li>Linear Regression Interactive Visualization</li> <li>Regression Analysis Explained Visually</li> <li>Kaggle Learn: Introduction to Machine Learning</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#video-resources","title":"Video Resources","text":"<ol> <li>StatQuest: Linear Regression</li> <li>3Blue1Brown: Linear Algebra Essence</li> <li>MIT OpenCourseWare: Statistics</li> </ol>"},{"location":"Machine-Learning/Linear%20Regression/#practical-applications","title":"Practical Applications","text":"<ol> <li>Real Estate Price Prediction</li> <li>Medical Research Applications</li> <li>Business Analytics Case Studies</li> </ol>"},{"location":"Machine-Learning/Logistic%20Regression/","title":"\ud83d\udd25 Logistic Regression","text":"<p>Logistic Regression is a statistical method used for binary and multiclass classification problems that models the probability of class membership using the logistic function.</p> <p>Resources: Scikit-learn Logistic Regression | Stanford CS229 Notes</p>"},{"location":"Machine-Learning/Logistic%20Regression/#_1","title":"Logistic Regression","text":"<p>\u000f Summary</p> <p>Logistic Regression is a linear classifier that uses the logistic function (sigmoid) to map any real-valued number into a value between 0 and 1, making it suitable for probability estimation and classification tasks.</p> <p>Key characteristics: - Probabilistic: Outputs probabilities rather than direct classifications - Linear decision boundary: Creates linear decision boundaries in feature space - No distributional assumptions: Unlike linear regression, doesn't assume normal distribution of errors - Robust to outliers: Less sensitive to outliers compared to linear regression - Interpretable: Coefficients have direct interpretation as log-odds ratios</p> <p>Applications: - Medical diagnosis (disease/no disease) - Marketing (click/no click, buy/don't buy) - Finance (default/no default) - Email classification (spam/ham) - Customer churn prediction - A/B test analysis</p> <p>Types: - Binary Logistic Regression: Two classes (0 or 1) - Multinomial Logistic Regression: Multiple classes (&gt;2) - Ordinal Logistic Regression: Ordered categories</p>"},{"location":"Machine-Learning/Logistic%20Regression/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Logistic%20Regression/#how-logistic-regression-works","title":"How Logistic Regression Works","text":"<p>While linear regression predicts continuous values, logistic regression predicts the probability that an instance belongs to a particular category. It uses the logistic (sigmoid) function to constrain outputs between 0 and 1.</p>"},{"location":"Machine-Learning/Logistic%20Regression/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Logistic%20Regression/#1-the-logistic-function-sigmoid","title":"1. The Logistic Function (Sigmoid)","text":"<p>The sigmoid function maps any real number to a value between 0 and 1:</p> \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] <p>Where \\(z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p\\)</p>"},{"location":"Machine-Learning/Logistic%20Regression/#2-odds-and-log-odds","title":"2. Odds and Log-Odds","text":"<p>Odds represent the ratio of probability of success to probability of failure: \\(\\(\\text{Odds} = \\frac{p}{1-p}\\)\\)</p> <p>Log-odds (logit) is the natural logarithm of odds: \\(\\(\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = z\\)\\)</p>"},{"location":"Machine-Learning/Logistic%20Regression/#3-the-logistic-regression-model","title":"3. The Logistic Regression Model","text":"<p>For binary classification: \\(\\(P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_px_p)}}\\)\\)</p> <p>Key insight: Linear combination of features determines the log-odds, while the sigmoid function converts it to probability.</p>"},{"location":"Machine-Learning/Logistic%20Regression/#4-maximum-likelihood-estimation","title":"4. Maximum Likelihood Estimation","text":"<p>Logistic regression uses maximum likelihood estimation (MLE) to find optimal parameters. The likelihood function for \\(n\\) observations is:</p> \\[L(\\beta) = \\prod_{i=1}^{n} P(y_i|x_i)^{y_i} \\cdot (1-P(y_i|x_i))^{1-y_i}\\] <p>Log-likelihood (easier to optimize): \\(\\(\\ell(\\beta) = \\sum_{i=1}^{n} [y_i \\log(P(y_i|x_i)) + (1-y_i) \\log(1-P(y_i|x_i))]\\)\\)</p>"},{"location":"Machine-Learning/Logistic%20Regression/#5-cost-function","title":"5. Cost Function","text":"<p>The cost function (negative log-likelihood) for logistic regression is: \\(\\(J(\\beta) = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(h_\\beta(x_i)) + (1-y_i) \\log(1-h_\\beta(x_i))]\\)\\)</p> <p>Where \\(h_\\beta(x_i) = \\sigma(\\beta^T x_i)\\) is the hypothesis function.</p>"},{"location":"Machine-Learning/Logistic%20Regression/#6-gradient-descent","title":"6. Gradient Descent","text":"<p>The gradient of the cost function with respect to parameters: \\(\\(\\frac{\\partial J(\\beta)}{\\partial \\beta_j} = \\frac{1}{n} \\sum_{i=1}^{n} (h_\\beta(x_i) - y_i) x_{ij}\\)\\)</p> <p>Update rule: \\(\\(\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}\\)\\)</p>"},{"location":"Machine-Learning/Logistic%20Regression/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Initialize parameters \\(\\beta\\) randomly or to zero</li> <li>Forward propagation: Calculate predictions using sigmoid function</li> <li>Calculate cost using log-likelihood</li> <li>Backward propagation: Calculate gradients</li> <li>Update parameters using gradient descent</li> <li>Repeat until convergence</li> </ol>"},{"location":"Machine-Learning/Logistic%20Regression/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/Logistic%20Regression/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, classification_report, \n                           confusion_matrix, roc_curve, auc, \n                           precision_recall_curve)\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Binary Classification Example\nprint(\"=\" * 50)\nprint(\"BINARY LOGISTIC REGRESSION\")\nprint(\"=\" * 50)\n\n# Generate sample data\nX, y = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Scale features (important for logistic regression)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train model\nlog_reg = LogisticRegression(\n    random_state=42,\n    max_iter=1000,\n    solver='lbfgs'  # Good for small datasets\n)\n\nlog_reg.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = log_reg.predict(X_test_scaled)\ny_pred_proba = log_reg.predict_proba(X_test_scaled)\n\n# Evaluate model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"\\nCoefficients: {log_reg.coef_[0]}\")\nprint(f\"Intercept: {log_reg.intercept_[0]:.3f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.subplot(1, 3, 2)\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\n\n# Decision boundary visualization\nplt.subplot(1, 3, 3)\nh = 0.02\nx_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\ny_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = log_reg.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\nplt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], \n           c=y_train, cmap='RdYlBu', edgecolors='black')\nplt.title('Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n# Real-world example: Breast Cancer Dataset\nprint(\"\\n\" + \"=\" * 50)\nprint(\"REAL-WORLD EXAMPLE: BREAST CANCER CLASSIFICATION\")\nprint(\"=\" * 50)\n\n# Load dataset\ncancer = load_breast_cancer()\nX_cancer = cancer.data\ny_cancer = cancer.target\n\n# Use subset of features for interpretability\nfeature_names = cancer.feature_names[:10]  # First 10 features\nX_cancer_subset = X_cancer[:, :10]\n\nprint(f\"Dataset shape: {X_cancer_subset.shape}\")\nprint(f\"Features: {list(feature_names)}\")\nprint(f\"Classes: {cancer.target_names}\")\n\n# Split and scale\nX_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n    X_cancer_subset, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n)\n\nscaler_cancer = StandardScaler()\nX_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\nX_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n\n# Train model\ncancer_model = LogisticRegression(random_state=42, max_iter=1000)\ncancer_model.fit(X_train_cancer_scaled, y_train_cancer)\n\n# Predictions\ny_pred_cancer = cancer_model.predict(X_test_cancer_scaled)\ny_pred_proba_cancer = cancer_model.predict_proba(X_test_cancer_scaled)\n\nprint(f\"Cancer Classification Accuracy: {accuracy_score(y_test_cancer, y_pred_cancer):.3f}\")\n\n# Feature importance (coefficients)\nfeature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': cancer_model.coef_[0],\n    'Abs_Coefficient': np.abs(cancer_model.coef_[0])\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(\"\\nFeature Importance (by coefficient magnitude):\")\nprint(feature_importance)\n\n# Multiclass Classification Example\nprint(\"\\n\" + \"=\" * 50)\nprint(\"MULTICLASS LOGISTIC REGRESSION\")\nprint(\"=\" * 50)\n\nfrom sklearn.datasets import make_classification\n\n# Generate multiclass data\nX_multi, y_multi = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_informative=3,\n    n_redundant=1,\n    n_classes=3,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n)\n\nscaler_multi = StandardScaler()\nX_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)\nX_test_multi_scaled = scaler_multi.transform(X_test_multi)\n\n# Train multiclass model\nmulti_model = LogisticRegression(\n    multi_class='ovr',  # One-vs-Rest\n    random_state=42,\n    max_iter=1000\n)\n\nmulti_model.fit(X_train_multi_scaled, y_train_multi)\n\n# Evaluate\ny_pred_multi = multi_model.predict(X_test_multi_scaled)\nprint(f\"Multiclass Accuracy: {accuracy_score(y_test_multi, y_pred_multi):.3f}\")\nprint(\"\\nMulticlass Classification Report:\")\nprint(classification_report(y_test_multi, y_pred_multi))\n\n# Cross-validation\ncv_scores = cross_val_score(multi_model, X_train_multi_scaled, y_train_multi, cv=5)\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n</code></pre>"},{"location":"Machine-Learning/Logistic%20Regression/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n    'penalty': ['l1', 'l2'],        # Regularization type\n    'solver': ['liblinear', 'saga'] # Solvers that support both L1 and L2\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    LogisticRegression(random_state=42, max_iter=1000),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best cross-validation score:\", grid_search.best_score_)\n\n# Use best model\nbest_model = grid_search.best_estimator_\nbest_pred = best_model.predict(X_test_scaled)\nprint(\"Best model test accuracy:\", accuracy_score(y_test, best_pred))\n</code></pre>"},{"location":"Machine-Learning/Logistic%20Regression/#from-scratch-implementation","title":"\ud83d\udd27 From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nclass LogisticRegressionFromScratch:\n    \"\"\"Logistic Regression implementation from scratch\"\"\"\n\n    def __init__(self, learning_rate=0.01, max_iterations=1000, fit_intercept=True, verbose=False):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.fit_intercept = fit_intercept\n        self.verbose = verbose\n\n    def _add_intercept(self, X):\n        \"\"\"Add bias column to the feature matrix\"\"\"\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n\n    def _sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        # Clip z to prevent overflow\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def _cost_function(self, h, y):\n        \"\"\"Calculate the logistic regression cost function\"\"\"\n        # Avoid log(0) by adding small epsilon\n        epsilon = 1e-15\n        h = np.clip(h, epsilon, 1 - epsilon)\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n\n    def fit(self, X, y):\n        \"\"\"Train the logistic regression model\"\"\"\n        # Add intercept term if needed\n        if self.fit_intercept:\n            X = self._add_intercept(X)\n\n        # Initialize weights\n        self.weights = np.zeros(X.shape[1])\n\n        # Store cost history\n        self.cost_history = []\n\n        # Gradient descent\n        for i in range(self.max_iterations):\n            # Forward propagation\n            z = np.dot(X, self.weights)\n            h = self._sigmoid(z)\n\n            # Calculate cost\n            cost = self._cost_function(h, y)\n            self.cost_history.append(cost)\n\n            # Calculate gradient\n            gradient = np.dot(X.T, (h - y)) / y.size\n\n            # Update weights\n            self.weights -= self.learning_rate * gradient\n\n            # Print progress\n            if self.verbose and i % 100 == 0:\n                print(f\"Iteration {i}: Cost = {cost:.6f}\")\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities\"\"\"\n        if self.fit_intercept:\n            X = self._add_intercept(X)\n\n        probabilities = self._sigmoid(np.dot(X, self.weights))\n        return np.vstack([1 - probabilities, probabilities]).T\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        return (self.predict_proba(X)[:, 1] &gt;= 0.5).astype(int)\n\n    def score(self, X, y):\n        \"\"\"Calculate accuracy\"\"\"\n        return (self.predict(X) == y).mean()\n\n# Example usage of from-scratch implementation\nprint(\"=\" * 60)\nprint(\"FROM SCRATCH IMPLEMENTATION\")\nprint(\"=\" * 60)\n\n# Generate sample data\nnp.random.seed(42)\nX_scratch, y_scratch = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\n# Split and scale data\nX_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = train_test_split(\n    X_scratch, y_scratch, test_size=0.2, random_state=42\n)\n\nscaler_scratch = StandardScaler()\nX_train_scratch_scaled = scaler_scratch.fit_transform(X_train_scratch)\nX_test_scratch_scaled = scaler_scratch.transform(X_test_scratch)\n\n# Train custom logistic regression\ncustom_lr = LogisticRegressionFromScratch(\n    learning_rate=0.01,\n    max_iterations=1000,\n    verbose=True\n)\n\ncustom_lr.fit(X_train_scratch_scaled, y_train_scratch)\n\n# Make predictions\ny_pred_scratch = custom_lr.predict(X_test_scratch_scaled)\ny_pred_proba_scratch = custom_lr.predict_proba(X_test_scratch_scaled)\n\ncustom_accuracy = custom_lr.score(X_test_scratch_scaled, y_test_scratch)\nprint(f\"\\nCustom Logistic Regression Accuracy: {custom_accuracy:.3f}\")\nprint(f\"Final weights: {custom_lr.weights}\")\n\n# Compare with sklearn\nsklearn_lr = LogisticRegression(random_state=42, max_iter=1000)\nsklearn_lr.fit(X_train_scratch_scaled, y_train_scratch)\nsklearn_pred = sklearn_lr.predict(X_test_scratch_scaled)\nsklearn_accuracy = accuracy_score(y_test_scratch, sklearn_pred)\n\nprint(f\"Scikit-learn Accuracy: {sklearn_accuracy:.3f}\")\nprint(f\"Accuracy difference: {abs(custom_accuracy - sklearn_accuracy):.4f}\")\n\n# Plot cost function\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(custom_lr.cost_history)\nplt.title('Cost Function During Training')\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\nplt.grid(True)\n\n# Visualize decision boundary\nplt.subplot(1, 2, 2)\nh = 0.02\nx_min, x_max = X_train_scratch_scaled[:, 0].min() - 1, X_train_scratch_scaled[:, 0].max() + 1\ny_min, y_max = X_train_scratch_scaled[:, 1].min() - 1, X_train_scratch_scaled[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = custom_lr.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\nplt.scatter(X_train_scratch_scaled[:, 0], X_train_scratch_scaled[:, 1], \n           c=y_train_scratch, cmap='RdYlBu', edgecolors='black')\nplt.title('Custom Implementation Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n# Advanced from-scratch implementation with regularization\nclass RegularizedLogisticRegression:\n    \"\"\"Logistic Regression with L1 and L2 regularization\"\"\"\n\n    def __init__(self, learning_rate=0.01, max_iterations=1000, \n                 regularization=None, lambda_reg=0.01, fit_intercept=True):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.regularization = regularization  # 'l1', 'l2', or None\n        self.lambda_reg = lambda_reg\n        self.fit_intercept = fit_intercept\n\n    def _add_intercept(self, X):\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n\n    def _sigmoid(self, z):\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def _cost_function(self, h, y):\n        epsilon = 1e-15\n        h = np.clip(h, epsilon, 1 - epsilon)\n        cost = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n\n        # Add regularization term\n        if self.regularization == 'l1':\n            # Don't regularize intercept term\n            reg_term = self.lambda_reg * np.sum(np.abs(self.weights[1:]))\n        elif self.regularization == 'l2':\n            reg_term = self.lambda_reg * np.sum(self.weights[1:] ** 2)\n        else:\n            reg_term = 0\n\n        return cost + reg_term\n\n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = self._add_intercept(X)\n\n        self.weights = np.zeros(X.shape[1])\n        self.cost_history = []\n\n        for i in range(self.max_iterations):\n            # Forward propagation\n            z = np.dot(X, self.weights)\n            h = self._sigmoid(z)\n\n            # Calculate cost\n            cost = self._cost_function(h, y)\n            self.cost_history.append(cost)\n\n            # Calculate gradient\n            gradient = np.dot(X.T, (h - y)) / y.size\n\n            # Add regularization to gradient\n            if self.regularization == 'l1':\n                gradient[1:] += self.lambda_reg * np.sign(self.weights[1:])\n            elif self.regularization == 'l2':\n                gradient[1:] += 2 * self.lambda_reg * self.weights[1:]\n\n            # Update weights\n            self.weights -= self.learning_rate * gradient\n\n    def predict_proba(self, X):\n        if self.fit_intercept:\n            X = self._add_intercept(X)\n        probabilities = self._sigmoid(np.dot(X, self.weights))\n        return np.vstack([1 - probabilities, probabilities]).T\n\n    def predict(self, X):\n        return (self.predict_proba(X)[:, 1] &gt;= 0.5).astype(int)\n\n# Test regularized implementation\nprint(\"\\n\" + \"=\" * 60)\nprint(\"REGULARIZED IMPLEMENTATION\")\nprint(\"=\" * 60)\n\n# Test L1 regularization\nl1_model = RegularizedLogisticRegression(\n    learning_rate=0.01,\n    max_iterations=1000,\n    regularization='l1',\n    lambda_reg=0.01\n)\n\nl1_model.fit(X_train_scratch_scaled, y_train_scratch)\nl1_accuracy = l1_model.score(X_test_scratch_scaled, y_test_scratch)\n\n# Test L2 regularization\nl2_model = RegularizedLogisticRegression(\n    learning_rate=0.01,\n    max_iterations=1000,\n    regularization='l2',\n    lambda_reg=0.01\n)\n\nl2_model.fit(X_train_scratch_scaled, y_train_scratch)\nl2_accuracy = l2_model.score(X_test_scratch_scaled, y_test_scratch)\n\nprint(f\"L1 Regularized Accuracy: {l1_accuracy:.3f}\")\nprint(f\"L2 Regularized Accuracy: {l2_accuracy:.3f}\")\nprint(f\"No Regularization Accuracy: {custom_accuracy:.3f}\")\n\nprint(f\"\\nL1 weights: {l1_model.weights}\")\nprint(f\"L2 weights: {l2_model.weights}\")\nprint(f\"No reg weights: {custom_lr.weights}\")\n</code></pre>"},{"location":"Machine-Learning/Logistic%20Regression/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Logistic%20Regression/#assumptions","title":"Assumptions","text":"<ol> <li>Linear relationship between logit and features: The log-odds should be a linear combination of features</li> <li>Independence of observations: Each observation should be independent</li> <li>No multicollinearity: Features should not be highly correlated</li> <li>Large sample size: Generally needs larger sample sizes than linear regression</li> <li>Binary or ordinal outcome: Dependent variable should be categorical</li> </ol>"},{"location":"Machine-Learning/Logistic%20Regression/#limitations","title":"Limitations","text":"<ol> <li>Linear decision boundary:</li> <li>Can only create linear decision boundaries</li> <li> <p>Solution: Feature engineering, polynomial features, or non-linear algorithms</p> </li> <li> <p>Sensitive to outliers:</p> </li> <li>Extreme values can influence the model significantly</li> <li> <p>Solution: Robust scaling, outlier detection and removal</p> </li> <li> <p>Assumes no missing values:</p> </li> <li>Cannot handle missing data directly</li> <li> <p>Solution: Imputation or algorithms that handle missing values</p> </li> <li> <p>Requires feature scaling:</p> </li> <li>Features on different scales can bias the model</li> <li> <p>Solution: Standardization or normalization</p> </li> <li> <p>Perfect separation problems:</p> </li> <li>When classes are perfectly separable, coefficients can become infinite</li> <li>Solution: Regularization (L1/L2)</li> </ol>"},{"location":"Machine-Learning/Logistic%20Regression/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Algorithm Interpretability Speed Non-linear Probability Output Overfitting Risk Logistic Regression PPPPP PPPPP L \u0005 PP Decision Trees PPPP PPPP \u0005 \u0005 PPPP Random Forest PP PPP \u0005 \u0005 PP SVM PP PP \u0005 L PPP Neural Networks P PP \u0005 \u0005 PPPPP <p>When to use Logistic Regression: - \u0005 When you need interpretable results - \u0005 For baseline models - \u0005 When you have linear relationships - \u0005 When you need probability estimates - \u0005 With limited training data</p> <p>When to avoid: - L When relationships are highly non-linear - L When you have very high-dimensional data - L When interpretability is not important and accuracy is paramount</p>"},{"location":"Machine-Learning/Logistic%20Regression/#interview-questions","title":"\u2753 Interview Questions","text":"1. Explain the difference between Linear Regression and Logistic Regression. <p>Key Differences:</p> Aspect Linear Regression Logistic Regression Purpose Predicts continuous values Predicts probabilities/classes Output range (-\u001e, +\u001e) [0, 1] Function Linear: y = \u00c2\u00b2X + \u00c2\u00b5 Logistic: p = 1/(1 + e^(-\u00c2\u00b2X)) Error distribution Normal Binomial Cost function Mean Squared Error Log-likelihood Parameters estimation Least squares Maximum likelihood Decision boundary Not applicable Linear <p>Mathematical relationship: <pre><code>Linear Regression: y = \u00c2\u00b2\u00c2\u0080 + \u00c2\u00b2\u00c2\u0081x\u00c2\u0081 + \u00c2\u00b2\u00c2\u0082x\u00c2\u0082 + ... + \u00c2\u00b2\u00c2\u009ax\u00c2\u009a + \u00c2\u00b5\n\nLogistic Regression: log(p/(1-p)) = \u00c2\u00b2\u00c2\u0080 + \u00c2\u00b2\u00c2\u0081x\u00c2\u0081 + \u00c2\u00b2\u00c2\u0082x\u00c2\u0082 + ... + \u00c2\u00b2\u00c2\u009ax\u00c2\u009a\n</code></pre></p> <p>When to use each: - Linear Regression: Predicting house prices, temperatures, stock prices - Logistic Regression: Email spam detection, medical diagnosis, customer churn</p> 2. What is the sigmoid function and why is it used in logistic regression? <p>Sigmoid Function: \\(\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\\)</p> <p>Properties:</p> <ol> <li>Range [0,1]: Perfect for probability estimation</li> <li>S-shaped curve: Smooth transition between 0 and 1  </li> <li>Differentiable: Enables gradient descent optimization</li> <li>Asymptotic: Approaches 0 and 1 but never reaches them</li> </ol> <p>Why sigmoid is used:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-10, 10, 100)\ny = sigmoid(z)\n\nplt.figure(figsize=(8, 5))\nplt.plot(z, y, 'b-', linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision boundary')\nplt.xlabel('z (linear combination)')\nplt.ylabel('\u00c3\u0083(z) (probability)')\nplt.title('Sigmoid Function')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n</code></pre> <p>Mathematical advantages: - Maps any real number to (0,1) - Derivative: \u00c3\u0083'(z) = \u00c3\u0083(z)(1 - \u00c3\u0083(z)) - Smooth gradient for optimization - Interpretable as probability</p> 3. How do you interpret the coefficients in logistic regression? <p>Coefficient Interpretation:</p> <p>Raw Coefficients (\u00c2\u00b2): - Represent change in log-odds per unit change in feature - If \u00c2\u00b2\u00c2\u0081 = 0.5, then one unit increase in x\u00c2\u0081 increases log-odds by 0.5</p> <p>Odds Ratios (e^\u00c2\u00b2): - More interpretable than raw coefficients - If OR = e^\u00c2\u00b2 = 2, the odds double with one unit increase in feature</p> <p>Example interpretation: <pre><code># Example: Email spam classification\n# Features: [word_count, has_links, sender_reputation]\n# Coefficients: [0.1, 1.2, -0.8]\n\ncoefficients = [0.1, 1.2, -0.8]\nodds_ratios = np.exp(coefficients)\n\ninterpretations = [\n    f\"word_count: \u00c2\u00b2={coefficients[0]}, OR={odds_ratios[0]:.2f}\",\n    f\"has_links: \u00c2\u00b2={coefficients[1]}, OR={odds_ratios[1]:.2f}\", \n    f\"sender_reputation: \u00c2\u00b2={coefficients[2]}, OR={odds_ratios[2]:.2f}\"\n]\n\nfor interp in interpretations:\n    print(interp)\n</code></pre></p> <p>Interpretation: - word_count (\u00c2\u00b2=0.1): Each additional word increases spam odds by 10% - has_links (\u00c2\u00b2=1.2): Having links increases spam odds by 232% - sender_reputation (\u00c2\u00b2=-0.8): Better reputation decreases spam odds by 55%</p> <p>Key points: - Positive \u00c2\u00b2: Increases probability of positive class - Negative \u00c2\u00b2: Decreases probability of positive class - Magnitude indicates strength of effect - Sign indicates direction of effect</p> 4. What is the difference between odds and probability? <p>Definitions:</p> <p>Probability (p):  - Range: [0, 1] - P(event occurs) = number of favorable outcomes / total outcomes</p> <p>Odds: - Range: [0, \u001e] - Odds = P(event occurs) / P(event doesn't occur) = p / (1-p)</p> <p>Mathematical relationship: <pre><code>def prob_to_odds(p):\n    return p / (1 - p)\n\ndef odds_to_prob(odds):\n    return odds / (1 + odds)\n\n# Examples\nprobabilities = [0.1, 0.25, 0.5, 0.75, 0.9]\n\nprint(\"Probability \u00c2\u0092 Odds conversion:\")\nfor p in probabilities:\n    odds = prob_to_odds(p)\n    print(f\"P = {p:.2f} \u00c2\u0092 Odds = {odds:.2f}\")\n\n# Output:\n# P = 0.10 \u00c2\u0092 Odds = 0.11  (1:9 against)\n# P = 0.25 \u00c2\u0092 Odds = 0.33  (1:3 against) \n# P = 0.50 \u00c2\u0092 Odds = 1.00  (1:1 even)\n# P = 0.75 \u00c2\u0092 Odds = 3.00  (3:1 for)\n# P = 0.90 \u00c2\u0092 Odds = 9.00  (9:1 for)\n</code></pre></p> <p>Log-odds (logit): - Range: (-\u001e, +\u001e) - logit(p) = log(p/(1-p)) = log(odds) - This is what logistic regression actually models</p> <p>Why this matters: - Logistic regression predicts log-odds (linear combination) - Sigmoid converts log-odds back to probability - Coefficients represent changes in log-odds, not probability</p> 5. How does Maximum Likelihood Estimation work in logistic regression? <p>Maximum Likelihood Estimation (MLE):</p> <p>Concept: Find parameters that make the observed data most likely.</p> <p>Likelihood function: \\(\\(L(\\beta) = \\prod_{i=1}^{n} P(y_i|x_i)^{y_i} \\cdot (1-P(y_i|x_i))^{1-y_i}\\)\\)</p> <p>Log-likelihood (easier to optimize): \\(\\(\\ell(\\beta) = \\sum_{i=1}^{n} [y_i \\log(P(y_i|x_i)) + (1-y_i) \\log(1-P(y_i|x_i))]\\)\\)</p> <p>Step-by-step process:</p> <pre><code>def log_likelihood(y_true, y_pred):\n    # Avoid log(0) by clipping predictions\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n\n    ll = np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return ll\n\n# Example with simple data\ny_true = np.array([0, 0, 1, 1])\n\n# Poor predictions\ny_pred_bad = np.array([0.9, 0.8, 0.2, 0.1])\nll_bad = log_likelihood(y_true, y_pred_bad)\n\n# Good predictions  \ny_pred_good = np.array([0.1, 0.2, 0.8, 0.9])\nll_good = log_likelihood(y_true, y_pred_good)\n\nprint(f\"Bad predictions log-likelihood: {ll_bad:.3f}\")\nprint(f\"Good predictions log-likelihood: {ll_good:.3f}\")\nprint(f\"Good predictions have higher likelihood!\")\n</code></pre> <p>Why MLE over least squares: - Least squares assumes normal distribution of errors - MLE is appropriate for binary outcomes - Provides asymptotic properties (consistency, efficiency) - Naturally handles the [0,1] constraint of probabilities</p> <p>Optimization: - No closed-form solution (unlike linear regression) - Uses iterative methods: Newton-Raphson, gradient descent - Requires numerical optimization algorithms</p> 6. What is regularization in logistic regression and why is it needed? <p>Regularization: Technique to prevent overfitting by adding penalty term to cost function.</p> <p>Why regularization is needed:</p> <ol> <li>Perfect separation: When classes are linearly separable, coefficients \u00c2\u0092 \u001e</li> <li>Overfitting: High-dimensional data with few samples</li> <li>Multicollinearity: Correlated features cause unstable estimates</li> <li>Numerical stability: Prevents extreme coefficient values</li> </ol> <p>Types of regularization:</p> <p>L1 Regularization (Lasso): \\(\\(J(\\beta) = -\\ell(\\beta) + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\)\\)</p> <pre><code># L1 regularization promotes sparsity\nfrom sklearn.linear_model import LogisticRegression\n\n# Strong L1 regularization\nl1_model = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\nl1_model.fit(X_train, y_train)\n\nprint(\"L1 Coefficients:\", l1_model.coef_[0])\nprint(\"Number of zero coefficients:\", np.sum(l1_model.coef_[0] == 0))\n</code></pre> <p>L2 Regularization (Ridge): \\(\\(J(\\beta) = -\\ell(\\beta) + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\)\\)</p> <pre><code># L2 regularization shrinks coefficients  \nl2_model = LogisticRegression(penalty='l2', C=0.1)\nl2_model.fit(X_train, y_train)\n\nprint(\"L2 Coefficients:\", l2_model.coef_[0])\nprint(\"Coefficient magnitudes:\", np.abs(l2_model.coef_[0]))\n</code></pre> <p>Elastic Net (L1 + L2): \\(\\(J(\\beta) = -\\ell(\\beta) + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\\)\\)</p> <p>Key differences:</p> Regularization Effect Use Case Parameter in sklearn L1 Feature selection, sparse High-dim data, feature selection penalty='l1' L2 Shrinks coefficients Multicollinearity, general penalty='l2' Elastic Net Combines both Best of both worlds penalty='elasticnet' <p>C parameter: Inverse of regularization strength - Large C = Less regularization (more complex model) - Small C = More regularization (simpler model)</p> 7. How do you handle multiclass classification with logistic regression? <p>Strategies for multiclass classification:</p> <p>1. One-vs-Rest (OvR): - Train K binary classifiers (K = number of classes) - Each classifier: \"Class i vs all other classes\" - Prediction: Class with highest probability</p> <pre><code># One-vs-Rest implementation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX, y = iris.data, iris.target\n\n# OvR is default for multiclass\novr_model = LogisticRegression(multi_class='ovr')\novr_model.fit(X, y)\n\nprint(\"OvR model shape:\", ovr_model.coef_.shape)  # (3, 4) - 3 classes, 4 features\nprint(\"Classes:\", iris.target_names)\n</code></pre> <p>2. One-vs-One (OvO): - Train K(K-1)/2 binary classifiers - Each classifier: \"Class i vs Class j\" - Prediction: Majority voting</p> <p>3. Multinomial Logistic Regression: - Single model that directly handles multiple classes - Uses softmax function instead of sigmoid - More efficient than OvR/OvO</p> <pre><code># Multinomial approach\nmultinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nmultinomial_model.fit(X, y)\n\n# Softmax probabilities\nprobabilities = multinomial_model.predict_proba(X[:5])\nprint(\"Softmax probabilities:\")\nprint(probabilities)\nprint(\"Row sums (should be 1):\", probabilities.sum(axis=1))\n</code></pre> <p>Softmax function (for multinomial): \\(\\(P(y_i = k) = \\frac{e^{z_{ik}}}{\\sum_{j=1}^{K} e^{z_{ij}}}\\)\\)</p> <p>Comparison:</p> Method # Models Training Time Prediction Speed Memory OvR K Fast Fast Low OvO K(K-1)/2 Slow Medium High Multinomial 1 Medium Very Fast Very Low <p>When to use each: - OvR: Default choice, works well in practice - OvO: When individual binary problems are easier - Multinomial: When classes are mutually exclusive and exhaustive</p> 8. How do you evaluate a logistic regression model? <p>Evaluation metrics for logistic regression:</p> <p>1. Classification Accuracy: <pre><code>from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_true, y_pred)\n</code></pre></p> <p>2. Confusion Matrix: <pre><code>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_true, y_pred)\nConfusionMatrixDisplay(cm).plot()\n</code></pre></p> <p>3. Precision, Recall, F1-Score: <pre><code>from sklearn.metrics import classification_report, precision_recall_fscore_support\n\nprint(classification_report(y_true, y_pred))\nprecision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n</code></pre></p> <p>4. ROC Curve and AUC: <pre><code>from sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# For binary classification\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Direct calculation\nauc_score = roc_auc_score(y_true, y_pred_proba[:, 1])\n</code></pre></p> <p>5. Precision-Recall Curve: <pre><code>from sklearn.metrics import precision_recall_curve, average_precision_score\n\nprecision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])\navg_precision = average_precision_score(y_true, y_pred_proba[:, 1])\n</code></pre></p> <p>6. Log-Loss: <pre><code>from sklearn.metrics import log_loss\n\n# Measures quality of probability predictions\nlogloss = log_loss(y_true, y_pred_proba[:, 1])\n</code></pre></p> <p>7. Cross-Validation: <pre><code>from sklearn.model_selection import cross_val_score, StratifiedKFold\n\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n</code></pre></p> <p>When to use each metric:</p> Metric Use Case Important When Accuracy Balanced datasets Equal misclassification costs Precision False positives costly Spam detection, medical screening Recall False negatives costly Disease diagnosis, fraud detection F1-Score Imbalanced data Balance precision and recall AUC-ROC Ranking quality Overall discriminative ability PR-AUC Imbalanced data Focus on positive class Log-Loss Probability quality Calibrated probabilities needed 9. What are the assumptions of logistic regression and how do you check them? <p>Assumptions of Logistic Regression:</p> <p>1. Independence of observations: - Each observation should be independent</p> <p>Check:  - Review data collection process - Look for time series or clustered data - Use Durbin-Watson test for time series</p> <pre><code>from statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Check for autocorrelation in residuals\nresiduals = y_true - y_pred_proba[:, 1]\nljung_box = acorr_ljungbox(residuals, lags=10)\nprint(\"Ljung-Box test p-values:\", ljung_box['lb_pvalue'])\n</code></pre> <p>2. Linear relationship between logit and features: - Log-odds should be linear combination of features</p> <p>Check: Box-Tidwell test, visual inspection <pre><code># Visual check: logit vs continuous features\ndef logit(p):\n    return np.log(p / (1 - p))\n\n# Group data by feature quantiles and calculate logit\nfor feature in continuous_features:\n    plt.figure(figsize=(8, 5))\n\n    # Create quantile groups\n    quantiles = pd.qcut(X[feature], q=10, duplicates='drop')\n    grouped_mean = y.groupby(quantiles).mean()\n\n    # Calculate logit (avoid 0 and 1)\n    grouped_mean = np.clip(grouped_mean, 0.01, 0.99)\n    logit_values = logit(grouped_mean)\n\n    plt.scatter(grouped_mean.index, logit_values)\n    plt.xlabel(f'{feature} (quantiles)')\n    plt.ylabel('Logit')\n    plt.title(f'Linearity Check: {feature}')\n    plt.show()\n</code></pre></p> <p>3. No multicollinearity: - Features should not be highly correlated</p> <p>Check: VIF (Variance Inflation Factor) <pre><code>from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                   for i in range(X.shape[1])]\n\nprint(\"VIF values:\")\nprint(vif_data.sort_values('VIF', ascending=False))\nprint(\"\\nRule of thumb: VIF &gt; 10 indicates multicollinearity\")\n</code></pre></p> <p>4. Large sample size: - Need adequate samples per parameter</p> <p>Rule of thumb: At least 10-20 events per predictor variable</p> <pre><code># Check sample size adequacy\nn_samples, n_features = X.shape\nn_events = np.sum(y == 1)  # For binary classification\n\nratio = n_events / n_features\nprint(f\"Events per predictor: {ratio:.1f}\")\nprint(\"Adequate if &gt; 10-20\")\n</code></pre> <p>5. No influential outliers: - Extreme values shouldn't dominate the model</p> <p>Check: Cook's distance, standardized residuals <pre><code>from scipy import stats\n\n# Calculate standardized residuals\ny_pred_prob = model.predict_proba(X)[:, 1]\nresiduals = y - y_pred_prob\nstd_residuals = residuals / np.sqrt(y_pred_prob * (1 - y_pred_prob))\n\n# Identify outliers\noutlier_threshold = 2.5\noutliers = np.abs(std_residuals) &gt; outlier_threshold\n\nprint(f\"Number of potential outliers: {np.sum(outliers)}\")\nprint(f\"Percentage of outliers: {np.mean(outliers) * 100:.1f}%\")\n</code></pre></p> <p>What to do if assumptions are violated:</p> Assumption Violated Solutions Independence Use mixed-effects models, cluster-robust errors Linearity Add polynomial terms, splines, or transform variables Multicollinearity Remove correlated features, PCA, regularization Sample size Collect more data, use regularization, simpler model Outliers Remove outliers, use robust methods, transform data 10. How does logistic regression handle imbalanced datasets? <p>Challenges with imbalanced data: - Model biased toward majority class - High accuracy but poor minority class recall - Misleading performance metrics</p> <p>Solutions:</p> <p>1. Class weighting: <pre><code># Automatically balance class weights\nbalanced_model = LogisticRegression(class_weight='balanced')\nbalanced_model.fit(X_train, y_train)\n\n# Manual class weights\nmanual_weights = {0: 1, 1: 10}  # Give 10x weight to minority class\nweighted_model = LogisticRegression(class_weight=manual_weights)\n</code></pre></p> <p>2. Resampling techniques: <pre><code>from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\n\n# Oversampling minority class\nsmote = SMOTE(random_state=42)\nX_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n\n# Undersampling majority class\nundersampler = RandomUnderSampler(random_state=42)\nX_under, y_under = undersampler.fit_resample(X_train, y_train)\n\n# Combined approach\ncombined = SMOTETomek(random_state=42)\nX_combined, y_combined = combined.fit_resample(X_train, y_train)\n</code></pre></p> <p>3. Threshold tuning: <pre><code>from sklearn.metrics import precision_recall_curve\n\n# Find optimal threshold\ny_pred_proba = model.predict_proba(X_test)[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\n# Maximize F1-score\nf1_scores = 2 * precision * recall / (precision + recall)\noptimal_idx = np.argmax(f1_scores)\noptimal_threshold = thresholds[optimal_idx]\n\n# Use optimal threshold for predictions\ny_pred_optimal = (y_pred_proba &gt;= optimal_threshold).astype(int)\n</code></pre></p> <p>4. Ensemble methods: <pre><code>from sklearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n\n# Balanced bagging\nbalanced_bagging = BalancedBaggingClassifier(\n    base_estimator=LogisticRegression(),\n    random_state=42\n)\n\n# Balanced random forest\nbalanced_rf = BalancedRandomForestClassifier(random_state=42)\n</code></pre></p> <p>5. Evaluation metrics for imbalanced data: <pre><code>from sklearn.metrics import (classification_report, confusion_matrix, \n                            roc_auc_score, average_precision_score)\n\n# Focus on minority class performance\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# ROC-AUC (less affected by imbalance)\nauc_roc = roc_auc_score(y_test, y_pred_proba)\n\n# Precision-Recall AUC (better for imbalanced data)\nauc_pr = average_precision_score(y_test, y_pred_proba)\n\nprint(f\"ROC-AUC: {auc_roc:.3f}\")\nprint(f\"PR-AUC: {auc_pr:.3f}\")\n</code></pre></p> <p>Comparison of approaches:</p> Method Pros Cons When to Use Class Weighting Simple, fast May overfit minority Small to moderate imbalance SMOTE Creates synthetic samples Potential overfitting Moderate imbalance Undersampling Fast, simple Loss of information Large datasets Threshold Tuning No data modification Requires validation set Any imbalance level Ensemble Often best performance More complex Severe imbalance <p>Best practices: - Use stratified cross-validation - Focus on precision, recall, F1-score, not just accuracy - Consider business cost of false positives vs false negatives - Use PR-AUC over ROC-AUC for severe imbalance - Combine multiple approaches (e.g., SMOTE + class weighting)</p>"},{"location":"Machine-Learning/Logistic%20Regression/#examples","title":"\ud83d\udca1 Examples","text":""},{"location":"Machine-Learning/Logistic%20Regression/#example-1-customer-churn-prediction","title":"Example 1: Customer Churn Prediction","text":"<pre><code># Customer churn prediction using logistic regression\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n\n# Simulate customer churn data\nnp.random.seed(42)\nn_customers = 2000\n\n# Generate synthetic customer data\ndata = {\n    'age': np.random.normal(40, 12, n_customers).astype(int),\n    'tenure_months': np.random.exponential(24, n_customers).astype(int),\n    'monthly_charges': np.random.normal(65, 20, n_customers),\n    'total_charges': np.random.normal(1500, 800, n_customers),\n    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n                                     n_customers, p=[0.5, 0.3, 0.2]),\n    'payment_method': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],\n                                      n_customers, p=[0.4, 0.2, 0.2, 0.2]),\n    'customer_service_calls': np.random.poisson(2, n_customers),\n    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n                                        n_customers, p=[0.4, 0.4, 0.2])\n}\n\n# Create churn based on logical rules (with noise)\nchurn_probability = (\n    (data['contract_type'] == 'Month-to-month') * 0.3 +\n    (data['customer_service_calls'] &gt; 3) * 0.2 +\n    (data['monthly_charges'] &gt; 80) * 0.15 +\n    (data['tenure_months'] &lt; 12) * 0.25 +\n    np.random.normal(0, 0.1, n_customers)  # Add noise\n)\n\nchurn = (churn_probability &gt; 0.5).astype(int)\n\n# Create DataFrame\ndf_churn = pd.DataFrame(data)\ndf_churn['churn'] = churn\n\nprint(\"Customer Churn Dataset:\")\nprint(df_churn.head())\nprint(f\"\\nChurn rate: {df_churn['churn'].mean():.2%}\")\nprint(f\"Dataset shape: {df_churn.shape}\")\n\n# Exploratory Data Analysis\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Numerical features\nnumerical_features = ['age', 'tenure_months', 'monthly_charges', 'total_charges']\nfor i, feature in enumerate(numerical_features):\n    ax = axes[i//3, i%3]\n    df_churn.boxplot(column=feature, by='churn', ax=ax)\n    ax.set_title(f'{feature} by Churn Status')\n    ax.set_xlabel('Churn (0=No, 1=Yes)')\n\n# Categorical features\ndf_churn['contract_type'].value_counts().plot(kind='bar', ax=axes[1, 2])\naxes[1, 2].set_title('Contract Type Distribution')\naxes[1, 2].set_xlabel('Contract Type')\n\n# Customer service calls vs churn\nchurn_by_calls = df_churn.groupby('customer_service_calls')['churn'].mean()\naxes[1, 1].bar(churn_by_calls.index, churn_by_calls.values)\naxes[1, 1].set_title('Churn Rate by Customer Service Calls')\naxes[1, 1].set_xlabel('Number of Calls')\naxes[1, 1].set_ylabel('Churn Rate')\n\nplt.tight_layout()\nplt.show()\n\n# Data preprocessing\n# Encode categorical variables\nle_contract = LabelEncoder()\nle_payment = LabelEncoder()\nle_internet = LabelEncoder()\n\ndf_processed = df_churn.copy()\ndf_processed['contract_type_encoded'] = le_contract.fit_transform(df_churn['contract_type'])\ndf_processed['payment_method_encoded'] = le_payment.fit_transform(df_churn['payment_method'])\ndf_processed['internet_service_encoded'] = le_internet.fit_transform(df_churn['internet_service'])\n\n# Select features for modeling\nfeature_columns = ['age', 'tenure_months', 'monthly_charges', 'total_charges',\n                  'contract_type_encoded', 'payment_method_encoded', \n                  'customer_service_calls', 'internet_service_encoded']\n\nX = df_processed[feature_columns]\ny = df_processed['churn']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train logistic regression model\nchurn_model = LogisticRegression(\n    random_state=42,\n    class_weight='balanced',  # Handle class imbalance\n    max_iter=1000\n)\n\nchurn_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = churn_model.predict(X_test_scaled)\ny_pred_proba = churn_model.predict_proba(X_test_scaled)[:, 1]\n\n# Evaluate model\nprint(\"\\n\" + \"=\"*50)\nprint(\"CHURN PREDICTION RESULTS\")\nprint(\"=\"*50)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, \n                          target_names=['No Churn', 'Churn']))\n\n# Feature importance analysis\nfeature_importance = pd.DataFrame({\n    'Feature': feature_columns,\n    'Coefficient': churn_model.coef_[0],\n    'Abs_Coefficient': np.abs(churn_model.coef_[0]),\n    'Odds_Ratio': np.exp(churn_model.coef_[0])\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Interpret results\nprint(\"\\nBusiness Insights:\")\nfor _, row in feature_importance.head(3).iterrows():\n    feature = row['Feature']\n    coef = row['Coefficient']\n    odds_ratio = row['Odds_Ratio']\n\n    if coef &gt; 0:\n        impact = \"increases\"\n    else:\n        impact = \"decreases\"\n\n    print(f\"- {feature}: {impact} churn odds by {abs(odds_ratio-1)*100:.1f}%\")\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label=f'ROC curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Customer Churn - ROC Curve')\nplt.legend(loc=\"lower right\")\n\n# Feature importance visualization\nplt.subplot(1, 2, 2)\ntop_features = feature_importance.head(6)\ncolors = ['red' if x &lt; 0 else 'green' for x in top_features['Coefficient']]\nplt.barh(range(len(top_features)), top_features['Coefficient'], color=colors)\nplt.yticks(range(len(top_features)), top_features['Feature'])\nplt.xlabel('Coefficient Value')\nplt.title('Feature Importance (Logistic Regression Coefficients)')\nplt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Customer segments analysis\nprint(\"\\n\" + \"=\"*50)\nprint(\"CUSTOMER SEGMENTATION ANALYSIS\")\nprint(\"=\"*50)\n\n# Predict churn for different customer segments\nsegments = {\n    'New Month-to-month': {\n        'age': 35, 'tenure_months': 3, 'monthly_charges': 70, \n        'total_charges': 210, 'contract_type_encoded': le_contract.transform(['Month-to-month'])[0],\n        'payment_method_encoded': le_payment.transform(['Electronic check'])[0],\n        'customer_service_calls': 5, 'internet_service_encoded': le_internet.transform(['Fiber optic'])[0]\n    },\n    'Loyal Two-year': {\n        'age': 45, 'tenure_months': 36, 'monthly_charges': 60,\n        'total_charges': 2160, 'contract_type_encoded': le_contract.transform(['Two year'])[0],\n        'payment_method_encoded': le_payment.transform(['Bank transfer'])[0],\n        'customer_service_calls': 1, 'internet_service_encoded': le_internet.transform(['DSL'])[0]\n    }\n}\n\nfor segment_name, segment_data in segments.items():\n    segment_features = np.array([[segment_data[col] for col in feature_columns]])\n    segment_scaled = scaler.transform(segment_features)\n    churn_prob = churn_model.predict_proba(segment_scaled)[0, 1]\n\n    print(f\"{segment_name} customer:\")\n    print(f\"  Churn probability: {churn_prob:.1%}\")\n    print(f\"  Risk level: {'High' if churn_prob &gt; 0.7 else 'Medium' if churn_prob &gt; 0.3 else 'Low'}\")\n    print()\n</code></pre>"},{"location":"Machine-Learning/Logistic%20Regression/#example-2-medical-diagnosis-classification","title":"Example 2: Medical Diagnosis Classification","text":"<pre><code># Medical diagnosis using logistic regression\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\nimport numpy as np\n\nprint(\"=\"*60)\nprint(\"MEDICAL DIAGNOSIS: BREAST CANCER CLASSIFICATION\")\nprint(\"=\"*60)\n\n# Load breast cancer dataset\ncancer = load_breast_cancer()\nX_medical = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ny_medical = cancer.target\n\nprint(f\"Dataset Information:\")\nprint(f\"- Samples: {len(X_medical)}\")\nprint(f\"- Features: {len(cancer.feature_names)}\")\nprint(f\"- Classes: {cancer.target_names}\")\nprint(f\"- Class distribution: {np.bincount(y_medical)}\")\n\n# Focus on most clinically relevant features\nclinical_features = [\n    'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n    'mean compactness', 'mean concavity', 'mean concave points',\n    'worst radius', 'worst perimeter', 'worst area'\n]\n\nX_clinical = X_medical[clinical_features]\n\n# Split data\nX_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n    X_clinical, y_medical, test_size=0.2, random_state=42, stratify=y_medical\n)\n\n# Scale features\nscaler_med = StandardScaler()\nX_train_med_scaled = scaler_med.fit_transform(X_train_med)\nX_test_med_scaled = scaler_med.transform(X_test_med)\n\n# Train medical diagnosis model\nmedical_model = LogisticRegression(\n    random_state=42,\n    max_iter=1000,\n    C=1.0  # No strong regularization for medical application\n)\n\nmedical_model.fit(X_train_med_scaled, y_train_med)\n\n# Predictions\ny_pred_med = medical_model.predict(X_test_med_scaled)\ny_pred_proba_med = medical_model.predict_proba(X_test_med_scaled)\n\nprint(f\"\\nDiagnostic Model Performance:\")\nprint(f\"Accuracy: {accuracy_score(y_test_med, y_pred_med):.3f}\")\n\n# Medical-specific metrics\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\ncm_med = confusion_matrix(y_test_med, y_pred_med)\ntn, fp, fn, tp = cm_med.ravel()\n\n# Medical terminology\nsensitivity = recall_score(y_test_med, y_pred_med)  # True Positive Rate\nspecificity = tn / (tn + fp)  # True Negative Rate\nppv = precision_score(y_test_med, y_pred_med)  # Positive Predictive Value\nnpv = tn / (tn + fn)  # Negative Predictive Value\n\nprint(f\"\\nMedical Performance Metrics:\")\nprint(f\"Sensitivity (Recall): {sensitivity:.3f}\")\nprint(f\"Specificity: {specificity:.3f}\")\nprint(f\"Positive Predictive Value: {ppv:.3f}\")\nprint(f\"Negative Predictive Value: {npv:.3f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"                 Predicted\")\nprint(f\"               Benign  Malignant\")\nprint(f\"Actual Benign    {tn:2d}      {fp:2d}\")\nprint(f\"    Malignant    {fn:2d}      {tp:2d}\")\n\n# Clinical interpretation of features\nfeature_clinical = pd.DataFrame({\n    'Clinical_Feature': clinical_features,\n    'Coefficient': medical_model.coef_[0],\n    'Odds_Ratio': np.exp(medical_model.coef_[0]),\n    'Clinical_Impact': ['Increases malignancy risk' if c &gt; 0 else 'Decreases malignancy risk' \n                       for c in medical_model.coef_[0]]\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(f\"\\nClinical Feature Analysis:\")\nprint(feature_clinical)\n\n# Risk stratification\nrisk_thresholds = [0.3, 0.7]\nrisk_levels = []\n\nfor prob in y_pred_proba_med[:, 1]:\n    if prob &lt; risk_thresholds[0]:\n        risk_levels.append('Low Risk')\n    elif prob &lt; risk_thresholds[1]:\n        risk_levels.append('Moderate Risk')\n    else:\n        risk_levels.append('High Risk')\n\nrisk_df = pd.DataFrame({\n    'Patient_ID': range(len(y_test_med)),\n    'Actual': ['Malignant' if y == 1 else 'Benign' for y in y_test_med],\n    'Predicted_Probability': y_pred_proba_med[:, 1],\n    'Risk_Level': risk_levels\n})\n\nprint(f\"\\nRisk Stratification Summary:\")\nprint(risk_df['Risk_Level'].value_counts())\n\n# Show some example cases\nprint(f\"\\nExample High-Risk Cases:\")\nhigh_risk_cases = risk_df[risk_df['Risk_Level'] == 'High Risk'].head(3)\nfor _, case in high_risk_cases.iterrows():\n    print(f\"Patient {case['Patient_ID']}: {case['Predicted_Probability']:.1%} malignancy risk \"\n          f\"(Actual: {case['Actual']})\")\n</code></pre>"},{"location":"Machine-Learning/Logistic%20Regression/#references","title":"\ud83d\udcda References","text":"<ol> <li>Books:</li> <li>The Elements of Statistical Learning - Hastie, Tibshirani, Friedman</li> <li>An Introduction to Statistical Learning - James, Witten, Hastie, Tibshirani</li> <li> <p>Applied Logistic Regression - Hosmer, Lemeshow, Sturdivant</p> </li> <li> <p>Academic Papers:</p> </li> <li>Maximum Likelihood Estimation - Original MLE theory</li> <li> <p>Regularization Paths for Generalized Linear Models - Friedman et al.</p> </li> <li> <p>Online Resources:</p> </li> <li>Scikit-learn Logistic Regression</li> <li>Stanford CS229 - Machine Learning</li> <li> <p>MIT 6.034 - Logistic Regression</p> </li> <li> <p>Interactive Tools:</p> </li> <li>Logistic Regression Visualization</li> <li> <p>Seeing Theory - Regression</p> </li> <li> <p>Video Lectures:</p> </li> <li>Andrew Ng - Machine Learning Course</li> <li>StatQuest - Logistic Regression</li> <li> <p>3Blue1Brown - Neural Networks</p> </li> <li> <p>Documentation:</p> </li> <li>Statsmodels - Logistic Regression</li> <li>TensorFlow - Classification</li> </ol>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/","title":"\ud83d\udcca Loss Functions - MAE, RMSE","text":"<p>Loss functions quantify the difference between predicted and actual values, serving as the foundation for training machine learning models through optimization algorithms.</p> <p>Resources: Scikit-learn Metrics | Deep Learning Book - Chapter 5</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#_1","title":"Loss Functions - MAE, RMSE","text":"<p>\u000f Summary</p> <p>Loss functions are mathematical functions that measure the discrepancy between predicted values and true values in machine learning models. MAE and RMSE are two fundamental regression loss functions:</p> <p>Mean Absolute Error (MAE): - Measures the average magnitude of errors in predictions - Less sensitive to outliers - Provides uniform penalty for all errors - Also known as L1 loss</p> <p>Root Mean Square Error (RMSE): - Measures the square root of the average squared differences - More sensitive to outliers due to squaring - Penalizes larger errors more heavily - Related to L2 loss (MSE)</p> <p>Applications: - Regression model evaluation - Neural network training objectives - Time series forecasting assessment - Computer vision tasks - Financial modeling - Performance benchmarking</p> <p>When to use which: - MAE: When all errors are equally important and outliers should not dominate - RMSE: When larger errors are more problematic and should be penalized heavily</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Definition: \\(\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\\)\\)</p> <p>Where: - \\(n\\) is the number of samples - \\(y_i\\) is the true value - \\(\\hat{y}_i\\) is the predicted value - \\(|\u00b7|\\) denotes absolute value</p> <p>Properties: - Linear penalty: Each unit of error contributes equally - Robust to outliers: Outliers don't disproportionately affect the loss - Non-differentiable at zero: Gradient-based optimization can be challenging - Interpretable: Same units as the target variable</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#root-mean-square-error-rmse","title":"Root Mean Square Error (RMSE)","text":"<p>Definition: \\(\\(\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\)\\)</p> <p>Relation to Mean Square Error (MSE): \\(\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\\) \\(\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\)\\)</p> <p>Properties: - Quadratic penalty: Larger errors are penalized exponentially more - Sensitive to outliers: Large errors dominate the loss function - Differentiable everywhere: Smooth optimization landscape - Interpretable units: Same units as the target variable (unlike MSE)</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>MAE (L1 Loss): - Forms diamond-shaped contours in parameter space - Encourages sparse solutions - Equal penalty regardless of error magnitude</p> <p>RMSE/MSE (L2 Loss): - Forms circular contours in parameter space - Smooth gradients everywhere - Increasing penalty with error magnitude</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#gradient-analysis","title":"Gradient Analysis","text":"<p>MAE Gradient: \\(\\(\\frac{\\partial \\text{MAE}}{\\partial \\hat{y}_i} = \\frac{1}{n} \\cdot \\text{sign}(y_i - \\hat{y}_i)\\)\\)</p> <p>MSE Gradient: \\(\\(\\frac{\\partial \\text{MSE}}{\\partial \\hat{y}_i} = \\frac{2}{n} (y_i - \\hat{y}_i)\\)\\)</p> <p>The MAE gradient is constant (\u00b11/n), while MSE gradient is proportional to the error magnitude.</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#implementation-using-libraries","title":"\ud83d\udcbb Implementation using Libraries","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_regression\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate sample regression data\nX, y = make_regression(n_samples=1000, n_features=5, noise=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Calculate MAE and RMSE\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Mean Square Error (MSE): {mse:.4f}\")\nprint(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n\n# Demonstrate outlier sensitivity\n# Add some outliers to predictions\ny_pred_with_outliers = y_pred.copy()\ny_pred_with_outliers[:5] += 100  # Add large errors to first 5 predictions\n\nmae_outliers = mean_absolute_error(y_test, y_pred_with_outliers)\nrmse_outliers = np.sqrt(mean_squared_error(y_test, y_pred_with_outliers))\n\nprint(f\"\\nWith Outliers:\")\nprint(f\"MAE: {mae:.4f} -&gt; {mae_outliers:.4f} (increase: {mae_outliers/mae:.2f}x)\")\nprint(f\"RMSE: {rmse:.4f} -&gt; {rmse_outliers:.4f} (increase: {rmse_outliers/rmse:.2f}x)\")\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#using-tensorflowkeras","title":"Using TensorFlow/Keras","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\n# Custom loss functions\ndef mae_loss(y_true, y_pred):\n    \"\"\"Mean Absolute Error loss function\"\"\"\n    return tf.reduce_mean(tf.abs(y_true - y_pred))\n\ndef rmse_loss(y_true, y_pred):\n    \"\"\"Root Mean Square Error loss function\"\"\"\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n\n# Example neural network with different loss functions\ndef create_model(loss_fn):\n    model = keras.Sequential([\n        keras.layers.Dense(64, activation='relu', input_shape=(5,)),\n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.Dense(1)\n    ])\n\n    model.compile(optimizer='adam', loss=loss_fn, metrics=[mae_loss, rmse_loss])\n    return model\n\n# Train models with different loss functions\nmae_model = create_model(mae_loss)\nrmse_model = create_model('mse')  # MSE is equivalent to RMSE for optimization\n\nprint(\"MAE Model:\")\nmae_history = mae_model.fit(X_train, y_train, epochs=50, batch_size=32, \n                           validation_split=0.2, verbose=0)\n\nprint(\"RMSE Model:\")\nrmse_history = rmse_model.fit(X_train, y_train, epochs=50, batch_size=32, \n                             validation_split=0.2, verbose=0)\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#plotting-loss-functions","title":"Plotting Loss Functions","text":"<pre><code># Visualize how MAE and RMSE behave with different error magnitudes\nerrors = np.linspace(-5, 5, 100)\nmae_values = np.abs(errors)\nmse_values = errors**2\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(errors, mae_values, label='MAE = |error|', linewidth=2)\nplt.plot(errors, mse_values, label='MSE = error\u00b2', linewidth=2)\nplt.xlabel('Prediction Error')\nplt.ylabel('Loss Value')\nplt.title('MAE vs MSE Loss Functions')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(errors[errors != 0], 1 * np.ones_like(errors[errors != 0]), \n         label='MAE Gradient = \u00b11', linewidth=2)\nplt.plot(errors, 2 * errors, label='MSE Gradient = 2\u00d7error', linewidth=2)\nplt.xlabel('Prediction Error')\nplt.ylabel('Gradient')\nplt.title('Gradient Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#from-scratch-implementation","title":"\ud83d\udd27 From Scratch Implementation","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#pure-python-implementation","title":"Pure Python Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple\n\nclass LossFunctions:\n    \"\"\"\n    Implementation of MAE and RMSE loss functions from scratch\n    \"\"\"\n\n    @staticmethod\n    def mae(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate Mean Absolute Error\n\n        Args:\n            y_true: True values\n            y_pred: Predicted values\n\n        Returns:\n            MAE value\n        \"\"\"\n        if len(y_true) != len(y_pred):\n            raise ValueError(\"y_true and y_pred must have same length\")\n\n        errors = y_true - y_pred\n        absolute_errors = np.abs(errors)\n        mae = np.mean(absolute_errors)\n\n        return mae\n\n    @staticmethod\n    def mse(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate Mean Square Error\n\n        Args:\n            y_true: True values\n            y_pred: Predicted values\n\n        Returns:\n            MSE value\n        \"\"\"\n        if len(y_true) != len(y_pred):\n            raise ValueError(\"y_true and y_pred must have same length\")\n\n        errors = y_true - y_pred\n        squared_errors = errors ** 2\n        mse = np.mean(squared_errors)\n\n        return mse\n\n    @staticmethod\n    def rmse(y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate Root Mean Square Error\n\n        Args:\n            y_true: True values\n            y_pred: Predicted values\n\n        Returns:\n            RMSE value\n        \"\"\"\n        mse_value = LossFunctions.mse(y_true, y_pred)\n        rmse = np.sqrt(mse_value)\n\n        return rmse\n\n    @staticmethod\n    def mae_gradient(y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Calculate gradient of MAE with respect to predictions\n\n        Args:\n            y_true: True values\n            y_pred: Predicted values\n\n        Returns:\n            Gradient array\n        \"\"\"\n        errors = y_true - y_pred\n        gradients = np.zeros_like(errors)\n\n        gradients[errors &gt; 0] = -1  # If error is positive, gradient is -1\n        gradients[errors &lt; 0] = 1   # If error is negative, gradient is +1\n        gradients[errors == 0] = 0  # If error is zero, gradient is 0\n\n        return gradients / len(y_true)\n\n    @staticmethod\n    def mse_gradient(y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Calculate gradient of MSE with respect to predictions\n\n        Args:\n            y_true: True values\n            y_pred: Predicted values\n\n        Returns:\n            Gradient array\n        \"\"\"\n        errors = y_true - y_pred\n        gradients = -2 * errors  # Derivative of (y_true - y_pred)\u00b2\n\n        return gradients / len(y_true)\n\n# Demonstration of custom implementation\ndef demonstrate_loss_functions():\n    \"\"\"Demonstrate the custom loss function implementation\"\"\"\n    # Create sample data\n    np.random.seed(42)\n    y_true = np.random.normal(50, 10, 100)\n    y_pred = y_true + np.random.normal(0, 5, 100)  # Add some noise\n\n    # Calculate losses using our implementation\n    loss_calc = LossFunctions()\n\n    mae_value = loss_calc.mae(y_true, y_pred)\n    mse_value = loss_calc.mse(y_true, y_pred)\n    rmse_value = loss_calc.rmse(y_true, y_pred)\n\n    print(\"Custom Implementation Results:\")\n    print(f\"MAE: {mae_value:.4f}\")\n    print(f\"MSE: {mse_value:.4f}\")\n    print(f\"RMSE: {rmse_value:.4f}\")\n\n    # Compare with sklearn\n    from sklearn.metrics import mean_absolute_error, mean_squared_error\n\n    sklearn_mae = mean_absolute_error(y_true, y_pred)\n    sklearn_mse = mean_squared_error(y_true, y_pred)\n    sklearn_rmse = np.sqrt(sklearn_mse)\n\n    print(\"\\nSklearn Results:\")\n    print(f\"MAE: {sklearn_mae:.4f}\")\n    print(f\"MSE: {sklearn_mse:.4f}\")\n    print(f\"RMSE: {sklearn_rmse:.4f}\")\n\n    print(f\"\\nDifferences (should be ~0):\")\n    print(f\"MAE diff: {abs(mae_value - sklearn_mae):.10f}\")\n    print(f\"MSE diff: {abs(mse_value - sklearn_mse):.10f}\")\n    print(f\"RMSE diff: {abs(rmse_value - sklearn_rmse):.10f}\")\n\n    # Demonstrate gradient calculation\n    mae_grad = loss_calc.mae_gradient(y_true, y_pred)\n    mse_grad = loss_calc.mse_gradient(y_true, y_pred)\n\n    print(f\"\\nGradient Statistics:\")\n    print(f\"MAE gradient mean: {np.mean(mae_grad):.6f}\")\n    print(f\"MSE gradient mean: {np.mean(mse_grad):.6f}\")\n    print(f\"MAE gradient std: {np.std(mae_grad):.6f}\")\n    print(f\"MSE gradient std: {np.std(mse_grad):.6f}\")\n\n# Run demonstration\ndemonstrate_loss_functions()\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#robust-loss-function-implementation","title":"Robust Loss Function Implementation","text":"<pre><code>class RobustLossFunctions:\n    \"\"\"\n    Enhanced implementation with robust error handling and additional metrics\n    \"\"\"\n\n    def __init__(self):\n        self.history = []\n\n    def calculate_all_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; dict:\n        \"\"\"\n        Calculate comprehensive error metrics\n\n        Returns:\n            Dictionary containing all metrics\n        \"\"\"\n        errors = y_true - y_pred\n        abs_errors = np.abs(errors)\n        squared_errors = errors ** 2\n\n        metrics = {\n            'mae': np.mean(abs_errors),\n            'mse': np.mean(squared_errors),\n            'rmse': np.sqrt(np.mean(squared_errors)),\n            'median_ae': np.median(abs_errors),\n            'max_error': np.max(abs_errors),\n            'mean_error': np.mean(errors),  # Bias\n            'std_error': np.std(errors),\n            'r2_score': self._r2_score(y_true, y_pred)\n        }\n\n        self.history.append(metrics)\n        return metrics\n\n    def _r2_score(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"Calculate R\u00b2 score\"\"\"\n        ss_res = np.sum((y_true - y_pred) ** 2)\n        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n        r2 = 1 - (ss_res / ss_tot)\n        return r2\n\n    def compare_with_baseline(self, y_true: np.ndarray, y_pred: np.ndarray, \n                            baseline_pred: np.ndarray) -&gt; dict:\n        \"\"\"\n        Compare model performance with a baseline\n\n        Args:\n            y_true: True values\n            y_pred: Model predictions\n            baseline_pred: Baseline predictions (e.g., mean prediction)\n\n        Returns:\n            Comparison metrics\n        \"\"\"\n        model_metrics = self.calculate_all_metrics(y_true, y_pred)\n        baseline_metrics = self.calculate_all_metrics(y_true, baseline_pred)\n\n        comparison = {}\n        for metric in model_metrics:\n            if metric in ['r2_score']:  # Higher is better\n                improvement = model_metrics[metric] - baseline_metrics[metric]\n            else:  # Lower is better\n                improvement = baseline_metrics[metric] - model_metrics[metric]\n\n            comparison[f'{metric}_improvement'] = improvement\n            comparison[f'{metric}_improvement_pct'] = (improvement / baseline_metrics[metric]) * 100\n\n        return comparison\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#assumptions","title":"Assumptions","text":"<p>MAE: - All prediction errors are equally important - Outliers should not dominate the loss function - The cost of over-prediction equals the cost of under-prediction - Linear penalty structure is appropriate for the problem</p> <p>RMSE: - Larger errors are more problematic and should be penalized heavily - The relationship between error magnitude and penalty should be quadratic - Gaussian error distribution is assumed (for probabilistic interpretation) - Differentiability of loss function is required for optimization</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#limitations","title":"Limitations","text":"<p>MAE Limitations: - Non-differentiable at zero: Makes gradient-based optimization challenging - Equal weighting: May not reflect real-world cost structures where large errors are disproportionately costly - Slower convergence: Constant gradients can lead to slower optimization - Less sensitive to small improvements: May not distinguish between models with similar performance</p> <p>RMSE Limitations: - Outlier sensitivity: Few extreme values can dominate the loss - Unit dependency: Values are affected by the scale of the target variable - Overemphasis on large errors: May ignore many small errors - Not robust: Performance degrades significantly with outliers</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#comparison-with-other-loss-functions","title":"Comparison with Other Loss Functions","text":"Loss Function Outlier Sensitivity Differentiability Interpretability Use Case MAE Low No (at 0) High Robust regression RMSE/MSE High Yes Medium Standard regression Huber Medium Yes Medium Robust with smoothness Quantile Variable Yes High Risk-aware prediction"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#when-to-use-which","title":"When to Use Which","text":"<p>Use MAE when: - Outliers are present and shouldn't dominate - All errors are equally costly - You need robust estimates - Interpretability is crucial - Working with heavy-tailed distributions</p> <p>Use RMSE when: - Large errors are more problematic - You have clean data without extreme outliers - Need smooth gradients for optimization - Working with Gaussian-like distributions - Standard benchmarking is required</p>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#interview-questions","title":"\u2753 Interview Questions","text":"Q1: What is the main difference between MAE and RMSE in terms of outlier sensitivity? <p>Answer: </p> <p>MAE (L1 loss) is less sensitive to outliers because it uses absolute values, giving equal weight to all errors. Each error contributes linearly to the total loss.</p> <p>RMSE (L2 loss) is highly sensitive to outliers because it squares the errors before averaging. This means large errors are penalized exponentially more than small errors. A single large outlier can dominate the entire loss value.</p> <p>Mathematical example: - Normal errors: [1, 1, 1, 1, 1] \u0092 MAE = 1, RMSE = 1 - With outlier: [1, 1, 1, 1, 10] \u0092 MAE = 2.8, RMSE = 4.6</p> <p>The outlier causes RMSE to increase by 4.6x while MAE only increases by 2.8x.</p> Q2: Why is MAE non-differentiable at zero and how does this affect optimization? <p>Answer:</p> <p>MAE uses the absolute value function |x|, which has a sharp corner at x=0. At this point, the left derivative is -1 and the right derivative is +1, making the function non-differentiable.</p> <p>Impact on optimization: - Gradient-based optimizers (SGD, Adam) struggle near zero error - Can cause oscillations around the optimal solution - May require smaller learning rates - Subgradient methods or smoothed versions (like Huber loss) are often used instead</p> <p>Solutions: - Use subgradient descent - Implement Huber loss (smooth approximation) - Use specialized optimizers designed for non-smooth functions</p> Q3: In what scenarios would you choose MAE over RMSE for model evaluation? <p>Answer:</p> <p>Choose MAE when: 1. Outliers are present: MAE provides more robust evaluation 2. Equal error costs: All prediction errors have the same business impact 3. Heavy-tailed distributions: Data doesn't follow normal distribution 4. Interpretability matters: MAE is in the same units as the target variable 5. Median-based predictions: MAE aligns with median-based models</p> <p>Real-world examples: - Sales forecasting: Missing by $100 or $1000 might have similar operational impact - Medical dosage: All dosage errors are equally concerning - Robust regression: When data contains measurement errors or anomalies</p> Q4: How do MAE and RMSE relate to different types of statistical estimators? <p>Answer:</p> <p>MAE (L1 loss): - Corresponds to median estimator - Minimizing MAE gives the median of the target distribution - Robust to outliers, represents the \"typical\" error - Related to Laplace distribution assumption</p> <p>RMSE/MSE (L2 loss): - Corresponds to mean estimator - Minimizing MSE gives the mean of the target distribution - Optimal under Gaussian noise assumption - Related to maximum likelihood estimation for normal distribution</p> <p>Practical implication: If your target variable is skewed or has outliers, MAE might give more representative results than RMSE.</p> Q5: How do you implement a custom loss function that combines MAE and RMSE? <p>Answer:</p> <p>Huber Loss is a common combination that transitions from MAE to MSE:</p> <pre><code>def huber_loss(y_true, y_pred, delta=1.0):\n    errors = y_true - y_pred\n    abs_errors = np.abs(errors)\n\n    # Use MSE for small errors, MAE for large errors\n    mask = abs_errors &lt;= delta\n\n    loss = np.where(mask, \n                   0.5 * errors**2,  # MSE region\n                   delta * abs_errors - 0.5 * delta**2)  # MAE region\n\n    return np.mean(loss)\n</code></pre> <p>Other combinations: - Weighted combination: <code>\u00b1 * MAE + (1-\u00b1) * MSE</code> - Log-cosh loss: <code>log(cosh(y_pred - y_true))</code> - Quantile loss: For asymmetric penalty structures</p> Q6: What is the relationship between RMSE and standard deviation? <p>Answer:</p> <p>RMSE and standard deviation are closely related but serve different purposes:</p> <p>Similarities: - Both use squared differences - Both are in the same units as the original data - Both penalize large deviations more heavily</p> <p>Key differences: - RMSE: Measures prediction error (predicted vs actual) - Standard deviation: Measures variability around the mean - RMSE formula: <code>\u001a(\u00a3(y_actual - y_pred)\u00b2/n)</code> - Std dev formula: <code>\u001a(\u00a3(x - \u00bc)\u00b2/n)</code></p> <p>Interpretation: RMSE can be thought of as the \"standard deviation of prediction errors.\"</p> Q7: How do you handle the scale dependency of MAE and RMSE? <p>Answer:</p> <p>Both MAE and RMSE are affected by the scale of the target variable, making cross-dataset comparison difficult.</p> <p>Solutions:</p> <ol> <li> <p>Normalized RMSE (NRMSE): <pre><code>nrmse = rmse / (y_max - y_min)  # or / y_mean\n</code></pre></p> </li> <li> <p>Mean Absolute Percentage Error (MAPE): <pre><code>mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n</code></pre></p> </li> <li> <p>Coefficient of Variation of RMSE: <pre><code>cv_rmse = rmse / np.mean(y_true)\n</code></pre></p> </li> <li> <p>Standardized errors: <pre><code>standardized_mae = mae / np.std(y_true)\n</code></pre></p> </li> </ol> Q8: What are the computational complexities of MAE and RMSE? <p>Answer:</p> <p>Time Complexity: - MAE: O(n) - linear scan through errors, absolute value operation - RMSE: O(n) - linear scan through errors, square and square root operations</p> <p>Space Complexity: - Both: O(1) additional space (can compute incrementally) - Or O(n) if storing all errors for analysis</p> <p>Computational considerations: - RMSE requires more expensive operations (squaring, square root) - MAE operations are simpler but may require specialized handling for gradients - Both can be computed incrementally for streaming data - Vectorized implementations (NumPy, GPU) make the difference negligible</p> Q9: How do MAE and RMSE behave differently during model training? <p>Answer:</p> <p>Convergence patterns: - RMSE-based training: Smooth convergence, large errors get immediate attention - MAE-based training: Can have slower convergence due to constant gradients</p> <p>Learning dynamics: - RMSE: Model focuses on reducing largest errors first - MAE: Model treats all errors equally, more balanced learning</p> <p>Practical implications: <pre><code># RMSE training might show:\n# Epoch 1: Large errors dominate loss\n# Epoch 50: Focuses on medium errors  \n# Epoch 100: Fine-tuning small errors\n\n# MAE training might show:\n# More consistent error reduction across all samples\n# Less dramatic early improvements\n# Better final performance on median metrics\n</code></pre></p> Q10: What are some advanced variants and alternatives to standard MAE and RMSE? <p>Answer:</p> <p>Advanced variants:</p> <ol> <li> <p>Weighted MAE/RMSE: Different weights for different samples    <pre><code>weighted_mae = np.mean(weights * np.abs(y_true - y_pred))\n</code></pre></p> </li> <li> <p>Trimmed MAE/RMSE: Remove extreme values before calculation</p> </li> <li> <p>Quantile Loss: Asymmetric loss function    <pre><code>def quantile_loss(y_true, y_pred, quantile=0.5):\n    errors = y_true - y_pred\n    return np.mean(np.maximum(quantile * errors, (quantile - 1) * errors))\n</code></pre></p> </li> <li> <p>Log-cosh Loss: Smooth approximation of MAE    <pre><code>def log_cosh_loss(y_true, y_pred):\n    return np.mean(np.log(np.cosh(y_pred - y_true)))\n</code></pre></p> </li> <li> <p>Huber Loss: Combines MAE and MSE benefits</p> </li> <li> <p>Fair Loss: Less sensitive to outliers than MSE, smoother than MAE</p> </li> </ol>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#examples","title":"\ud83d\udca1 Examples","text":""},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#real-world-example-house-price-prediction","title":"Real-world Example: House Price Prediction","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, HuberRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Load California housing dataset\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Add some artificial outliers to demonstrate difference\nnp.random.seed(42)\noutlier_indices = np.random.choice(len(y), size=50, replace=False)\ny_with_outliers = y.copy()\ny_with_outliers[outlier_indices] *= 5  # Make some house prices 5x higher\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_with_outliers, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train different models\nlr_model = LinearRegression()\nhuber_model = HuberRegressor(epsilon=1.5)\n\nlr_model.fit(X_train_scaled, y_train)\nhuber_model.fit(X_train_scaled, y_train)\n\n# Make predictions\nlr_pred = lr_model.predict(X_test_scaled)\nhuber_pred = huber_model.predict(X_test_scaled)\n\n# Calculate metrics\ndef calculate_metrics(y_true, y_pred, model_name):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n\n    print(f\"\\n{model_name} Results:\")\n    print(f\"MAE: ${mae:.3f}k\")\n    print(f\"MSE: ${mse:.3f}k\u00b2\")\n    print(f\"RMSE: ${rmse:.3f}k\")\n\n    return mae, mse, rmse\n\n# Compare models\nlr_metrics = calculate_metrics(y_test, lr_pred, \"Linear Regression\")\nhuber_metrics = calculate_metrics(y_test, huber_pred, \"Huber Regression\")\n\n# Visualization\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, lr_pred, alpha=0.6, label='Linear Regression')\nplt.scatter(y_test, huber_pred, alpha=0.6, label='Huber Regression')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('True House Price ($100k)')\nplt.ylabel('Predicted House Price ($100k)')\nplt.title('Predictions vs True Values')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nlr_errors = y_test - lr_pred\nhuber_errors = y_test - huber_pred\nplt.hist(lr_errors, bins=50, alpha=0.7, label='Linear Regression', density=True)\nplt.hist(huber_errors, bins=50, alpha=0.7, label='Huber Regression', density=True)\nplt.xlabel('Prediction Error ($100k)')\nplt.ylabel('Density')\nplt.title('Error Distribution')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nmodels = ['Linear Regression', 'Huber Regression']\nmae_values = [lr_metrics[0], huber_metrics[0]]\nrmse_values = [lr_metrics[2], huber_metrics[2]]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nplt.bar(x - width/2, mae_values, width, label='MAE', alpha=0.8)\nplt.bar(x + width/2, rmse_values, width, label='RMSE', alpha=0.8)\n\nplt.xlabel('Models')\nplt.ylabel('Error ($100k)')\nplt.title('MAE vs RMSE Comparison')\nplt.xticks(x, models)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Outlier analysis\noutlier_mask = np.abs(y_test - lr_pred) &gt; np.percentile(np.abs(y_test - lr_pred), 95)\nprint(f\"\\nOutlier Analysis (top 5% errors):\")\nprint(f\"Number of outlier predictions: {np.sum(outlier_mask)}\")\nprint(f\"MAE on outliers: ${mean_absolute_error(y_test[outlier_mask], lr_pred[outlier_mask]):.3f}k\")\nprint(f\"MAE on non-outliers: ${mean_absolute_error(y_test[~outlier_mask], lr_pred[~outlier_mask]):.3f}k\")\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#time-series-forecasting-example","title":"Time Series Forecasting Example","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Generate synthetic time series data\nnp.random.seed(42)\ndates = pd.date_range('2020-01-01', periods=365, freq='D')\n\n# Create trend + seasonality + noise\ntrend = np.linspace(100, 120, 365)\nseasonality = 10 * np.sin(2 * np.pi * np.arange(365) / 365.25 * 4)  # Quarterly pattern\nnoise = np.random.normal(0, 2, 365)\noutliers = np.zeros(365)\noutliers[100] = 20  # Add a large outlier\noutliers[200] = -15  # Add a negative outlier\n\ntime_series = trend + seasonality + noise + outliers\n\n# Simple moving average prediction\nwindow = 30\npredictions = []\nactuals = []\n\nfor i in range(window, len(time_series)):\n    # Predict using simple moving average\n    pred = np.mean(time_series[i-window:i])\n    predictions.append(pred)\n    actuals.append(time_series[i])\n\npredictions = np.array(predictions)\nactuals = np.array(actuals)\n\n# Calculate rolling metrics\nwindow_size = 30\nrolling_mae = []\nrolling_rmse = []\n\nfor i in range(window_size, len(predictions)):\n    window_actuals = actuals[i-window_size:i]\n    window_preds = predictions[i-window_size:i]\n\n    mae = mean_absolute_error(window_actuals, window_preds)\n    rmse = np.sqrt(mean_squared_error(window_actuals, window_preds))\n\n    rolling_mae.append(mae)\n    rolling_rmse.append(rmse)\n\n# Visualization\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.plot(dates[window:], actuals, label='Actual', alpha=0.7)\nplt.plot(dates[window:], predictions, label='Predicted (MA)', alpha=0.7)\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.title('Time Series Forecasting')\nplt.legend()\n\nplt.subplot(2, 2, 2)\nerrors = actuals - predictions\nplt.plot(dates[window:], errors, alpha=0.7)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Date')\nplt.ylabel('Prediction Error')\nplt.title('Prediction Errors Over Time')\n\nplt.subplot(2, 2, 3)\nplt.plot(dates[window+window_size:], rolling_mae, label='Rolling MAE', linewidth=2)\nplt.plot(dates[window+window_size:], rolling_rmse, label='Rolling RMSE', linewidth=2)\nplt.xlabel('Date')\nplt.ylabel('Error Metric')\nplt.title('Rolling Error Metrics')\nplt.legend()\n\nplt.subplot(2, 2, 4)\nplt.hist(errors, bins=30, alpha=0.7, density=True)\nplt.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'Mean: {np.mean(errors):.2f}')\nplt.axvline(x=np.median(errors), color='g', linestyle='--', label=f'Median: {np.median(errors):.2f}')\nplt.xlabel('Prediction Error')\nplt.ylabel('Density')\nplt.title('Error Distribution')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"Time Series Forecasting Results:\")\nprint(f\"Overall MAE: {mean_absolute_error(actuals, predictions):.3f}\")\nprint(f\"Overall RMSE: {np.sqrt(mean_squared_error(actuals, predictions)):.3f}\")\nprint(f\"Mean Error (Bias): {np.mean(errors):.3f}\")\nprint(f\"Std Error: {np.std(errors):.3f}\")\nprint(f\"Max Error: {np.max(np.abs(errors)):.3f}\")\nprint(f\"Median Absolute Error: {np.median(np.abs(errors)):.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/#references","title":"\ud83d\udcda References","text":"<p>Books: - The Elements of Statistical Learning - Hastie, Tibshirani, Friedman - Pattern Recognition and Machine Learning - Christopher Bishop - Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville</p> <p>Academic Papers: - Huber Loss Function - Peter Huber (1964) - Quantile Regression - Roger Koenker and Gilbert Bassett (1978)</p> <p>Online Resources: - Scikit-learn Model Evaluation - TensorFlow Loss Functions - PyTorch Loss Functions - Loss Functions for Regression</p> <p>Tutorials and Blogs: - Understanding Different Loss Functions - MAE vs RMSE vs MAPE - Robust Loss Functions for Deep Learning</p>"},{"location":"Machine-Learning/Neural%20Networks/","title":"\ud83e\udde0 Neural Networks","text":"<p>Neural Networks are computing systems inspired by biological neural networks, consisting of interconnected nodes (neurons) that learn complex patterns through iterative weight adjustments using backpropagation.</p> <p>Resources: Deep Learning Book | Neural Networks and Deep Learning | TensorFlow Tutorial</p>"},{"location":"Machine-Learning/Neural%20Networks/#_1","title":"Neural Networks","text":"<p>\u000f Summary</p> <p>Neural Networks (also known as Artificial Neural Networks or ANNs) are computational models inspired by the human brain's structure and function. They consist of interconnected processing units called neurons or nodes, organized in layers that transform input data through weighted connections and activation functions.</p> <p>Key Components: - Neurons/Nodes: Basic processing units that receive inputs, apply weights, and produce outputs - Layers: Collections of neurons (input layer, hidden layers, output layer) - Weights: Parameters that determine the strength of connections between neurons - Biases: Additional parameters that help shift the activation function - Activation Functions: Non-linear functions that introduce complexity to the model</p> <p>Types of Neural Networks: - Feedforward Networks: Information flows in one direction from input to output - Convolutional Neural Networks (CNNs): Specialized for image processing - Recurrent Neural Networks (RNNs): Handle sequential data with memory - Long Short-Term Memory (LSTM): Advanced RNNs for long sequences - Autoencoders: Learn compressed representations of data - Generative Adversarial Networks (GANs): Generate new data samples</p> <p>Applications: - Image recognition and computer vision - Natural language processing - Speech recognition - Recommendation systems - Time series prediction - Game playing (AlphaGo, chess) - Medical diagnosis - Autonomous vehicles</p> <p>Advantages: - Can learn complex non-linear relationships - Universal function approximators - Automatic feature learning - Scalable to large datasets - Versatile across domains</p>"},{"location":"Machine-Learning/Neural%20Networks/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Neural%20Networks/#biological-inspiration","title":"Biological Inspiration","text":"<p>Neural networks are inspired by how biological neurons work: - Biological neuron: Receives signals through dendrites, processes them in the cell body, and sends output through axons - Artificial neuron: Receives inputs, applies weights and bias, passes through activation function, and produces output</p>"},{"location":"Machine-Learning/Neural%20Networks/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Neural%20Networks/#single-neuron-perceptron","title":"Single Neuron (Perceptron)","text":"<p>A single neuron computes: \\(\\(y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = f(w^T x + b)\\)\\)</p> <p>Where: - \\(x_i\\) are input features - \\(w_i\\) are weights - \\(b\\) is bias - \\(f\\) is the activation function - \\(y\\) is the output</p>"},{"location":"Machine-Learning/Neural%20Networks/#multi-layer-neural-network","title":"Multi-layer Neural Network","text":"<p>For a network with \\(L\\) layers:</p> <p>Forward Propagation: \\(\\(a^{(l)} = f^{(l)}\\left(W^{(l)} a^{(l-1)} + b^{(l)}\\right)\\)\\)</p> <p>Where: - \\(a^{(l)}\\) is the activation of layer \\(l\\) - \\(W^{(l)}\\) is the weight matrix for layer \\(l\\) - \\(b^{(l)}\\) is the bias vector for layer \\(l\\) - \\(f^{(l)}\\) is the activation function for layer \\(l\\)</p>"},{"location":"Machine-Learning/Neural%20Networks/#activation-functions","title":"Activation Functions","text":"<p>Sigmoid: \\(\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\\)</p> <p>Hyperbolic Tangent (tanh): \\(\\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\\)</p> <p>ReLU (Rectified Linear Unit): \\(\\(\\text{ReLU}(x) = \\max(0, x)\\)\\)</p> <p>Leaky ReLU: \\(\\(\\text{LeakyReLU}(x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\)\\)</p> <p>Softmax (for multi-class output): \\(\\(\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}\\)\\)</p>"},{"location":"Machine-Learning/Neural%20Networks/#loss-functions","title":"Loss Functions","text":"<p>Mean Squared Error (Regression): \\(\\(L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\\)</p> <p>Cross-entropy (Classification): \\(\\(L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\\)\\)</p>"},{"location":"Machine-Learning/Neural%20Networks/#backpropagation-algorithm","title":"Backpropagation Algorithm","text":"<p>Chain Rule Application: \\(\\(\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}}\\)\\)</p> <p>Weight Update Rule: \\(\\(w_{ij}^{(l)} = w_{ij}^{(l)} - \\alpha \\frac{\\partial L}{\\partial w_{ij}^{(l)}}\\)\\)</p> <p>Where \\(\\alpha\\) is the learning rate.</p>"},{"location":"Machine-Learning/Neural%20Networks/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<p>Neural networks with at least one hidden layer containing sufficient neurons can approximate any continuous function to arbitrary accuracy, making them powerful universal function approximators.</p>"},{"location":"Machine-Learning/Neural%20Networks/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/Neural%20Networks/#using-tensorflowkeras","title":"Using TensorFlow/Keras","text":"<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.datasets import make_classification, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, \n                         n_informative=15, n_redundant=5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create a simple feedforward neural network\ndef create_model(input_dim, hidden_layers=[64, 32], output_dim=1, activation='relu'):\n    \"\"\"\n    Create a feedforward neural network\n\n    Args:\n        input_dim: Number of input features\n        hidden_layers: List of neurons in each hidden layer\n        output_dim: Number of output neurons\n        activation: Activation function for hidden layers\n    \"\"\"\n    model = keras.Sequential()\n\n    # Input layer\n    model.add(keras.layers.Dense(hidden_layers[0], \n                               activation=activation, \n                               input_dim=input_dim))\n    model.add(keras.layers.Dropout(0.3))\n\n    # Hidden layers\n    for neurons in hidden_layers[1:]:\n        model.add(keras.layers.Dense(neurons, activation=activation))\n        model.add(keras.layers.Dropout(0.3))\n\n    # Output layer\n    if output_dim == 1:\n        model.add(keras.layers.Dense(1, activation='sigmoid'))\n        loss = 'binary_crossentropy'\n        metrics = ['accuracy']\n    else:\n        model.add(keras.layers.Dense(output_dim, activation='softmax'))\n        loss = 'sparse_categorical_crossentropy'\n        metrics = ['accuracy']\n\n    # Compile model\n    model.compile(optimizer='adam', loss=loss, metrics=metrics)\n\n    return model\n\n# Create and train model\nmodel = create_model(input_dim=X_train_scaled.shape[1])\nprint(\"Model Architecture:\")\nmodel.summary()\n\n# Train the model\nhistory = model.fit(X_train_scaled, y_train,\n                   batch_size=32,\n                   epochs=50,\n                   validation_split=0.2,\n                   verbose=0)\n\n# Evaluate the model\ntrain_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\ntest_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n\nprint(f\"\\nTraining Accuracy: {train_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#multi-class-classification-with-iris-dataset","title":"Multi-class Classification with Iris Dataset","text":"<pre><code># Load Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split and scale data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create multi-class model\nmulticlass_model = create_model(input_dim=4, \n                              hidden_layers=[10, 8], \n                              output_dim=3)\n\n# Train model\nhistory = multiclass_model.fit(X_train_scaled, y_train,\n                              epochs=100,\n                              batch_size=16,\n                              validation_split=0.2,\n                              verbose=0)\n\n# Predictions\npredictions = multiclass_model.predict(X_test_scaled)\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"\\nMulti-class Classification Results:\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, predicted_classes, \n                          target_names=iris.target_names))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, predicted_classes))\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#using-pytorch","title":"Using PyTorch","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass SimpleNN(nn.Module):\n    \"\"\"\n    Simple feedforward neural network in PyTorch\n    \"\"\"\n    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.3):\n        super(SimpleNN, self).__init__()\n\n        layers = []\n        prev_size = input_size\n\n        # Hidden layers\n        for hidden_size in hidden_sizes:\n            layers.extend([\n                nn.Linear(prev_size, hidden_size),\n                nn.ReLU(),\n                nn.Dropout(dropout_prob)\n            ])\n            prev_size = hidden_size\n\n        # Output layer\n        layers.append(nn.Linear(prev_size, output_size))\n\n        if output_size == 1:\n            layers.append(nn.Sigmoid())\n        else:\n            layers.append(nn.Softmax(dim=1))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train_scaled)\ny_train_tensor = torch.LongTensor(y_train)\nX_test_tensor = torch.FloatTensor(X_test_scaled)\ny_test_tensor = torch.LongTensor(y_test)\n\n# Create data loaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Initialize model\npytorch_model = SimpleNN(input_size=4, hidden_sizes=[10, 8], output_size=3)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 100\ntrain_losses = []\n\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    for batch_X, batch_y in train_loader:\n        # Forward pass\n        outputs = pytorch_model(batch_X)\n        loss = criterion(outputs, batch_y)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    train_losses.append(epoch_loss / len(train_loader))\n\n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n\n# Evaluate PyTorch model\nwith torch.no_grad():\n    test_outputs = pytorch_model(X_test_tensor)\n    _, predicted = torch.max(test_outputs.data, 1)\n    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n    print(f'PyTorch Model Test Accuracy: {accuracy:.4f}')\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#from-scratch-implementation","title":"\u0099\u000f From Scratch Implementation","text":""},{"location":"Machine-Learning/Neural%20Networks/#complete-neural-network-from-scratch","title":"Complete Neural Network from Scratch","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nclass NeuralNetwork:\n    \"\"\"\n    Neural Network implementation from scratch using NumPy\n    \"\"\"\n\n    def __init__(self, layers, learning_rate=0.01, random_seed=42):\n        \"\"\"\n        Initialize neural network\n\n        Args:\n            layers: List of integers representing number of neurons in each layer\n            learning_rate: Learning rate for gradient descent\n            random_seed: Random seed for reproducibility\n        \"\"\"\n        np.random.seed(random_seed)\n        self.layers = layers\n        self.learning_rate = learning_rate\n        self.num_layers = len(layers)\n\n        # Initialize weights and biases using He initialization\n        self.weights = {}\n        self.biases = {}\n\n        for i in range(1, self.num_layers):\n            # He initialization for ReLU activation\n            self.weights[f'W{i}'] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2/layers[i-1])\n            self.biases[f'b{i}'] = np.zeros((1, layers[i]))\n\n        # Store activations and gradients\n        self.activations = {}\n        self.gradients = {}\n\n    def relu(self, z):\n        \"\"\"ReLU activation function\"\"\"\n        return np.maximum(0, z)\n\n    def relu_derivative(self, z):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (z &gt; 0).astype(float)\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        # Clip z to prevent overflow\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_derivative(self, z):\n        \"\"\"Derivative of sigmoid\"\"\"\n        s = self.sigmoid(z)\n        return s * (1 - s)\n\n    def softmax(self, z):\n        \"\"\"Softmax activation function\"\"\"\n        # Numerical stability\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward_propagation(self, X):\n        \"\"\"\n        Forward propagation through the network\n\n        Args:\n            X: Input data of shape (m, n_features)\n\n        Returns:\n            Final output of the network\n        \"\"\"\n        self.activations['A0'] = X\n\n        for i in range(1, self.num_layers):\n            # Linear transformation\n            Z = np.dot(self.activations[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']\n            self.activations[f'Z{i}'] = Z\n\n            # Apply activation function\n            if i == self.num_layers - 1:  # Output layer\n                if self.layers[-1] == 1:  # Binary classification\n                    A = self.sigmoid(Z)\n                else:  # Multi-class classification\n                    A = self.softmax(Z)\n            else:  # Hidden layers\n                A = self.relu(Z)\n\n            self.activations[f'A{i}'] = A\n\n        return self.activations[f'A{self.num_layers-1}']\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Compute loss function\n\n        Args:\n            y_true: True labels\n            y_pred: Predicted probabilities\n\n        Returns:\n            Loss value\n        \"\"\"\n        m = y_true.shape[0]\n\n        if self.layers[-1] == 1:  # Binary classification\n            # Binary cross-entropy\n            epsilon = 1e-15  # Small value to prevent log(0)\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        else:  # Multi-class classification\n            # Categorical cross-entropy\n            epsilon = 1e-15\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n\n        return loss\n\n    def backward_propagation(self, X, y):\n        \"\"\"\n        Backward propagation to compute gradients\n\n        Args:\n            X: Input data\n            y: True labels\n        \"\"\"\n        m = X.shape[0]\n\n        # Output layer gradient\n        if self.layers[-1] == 1:  # Binary classification\n            dZ = self.activations[f'A{self.num_layers-1}'] - y.reshape(-1, 1)\n        else:  # Multi-class classification\n            dZ = self.activations[f'A{self.num_layers-1}'] - y\n\n        # Backpropagate through layers\n        for i in range(self.num_layers - 1, 0, -1):\n            # Compute gradients\n            self.gradients[f'dW{i}'] = (1/m) * np.dot(self.activations[f'A{i-1}'].T, dZ)\n            self.gradients[f'db{i}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n\n            if i &gt; 1:  # Not the first layer\n                # Compute dA for previous layer\n                dA_prev = np.dot(dZ, self.weights[f'W{i}'].T)\n                # Compute dZ for previous layer (ReLU derivative)\n                dZ = dA_prev * self.relu_derivative(self.activations[f'Z{i-1}'])\n\n    def update_parameters(self):\n        \"\"\"Update weights and biases using gradients\"\"\"\n        for i in range(1, self.num_layers):\n            self.weights[f'W{i}'] -= self.learning_rate * self.gradients[f'dW{i}']\n            self.biases[f'b{i}'] -= self.learning_rate * self.gradients[f'db{i}']\n\n    def fit(self, X, y, epochs=1000, verbose=True):\n        \"\"\"\n        Train the neural network\n\n        Args:\n            X: Training data\n            y: Training labels\n            epochs: Number of training epochs\n            verbose: Whether to print training progress\n        \"\"\"\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward propagation\n            y_pred = self.forward_propagation(X)\n\n            # Compute loss\n            loss = self.compute_loss(y, y_pred)\n            losses.append(loss)\n\n            # Backward propagation\n            self.backward_propagation(X, y)\n\n            # Update parameters\n            self.update_parameters()\n\n            # Print progress\n            if verbose and epoch % 100 == 0:\n                accuracy = self.accuracy(y, y_pred)\n                print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n\n        return losses\n\n    def predict(self, X):\n        \"\"\"Make predictions on new data\"\"\"\n        y_pred = self.forward_propagation(X)\n\n        if self.layers[-1] == 1:  # Binary classification\n            return (y_pred &gt; 0.5).astype(int)\n        else:  # Multi-class classification\n            return np.argmax(y_pred, axis=1)\n\n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward_propagation(X)\n\n    def accuracy(self, y_true, y_pred):\n        \"\"\"Compute accuracy\"\"\"\n        if self.layers[-1] == 1:  # Binary classification\n            predictions = (y_pred &gt; 0.5).astype(int)\n            return np.mean(predictions == y_true.reshape(-1, 1))\n        else:  # Multi-class classification\n            predictions = np.argmax(y_pred, axis=1)\n            y_true_labels = np.argmax(y_true, axis=1) if y_true.ndim &gt; 1 else y_true\n            return np.mean(predictions == y_true_labels)\n\n# Demonstration with Moon dataset\ndef demo_neural_network():\n    \"\"\"Demonstrate neural network on moon dataset\"\"\"\n    # Generate moon dataset\n    X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Split data\n    split_idx = int(0.8 * len(X))\n    X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Create and train neural network\n    nn = NeuralNetwork(layers=[2, 10, 8, 1], learning_rate=0.1)\n\n    print(\"Training Neural Network...\")\n    losses = nn.fit(X_train, y_train, epochs=1000, verbose=True)\n\n    # Make predictions\n    train_pred = nn.predict(X_train)\n    test_pred = nn.predict(X_test)\n\n    train_accuracy = np.mean(train_pred == y_train.reshape(-1, 1))\n    test_accuracy = np.mean(test_pred == y_test.reshape(-1, 1))\n\n    print(f\"\\nFinal Results:\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n    # Visualize results\n    plt.figure(figsize=(15, 5))\n\n    # Plot loss curve\n    plt.subplot(1, 3, 1)\n    plt.plot(losses)\n    plt.title('Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n\n    # Plot original data\n    plt.subplot(1, 3, 2)\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n    plt.title('Original Data')\n    plt.colorbar(scatter)\n\n    # Plot decision boundary\n    plt.subplot(1, 3, 3)\n    h = 0.02\n    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = nn.predict_proba(mesh_points)\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')\n    scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', edgecolors='black')\n    plt.title('Decision Boundary')\n    plt.colorbar(scatter)\n\n    plt.tight_layout()\n    plt.show()\n\n    return nn\n\n# Run demonstration\nneural_network = demo_neural_network()\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#advanced-features-implementation","title":"Advanced Features Implementation","text":"<pre><code>class AdvancedNeuralNetwork(NeuralNetwork):\n    \"\"\"\n    Extended neural network with advanced features\n    \"\"\"\n\n    def __init__(self, layers, learning_rate=0.01, momentum=0.9, \n                 regularization=0.01, dropout_rate=0.5, random_seed=42):\n        super().__init__(layers, learning_rate, random_seed)\n\n        self.momentum = momentum\n        self.regularization = regularization\n        self.dropout_rate = dropout_rate\n\n        # Initialize momentum terms\n        self.velocity_w = {}\n        self.velocity_b = {}\n\n        for i in range(1, self.num_layers):\n            self.velocity_w[f'W{i}'] = np.zeros_like(self.weights[f'W{i}'])\n            self.velocity_b[f'b{i}'] = np.zeros_like(self.biases[f'b{i}'])\n\n    def dropout(self, A, training=True):\n        \"\"\"Apply dropout regularization\"\"\"\n        if training and self.dropout_rate &gt; 0:\n            mask = np.random.rand(*A.shape) &gt; self.dropout_rate\n            return A * mask / (1 - self.dropout_rate)\n        return A\n\n    def forward_propagation(self, X, training=True):\n        \"\"\"Forward propagation with dropout\"\"\"\n        self.activations['A0'] = X\n\n        for i in range(1, self.num_layers):\n            Z = np.dot(self.activations[f'A{i-1}'], self.weights[f'W{i}']) + self.biases[f'b{i}']\n            self.activations[f'Z{i}'] = Z\n\n            if i == self.num_layers - 1:  # Output layer\n                if self.layers[-1] == 1:\n                    A = self.sigmoid(Z)\n                else:\n                    A = self.softmax(Z)\n            else:  # Hidden layers\n                A = self.relu(Z)\n                A = self.dropout(A, training)  # Apply dropout\n\n            self.activations[f'A{i}'] = A\n\n        return self.activations[f'A{self.num_layers-1}']\n\n    def compute_loss_with_regularization(self, y_true, y_pred):\n        \"\"\"Compute loss with L2 regularization\"\"\"\n        base_loss = self.compute_loss(y_true, y_pred)\n\n        # Add L2 regularization\n        l2_penalty = 0\n        for i in range(1, self.num_layers):\n            l2_penalty += np.sum(self.weights[f'W{i}'] ** 2)\n\n        regularized_loss = base_loss + (self.regularization / 2) * l2_penalty\n        return regularized_loss\n\n    def update_parameters_with_momentum(self):\n        \"\"\"Update parameters using momentum\"\"\"\n        for i in range(1, self.num_layers):\n            # Add L2 regularization to gradients\n            reg_dW = self.gradients[f'dW{i}'] + self.regularization * self.weights[f'W{i}']\n\n            # Update velocity\n            self.velocity_w[f'W{i}'] = (self.momentum * self.velocity_w[f'W{i}'] - \n                                      self.learning_rate * reg_dW)\n            self.velocity_b[f'b{i}'] = (self.momentum * self.velocity_b[f'b{i}'] - \n                                      self.learning_rate * self.gradients[f'db{i}'])\n\n            # Update parameters\n            self.weights[f'W{i}'] += self.velocity_w[f'W{i}']\n            self.biases[f'b{i}'] += self.velocity_b[f'b{i}']\n\n    def fit(self, X, y, epochs=1000, verbose=True):\n        \"\"\"Train with advanced features\"\"\"\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward propagation (with dropout)\n            y_pred = self.forward_propagation(X, training=True)\n\n            # Compute loss with regularization\n            loss = self.compute_loss_with_regularization(y, y_pred)\n            losses.append(loss)\n\n            # Backward propagation\n            self.backward_propagation(X, y)\n\n            # Update parameters with momentum\n            self.update_parameters_with_momentum()\n\n            # Print progress\n            if verbose and epoch % 100 == 0:\n                # Use forward propagation without dropout for accuracy calculation\n                y_pred_eval = self.forward_propagation(X, training=False)\n                accuracy = self.accuracy(y, y_pred_eval)\n                print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n\n        return losses\n\n    def predict(self, X):\n        \"\"\"Make predictions without dropout\"\"\"\n        y_pred = self.forward_propagation(X, training=False)\n\n        if self.layers[-1] == 1:\n            return (y_pred &gt; 0.5).astype(int)\n        else:\n            return np.argmax(y_pred, axis=1)\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#assumptions-and-limitations","title":"\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Neural%20Networks/#assumptions","title":"Assumptions","text":"<p>Data Assumptions: - Independent and identically distributed (IID) data: Training and test data come from the same distribution - Sufficient training data: Need enough data to learn complex patterns without overfitting - Feature relevance: Input features contain useful information for the target variable - Stationarity: Data distribution doesn't change significantly over time</p> <p>Model Assumptions: - Universal approximation: Any continuous function can be approximated with sufficient neurons - Differentiability: Loss function and activations should be differentiable for backpropagation - Local minima acceptability: Finding global minimum is not required for good performance - Feature scaling: Input features should be normalized for optimal performance</p>"},{"location":"Machine-Learning/Neural%20Networks/#limitations","title":"Limitations","text":"<p>Computational Limitations: - High computational cost: Training can be expensive, especially for large networks - Memory requirements: Need to store activations, gradients, and parameters - Training time: Can take hours or days for complex problems - Hardware dependency: Performance varies significantly across different hardware</p> <p>Theoretical Limitations: - Black box nature: Difficult to interpret decisions and understand learned features - Overfitting tendency: Can memorize training data instead of learning generalizable patterns - Hyperparameter sensitivity: Performance highly dependent on architecture and parameter choices - Local minima: Gradient descent may get stuck in suboptimal solutions</p> <p>Practical Limitations: - Data hunger: Require large amounts of labeled data - Vanishing/exploding gradients: Deep networks suffer from gradient flow problems - Catastrophic forgetting: Forget previously learned tasks when learning new ones - Adversarial vulnerability: Small input perturbations can cause misclassification</p>"},{"location":"Machine-Learning/Neural%20Networks/#common-problems-and-solutions","title":"Common Problems and Solutions","text":"Problem Cause Solutions Overfitting Too complex model, insufficient data Dropout, regularization, early stopping, data augmentation Underfitting Too simple model, insufficient training More layers/neurons, longer training, reduce regularization Vanishing Gradients Deep networks, saturating activations ReLU, ResNet, LSTM, batch normalization Exploding Gradients Poor weight initialization, high learning rate Gradient clipping, proper initialization, lower learning rate Slow Convergence Poor optimization settings Adam optimizer, learning rate scheduling, batch normalization"},{"location":"Machine-Learning/Neural%20Networks/#when-to-use-neural-networks","title":"When to Use Neural Networks","text":"<p>Best suited for: - Large datasets with complex patterns - Image, text, and speech recognition - Non-linear relationships - Automatic feature learning - High-dimensional data</p> <p>Not ideal for: - Small datasets (&lt; 1000 samples) - Linear relationships - Interpretability is crucial - Limited computational resources - Simple problems with clear patterns</p>"},{"location":"Machine-Learning/Neural%20Networks/#interview-questions","title":"\u2753 Interview Questions","text":"Q1: Explain the backpropagation algorithm and its mathematical foundation. <p>Answer:</p> <p>Backpropagation is the algorithm used to train neural networks by computing gradients of the loss function with respect to network parameters.</p> <p>Mathematical Foundation: Uses the chain rule of calculus to compute partial derivatives:</p> \\[\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}}\\] <p>Steps: 1. Forward pass: Compute activations for all layers 2. Loss computation: Calculate loss at output layer 3. Backward pass: Compute gradients layer by layer from output to input 4. Parameter update: Update weights and biases using computed gradients</p> <p>Key insight: Error signals propagate backward through the network, with each layer's gradients depending on the subsequent layer's gradients.</p> Q2: What is the vanishing gradient problem and how can it be addressed? <p>Answer:</p> <p>Vanishing Gradient Problem: In deep networks, gradients become exponentially smaller as they propagate backward through layers, making early layers learn very slowly or not at all.</p> <p>Causes: - Sigmoid/tanh activation functions (derivatives d 0.25) - Weight initialization issues - Deep network architectures</p> <p>Solutions:</p> <ol> <li>ReLU Activation: <code>ReLU(x) = max(0, x)</code> has gradient 1 for positive inputs</li> <li>Proper Weight Initialization: He/Xavier initialization</li> <li>Batch Normalization: Normalizes inputs to each layer</li> <li>Residual Connections: Skip connections in ResNets</li> <li>LSTM/GRU: For sequential data</li> <li>Gradient Clipping: Prevent exploding gradients</li> </ol> <pre><code># Example: ReLU vs Sigmoid gradient\ndef sigmoid_derivative(x):\n    s = 1 / (1 + np.exp(-x))\n    return s * (1 - s)  # Max value: 0.25\n\ndef relu_derivative(x):\n    return (x &gt; 0).astype(float)  # Value: 0 or 1\n</code></pre> Q3: Compare different activation functions and their use cases. <p>Answer:</p> Activation Formula Range Derivative Use Case Pros Cons Sigmoid \\(\\frac{1}{1+e^{-x}}\\) (0,1) \\(\\sigma(x)(1-\\sigma(x))\\) Binary classification output Smooth, interpretable probabilities Vanishing gradients, not zero-centered Tanh \\(\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\) (-1,1) \\(1-\\tanh^2(x)\\) Hidden layers (legacy) Zero-centered, smooth Vanishing gradients ReLU \\(\\max(0,x)\\) [0,\u001e) \\(\\begin{cases} 1 &amp; x &gt; 0 \\\\ 0 &amp; x \\leq 0 \\end{cases}\\) Hidden layers Simple, no vanishing gradients Dead neurons, not zero-centered Leaky ReLU \\(\\begin{cases} x &amp; x &gt; 0 \\\\ \\alpha x &amp; x \\leq 0 \\end{cases}\\) (-\u001e,\u001e) \\(\\begin{cases} 1 &amp; x &gt; 0 \\\\ \\alpha &amp; x \\leq 0 \\end{cases}\\) Hidden layers Fixes dead ReLU problem Hyperparameter \u00b1 Softmax \\(\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\) (0,1), \\(\\sum=1\\) Complex Multi-class output Probability distribution Only for output layer <p>Recommendations: - Hidden layers: ReLU or Leaky ReLU - Binary output: Sigmoid - Multi-class output: Softmax - Regression output: Linear (no activation)</p> Q4: How do you prevent overfitting in neural networks? <p>Answer:</p> <p>Regularization Techniques:</p> <ol> <li> <p>Dropout: Randomly set neurons to zero during training    <pre><code>def dropout(x, keep_prob=0.5, training=True):\n    if training:\n        mask = np.random.binomial(1, keep_prob, x.shape) / keep_prob\n        return x * mask\n    return x\n</code></pre></p> </li> <li> <p>L1/L2 Regularization: Add penalty to loss function    \\(\\(L_{total} = L_{original} + \\lambda \\sum_{i} |w_i|\\)\\) (L1)    \\(\\(L_{total} = L_{original} + \\lambda \\sum_{i} w_i^2\\)\\) (L2)</p> </li> <li> <p>Early Stopping: Stop training when validation loss stops improving</p> </li> <li> <p>Data Augmentation: Artificially increase training data</p> </li> <li> <p>Batch Normalization: Normalize inputs to each layer</p> </li> <li> <p>Reduce Model Complexity: Fewer layers/neurons</p> </li> <li> <p>Cross-validation: Use k-fold validation for model selection</p> </li> </ol> <p>Implementation: <pre><code>model.add(keras.layers.Dropout(0.5))\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              regularizers=keras.regularizers.l2(0.01))\n</code></pre></p> Q5: Explain the differences between batch, mini-batch, and stochastic gradient descent. <p>Answer:</p> <p>Gradient Descent Variants:</p> <ol> <li>Batch Gradient Descent:</li> <li>Uses entire dataset for each update</li> <li>Formula: \\(w = w - \\alpha \\nabla_w J(w)\\)</li> <li>Pros: Stable convergence, guaranteed global minimum for convex functions</li> <li> <p>Cons: Slow for large datasets, memory intensive</p> </li> <li> <p>Stochastic Gradient Descent (SGD):</p> </li> <li>Uses one sample at a time</li> <li>Formula: \\(w = w - \\alpha \\nabla_w J(w; x^{(i)}, y^{(i)})\\)</li> <li>Pros: Fast updates, can escape local minima</li> <li> <p>Cons: Noisy updates, may oscillate around minimum</p> </li> <li> <p>Mini-batch Gradient Descent:</p> </li> <li>Uses small batches (typically 32-256 samples)</li> <li>Combines benefits of both approaches</li> <li>Pros: Balanced speed and stability, vectorization benefits</li> <li>Cons: Additional hyperparameter (batch size)</li> </ol> <p>Comparison: <pre><code># Batch size effects\nbatch_sizes = [1, 32, 128, len(X_train)]  # SGD, mini-batch, mini-batch, batch\nnames = ['SGD', 'Mini-batch (32)', 'Mini-batch (128)', 'Batch GD']\n</code></pre></p> <p>Modern Practice: Mini-batch GD with adaptive optimizers (Adam, RMSprop) is most common.</p> Q6: What is the Universal Approximation Theorem and what does it mean for neural networks? <p>Answer:</p> <p>Universal Approximation Theorem: A feedforward neural network with: - At least one hidden layer - Sufficient number of neurons - Non-linear activation functions</p> <p>Can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\) to arbitrary accuracy.</p> <p>Mathematical Statement: For any continuous function \\(f: [0,1]^n \\to \\mathbb{R}\\) and \\(\\epsilon &gt; 0\\), there exists a neural network \\(F\\) such that: \\(\\(|F(x) - f(x)| &lt; \\epsilon \\text{ for all } x \\in [0,1]^n\\)\\)</p> <p>Implications: - Theoretical: Neural networks are universal function approximators - Practical: Width vs depth trade-offs exist - Limitation: Says nothing about learnability or generalization - Reality: Need appropriate architecture, optimization, and data</p> <p>Important Notes: - Theorem guarantees approximation exists, not that SGD will find it - Doesn't specify required network size - Doesn't guarantee good generalization</p> Q7: How do you initialize weights in neural networks and why is it important? <p>Answer:</p> <p>Why Initialization Matters: - Breaks symmetry between neurons - Prevents vanishing/exploding gradients - Affects convergence speed and final performance</p> <p>Common Initialization Methods:</p> <ol> <li>Zero Initialization: </li> <li>All weights = 0</li> <li> <p>Problem: All neurons learn the same features (symmetry)</p> </li> <li> <p>Random Initialization:    <pre><code>W = np.random.randn(n_in, n_out) * 0.01\n</code></pre></p> </li> <li> <p>Problem: May cause vanishing gradients</p> </li> <li> <p>Xavier/Glorot Initialization:    <pre><code>W = np.random.randn(n_in, n_out) * np.sqrt(1/n_in)\n# or\nW = np.random.randn(n_in, n_out) * np.sqrt(2/(n_in + n_out))\n</code></pre></p> </li> <li> <p>Best for: Sigmoid, tanh activations</p> </li> <li> <p>He Initialization:    <pre><code>W = np.random.randn(n_in, n_out) * np.sqrt(2/n_in)\n</code></pre></p> </li> <li>Best for: ReLU activations</li> </ol> <p>Rule of thumb: Use He initialization with ReLU, Xavier with sigmoid/tanh.</p> Q8: Explain the concept of batch normalization and its benefits. <p>Answer:</p> <p>Batch Normalization: Normalizes inputs to each layer by adjusting and scaling activations.</p> <p>Mathematical Formula: For a layer with inputs \\(x_1, x_2, ..., x_m\\) (mini-batch):</p> \\[\\mu = \\frac{1}{m}\\sum_{i=1}^{m} x_i$$ $$\\sigma^2 = \\frac{1}{m}\\sum_{i=1}^{m} (x_i - \\mu)^2$$ $$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$ $$y_i = \\gamma \\hat{x}_i + \\beta\\] <p>Where \\(\\gamma\\) and \\(\\beta\\) are learnable parameters.</p> <p>Benefits: 1. Faster training: Higher learning rates possible 2. Reduced sensitivity: Less dependent on initialization 3. Regularization effect: Slight noise helps prevent overfitting 4. Gradient flow: Helps with vanishing gradient problem 5. Internal covariate shift: Reduces change in input distributions</p> <p>Implementation: <pre><code>model.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\n</code></pre></p> Q9: What are the differences between feed-forward, convolutional, and recurrent neural networks? <p>Answer:</p> Aspect Feedforward Convolutional (CNN) Recurrent (RNN) Architecture Layers connected sequentially Convolution + pooling layers Feedback connections Information Flow Input \u0092 Hidden \u0092 Output Local receptive fields Sequential processing Parameter Sharing No Yes (shared kernels) Yes (across time) Best For Tabular data, classification Images, spatial data Sequences, time series Key Advantage Simplicity, universal approximation Translation invariance Memory of past inputs Main Challenge Limited to fixed input sizes Large parameter count Vanishing gradients <p>Feedforward: <pre><code># Simple MLP\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(784,)),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n</code></pre></p> <p>CNN: <pre><code># For image classification\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n    MaxPooling2D((2,2)),\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Flatten(),\n    Dense(10, activation='softmax')\n])\n</code></pre></p> <p>RNN: <pre><code># For sequence data\nmodel = Sequential([\n    LSTM(50, return_sequences=True, input_shape=(timesteps, features)),\n    LSTM(50),\n    Dense(1)\n])\n</code></pre></p> Q10: How do you handle class imbalance in neural network classification? <p>Answer:</p> <p>Class Imbalance Strategies:</p> <ol> <li> <p>Class Weights: Penalize minority class errors more heavily    <pre><code>from sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight('balanced', \n                                  classes=np.unique(y_train), \n                                  y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\nmodel.fit(X_train, y_train, class_weight=class_weight_dict)\n</code></pre></p> </li> <li> <p>Resampling Techniques:</p> </li> <li>Oversampling: SMOTE, ADASYN</li> <li>Undersampling: Random undersampling</li> <li> <p>Combined: SMOTETomek</p> </li> <li> <p>Custom Loss Functions:    <pre><code>def weighted_binary_crossentropy(pos_weight):\n    def loss(y_true, y_pred):\n        return K.mean(-pos_weight * y_true * K.log(y_pred) - \n                    (1 - y_true) * K.log(1 - y_pred))\n    return loss\n</code></pre></p> </li> <li> <p>Focal Loss: Focuses on hard examples    <pre><code>def focal_loss(alpha=0.25, gamma=2.0):\n    def loss(y_true, y_pred):\n        pt = tf.where(y_true == 1, y_pred, 1 - y_pred)\n        return -alpha * (1 - pt) ** gamma * tf.log(pt)\n    return loss\n</code></pre></p> </li> <li> <p>Evaluation Metrics: Use precision, recall, F1-score, AUC-ROC instead of accuracy</p> </li> <li> <p>Threshold Tuning: Adjust classification threshold based on validation set</p> </li> </ol>"},{"location":"Machine-Learning/Neural%20Networks/#examples","title":"\ud83d\udca1 Examples","text":""},{"location":"Machine-Learning/Neural%20Networks/#real-world-example-image-classification-with-cifar-10","title":"Real-world Example: Image Classification with CIFAR-10","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load CIFAR-10 dataset\n(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\n# Class names\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Test data shape: {X_test.shape}\")\nprint(f\"Number of classes: {len(class_names)}\")\n\n# Normalize pixel values\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Convert labels to categorical\ny_train_cat = keras.utils.to_categorical(y_train, 10)\ny_test_cat = keras.utils.to_categorical(y_test, 10)\n\n# Create CNN model\ndef create_cnn_model():\n    model = keras.Sequential([\n        # First Convolutional Block\n        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(32, (3, 3), activation='relu'),\n        keras.layers.MaxPooling2D((2, 2)),\n        keras.layers.Dropout(0.25),\n\n        # Second Convolutional Block\n        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        keras.layers.MaxPooling2D((2, 2)),\n        keras.layers.Dropout(0.25),\n\n        # Third Convolutional Block\n        keras.layers.Conv2D(128, (3, 3), activation='relu'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.25),\n\n        # Dense Layers\n        keras.layers.Flatten(),\n        keras.layers.Dense(512, activation='relu'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    return model\n\n# Create and compile model\nmodel = create_cnn_model()\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(\"CNN Model Architecture:\")\nmodel.summary()\n\n# Data augmentation\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    zoom_range=0.1\n)\n\ndatagen.fit(X_train)\n\n# Callbacks\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, restore_best_weights=True)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n\n# Train model\nprint(\"Training CNN model...\")\nhistory = model.fit(datagen.flow(X_train, y_train_cat, batch_size=32),\n                    epochs=50,\n                    validation_data=(X_test, y_test_cat),\n                    callbacks=[early_stopping, reduce_lr],\n                    verbose=1)\n\n# Evaluate model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\nprint(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n\n# Make predictions\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test_cat, axis=1)\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true_classes, y_pred_classes, \n                          target_names=class_names))\n\n# Visualizations\nplt.figure(figsize=(18, 6))\n\n# Training history\nplt.subplot(1, 3, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Confusion matrix\nplt.subplot(1, 3, 3)\ncm = confusion_matrix(y_true_classes, y_pred_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nplt.tight_layout()\nplt.show()\n\n# Sample predictions visualization\ndef plot_predictions(images, true_labels, predicted_labels, class_names, num_samples=12):\n    plt.figure(figsize=(15, 8))\n    for i in range(num_samples):\n        plt.subplot(3, 4, i + 1)\n        plt.imshow(images[i])\n        plt.axis('off')\n\n        true_class = class_names[true_labels[i]]\n        pred_class = class_names[predicted_labels[i]]\n        confidence = np.max(y_pred[i]) * 100\n\n        color = 'green' if true_labels[i] == predicted_labels[i] else 'red'\n        plt.title(f'True: {true_class}\\nPred: {pred_class} ({confidence:.1f}%)', \n                 color=color, fontsize=10)\n\n    plt.tight_layout()\n    plt.show()\n\n# Show sample predictions\nplot_predictions(X_test[:12], y_true_classes[:12], y_pred_classes[:12], class_names)\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#time-series-prediction-with-rnnlstm","title":"Time Series Prediction with RNN/LSTM","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n# Generate synthetic time series data\ndef generate_time_series(n_samples=1000):\n    \"\"\"Generate synthetic time series with trend, seasonality, and noise\"\"\"\n    time = np.arange(n_samples)\n\n    # Trend component\n    trend = 0.02 * time\n\n    # Seasonal components\n    yearly = 10 * np.sin(2 * np.pi * time / 365.25)\n    monthly = 5 * np.sin(2 * np.pi * time / 30.4)\n    weekly = 3 * np.sin(2 * np.pi * time / 7)\n\n    # Noise\n    noise = np.random.normal(0, 2, n_samples)\n\n    # Combine components\n    series = 100 + trend + yearly + monthly + weekly + noise\n\n    return pd.Series(series, index=pd.date_range('2020-01-01', periods=n_samples, freq='D'))\n\n# Generate data\nts_data = generate_time_series(1000)\n\nprint(f\"Time series length: {len(ts_data)}\")\nprint(f\"Date range: {ts_data.index[0]} to {ts_data.index[-1]}\")\n\n# Prepare data for LSTM\ndef prepare_lstm_data(data, lookback_window=60, forecast_horizon=1):\n    \"\"\"\n    Prepare time series data for LSTM training\n\n    Args:\n        data: Time series data\n        lookback_window: Number of previous time steps to use as input\n        forecast_horizon: Number of time steps to predict\n\n    Returns:\n        X, y arrays for training\n    \"\"\"\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))\n\n    X, y = [], []\n\n    for i in range(lookback_window, len(scaled_data) - forecast_horizon + 1):\n        X.append(scaled_data[i-lookback_window:i, 0])\n        y.append(scaled_data[i:i+forecast_horizon, 0])\n\n    return np.array(X), np.array(y), scaler\n\n# Prepare data\nlookback = 60\nforecast_horizon = 10\n\nX, y, scaler = prepare_lstm_data(ts_data, lookback, forecast_horizon)\n\n# Reshape for LSTM (samples, timesteps, features)\nX = X.reshape((X.shape[0], X.shape[1], 1))\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Split data\ntrain_size = int(0.8 * len(X))\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Create LSTM model\ndef create_lstm_model(input_shape, forecast_horizon):\n    \"\"\"Create LSTM model for time series prediction\"\"\"\n    model = Sequential([\n        LSTM(50, return_sequences=True, input_shape=input_shape),\n        Dropout(0.2),\n        LSTM(50, return_sequences=True),\n        Dropout(0.2),\n        LSTM(50),\n        Dropout(0.2),\n        Dense(25),\n        Dense(forecast_horizon)\n    ])\n\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model\n\n# Build and train model\nlstm_model = create_lstm_model((lookback, 1), forecast_horizon)\n\nprint(\"LSTM Model Architecture:\")\nlstm_model.summary()\n\n# Train model\nhistory = lstm_model.fit(X_train, y_train,\n                        batch_size=32,\n                        epochs=50,\n                        validation_data=(X_test, y_test),\n                        verbose=1)\n\n# Make predictions\ntrain_predictions = lstm_model.predict(X_train)\ntest_predictions = lstm_model.predict(X_test)\n\n# Inverse transform predictions\ntrain_predictions = scaler.inverse_transform(train_predictions)\ntest_predictions = scaler.inverse_transform(test_predictions)\ny_train_orig = scaler.inverse_transform(y_train)\ny_test_orig = scaler.inverse_transform(y_test)\n\n# Calculate metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ntrain_mae = mean_absolute_error(y_train_orig.flatten(), train_predictions.flatten())\ntest_mae = mean_absolute_error(y_test_orig.flatten(), test_predictions.flatten())\ntrain_rmse = np.sqrt(mean_squared_error(y_train_orig.flatten(), train_predictions.flatten()))\ntest_rmse = np.sqrt(mean_squared_error(y_test_orig.flatten(), test_predictions.flatten()))\n\nprint(f\"\\nModel Performance:\")\nprint(f\"Train MAE: {train_mae:.4f}, Train RMSE: {train_rmse:.4f}\")\nprint(f\"Test MAE: {test_mae:.4f}, Test RMSE: {test_rmse:.4f}\")\n\n# Visualizations\nplt.figure(figsize=(18, 12))\n\n# Original time series\nplt.subplot(3, 2, 1)\nplt.plot(ts_data.index, ts_data.values)\nplt.title('Original Time Series')\nplt.xlabel('Date')\nplt.ylabel('Value')\n\n# Training history\nplt.subplot(3, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Training History')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Training predictions vs actual\nplt.subplot(3, 2, 3)\nplt.plot(y_train_orig[:, 0], label='Actual', alpha=0.7)\nplt.plot(train_predictions[:, 0], label='Predicted', alpha=0.7)\nplt.title('Training: Actual vs Predicted (First Step)')\nplt.xlabel('Sample')\nplt.ylabel('Value')\nplt.legend()\n\n# Test predictions vs actual\nplt.subplot(3, 2, 4)\nplt.plot(y_test_orig[:, 0], label='Actual', alpha=0.7)\nplt.plot(test_predictions[:, 0], label='Predicted', alpha=0.7)\nplt.title('Test: Actual vs Predicted (First Step)')\nplt.xlabel('Sample')\nplt.ylabel('Value')\nplt.legend()\n\n# Residuals plot\nplt.subplot(3, 2, 5)\ntest_residuals = y_test_orig[:, 0] - test_predictions[:, 0]\nplt.scatter(test_predictions[:, 0], test_residuals, alpha=0.5)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.title('Residuals Plot (Test Set)')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\n\n# Multi-step ahead predictions\nplt.subplot(3, 2, 6)\nsample_idx = 50\nactual_sequence = y_test_orig[sample_idx]\npredicted_sequence = test_predictions[sample_idx]\n\nplt.plot(range(len(actual_sequence)), actual_sequence, 'o-', label='Actual')\nplt.plot(range(len(predicted_sequence)), predicted_sequence, 's-', label='Predicted')\nplt.title(f'Multi-step Prediction (Sample {sample_idx})')\nplt.xlabel('Future Time Step')\nplt.ylabel('Value')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Feature importance analysis for time series\ndef analyze_lstm_importance(model, X_sample, scaler, n_steps=10):\n    \"\"\"Analyze which time steps are most important for prediction\"\"\"\n    baseline_pred = model.predict(X_sample.reshape(1, -1, 1))\n    importances = []\n\n    for i in range(len(X_sample)):\n        # Perturb each time step\n        X_perturbed = X_sample.copy()\n        X_perturbed[i] = np.mean(X_sample)  # Replace with mean\n\n        perturbed_pred = model.predict(X_perturbed.reshape(1, -1, 1))\n        importance = np.abs(baseline_pred - perturbed_pred).mean()\n        importances.append(importance)\n\n    return np.array(importances)\n\n# Analyze importance for a sample\nsample_importance = analyze_lstm_importance(lstm_model, X_test[0], scaler)\n\nplt.figure(figsize=(12, 4))\nplt.plot(range(len(sample_importance)), sample_importance)\nplt.title('Time Step Importance for Prediction')\nplt.xlabel('Time Step (from past)')\nplt.ylabel('Importance Score')\nplt.show()\n\nprint(f\"Most important time steps: {np.argsort(sample_importance)[-5:]}\")\n</code></pre>"},{"location":"Machine-Learning/Neural%20Networks/#references","title":"\ud83d\udcda References","text":"<p>Foundational Books: - Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville - Neural Networks and Deep Learning - Michael Nielsen - Pattern Recognition and Machine Learning - Christopher Bishop - The Elements of Statistical Learning - Hastie, Tibshirani, Friedman</p> <p>Classic Papers: - Backpropagation - Rumelhart, Hinton, Williams (1986) - Universal Approximation Theorem - Hornik, Stinchcombe, White (1989) - LSTM Networks - Hochreiter &amp; Schmidhuber (1997) - Dropout - Srivastava et al. (2014) - Batch Normalization - Ioffe &amp; Szegedy (2015)</p> <p>Modern Architectures: - ResNet - He et al. (2016) - Attention is All You Need - Vaswani et al. (2017) - BERT - Devlin et al. (2018) - GPT - Radford et al. (2018)</p> <p>Online Resources: - TensorFlow Tutorials - PyTorch Tutorials - Keras Documentation - CS231n: Convolutional Neural Networks - CS224n: Natural Language Processing</p> <p>Practical Guides: - Neural Networks and Deep Learning Course - Andrew Ng - FastAI Practical Deep Learning - MIT 6.034 Artificial Intelligence</p> <p>Specialized Topics: - Convolutional Neural Networks for Visual Recognition - Recurrent Neural Networks for Sequence Learning - Generative Adversarial Networks - Neural Architecture Search</p>"},{"location":"Machine-Learning/Normal%20Distribution/","title":"Normal Distribution","text":"<p>\ud83d\udd27#\ud83d\udd27 \ud83d\udd27\ud83d\udcca\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>The Normal Distribution (also called Gaussian Distribution) is the most important continuous probability distribution in statistics and machine learning, characterized by its symmetric bell-shaped curve and defined by two parameters: mean and standard deviation.</p> <p>Resources: SciPy Stats Documentation | Khan Academy Statistics | Elements of Statistical Learning</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27 \u000f Summary</p> <p>The Normal Distribution is a continuous probability distribution that is symmetric around its mean, with the shape determined by its standard deviation. It's fundamental to statistics and machine learning due to the Central Limit Theorem and its mathematical properties.</p> <p>Key Characteristics: - Bell-shaped curve: Symmetric around the mean - Unimodal: Single peak at the mean - Asymptotic: Tails approach zero but never reach it - Defined by two parameters: Mean (\u00bc) and standard deviation (\u00c3) - 68-95-99.7 rule: Empirical rule for data spread</p> <p>Standard Normal Distribution: - Mean (\u00bc) = 0 - Standard deviation (\u00c3) = 1 - Used for standardization and z-scores</p> <p>Applications in Machine Learning: - Assumption in algorithms: Linear regression, Naive Bayes, LDA - Initialization: Weight initialization in neural networks - Regularization: Gaussian priors in Bayesian methods - Noise modeling: Gaussian noise assumptions - Feature engineering: Normalization and standardization - Hypothesis testing: Statistical significance testing - Confidence intervals: Uncertainty quantification</p> <p>Real-world Examples: - Heights and weights of populations - Measurement errors in experiments - Financial returns (approximately) - IQ scores - Blood pressure measurements - Test scores and grades</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27\ud83e\udde0\ud83d\udd27 \ud83d\udd27I\ud83d\udd27n\ud83d\udd27t\ud83d\udd27u\ud83d\udd27i\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27M\ud83d\udd27a\ud83d\udd27t\ud83d\udd27h\ud83d\udd27e\ud83d\udd27m\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27F\ud83d\udd27o\ud83d\udd27u\ud83d\udd27n\ud83d\udd27d\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27b\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27D\ud83d\udd27e\ud83d\udd27n\ud83d\udd27s\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27F\ud83d\udd27u\ud83d\udd27n\ud83d\udd27c\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27(\ud83d\udd27P\ud83d\udd27D\ud83d\udd27F\ud83d\udd27)\ud83d\udd27</p> <p>The Normal Distribution is defined by its PDF:</p> \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] <p>Where: - \\(\\mu\\) is the mean (location parameter) - \\(\\sigma\\) is the standard deviation (scale parameter) - \\(\\sigma^2\\) is the variance - \\(e \\approx 2.718\\) (Euler's number) - \\(\\pi \\approx 3.14159\\)</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27t\ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27a\ud83d\udd27r\ud83d\udd27d\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>When \\(\\mu = 0\\) and \\(\\sigma = 1\\):</p> \\[\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}\\] <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27u\ud83d\udd27m\ud83d\udd27u\ud83d\udd27l\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27v\ud83d\udd27e\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27F\ud83d\udd27u\ud83d\udd27n\ud83d\udd27c\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27(\ud83d\udd27C\ud83d\udd27D\ud83d\udd27F\ud83d\udd27)\ud83d\udd27</p> \\[F(x) = P(X \\leq x) = \\int_{-\\infty}^{x} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(t-\\mu)^2}{2\\sigma^2}} dt\\] <p>For standard normal: \\(\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt\\)</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27Z\ud83d\udd27-\ud83d\udd27S\ud83d\udd27c\ud83d\udd27o\ud83d\udd27r\ud83d\udd27e\ud83d\udd27 \ud83d\udd27T\ud83d\udd27r\ud83d\udd27a\ud83d\udd27n\ud83d\udd27s\ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>Convert any normal distribution to standard normal:</p> \\[Z = \\frac{X - \\mu}{\\sigma}\\] <p>Where \\(Z \\sim N(0, 1)\\)</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27K\ud83d\udd27e\ud83d\udd27y\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27p\ud83d\udd27e\ud83d\udd27r\ud83d\udd27t\ud83d\udd27i\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27M\ud83d\udd27o\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27p\ud83d\udd27e\ud83d\udd27r\ud83d\udd27t\ud83d\udd27i\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>Mean (First Moment): \\(\\(E[X] = \\mu\\)\\)</p> <p>Variance (Second Central Moment): \\(\\(\\text{Var}(X) = E[(X-\\mu)^2] = \\sigma^2\\)\\)</p> <p>Skewness (Third Standardized Moment): \\(\\(\\text{Skewness} = E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right] = 0\\)\\)</p> <p>Kurtosis (Fourth Standardized Moment): \\(\\(\\text{Kurtosis} = E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right] = 3\\)\\)</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27T\ud83d\udd27h\ud83d\udd27e\ud83d\udd27 \ud83d\udd276\ud83d\udd278\ud83d\udd27-\ud83d\udd279\ud83d\udd275\ud83d\udd27-\ud83d\udd279\ud83d\udd279\ud83d\udd27.\ud83d\udd277\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27(\ud83d\udd27E\ud83d\udd27m\ud83d\udd27p\ud83d\udd27i\ud83d\udd27r\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27l\ud83d\udd27e\ud83d\udd27)\ud83d\udd27</p> <p>For any normal distribution: - 68% of data falls within 1 standard deviation: \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = 0.68\\) - 95% of data falls within 2 standard deviations: \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) = 0.95\\) - 99.7% of data falls within 3 standard deviations: \\(P(\\mu - 3\\sigma \\leq X \\leq \\mu + 3\\sigma) = 0.997\\)</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27n\ud83d\udd27e\ud83d\udd27a\ud83d\udd27r\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27m\ud83d\udd27b\ud83d\udd27i\ud83d\udd27n\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27</p> <p>If \\(X \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\) are independent:</p> \\[aX + bY \\sim N(a\\mu_1 + b\\mu_2, a^2\\sigma_1^2 + b^2\\sigma_2^2)\\] <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27m\ud83d\udd27i\ud83d\udd27t\ud83d\udd27 \ud83d\udd27T\ud83d\udd27h\ud83d\udd27e\ud83d\udd27o\ud83d\udd27r\ud83d\udd27e\ud83d\udd27m\ud83d\udd27</p> <p>For any population with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), the sampling distribution of the sample mean approaches normal as sample size increases:</p> \\[\\bar{X}_n \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ as } n \\to \\infty\\] <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27M\ud83d\udd27a\ud83d\udd27x\ud83d\udd27i\ud83d\udd27m\ud83d\udd27u\ud83d\udd27m\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27k\ud83d\udd27e\ud83d\udd27l\ud83d\udd27i\ud83d\udd27h\ud83d\udd27o\ud83d\udd27o\ud83d\udd27d\ud83d\udd27 \ud83d\udd27E\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27m\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>Given observations \\(x_1, x_2, ..., x_n\\) from \\(N(\\mu, \\sigma^2)\\):</p> <p>Log-likelihood: \\(\\(\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2\\)\\)</p> <p>MLE Estimators: \\(\\(\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\bar{x}\\)\\)</p> \\[\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\] <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27=\ud83d\udd27\"\ud83d\udd27 \ud83d\udd27I\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27u\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27b\ud83d\udd27r\ud83d\udd27a\ud83d\udd27r\ud83d\udd27i\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27U\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27N\ud83d\udd27u\ud83d\udd27m\ud83d\udd27P\ud83d\udd27y\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27 \ud83d\udd27S\ud83d\udd27c\ud83d\udd27i\ud83d\udd27P\ud83d\udd27y\ud83d\udd27</p> <pre><code>import numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27e\ud83d\udd27t\ud83d\udd27 \ud83d\udd27s\ud83d\udd27t\ud83d\udd27y\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27 \ud83d\udd27b\ud83d\udd27e\ud83d\udd27t\ud83d\udd27t\ud83d\udd27e\ud83d\udd27r\ud83d\udd27 \ud83d\udd27p\ud83d\udd27l\ud83d\udd27o\ud83d\udd27t\ud83d\udd27s\ud83d\udd27\nplt.style.use('seaborn-v0_8')\nnp.random.seed(42)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27G\ud83d\udd27e\ud83d\udd27n\ud83d\udd27e\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27s\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\ndef generate_normal_samples(mu=0, sigma=1, size=1000):\n    \"\"\"\n    Generate samples from normal distribution\n\n    Args:\n        mu: Mean parameter\n        sigma: Standard deviation parameter\n        size: Number of samples\n\n    Returns:\n        Array of samples\n    \"\"\"\n    return np.random.normal(mu, sigma, size)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27B\ud83d\udd27a\ud83d\udd27s\ud83d\udd27i\ud83d\udd27c\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27o\ud83d\udd27p\ud83d\udd27e\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nmu, sigma = 5, 2\nsamples = generate_normal_samples(mu, sigma, 10000)\n\nprint(f\"True parameters: \u00bc={mu}, \u00c3={sigma}\")\nprint(f\"Sample statistics: \u00bc={np.mean(samples):.3f}, \u00c3={np.std(samples, ddof=1):.3f}\")\nprint(f\"Sample size: {len(samples)}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27U\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27s\ud83d\udd27c\ud83d\udd27i\ud83d\udd27p\ud83d\udd27y\ud83d\udd27.\ud83d\udd27s\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27s\ud83d\udd27 \ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nnormal_dist = stats.norm(loc=mu, scale=sigma)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27a\ud83d\udd27l\ud83d\udd27c\ud83d\udd27u\ud83d\udd27l\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27p\ud83d\udd27r\ud83d\udd27o\ud83d\udd27b\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27i\ud83d\udd27e\ud83d\udd27s\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27 \ud83d\udd27q\ud83d\udd27u\ud83d\udd27a\ud83d\udd27n\ud83d\udd27t\ud83d\udd27i\ud83d\udd27l\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\nx_values = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\npdf_values = normal_dist.pdf(x_values)\ncdf_values = normal_dist.cdf(x_values)\n\nprint(f\"\\nProbability calculations:\")\nprint(f\"P(X d 7) = {normal_dist.cdf(7):.4f}\")\nprint(f\"P(X e 3) = {1 - normal_dist.cdf(3):.4f}\")\nprint(f\"P(3 d X d 7) = {normal_dist.cdf(7) - normal_dist.cdf(3):.4f}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27u\ud83d\udd27a\ud83d\udd27n\ud83d\udd27t\ud83d\udd27i\ud83d\udd27l\ud83d\udd27e\ud83d\udd27s\ud83d\udd27 \ud83d\udd27(\ud83d\udd27i\ud83d\udd27n\ud83d\udd27v\ud83d\udd27e\ud83d\udd27r\ud83d\udd27s\ud83d\udd27e\ud83d\udd27 \ud83d\udd27C\ud83d\udd27D\ud83d\udd27F\ud83d\udd27)\ud83d\udd27\nprint(f\"\\nQuantiles:\")\nprint(f\"25th percentile: {normal_dist.ppf(0.25):.3f}\")\nprint(f\"50th percentile (median): {normal_dist.ppf(0.5):.3f}\")\nprint(f\"75th percentile: {normal_dist.ppf(0.75):.3f}\")\nprint(f\"95th percentile: {normal_dist.ppf(0.95):.3f}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27E\ud83d\udd27m\ud83d\udd27p\ud83d\udd27i\ud83d\udd27r\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27r\ud83d\udd27u\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27v\ud83d\udd27e\ud83d\udd27r\ud83d\udd27i\ud83d\udd27f\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nwithin_1_sigma = np.sum(np.abs(samples - mu) &lt;= sigma) / len(samples)\nwithin_2_sigma = np.sum(np.abs(samples - mu) &lt;= 2*sigma) / len(samples)\nwithin_3_sigma = np.sum(np.abs(samples - mu) &lt;= 3*sigma) / len(samples)\n\nprint(f\"\\nEmpirical Rule Verification:\")\nprint(f\"Within 1\u00c3: {within_1_sigma:.3f} (expected: 0.683)\")\nprint(f\"Within 2\u00c3: {within_2_sigma:.3f} (expected: 0.954)\")\nprint(f\"Within 3\u00c3: {within_3_sigma:.3f} (expected: 0.997)\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27V\ud83d\udd27i\ud83d\udd27s\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27z\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.figure(figsize=(15, 12))\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27P\ud83d\udd27D\ud83d\udd27F\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27 \ud83d\udd27h\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27o\ud83d\udd27g\ud83d\udd27r\ud83d\udd27a\ud83d\udd27m\ud83d\udd27\nplt.subplot(3, 2, 1)\nplt.hist(samples, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\nplt.plot(x_values, pdf_values, 'r-', linewidth=2, label=f'PDF: N({mu}, {sigma}\u00b2)')\nplt.axvline(mu, color='red', linestyle='--', alpha=0.8, label=f'Mean = {mu}')\nplt.axvline(mu + sigma, color='orange', linestyle='--', alpha=0.8, label=f'\u00bc + \u00c3')\nplt.axvline(mu - sigma, color='orange', linestyle='--', alpha=0.8, label=f'\u00bc - \u00c3')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('Normal Distribution PDF with Histogram')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27D\ud83d\udd27F\ud83d\udd27\nplt.subplot(3, 2, 2)\nplt.plot(x_values, cdf_values, 'b-', linewidth=2, label='CDF')\nplt.axhline(0.5, color='red', linestyle='--', alpha=0.8, label='P = 0.5')\nplt.axvline(mu, color='red', linestyle='--', alpha=0.8, label=f'Median = {mu}')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.title('Normal Distribution CDF')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27-\ud83d\udd27Q\ud83d\udd27 \ud83d\udd27p\ud83d\udd27l\ud83d\udd27o\ud83d\udd27t\ud83d\udd27\nplt.subplot(3, 2, 3)\nstats.probplot(samples, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot: Testing Normality')\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27E\ud83d\udd27m\ud83d\udd27p\ud83d\udd27i\ud83d\udd27r\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27r\ud83d\udd27u\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27v\ud83d\udd27i\ud83d\udd27s\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27z\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.subplot(3, 2, 4)\nx_emp = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\ny_emp = stats.norm.pdf(x_emp, mu, sigma)\nplt.plot(x_emp, y_emp, 'k-', linewidth=2, label='PDF')\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27h\ud83d\udd27a\ud83d\udd27d\ud83d\udd27e\ud83d\udd27 \ud83d\udd27r\ud83d\udd27e\ud83d\udd27g\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nplt.fill_between(x_emp, 0, y_emp, where=((x_emp &gt;= mu-sigma) &amp; (x_emp &lt;= mu+sigma)), \n                alpha=0.3, color='blue', label='68% (1\u00c3)')\nplt.fill_between(x_emp, 0, y_emp, where=((x_emp &gt;= mu-2*sigma) &amp; (x_emp &lt;= mu+2*sigma)), \n                alpha=0.2, color='green', label='95% (2\u00c3)')\nplt.fill_between(x_emp, 0, y_emp, where=((x_emp &gt;= mu-3*sigma) &amp; (x_emp &lt;= mu+3*sigma)), \n                alpha=0.1, color='red', label='99.7% (3\u00c3)')\n\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('Empirical Rule (68-95-99.7)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27t\ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27a\ud83d\udd27r\ud83d\udd27d\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27c\ud83d\udd27o\ud83d\udd27m\ud83d\udd27p\ud83d\udd27a\ud83d\udd27r\ud83d\udd27i\ud83d\udd27s\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.subplot(3, 2, 5)\nz_scores = (samples - mu) / sigma\nplt.hist(z_scores, bins=50, density=True, alpha=0.7, color='lightgreen', edgecolor='black')\nx_std = np.linspace(-4, 4, 1000)\ny_std = stats.norm.pdf(x_std, 0, 1)\nplt.plot(x_std, y_std, 'r-', linewidth=2, label='Standard Normal N(0,1)')\nplt.xlabel('Z-score')\nplt.ylabel('Density')\nplt.title('Standardized Data vs Standard Normal')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27M\ud83d\udd27u\ud83d\udd27l\ud83d\udd27t\ud83d\udd27i\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27 \ud83d\udd27c\ud83d\udd27o\ud83d\udd27m\ud83d\udd27p\ud83d\udd27a\ud83d\udd27r\ud83d\udd27i\ud83d\udd27s\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.subplot(3, 2, 6)\nparams = [(0, 1), (0, 2), (2, 1), (-1, 0.5)]\ncolors = ['blue', 'red', 'green', 'orange']\nx_comp = np.linspace(-6, 6, 1000)\n\nfor (m, s), color in zip(params, colors):\n    y_comp = stats.norm.pdf(x_comp, m, s)\n    plt.plot(x_comp, y_comp, color=color, linewidth=2, \n            label=f'N({m}, {s}\u00b2)')\n\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.title('Comparison of Different Normal Distributions')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27T\ud83d\udd27e\ud83d\udd27s\ud83d\udd27t\ud83d\udd27s\ud83d\udd27 \ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27</p> <pre><code>from scipy.stats import shapiro, normaltest, anderson, kstest\nfrom sklearn.datasets import make_regression\n\ndef test_normality(data, alpha=0.05):\n    \"\"\"\n    Perform multiple tests for normality\n\n    Args:\n        data: Array of data to test\n        alpha: Significance level\n\n    Returns:\n        Dictionary with test results\n    \"\"\"\n    results = {}\n\n    # Shapiro-Wilk test\n    shapiro_stat, shapiro_p = shapiro(data)\n    results['Shapiro-Wilk'] = {\n        'statistic': shapiro_stat,\n        'p_value': shapiro_p,\n        'is_normal': shapiro_p &gt; alpha,\n        'interpretation': 'Normal' if shapiro_p &gt; alpha else 'Not Normal'\n    }\n\n    # D'Agostino-Pearson test\n    dp_stat, dp_p = normaltest(data)\n    results[\"D'Agostino-Pearson\"] = {\n        'statistic': dp_stat,\n        'p_value': dp_p,\n        'is_normal': dp_p &gt; alpha,\n        'interpretation': 'Normal' if dp_p &gt; alpha else 'Not Normal'\n    }\n\n    # Anderson-Darling test\n    ad_result = anderson(data, dist='norm')\n    # Use critical value for 5% significance level\n    critical_value = ad_result.critical_values[2]  # 5% level\n    results['Anderson-Darling'] = {\n        'statistic': ad_result.statistic,\n        'critical_value': critical_value,\n        'is_normal': ad_result.statistic &lt; critical_value,\n        'interpretation': 'Normal' if ad_result.statistic &lt; critical_value else 'Not Normal'\n    }\n\n    # Kolmogorov-Smirnov test\n    # First estimate parameters\n    mu_est, sigma_est = np.mean(data), np.std(data, ddof=1)\n    ks_stat, ks_p = kstest(data, lambda x: stats.norm.cdf(x, mu_est, sigma_est))\n    results['Kolmogorov-Smirnov'] = {\n        'statistic': ks_stat,\n        'p_value': ks_p,\n        'is_normal': ks_p &gt; alpha,\n        'interpretation': 'Normal' if ks_p &gt; alpha else 'Not Normal'\n    }\n\n    return results\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27T\ud83d\udd27e\ud83d\udd27s\ud83d\udd27t\ud83d\udd27 \ud83d\udd27w\ud83d\udd27i\ud83d\udd27t\ud83d\udd27h\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27f\ud83d\udd27f\ud83d\udd27e\ud83d\udd27r\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27 \ud83d\udd27t\ud83d\udd27y\ud83d\udd27p\ud83d\udd27e\ud83d\udd27s\ud83d\udd27 \ud83d\udd27o\ud83d\udd27f\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27t\ud83d\udd27a\ud83d\udd27\ndatasets = {\n    'Normal Data': np.random.normal(0, 1, 1000),\n    'Uniform Data': np.random.uniform(-2, 2, 1000),\n    'Exponential Data': np.random.exponential(1, 1000),\n    'Mixed Normal': np.concatenate([np.random.normal(-2, 1, 500), \n                                   np.random.normal(2, 1, 500)])\n}\n\nprint(\"Normality Test Results:\")\nprint(\"=\" * 80)\n\nfor name, data in datasets.items():\n    print(f\"\\n{name}:\")\n    print(f\"Mean: {np.mean(data):.3f}, Std: {np.std(data, ddof=1):.3f}\")\n\n    results = test_normality(data)\n\n    for test_name, result in results.items():\n        if 'p_value' in result:\n            print(f\"{test_name:20}: p={result['p_value']:.4f}, {result['interpretation']}\")\n        else:\n            print(f\"{test_name:20}: stat={result['statistic']:.4f}, {result['interpretation']}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27V\ud83d\udd27i\ud83d\udd27s\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27z\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27o\ud83d\udd27f\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27f\ud83d\udd27f\ud83d\udd27e\ud83d\udd27r\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27t\ud83d\udd27a\ud83d\udd27 \ud83d\udd27t\ud83d\udd27y\ud83d\udd27p\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\nplt.figure(figsize=(16, 10))\n\nfor i, (name, data) in enumerate(datasets.items()):\n    # Histogram\n    plt.subplot(2, 4, i+1)\n    plt.hist(data, bins=50, density=True, alpha=0.7, color=f'C{i}', edgecolor='black')\n\n    # Fit normal distribution\n    mu_fit, sigma_fit = stats.norm.fit(data)\n    x_fit = np.linspace(data.min(), data.max(), 100)\n    y_fit = stats.norm.pdf(x_fit, mu_fit, sigma_fit)\n    plt.plot(x_fit, y_fit, 'r-', linewidth=2, label=f'Fitted Normal')\n\n    plt.title(f'{name}')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.legend()\n\n    # Q-Q plot\n    plt.subplot(2, 4, i+5)\n    stats.probplot(data, dist=\"norm\", plot=plt)\n    plt.title(f'{name} - Q-Q Plot')\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27i\ud83d\udd27n\ud83d\udd27 \ud83d\udd27M\ud83d\udd27a\ud83d\udd27c\ud83d\udd27h\ud83d\udd27i\ud83d\udd27n\ud83d\udd27e\ud83d\udd27 \ud83d\udd27L\ud83d\udd27e\ud83d\udd27a\ud83d\udd27r\ud83d\udd27n\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27e\ud83d\udd27x\ud83d\udd27t\ud83d\udd27</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27D\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27a\ud83d\udd27s\ud83d\udd27s\ud83d\udd27u\ud83d\udd27m\ud83d\udd27p\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27 \ud83d\udd27i\ud83d\udd27n\ud83d\udd27 \ud83d\udd27M\ud83d\udd27L\ud83d\udd27\ndef demonstrate_ml_normality():\n    \"\"\"\n    Show how normal distribution is used in machine learning\n    \"\"\"\n\n    # Generate dataset\n    X, y = make_classification(n_samples=1000, n_features=4, n_redundant=0, \n                              n_informative=4, n_clusters_per_class=1, \n                              random_state=42)\n\n    feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]\n\n    print(\"Dataset Analysis:\")\n    print(f\"Dataset shape: {X.shape}\")\n    print(f\"Classes: {np.unique(y)}\")\n\n    # Analyze feature distributions\n    plt.figure(figsize=(16, 12))\n\n    for i in range(X.shape[1]):\n        # Histogram with normal overlay\n        plt.subplot(3, 4, i+1)\n        plt.hist(X[:, i], bins=30, density=True, alpha=0.7, color=f'C{i}')\n\n        # Fit and plot normal distribution\n        mu, sigma = stats.norm.fit(X[:, i])\n        x_range = np.linspace(X[:, i].min(), X[:, i].max(), 100)\n        plt.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 'r-', linewidth=2)\n        plt.title(f'{feature_names[i]} Distribution')\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n\n        # Q-Q plot\n        plt.subplot(3, 4, i+5)\n        stats.probplot(X[:, i], dist=\"norm\", plot=plt)\n        plt.title(f'{feature_names[i]} Q-Q Plot')\n\n        # Test normality\n        _, p_value = shapiro(X[:, i])\n        print(f\"{feature_names[i]} Shapiro-Wilk p-value: {p_value:.4f}\")\n\n    # Before and after standardization\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Plot comparison\n    for i in range(2):  # Just first 2 features for space\n        plt.subplot(3, 4, i+9)\n        plt.hist(X_scaled[:, i], bins=30, density=True, alpha=0.7, color='green')\n        x_std = np.linspace(-4, 4, 100)\n        plt.plot(x_std, stats.norm.pdf(x_std, 0, 1), 'r-', linewidth=2)\n        plt.title(f'{feature_names[i]} After Standardization')\n        plt.xlabel('Standardized Value')\n        plt.ylabel('Density')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train_scaled, X_test_scaled = train_test_split(X_scaled, test_size=0.2, random_state=42)\n\n    # Compare models with and without standardization\n    models = {\n        'Logistic Regression (Original)': LogisticRegression(random_state=42),\n        'Logistic Regression (Scaled)': LogisticRegression(random_state=42),\n        'Gaussian Naive Bayes (Original)': GaussianNB(),\n        'Gaussian Naive Bayes (Scaled)': GaussianNB()\n    }\n\n    results = {}\n\n    # Train and evaluate models\n    for i, (name, model) in enumerate(models.items()):\n        if 'Original' in name:\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n        else:\n            model.fit(X_train_scaled, y_train)\n            y_pred = model.predict(X_test_scaled)\n\n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y_test)\n        results[name] = accuracy\n\n        print(f\"\\n{name}:\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(\"\\nClassification Report:\")\n        print(classification_report(y_test, y_pred))\n\n    return results\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27n\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nml_results = demonstrate_ml_normality()\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27m\ud83d\udd27p\ud83d\udd27a\ud83d\udd27r\ud83d\udd27e\ud83d\udd27 \ud83d\udd27r\ud83d\udd27e\ud83d\udd27s\ud83d\udd27u\ud83d\udd27l\ud83d\udd27t\ud83d\udd27s\ud83d\udd27\nplt.figure(figsize=(10, 6))\nnames = list(ml_results.keys())\naccuracies = list(ml_results.values())\ncolors = ['blue', 'lightblue', 'red', 'lightcoral']\n\nbars = plt.bar(range(len(names)), accuracies, color=colors, alpha=0.7, edgecolor='black')\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.title('Model Performance: Original vs Standardized Features')\nplt.xticks(range(len(names)), [name.replace(' (Original)', '\\n(Original)').replace(' (Scaled)', '\\n(Scaled)') \n                               for name in names], rotation=0)\nplt.ylim(0, 1)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27A\ud83d\udd27d\ud83d\udd27d\ud83d\udd27 \ud83d\udd27v\ud83d\udd27a\ud83d\udd27l\ud83d\udd27u\ud83d\udd27e\ud83d\udd27 \ud83d\udd27l\ud83d\udd27a\ud83d\udd27b\ud83d\udd27e\ud83d\udd27l\ud83d\udd27s\ud83d\udd27 \ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27b\ud83d\udd27a\ud83d\udd27r\ud83d\udd27s\ud83d\udd27\nfor bar, acc in zip(bars, accuracies):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{acc:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27\u0099\ud83d\udd27\u000f\ud83d\udd27 \ud83d\udd27F\ud83d\udd27r\ud83d\udd27o\ud83d\udd27m\ud83d\udd27 \ud83d\udd27S\ud83d\udd27c\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27c\ud83d\udd27h\ud83d\udd27 \ud83d\udd27I\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27I\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Union, List, Tuple\n\nclass NormalDistribution:\n    \"\"\"\n    Complete implementation of Normal Distribution from scratch\n    \"\"\"\n\n    def __init__(self, mu: float = 0, sigma: float = 1):\n        \"\"\"\n        Initialize Normal Distribution\n\n        Args:\n            mu: Mean parameter\n            sigma: Standard deviation parameter (must be positive)\n        \"\"\"\n        if sigma &lt;= 0:\n            raise ValueError(\"Standard deviation must be positive\")\n\n        self.mu = mu\n        self.sigma = sigma\n        self.variance = sigma ** 2\n\n    def pdf(self, x: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Probability Density Function\n\n        Args:\n            x: Value(s) to evaluate\n\n        Returns:\n            PDF value(s)\n        \"\"\"\n        x = np.asarray(x)\n        coefficient = 1 / (self.sigma * np.sqrt(2 * np.pi))\n        exponent = -0.5 * ((x - self.mu) / self.sigma) ** 2\n        return coefficient * np.exp(exponent)\n\n    def cdf(self, x: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Cumulative Distribution Function using error function approximation\n\n        Args:\n            x: Value(s) to evaluate\n\n        Returns:\n            CDF value(s)\n        \"\"\"\n        x = np.asarray(x)\n        z = (x - self.mu) / (self.sigma * np.sqrt(2))\n        return 0.5 * (1 + self._erf(z))\n\n    def _erf(self, z: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Error function approximation using Abramowitz and Stegun formula\n\n        Args:\n            z: Input values\n\n        Returns:\n            Error function values\n        \"\"\"\n        # Constants for approximation\n        a1, a2, a3, a4, a5 = 0.254829592, -0.284496736, 1.421413741, -1.453152027, 1.061405429\n        p = 0.3275911\n\n        # Save the sign of z\n        sign = np.sign(z)\n        z = np.abs(z)\n\n        # A&amp;S formula 7.1.26\n        t = 1 / (1 + p * z)\n        y = 1 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * np.exp(-z * z)\n\n        return sign * y\n\n    def ppf(self, p: Union[float, np.ndarray]) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Percent Point Function (inverse CDF) using Beasley-Springer-Moro algorithm\n\n        Args:\n            p: Probability values (0 &lt; p &lt; 1)\n\n        Returns:\n            Quantile values\n        \"\"\"\n        p = np.asarray(p)\n\n        if np.any(p &lt;= 0) or np.any(p &gt;= 1):\n            raise ValueError(\"Probabilities must be between 0 and 1\")\n\n        # Convert to standard normal quantiles first\n        z = self._standard_normal_ppf(p)\n\n        # Transform to desired distribution\n        return self.mu + self.sigma * z\n\n    def _standard_normal_ppf(self, p: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Standard normal PPF using Beasley-Springer-Moro algorithm\n        \"\"\"\n        # Constants\n        a = [-3.969683028665376e+01, 2.209460984245205e+02, -2.759285104469687e+02,\n             1.383577518672690e+02, -3.066479806614716e+01, 2.506628277459239e+00]\n\n        b = [-5.447609879822406e+01, 1.615858368580409e+02, -1.556989798598866e+02,\n             6.680131188771972e+01, -1.328068155288572e+01]\n\n        c = [-7.784894002430293e-03, -3.223964580411365e-01, -2.400758277161838e+00,\n             -2.549732539343734e+00, 4.374664141464968e+00, 2.938163982698783e+00]\n\n        d = [7.784695709041462e-03, 3.224671290700398e-01, 2.445134137142996e+00,\n             3.754408661907416e+00]\n\n        p_low = 0.02425\n        p_high = 1 - p_low\n\n        result = np.zeros_like(p)\n\n        # Low region\n        mask_low = p &lt; p_low\n        if np.any(mask_low):\n            q = np.sqrt(-2 * np.log(p[mask_low]))\n            result[mask_low] = (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5]) / \\\n                              ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)\n\n        # Central region\n        mask_central = (p &gt;= p_low) &amp; (p &lt;= p_high)\n        if np.any(mask_central):\n            q = p[mask_central] - 0.5\n            r = q * q\n            result[mask_central] = (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5]) * q / \\\n                                  (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1)\n\n        # High region\n        mask_high = p &gt; p_high\n        if np.any(mask_high):\n            q = np.sqrt(-2 * np.log(1 - p[mask_high]))\n            result[mask_high] = -(((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5]) / \\\n                               ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)\n\n        return result\n\n    def sample(self, size: int = 1) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Generate random samples using Box-Muller transformation\n\n        Args:\n            size: Number of samples to generate\n\n        Returns:\n            Random samples\n        \"\"\"\n        # Box-Muller transformation\n        if size % 2 == 1:\n            size += 1\n            trim = True\n        else:\n            trim = False\n\n        # Generate uniform random numbers\n        u1 = np.random.uniform(0, 1, size // 2)\n        u2 = np.random.uniform(0, 1, size // 2)\n\n        # Box-Muller transformation\n        r = np.sqrt(-2 * np.log(u1))\n        theta = 2 * np.pi * u2\n\n        z1 = r * np.cos(theta)\n        z2 = r * np.sin(theta)\n\n        # Combine and transform to desired distribution\n        samples = np.concatenate([z1, z2])\n        samples = self.mu + self.sigma * samples\n\n        if trim:\n            samples = samples[:-1]\n\n        return samples[0] if len(samples) == 1 else samples\n\n    def fit(self, data: np.ndarray) -&gt; 'NormalDistribution':\n        \"\"\"\n        Fit normal distribution to data using Maximum Likelihood Estimation\n\n        Args:\n            data: Sample data\n\n        Returns:\n            Fitted NormalDistribution object\n        \"\"\"\n        data = np.asarray(data)\n        mu_mle = np.mean(data)\n        sigma_mle = np.std(data, ddof=0)  # MLE uses population std\n\n        return NormalDistribution(mu_mle, sigma_mle)\n\n    def log_likelihood(self, data: np.ndarray) -&gt; float:\n        \"\"\"\n        Calculate log-likelihood of data\n\n        Args:\n            data: Sample data\n\n        Returns:\n            Log-likelihood value\n        \"\"\"\n        data = np.asarray(data)\n        n = len(data)\n\n        ll = (-n/2) * np.log(2 * np.pi) - n * np.log(self.sigma) - \\\n             np.sum((data - self.mu)**2) / (2 * self.sigma**2)\n\n        return ll\n\n    def __str__(self) -&gt; str:\n        return f\"Normal(\u00bc={self.mu:.3f}, \u00c3={self.sigma:.3f})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"NormalDistribution(mu={self.mu}, sigma={self.sigma})\"\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27D\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27o\ud83d\udd27f\ud83d\udd27 \ud83d\udd27c\ud83d\udd27u\ud83d\udd27s\ud83d\udd27t\ud83d\udd27o\ud83d\udd27m\ud83d\udd27 \ud83d\udd27i\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\ndef demo_custom_normal():\n    \"\"\"Demonstrate custom normal distribution implementation\"\"\"\n\n    print(\"Custom Normal Distribution Implementation Demo\")\n    print(\"=\" * 50)\n\n    # Create distribution\n    norm = NormalDistribution(mu=2, sigma=1.5)\n    print(f\"Distribution: {norm}\")\n\n    # Generate samples\n    samples = norm.sample(10000)\n    print(f\"\\nGenerated {len(samples)} samples\")\n    print(f\"Sample mean: {np.mean(samples):.3f} (expected: {norm.mu})\")\n    print(f\"Sample std: {np.std(samples, ddof=1):.3f} (expected: {norm.sigma})\")\n\n    # Test PDF, CDF, PPF\n    test_values = np.array([-1, 0, 1, 2, 3, 4, 5])\n    print(f\"\\nFunction evaluations:\")\n    print(\"Value\\tPDF\\t\\tCDF\\t\\tPPF(CDF)\")\n\n    for val in test_values:\n        pdf_val = norm.pdf(val)\n        cdf_val = norm.cdf(val)\n        ppf_val = norm.ppf(cdf_val) if 0 &lt; cdf_val &lt; 1 else np.nan\n        print(f\"{val:.1f}\\t{pdf_val:.6f}\\t{cdf_val:.6f}\\t{ppf_val:.3f}\")\n\n    # Compare with scipy\n    import scipy.stats as stats\n    scipy_norm = stats.norm(norm.mu, norm.sigma)\n\n    print(f\"\\nComparison with SciPy (first 5 test values):\")\n    print(\"Value\\tCustom PDF\\tSciPy PDF\\tDiff PDF\\tCustom CDF\\tSciPy CDF\\tDiff CDF\")\n\n    for val in test_values[:5]:\n        custom_pdf = norm.pdf(val)\n        scipy_pdf = scipy_norm.pdf(val)\n        custom_cdf = norm.cdf(val)\n        scipy_cdf = scipy_norm.cdf(val)\n\n        print(f\"{val:.1f}\\t{custom_pdf:.6f}\\t{scipy_pdf:.6f}\\t{abs(custom_pdf-scipy_pdf):.2e}\\t\"\n              f\"{custom_cdf:.6f}\\t{scipy_cdf:.6f}\\t{abs(custom_cdf-scipy_cdf):.2e}\")\n\n    # Fit to data\n    fitted_norm = NormalDistribution().fit(samples)\n    print(f\"\\nFitted distribution: {fitted_norm}\")\n    print(f\"Log-likelihood: {fitted_norm.log_likelihood(samples):.2f}\")\n\n    # Visualization\n    plt.figure(figsize=(15, 10))\n\n    # PDF comparison\n    x = np.linspace(norm.mu - 4*norm.sigma, norm.mu + 4*norm.sigma, 1000)\n    custom_pdf = norm.pdf(x)\n    scipy_pdf = scipy_norm.pdf(x)\n\n    plt.subplot(2, 3, 1)\n    plt.plot(x, custom_pdf, 'b-', linewidth=2, label='Custom Implementation')\n    plt.plot(x, scipy_pdf, 'r--', linewidth=2, label='SciPy', alpha=0.7)\n    plt.xlabel('Value')\n    plt.ylabel('PDF')\n    plt.title('PDF Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # CDF comparison\n    custom_cdf = norm.cdf(x)\n    scipy_cdf = scipy_norm.cdf(x)\n\n    plt.subplot(2, 3, 2)\n    plt.plot(x, custom_cdf, 'b-', linewidth=2, label='Custom Implementation')\n    plt.plot(x, scipy_cdf, 'r--', linewidth=2, label='SciPy', alpha=0.7)\n    plt.xlabel('Value')\n    plt.ylabel('CDF')\n    plt.title('CDF Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Sample histogram with fitted PDF\n    plt.subplot(2, 3, 3)\n    plt.hist(samples, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n    plt.plot(x, norm.pdf(x), 'r-', linewidth=2, label='Original PDF')\n    plt.plot(x, fitted_norm.pdf(x), 'g--', linewidth=2, label='Fitted PDF')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.title('Samples with Original and Fitted PDF')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # PPF comparison\n    p_values = np.linspace(0.01, 0.99, 100)\n    custom_ppf = norm.ppf(p_values)\n    scipy_ppf = scipy_norm.ppf(p_values)\n\n    plt.subplot(2, 3, 4)\n    plt.plot(p_values, custom_ppf, 'b-', linewidth=2, label='Custom Implementation')\n    plt.plot(p_values, scipy_ppf, 'r--', linewidth=2, label='SciPy', alpha=0.7)\n    plt.xlabel('Probability')\n    plt.ylabel('Quantile')\n    plt.title('PPF (Quantile Function) Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Error analysis\n    pdf_errors = np.abs(custom_pdf - scipy_pdf)\n    cdf_errors = np.abs(custom_cdf - scipy_cdf)\n\n    plt.subplot(2, 3, 5)\n    plt.semilogy(x, pdf_errors, 'b-', linewidth=2, label='PDF Error')\n    plt.semilogy(x, cdf_errors, 'r-', linewidth=2, label='CDF Error')\n    plt.xlabel('Value')\n    plt.ylabel('Absolute Error (log scale)')\n    plt.title('Implementation Error Analysis')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Box-Muller samples vs normal samples\n    plt.subplot(2, 3, 6)\n    box_muller_samples = norm.sample(1000)\n    scipy_samples = scipy_norm.rvs(1000, random_state=42)\n\n    plt.hist(box_muller_samples, bins=30, alpha=0.5, label='Box-Muller', density=True)\n    plt.hist(scipy_samples, bins=30, alpha=0.5, label='SciPy', density=True)\n    plt.plot(x, norm.pdf(x), 'k-', linewidth=2, label='True PDF')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.title('Sample Generation Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return norm, samples\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27n\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\ncustom_norm, custom_samples = demo_custom_normal()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27A\ud83d\udd27d\ud83d\udd27v\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27e\ud83d\udd27d\ud83d\udd27 \ud83d\udd27S\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27M\ud83d\udd27e\ud83d\udd27t\ud83d\udd27h\ud83d\udd27o\ud83d\udd27d\ud83d\udd27s\ud83d\udd27</p> <pre><code>class AdvancedNormalAnalysis:\n    \"\"\"\n    Advanced methods for normal distribution analysis\n    \"\"\"\n\n    @staticmethod\n    def confidence_interval(data: np.ndarray, confidence: float = 0.95) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculate confidence interval for the mean\n\n        Args:\n            data: Sample data\n            confidence: Confidence level (0 &lt; confidence &lt; 1)\n\n        Returns:\n            Tuple of (lower_bound, upper_bound)\n        \"\"\"\n        n = len(data)\n        mean = np.mean(data)\n        std_err = np.std(data, ddof=1) / np.sqrt(n)\n\n        # For large n, use normal distribution\n        if n &gt;= 30:\n            from scipy import stats\n            z_score = stats.norm.ppf((1 + confidence) / 2)\n            margin_error = z_score * std_err\n        else:\n            # For small n, use t-distribution\n            from scipy import stats\n            t_score = stats.t.ppf((1 + confidence) / 2, n-1)\n            margin_error = t_score * std_err\n\n        return (mean - margin_error, mean + margin_error)\n\n    @staticmethod\n    def hypothesis_test_mean(data: np.ndarray, null_mean: float = 0, \n                           alternative: str = 'two-sided', alpha: float = 0.05) -&gt; dict:\n        \"\"\"\n        One-sample t-test for the mean\n\n        Args:\n            data: Sample data\n            null_mean: Hypothesized population mean\n            alternative: 'two-sided', 'greater', or 'less'\n            alpha: Significance level\n\n        Returns:\n            Dictionary with test results\n        \"\"\"\n        from scipy import stats\n\n        n = len(data)\n        sample_mean = np.mean(data)\n        sample_std = np.std(data, ddof=1)\n\n        # Test statistic\n        t_stat = (sample_mean - null_mean) / (sample_std / np.sqrt(n))\n\n        # P-value calculation\n        if alternative == 'two-sided':\n            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n-1))\n        elif alternative == 'greater':\n            p_value = 1 - stats.t.cdf(t_stat, n-1)\n        elif alternative == 'less':\n            p_value = stats.t.cdf(t_stat, n-1)\n        else:\n            raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n\n        # Critical value\n        if alternative == 'two-sided':\n            critical_value = stats.t.ppf(1 - alpha/2, n-1)\n        else:\n            critical_value = stats.t.ppf(1 - alpha, n-1)\n\n        reject_null = p_value &lt; alpha\n\n        return {\n            'sample_mean': sample_mean,\n            'null_mean': null_mean,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'critical_value': critical_value,\n            'reject_null': reject_null,\n            'conclusion': f\"{'Reject' if reject_null else 'Fail to reject'} the null hypothesis\",\n            'alpha': alpha,\n            'alternative': alternative\n        }\n\n    @staticmethod\n    def power_analysis(effect_size: float, alpha: float = 0.05, \n                      power: float = 0.8, alternative: str = 'two-sided') -&gt; int:\n        \"\"\"\n        Calculate required sample size for given power\n\n        Args:\n            effect_size: Cohen's d (standardized effect size)\n            alpha: Significance level\n            power: Desired statistical power\n            alternative: Type of test\n\n        Returns:\n            Required sample size\n        \"\"\"\n        from scipy import stats\n\n        # Z-scores for alpha and power\n        if alternative == 'two-sided':\n            z_alpha = stats.norm.ppf(1 - alpha/2)\n        else:\n            z_alpha = stats.norm.ppf(1 - alpha)\n\n        z_power = stats.norm.ppf(power)\n\n        # Sample size calculation\n        if alternative == 'two-sided':\n            n = ((z_alpha + z_power) / effect_size) ** 2\n        else:\n            n = ((z_alpha + z_power) / effect_size) ** 2\n\n        return int(np.ceil(n))\n\n    @staticmethod\n    def transformation_analysis(data: np.ndarray) -&gt; dict:\n        \"\"\"\n        Analyze data and suggest transformations to achieve normality\n\n        Args:\n            data: Sample data\n\n        Returns:\n            Dictionary with transformation analysis\n        \"\"\"\n        from scipy import stats\n\n        results = {\n            'original': {\n                'data': data,\n                'shapiro_p': stats.shapiro(data)[1],\n                'skewness': stats.skew(data),\n                'kurtosis': stats.kurtosis(data)\n            }\n        }\n\n        # Log transformation (for positive data)\n        if np.all(data &gt; 0):\n            log_data = np.log(data)\n            results['log'] = {\n                'data': log_data,\n                'shapiro_p': stats.shapiro(log_data)[1],\n                'skewness': stats.skew(log_data),\n                'kurtosis': stats.kurtosis(log_data)\n            }\n\n        # Square root transformation (for non-negative data)\n        if np.all(data &gt;= 0):\n            sqrt_data = np.sqrt(data)\n            results['sqrt'] = {\n                'data': sqrt_data,\n                'shapiro_p': stats.shapiro(sqrt_data)[1],\n                'skewness': stats.skew(sqrt_data),\n                'kurtosis': stats.kurtosis(sqrt_data)\n            }\n\n        # Box-Cox transformation\n        if np.all(data &gt; 0):\n            try:\n                boxcox_data, lambda_param = stats.boxcox(data)\n                results['boxcox'] = {\n                    'data': boxcox_data,\n                    'lambda': lambda_param,\n                    'shapiro_p': stats.shapiro(boxcox_data)[1],\n                    'skewness': stats.skew(boxcox_data),\n                    'kurtosis': stats.kurtosis(boxcox_data)\n                }\n            except:\n                pass\n\n        # Yeo-Johnson transformation (can handle negative values)\n        try:\n            yeojohnson_data, lambda_param = stats.yeojohnson(data)\n            results['yeojohnson'] = {\n                'data': yeojohnson_data,\n                'lambda': lambda_param,\n                'shapiro_p': stats.shapiro(yeojohnson_data)[1],\n                'skewness': stats.skew(yeojohnson_data),\n                'kurtosis': stats.kurtosis(yeojohnson_data)\n            }\n        except:\n            pass\n\n        # Find best transformation\n        best_transform = max(results.keys(), \n                           key=lambda k: results[k]['shapiro_p'])\n        results['best_transformation'] = best_transform\n\n        return results\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27D\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27o\ud83d\udd27f\ud83d\udd27 \ud83d\udd27a\ud83d\udd27d\ud83d\udd27v\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27e\ud83d\udd27d\ud83d\udd27 \ud83d\udd27m\ud83d\udd27e\ud83d\udd27t\ud83d\udd27h\ud83d\udd27o\ud83d\udd27d\ud83d\udd27s\ud83d\udd27\ndef demo_advanced_analysis():\n    \"\"\"Demonstrate advanced normal distribution analysis\"\"\"\n\n    # Generate different types of data\n    np.random.seed(42)\n\n    datasets = {\n        'Normal Data': np.random.normal(10, 2, 100),\n        'Skewed Data': np.random.exponential(1, 100),\n        'Heavy-tailed Data': np.random.standard_t(3, 100),\n        'Bimodal Data': np.concatenate([np.random.normal(5, 1, 50), \n                                       np.random.normal(15, 1, 50)])\n    }\n\n    analyzer = AdvancedNormalAnalysis()\n\n    print(\"Advanced Normal Distribution Analysis\")\n    print(\"=\" * 60)\n\n    for name, data in datasets.items():\n        print(f\"\\n{name}:\")\n        print(f\"Mean: {np.mean(data):.3f}, Std: {np.std(data, ddof=1):.3f}\")\n\n        # Confidence interval\n        ci = analyzer.confidence_interval(data, 0.95)\n        print(f\"95% CI for mean: ({ci[0]:.3f}, {ci[1]:.3f})\")\n\n        # Hypothesis test (test if mean = 10)\n        test_result = analyzer.hypothesis_test_mean(data, null_mean=10)\n        print(f\"T-test (H0: \u00bc=10): t={test_result['t_statistic']:.3f}, \"\n              f\"p={test_result['p_value']:.4f}, {test_result['conclusion']}\")\n\n        # Power analysis\n        effect_size = abs(np.mean(data) - 10) / np.std(data, ddof=1)\n        required_n = analyzer.power_analysis(effect_size, power=0.8)\n        print(f\"Required sample size for 80% power: {required_n}\")\n\n        # Transformation analysis\n        transforms = analyzer.transformation_analysis(data)\n        print(f\"Best transformation: {transforms['best_transformation']} \"\n              f\"(Shapiro p-value: {transforms[transforms['best_transformation']]['shapiro_p']:.4f})\")\n\n    # Visualization of transformations\n    plt.figure(figsize=(16, 12))\n\n    for i, (name, data) in enumerate(datasets.items()):\n        transforms = analyzer.transformation_analysis(data)\n\n        # Original data\n        plt.subplot(4, 4, i*4 + 1)\n        plt.hist(data, bins=20, density=True, alpha=0.7, color=f'C{i}')\n        plt.title(f'{name}\\n(Original)')\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n\n        # Best transformation\n        best_transform = transforms['best_transformation']\n        if best_transform != 'original':\n            best_data = transforms[best_transform]['data']\n\n            plt.subplot(4, 4, i*4 + 2)\n            plt.hist(best_data, bins=20, density=True, alpha=0.7, color=f'C{i}')\n            plt.title(f'{name}\\n({best_transform.title()})')\n            plt.xlabel('Transformed Value')\n            plt.ylabel('Density')\n\n            # Q-Q plots\n            plt.subplot(4, 4, i*4 + 3)\n            stats.probplot(data, dist=\"norm\", plot=plt)\n            plt.title('Original Q-Q Plot')\n\n            plt.subplot(4, 4, i*4 + 4)\n            stats.probplot(best_data, dist=\"norm\", plot=plt)\n            plt.title(f'{best_transform.title()} Q-Q Plot')\n        else:\n            plt.subplot(4, 4, i*4 + 2)\n            plt.text(0.5, 0.5, 'No transformation\\nneeded', \n                    ha='center', va='center', transform=plt.gca().transAxes)\n            plt.axis('off')\n\n            plt.subplot(4, 4, i*4 + 3)\n            stats.probplot(data, dist=\"norm\", plot=plt)\n            plt.title('Q-Q Plot')\n\n            plt.subplot(4, 4, i*4 + 4)\n            plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27n\ud83d\udd27 \ud83d\udd27a\ud83d\udd27d\ud83d\udd27v\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27e\ud83d\udd27d\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27m\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\ndemo_advanced_analysis()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27\u00a0\ud83d\udd27\u000f\ud83d\udd27 \ud83d\udd27A\ud83d\udd27s\ud83d\udd27s\ud83d\udd27u\ud83d\udd27m\ud83d\udd27p\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27d\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27m\ud83d\udd27i\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27A\ud83d\udd27s\ud83d\udd27s\ud83d\udd27u\ud83d\udd27m\ud83d\udd27p\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27</p> <p>Mathematical Assumptions: - Continuous data: Variables are measured on a continuous scale - Independence: Observations are independent of each other - Infinite support: Theoretically, values can range from -\u001e to +\u001e - Symmetry: Distribution is perfectly symmetric around the mean - Single mode: Only one peak in the distribution</p> <p>Statistical Assumptions in ML: - IID samples: Data points are independently and identically distributed - Stationarity: Distribution parameters don't change over time - Linearity: Linear relationships between variables (in linear models) - Homoscedasticity: Constant variance across all levels of independent variables - No outliers: Extreme values don't significantly affect the distribution</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27L\ud83d\udd27i\ud83d\udd27m\ud83d\udd27i\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27</p> <p>Theoretical Limitations: - Infinite tails: Assigns non-zero probability to extreme values (can be unrealistic) - Symmetry assumption: Real-world data often shows skewness - Single modality: Cannot model multimodal distributions - Parameter sensitivity: Small changes in \u00bc or \u00c3 can significantly affect probabilities - Curse of dimensionality: In high dimensions, most data lies far from the center</p> <p>Practical Limitations: - Finite data: Real datasets have finite ranges, unlike theoretical normal distribution - Measurement precision: Discrete measurements approximate continuous distributions - Outlier sensitivity: Sample statistics heavily influenced by extreme values - Model assumptions: Many statistical tests assume normality but real data may not follow this - Transformation needs: Data often requires preprocessing to achieve normality</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27W\ud83d\udd27h\ud83d\udd27e\ud83d\udd27n\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27A\ud83d\udd27s\ud83d\udd27s\ud83d\udd27u\ud83d\udd27m\ud83d\udd27p\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27 \ud83d\udd27F\ud83d\udd27a\ud83d\udd27i\ud83d\udd27l\ud83d\udd27</p> <p>Common Violations:</p> Issue Description Detection Solutions Skewness Asymmetric distribution Histogram, skewness statistic Log transform, Box-Cox Heavy tails More extreme values than expected Kurtosis, Q-Q plots Robust methods, t-distribution Multimodality Multiple peaks Histogram, density plots Mixture models, clustering Discrete data Integer or categorical values Data inspection Poisson, binomial models Bounded data Limited range (e.g., percentages) Domain knowledge Beta distribution, logit transform <p>Diagnostic Tools: - Visual: Histograms, Q-Q plots, box plots - Statistical: Shapiro-Wilk, Anderson-Darling, Kolmogorov-Smirnov tests - Descriptive: Skewness, kurtosis, range checks</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27o\ud83d\udd27b\ud83d\udd27u\ud83d\udd27s\ud83d\udd27t\ud83d\udd27 \ud83d\udd27A\ud83d\udd27l\ud83d\udd27t\ud83d\udd27e\ud83d\udd27r\ud83d\udd27n\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27v\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>When to Use Alternatives:</p> <ol> <li>t-Distribution: For heavy-tailed data or small samples</li> <li>Log-normal: For positively skewed data</li> <li>Gamma/Exponential: For non-negative, skewed data  </li> <li>Beta: For bounded data (0,1)</li> <li>Mixture Models: For multimodal data</li> <li>Non-parametric Methods: When no distributional assumptions can be made</li> </ol> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27=\ud83d\udd27\u00a1\ud83d\udd27 \ud83d\udd27I\ud83d\udd27n\ud83d\udd27t\ud83d\udd27e\ud83d\udd27r\ud83d\udd27v\ud83d\udd27i\ud83d\udd27e\ud83d\udd27w\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27u\ud83d\udd27e\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27</p> Q1: Explain the difference between normal distribution and standard normal distribution. <p>Answer:</p> <p>Normal Distribution: - General form: \\(N(\\mu, \\sigma^2)\\) - Can have any mean \\(\\mu\\) and standard deviation \\(\\sigma &gt; 0\\) - PDF: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> <p>Standard Normal Distribution: - Special case: \\(N(0, 1)\\) - Mean = 0, Standard deviation = 1 - PDF: \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}\\) - Also called z-distribution</p> <p>Relationship: Any normal distribution can be converted to standard normal using z-score transformation: \\(\\(Z = \\frac{X - \\mu}{\\sigma}\\)\\)</p> <p>Why it's important: - Standardizes comparisons across different scales - Simplifies probability calculations - Used in hypothesis testing and confidence intervals - Tables and software often reference standard normal</p> Q2: What is the Central Limit Theorem and why is it important for the normal distribution? <p>Answer:</p> <p>Central Limit Theorem (CLT) states: For any population with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), the distribution of sample means approaches normal as sample size increases:</p> \\[\\bar{X}_n \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ as } n \\to \\infty\\] <p>Key Points: - Original population can have ANY distribution - Sample size typically needs to be e30 for good approximation - Larger samples \u0092 better normal approximation - Standard error decreases as \\(\\frac{\\sigma}{\\sqrt{n}}\\)</p> <p>Importance: 1. Statistical Inference: Enables confidence intervals and hypothesis tests 2. Machine Learning: Justifies normality assumptions in many algorithms 3. Quality Control: Control charts based on sample means 4. A/B Testing: Comparison of group means</p> <p>Example: Even if individual heights are not perfectly normal, the average height of groups of 30+ people will be approximately normal.</p> Q3: How do you test if data follows a normal distribution? <p>Answer:</p> <p>Visual Methods: 1. Histogram: Should show bell-shaped curve 2. Q-Q Plot: Points should lie on straight line 3. Box Plot: Should be symmetric with few outliers</p> <p>Statistical Tests:</p> <ol> <li>Shapiro-Wilk Test (best for n &lt; 50):</li> <li>H\u0080: Data is normally distributed</li> <li>Most powerful test for normality</li> <li> <p><code>scipy.stats.shapiro(data)</code></p> </li> <li> <p>Anderson-Darling Test:</p> </li> <li>More sensitive to tail deviations</li> <li> <p><code>scipy.stats.anderson(data, dist='norm')</code></p> </li> <li> <p>Kolmogorov-Smirnov Test:</p> </li> <li>Tests against fitted normal distribution</li> <li> <p>Less powerful than others</p> </li> <li> <p>D'Agostino-Pearson Test:</p> </li> <li>Based on skewness and kurtosis</li> <li><code>scipy.stats.normaltest(data)</code></li> </ol> <p>Rule of Thumb: - Use multiple methods together - Visual inspection is crucial - Tests may reject normality for large samples due to minor deviations - Consider practical significance, not just statistical significance</p> Q4: What are the parameters of normal distribution and how do they affect the shape? <p>Answer:</p> <p>Parameters:</p> <ol> <li>Mean (\u00bc) - Location parameter:</li> <li>Determines center of distribution</li> <li>Peak of bell curve occurs at \u00bc</li> <li>Shifting \u00bc moves entire curve left/right</li> <li> <p>Range: \\(-\\infty &lt; \\mu &lt; \\infty\\)</p> </li> <li> <p>Standard Deviation (\u00c3) - Scale parameter:</p> </li> <li>Determines spread/width of distribution</li> <li>Controls how dispersed values are around mean</li> <li>Larger \u00c3 \u0092 wider, flatter curve</li> <li>Smaller \u00c3 \u0092 narrower, taller curve</li> <li>Range: \\(\u00c3 &gt; 0\\)</li> </ol> <p>Shape Effects: <pre><code>\u00bc = 0, \u00c3 = 1: Standard normal (tall, narrow)\n\u00bc = 0, \u00c3 = 2: Same center, wider spread\n\u00bc = 5, \u00c3 = 1: Shifted right, same spread\n\u00bc = 5, \u00c3 = 2: Shifted right, wider spread\n</code></pre></p> <p>Mathematical Properties: - Mode = Median = Mean = \u00bc - Inflection points at \u00bc \u00b1 \u00c3 - 68% of data within \u00bc \u00b1 \u00c3 - 95% of data within \u00bc \u00b1 2\u00c3 - 99.7% of data within \u00bc \u00b1 3\u00c3</p> Q5: Explain the 68-95-99.7 rule (Empirical Rule) and its applications. <p>Answer:</p> <p>The Empirical Rule states: For any normal distribution: - 68% of values lie within 1 standard deviation: \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = 0.6827\\) - 95% of values lie within 2 standard deviations: \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) = 0.9545\\) - 99.7% of values lie within 3 standard deviations: \\(P(\\mu - 3\\sigma \\leq X \\leq \\mu + 3\\sigma) = 0.9973\\)</p> <p>Applications:</p> <ol> <li>Quality Control:</li> <li>Products outside 3\u00c3 limits considered defective</li> <li> <p>Six Sigma methodology aims for 6\u00c3 quality</p> </li> <li> <p>Outlier Detection:</p> </li> <li>Values beyond 2\u00c3 or 3\u00c3 flagged as outliers</li> <li> <p>Z-scores &gt; 3 are rare (0.3% probability)</p> </li> <li> <p>Risk Assessment:</p> </li> <li>Financial returns: VaR calculations</li> <li> <p>Insurance: Claim amount predictions</p> </li> <li> <p>Educational Testing:</p> </li> <li>Standardized test scores (SAT, GRE)</li> <li> <p>Grade curving and percentile ranks</p> </li> <li> <p>Medical Diagnostics:</p> </li> <li>Normal ranges for lab values</li> <li>Growth charts for children</li> </ol> <p>Example: If IQ scores are N(100, 15): - 68% of people have IQ between 85-115 - 95% have IQ between 70-130 - 99.7% have IQ between 55-145</p> Q6: How is normal distribution used in machine learning algorithms? <p>Answer:</p> <p>Direct Usage:</p> <ol> <li>Naive Bayes Classifier:</li> <li>Assumes features follow normal distribution</li> <li> <p>Uses Gaussian likelihood: \\(P(x_i|y) = N(\\mu_{i,y}, \\sigma_{i,y}^2)\\)</p> </li> <li> <p>Linear Regression:</p> </li> <li>Assumes residuals are normally distributed</li> <li> <p>Enables confidence intervals and hypothesis tests</p> </li> <li> <p>Discriminant Analysis (LDA/QDA):</p> </li> <li>Assumes classes have multivariate normal distributions</li> <li>Decision boundaries based on Gaussian densities</li> </ol> <p>Indirect Usage:</p> <ol> <li>Weight Initialization:</li> <li>Neural networks: Xavier/He initialization</li> <li> <p>Random weights from normal distribution</p> </li> <li> <p>Regularization:</p> </li> <li>Gaussian priors in Bayesian methods</li> <li> <p>L2 regularization equivalent to normal prior</p> </li> <li> <p>Feature Engineering:</p> </li> <li>Box-Cox transformation to achieve normality</li> <li> <p>StandardScaler assumes normal-like distribution</p> </li> <li> <p>Uncertainty Quantification:</p> </li> <li>Bayesian neural networks</li> <li> <p>Gaussian processes</p> </li> <li> <p>Generative Models:</p> </li> <li>VAE latent space often assumed normal</li> <li>Normalizing flows</li> </ol> <p>Why Normal Distribution is Preferred: - Mathematical tractability - Central Limit Theorem justification - Maximum entropy for given mean and variance - Conjugate priors in Bayesian inference</p> Q7: What is the relationship between normal distribution and maximum likelihood estimation? <p>Answer:</p> <p>MLE for Normal Distribution:</p> <p>Given samples \\(x_1, x_2, ..., x_n\\) from \\(N(\\mu, \\sigma^2)\\):</p> <p>Likelihood Function: \\(\\(L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\\)\\)</p> <p>Log-Likelihood: \\(\\(\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - n\\ln(\\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i-\\mu)^2\\)\\)</p> <p>MLE Estimators: Taking derivatives and setting to zero:</p> \\[\\hat{\\mu}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\bar{x}\\] \\[\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\] <p>Key Properties: - Sample mean is unbiased: \\(E[\\hat{\\mu}] = \\mu\\) - MLE variance estimator is biased: \\(E[\\hat{\\sigma}^2_{MLE}] = \\frac{n-1}{n}\\sigma^2\\) - Unbiased estimator uses \\(n-1\\) in denominator - Both estimators are consistent and efficient</p> <p>Connection to Machine Learning: - Linear regression with Gaussian noise assumption - Foundation for many statistical tests - Basis for confidence intervals</p> Q8: How do you handle non-normal data in machine learning? <p>Answer:</p> <p>Detection Methods: - Visual inspection (histograms, Q-Q plots) - Statistical tests (Shapiro-Wilk, Anderson-Darling) - Skewness and kurtosis analysis</p> <p>Transformation Techniques:</p> <ol> <li>Log Transformation: For right-skewed data</li> <li>\\(y = \\log(x)\\) (requires \\(x &gt; 0\\))</li> <li> <p>Reduces positive skewness</p> </li> <li> <p>Square Root: For count data or mild skewness</p> </li> <li> <p>\\(y = \\sqrt{x}\\) (requires \\(x \\geq 0\\))</p> </li> <li> <p>Box-Cox Transformation: For positive data</p> </li> <li>\\(y = \\frac{x^{\\lambda} - 1}{\\lambda}\\) (if \\(\\lambda \\neq 0\\))</li> <li>\\(y = \\log(x)\\) (if \\(\\lambda = 0\\))</li> <li> <p>Automatically finds optimal \u00bb</p> </li> <li> <p>Yeo-Johnson: Handles negative values</p> </li> <li>Extension of Box-Cox for all real numbers</li> </ol> <p>Alternative Approaches:</p> <ol> <li>Robust Methods:</li> <li>Use median instead of mean</li> <li>Robust regression (Huber loss)</li> <li> <p>Trimmed statistics</p> </li> <li> <p>Non-parametric Methods:</p> </li> <li>Random Forest, SVM</li> <li>k-NN, Decision Trees</li> <li> <p>No distributional assumptions</p> </li> <li> <p>Different Distributions:</p> </li> <li>Poisson for count data</li> <li>Binomial for binary outcomes  </li> <li> <p>Exponential for survival times</p> </li> <li> <p>Ensemble Methods:</p> </li> <li>Bootstrap aggregating</li> <li>Less sensitive to individual distributions</li> </ol> Q9: What is standardization and why is it important when features follow normal distributions? <p>Answer:</p> <p>Standardization (Z-score normalization): Transform features to have mean=0 and std=1:</p> \\[z = \\frac{x - \\mu}{\\sigma}\\] <p>Why Important for Normal Data:</p> <ol> <li>Scale Independence:</li> <li>Features with different units become comparable</li> <li> <p>Example: Age (0-100) vs Income (\\(0-\\)100,000)</p> </li> <li> <p>Algorithm Performance:</p> </li> <li>Distance-based algorithms (k-NN, SVM, k-means)</li> <li>Gradient descent convergence</li> <li> <p>Neural network training stability</p> </li> <li> <p>Mathematical Properties:</p> </li> <li>Preserves normal distribution shape</li> <li>Standardized normal has known properties</li> <li>Enables use of z-tables and standard formulas</li> </ol> <p>When Normal Assumption Helps: - 68-95-99.7 rule applies after standardization - Outlier detection using z-scores - Statistical tests and confidence intervals - Feature importance comparison</p> <p>Implementation: <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre></p> <p>Alternative: Min-Max Normalization: - Scales to [0,1] range - Better when distribution is not normal - Preserves original distribution shape</p> Q10: Explain the concept of multivariate normal distribution and its applications. <p>Answer:</p> <p>Multivariate Normal Distribution: Extension of normal distribution to multiple variables:</p> \\[\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\] <p>PDF: \\(\\(f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{k/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)\\)\\)</p> <p>Where: - \\(\\mathbf{x} = [x_1, x_2, ..., x_k]^T\\) is k-dimensional vector - \\(\\boldsymbol{\\mu}\\) is mean vector - \\(\\boldsymbol{\\Sigma}\\) is covariance matrix</p> <p>Key Properties: - Marginal distributions are normal - Linear combinations are normal - Conditional distributions are normal - Zero correlation implies independence</p> <p>Applications in ML:</p> <ol> <li>Gaussian Mixture Models (GMM):</li> <li>Clustering with probabilistic assignments</li> <li> <p>Each cluster is a multivariate normal</p> </li> <li> <p>Principal Component Analysis (PCA):</p> </li> <li>Assumes data follows multivariate normal</li> <li> <p>Finds principal directions of variation</p> </li> <li> <p>Linear Discriminant Analysis (LDA):</p> </li> <li>Classes assumed multivariate normal</li> <li> <p>Same covariance matrix across classes</p> </li> <li> <p>Gaussian Processes:</p> </li> <li>Function values follow multivariate normal</li> <li> <p>Used in Bayesian optimization</p> </li> <li> <p>Kalman Filters:</p> </li> <li>State estimation in time series</li> <li>System and observation noise assumed normal</li> </ol> <p>Covariance Matrix Interpretation: - Diagonal: Individual variable variances - Off-diagonal: Correlations between variables - Eigenvalues: Principal component variances - Eigenvectors: Principal directions</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27\ud83e\udde0\ud83d\udd27 \ud83d\udd27E\ud83d\udd27x\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27e\ud83d\udd27a\ud83d\udd27l\ud83d\udd27-\ud83d\udd27w\ud83d\udd27o\ud83d\udd27r\ud83d\udd27l\ud83d\udd27d\ud83d\udd27 \ud83d\udd27E\ud83d\udd27x\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27:\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27i\ud83d\udd27n\ud83d\udd27 \ud83d\udd27M\ud83d\udd27a\ud83d\udd27n\ud83d\udd27u\ud83d\udd27f\ud83d\udd27a\ud83d\udd27c\ud83d\udd27t\ud83d\udd27u\ud83d\udd27r\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27M\ud83d\udd27a\ud83d\udd27n\ud83d\udd27u\ud83d\udd27f\ud83d\udd27a\ud83d\udd27c\ud83d\udd27t\ud83d\udd27u\ud83d\udd27r\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27E\ud83d\udd27x\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27\nnp.random.seed(42)\n\ndef simulate_manufacturing_data():\n    \"\"\"\n    Simulate manufacturing data with normal distribution assumptions\n    \"\"\"\n    # Simulate production data over 30 days\n    n_days = 30\n    samples_per_day = 50\n\n    # Target specifications\n    target_length = 100.0  # mm\n    tolerance = \u00b12.0  # mm (so acceptable range is 98-102 mm)\n    process_std = 0.8  # mm (process standard deviation)\n\n    # Simulate daily production\n    production_data = []\n\n    for day in range(1, n_days + 1):\n        # Daily mean might drift slightly (process variation)\n        daily_mean = target_length + np.random.normal(0, 0.2)\n\n        # Generate samples for the day\n        daily_samples = np.random.normal(daily_mean, process_std, samples_per_day)\n\n        for sample in daily_samples:\n            production_data.append({\n                'day': day,\n                'length': sample,\n                'within_spec': 98 &lt;= sample &lt;= 102,\n                'defect_type': 'none' if 98 &lt;= sample &lt;= 102 else ('short' if sample &lt; 98 else 'long')\n            })\n\n    return pd.DataFrame(production_data)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27G\ud83d\udd27e\ud83d\udd27n\ud83d\udd27e\ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27m\ud83d\udd27a\ud83d\udd27n\ud83d\udd27u\ud83d\udd27f\ud83d\udd27a\ud83d\udd27c\ud83d\udd27t\ud83d\udd27u\ud83d\udd27r\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27t\ud83d\udd27a\ud83d\udd27\nmanufacturing_df = simulate_manufacturing_data()\n\nprint(\"Manufacturing Quality Control Analysis\")\nprint(\"=\" * 50)\nprint(f\"Total samples: {len(manufacturing_df)}\")\nprint(f\"Target length: 100.0 mm \u00b1 2.0 mm\")\nprint(f\"Overall mean: {manufacturing_df['length'].mean():.3f} mm\")\nprint(f\"Overall std: {manufacturing_df['length'].std():.3f} mm\")\nprint(f\"Defect rate: {(~manufacturing_df['within_spec']).mean():.1%}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27D\ud83d\udd27a\ud83d\udd27i\ud83d\udd27l\ud83d\udd27y\ud83d\udd27 \ud83d\udd27s\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27s\ud83d\udd27\ndaily_stats = manufacturing_df.groupby('day')['length'].agg(['mean', 'std', 'count'])\ndaily_defect_rate = manufacturing_df.groupby('day')['within_spec'].apply(lambda x: (~x).mean())\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27c\ud83d\udd27h\ud83d\udd27a\ud83d\udd27r\ud83d\udd27t\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27 \ud83d\udd27u\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\noverall_mean = manufacturing_df['length'].mean()\noverall_std = manufacturing_df['length'].std()\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27l\ud83d\udd27i\ud83d\udd27m\ud83d\udd27i\ud83d\udd27t\ud83d\udd27s\ud83d\udd27 \ud83d\udd27(\ud83d\udd273\ud83d\udd27-\ud83d\udd27s\ud83d\udd27i\ud83d\udd27g\ud83d\udd27m\ud83d\udd27a\ud83d\udd27 \ud83d\udd27r\ud83d\udd27u\ud83d\udd27l\ud83d\udd27e\ud83d\udd27)\ud83d\udd27\nucl = overall_mean + 3 * overall_std  # Upper Control Limit\nlcl = overall_mean - 3 * overall_std  # Lower Control Limit\nusl = 102  # Upper Specification Limit\nlsl = 98   # Lower Specification Limit\n\nprint(f\"\\nControl Chart Limits:\")\nprint(f\"Upper Control Limit (UCL): {ucl:.3f} mm\")\nprint(f\"Lower Control Limit (LCL): {lcl:.3f} mm\")\nprint(f\"Upper Specification Limit (USL): {usl:.1f} mm\")\nprint(f\"Lower Specification Limit (LSL): {lsl:.1f} mm\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27c\ud83d\udd27e\ud83d\udd27s\ud83d\udd27s\ud83d\udd27 \ud83d\udd27c\ud83d\udd27a\ud83d\udd27p\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27\ncp = (usl - lsl) / (6 * overall_std)  # Process capability\ncpk = min((usl - overall_mean)/(3 * overall_std), \n          (overall_mean - lsl)/(3 * overall_std))  # Process capability index\n\nprint(f\"\\nProcess Capability:\")\nprint(f\"Cp (potential capability): {cp:.3f}\")\nprint(f\"Cpk (actual capability): {cpk:.3f}\")\nprint(f\"Process capability interpretation:\")\nif cpk &gt;= 1.33:\n    print(\"  Excellent process (&lt; 63 PPM defects)\")\nelif cpk &gt;= 1.0:\n    print(\"  Adequate process (&lt; 2,700 PPM defects)\")\nelse:\n    print(\"  Poor process (&gt; 2,700 PPM defects)\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27a\ud83d\udd27l\ud83d\udd27c\ud83d\udd27u\ud83d\udd27l\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27t\ud83d\udd27h\ud83d\udd27e\ud83d\udd27o\ud83d\udd27r\ud83d\udd27e\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27f\ud83d\udd27e\ud83d\udd27c\ud83d\udd27t\ud83d\udd27 \ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27s\ud83d\udd27 \ud83d\udd27u\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nz_usl = (usl - overall_mean) / overall_std\nz_lsl = (lsl - overall_mean) / overall_std\n\nprob_exceed_usl = 1 - stats.norm.cdf(z_usl)\nprob_below_lsl = stats.norm.cdf(z_lsl)\ntheoretical_defect_rate = prob_exceed_usl + prob_below_lsl\n\nprint(f\"\\nDefect Rate Analysis:\")\nprint(f\"Observed defect rate: {(~manufacturing_df['within_spec']).mean():.1%}\")\nprint(f\"Theoretical defect rate (based on normal): {theoretical_defect_rate:.1%}\")\nprint(f\"Theoretical PPM: {theoretical_defect_rate * 1e6:.0f}\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27V\ud83d\udd27i\ud83d\udd27s\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27z\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.figure(figsize=(20, 15))\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd271\ud83d\udd27.\ud83d\udd27 \ud83d\udd27O\ud83d\udd27v\ud83d\udd27e\ud83d\udd27r\ud83d\udd27a\ud83d\udd27l\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27w\ud83d\udd27i\ud83d\udd27t\ud83d\udd27h\ud83d\udd27 \ud83d\udd27s\ud83d\udd27p\ud83d\udd27e\ud83d\udd27c\ud83d\udd27i\ud83d\udd27f\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 1)\nplt.hist(manufacturing_df['length'], bins=50, density=True, alpha=0.7, \n         color='lightblue', edgecolor='black', label='Observed Data')\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27F\ud83d\udd27i\ud83d\udd27t\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nmu_fit, sigma_fit = stats.norm.fit(manufacturing_df['length'])\nx = np.linspace(manufacturing_df['length'].min() - 1, manufacturing_df['length'].max() + 1, 1000)\nplt.plot(x, stats.norm.pdf(x, mu_fit, sigma_fit), 'r-', linewidth=2, label='Fitted Normal')\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27A\ud83d\udd27d\ud83d\udd27d\ud83d\udd27 \ud83d\udd27s\ud83d\udd27p\ud83d\udd27e\ud83d\udd27c\ud83d\udd27i\ud83d\udd27f\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27 \ud83d\udd27l\ud83d\udd27i\ud83d\udd27m\ud83d\udd27i\ud83d\udd27t\ud83d\udd27s\ud83d\udd27\nplt.axvline(lsl, color='red', linestyle='--', linewidth=2, label='Spec Limits')\nplt.axvline(usl, color='red', linestyle='--', linewidth=2)\nplt.axvline(overall_mean, color='green', linestyle='-', linewidth=2, label='Process Mean')\n\nplt.xlabel('Length (mm)')\nplt.ylabel('Density')\nplt.title('Overall Distribution vs Specifications')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd272\ud83d\udd27.\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27c\ud83d\udd27h\ud83d\udd27a\ud83d\udd27r\ud83d\udd27t\ud83d\udd27 \ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27i\ud83d\udd27l\ud83d\udd27y\ud83d\udd27 \ud83d\udd27m\ud83d\udd27e\ud83d\udd27a\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 2)\nplt.plot(daily_stats.index, daily_stats['mean'], 'bo-', markersize=4)\nplt.axhline(overall_mean, color='green', linestyle='-', label='Grand Mean')\nplt.axhline(overall_mean + 3*overall_std/np.sqrt(50), color='red', linestyle='--', label='\u00b13\u00c3 limits')\nplt.axhline(overall_mean - 3*overall_std/np.sqrt(50), color='red', linestyle='--')\nplt.xlabel('Day')\nplt.ylabel('Daily Mean Length (mm)')\nplt.title('X-bar Control Chart')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd273\ud83d\udd27.\ud83d\udd27 \ud83d\udd27C\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27c\ud83d\udd27h\ud83d\udd27a\ud83d\udd27r\ud83d\udd27t\ud83d\udd27 \ud83d\udd27f\ud83d\udd27o\ud83d\udd27r\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27i\ud83d\udd27l\ud83d\udd27y\ud83d\udd27 \ud83d\udd27r\ud83d\udd27a\ud83d\udd27n\ud83d\udd27g\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 3)\nplt.plot(daily_stats.index, daily_stats['std'], 'go-', markersize=4)\nplt.axhline(overall_std, color='blue', linestyle='-', label='Grand Std')\nplt.xlabel('Day')\nplt.ylabel('Daily Std (mm)')\nplt.title('S Control Chart')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd274\ud83d\udd27.\ud83d\udd27 \ud83d\udd27D\ud83d\udd27a\ud83d\udd27i\ud83d\udd27l\ud83d\udd27y\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27f\ud83d\udd27e\ud83d\udd27c\ud83d\udd27t\ud83d\udd27 \ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 4)\nplt.bar(daily_defect_rate.index, daily_defect_rate.values * 100, alpha=0.7, color='orange')\nplt.axhline(theoretical_defect_rate * 100, color='red', linestyle='--', \n           label=f'Expected: {theoretical_defect_rate:.1%}')\nplt.xlabel('Day')\nplt.ylabel('Defect Rate (%)')\nplt.title('Daily Defect Rates')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd275\ud83d\udd27.\ud83d\udd27 \ud83d\udd27Q\ud83d\udd27-\ud83d\udd27Q\ud83d\udd27 \ud83d\udd27p\ud83d\udd27l\ud83d\udd27o\ud83d\udd27t\ud83d\udd27 \ud83d\udd27t\ud83d\udd27o\ud83d\udd27 \ud83d\udd27v\ud83d\udd27e\ud83d\udd27r\ud83d\udd27i\ud83d\udd27f\ud83d\udd27y\ud83d\udd27 \ud83d\udd27n\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27\nplt.subplot(3, 4, 5)\nstats.probplot(manufacturing_df['length'], dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot: Normality Check')\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd276\ud83d\udd27.\ud83d\udd27 \ud83d\udd27I\ud83d\udd27n\ud83d\udd27d\ud83d\udd27i\ud83d\udd27v\ud83d\udd27i\ud83d\udd27d\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27m\ud83d\udd27e\ud83d\udd27a\ud83d\udd27s\ud83d\udd27u\ud83d\udd27r\ud83d\udd27e\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27s\ud83d\udd27 \ud83d\udd27c\ud83d\udd27o\ud83d\udd27n\ud83d\udd27t\ud83d\udd27r\ud83d\udd27o\ud83d\udd27l\ud83d\udd27 \ud83d\udd27c\ud83d\udd27h\ud83d\udd27a\ud83d\udd27r\ud83d\udd27t\ud83d\udd27\nplt.subplot(3, 4, 6)\nsample_subset = manufacturing_df.head(100)  # First 100 samples\nplt.plot(range(len(sample_subset)), sample_subset['length'], 'b-', alpha=0.6)\nplt.axhline(overall_mean, color='green', linestyle='-', label='Mean')\nplt.axhline(ucl, color='red', linestyle='--', label='Control Limits')\nplt.axhline(lcl, color='red', linestyle='--')\nplt.axhline(usl, color='orange', linestyle=':', label='Spec Limits')\nplt.axhline(lsl, color='orange', linestyle=':')\nplt.xlabel('Sample Number')\nplt.ylabel('Length (mm)')\nplt.title('Individual Measurements (First 100)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd277\ud83d\udd27.\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27c\ud83d\udd27e\ud83d\udd27s\ud83d\udd27s\ud83d\udd27 \ud83d\udd27c\ud83d\udd27a\ud83d\udd27p\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27v\ud83d\udd27i\ud83d\udd27s\ud83d\udd27u\ud83d\udd27a\ud83d\udd27l\ud83d\udd27i\ud83d\udd27z\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nplt.subplot(3, 4, 7)\nx_cap = np.linspace(94, 106, 1000)\ny_cap = stats.norm.pdf(x_cap, overall_mean, overall_std)\nplt.plot(x_cap, y_cap, 'b-', linewidth=2, label='Process Distribution')\nplt.fill_between(x_cap, 0, y_cap, where=((x_cap &gt;= lsl) &amp; (x_cap &lt;= usl)), \n                alpha=0.3, color='green', label='Within Spec')\nplt.fill_between(x_cap, 0, y_cap, where=(x_cap &lt; lsl), \n                alpha=0.3, color='red', label='Below LSL')\nplt.fill_between(x_cap, 0, y_cap, where=(x_cap &gt; usl), \n                alpha=0.3, color='red', label='Above USL')\nplt.axvline(lsl, color='red', linestyle='--', linewidth=2)\nplt.axvline(usl, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Length (mm)')\nplt.ylabel('Density')\nplt.title(f'Process Capability (Cpk = {cpk:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd278\ud83d\udd27.\ud83d\udd27 \ud83d\udd27H\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27o\ud83d\udd27g\ud83d\udd27r\ud83d\udd27a\ud83d\udd27m\ud83d\udd27 \ud83d\udd27b\ud83d\udd27y\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27f\ud83d\udd27e\ud83d\udd27c\ud83d\udd27t\ud83d\udd27 \ud83d\udd27t\ud83d\udd27y\ud83d\udd27p\ud83d\udd27e\ud83d\udd27\nplt.subplot(3, 4, 8)\ndefect_counts = manufacturing_df['defect_type'].value_counts()\ncolors = {'none': 'green', 'short': 'red', 'long': 'orange'}\nbars = plt.bar(defect_counts.index, defect_counts.values, \n               color=[colors[x] for x in defect_counts.index])\nplt.xlabel('Defect Type')\nplt.ylabel('Count')\nplt.title('Distribution by Defect Type')\nfor bar, count in zip(bars, defect_counts.values):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n             str(count), ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd279\ud83d\udd27.\ud83d\udd27 \ud83d\udd27M\ud83d\udd27o\ud83d\udd27v\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27a\ud83d\udd27v\ud83d\udd27e\ud83d\udd27r\ud83d\udd27a\ud83d\udd27g\ud83d\udd27e\ud83d\udd27 \ud83d\udd27t\ud83d\udd27r\ud83d\udd27e\ud83d\udd27n\ud83d\udd27d\ud83d\udd27\nplt.subplot(3, 4, 9)\nmanufacturing_df['moving_avg'] = manufacturing_df['length'].rolling(window=20, center=True).mean()\nplt.plot(range(len(manufacturing_df)), manufacturing_df['length'], 'b-', alpha=0.3, label='Individual')\nplt.plot(range(len(manufacturing_df)), manufacturing_df['moving_avg'], 'r-', linewidth=2, label='Moving Avg (20)')\nplt.axhline(target_length, color='green', linestyle='--', label='Target')\nplt.xlabel('Sample Number')\nplt.ylabel('Length (mm)')\nplt.title('Process Trend Analysis')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd271\ud83d\udd270\ud83d\udd27.\ud83d\udd27 \ud83d\udd27P\ud83d\udd27r\ud83d\udd27o\ud83d\udd27b\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27c\ud83d\udd27u\ud83d\udd27l\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 10)\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27r\ud83d\udd27e\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27 \ud83d\udd27p\ud83d\udd27r\ud83d\udd27o\ud83d\udd27b\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27n\ud83d\udd27s\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27a\ud83d\udd27r\ud83d\udd27e\ud83d\udd27a\ud83d\udd27s\ud83d\udd27\nx_prob = np.linspace(96, 104, 1000)\ny_prob = stats.norm.pdf(x_prob, overall_mean, overall_std)\n\nplt.plot(x_prob, y_prob, 'b-', linewidth=2)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27h\ud83d\udd27a\ud83d\udd27d\ud83d\udd27e\ud83d\udd27 \ud83d\udd27d\ud83d\udd27i\ud83d\udd27f\ud83d\udd27f\ud83d\udd27e\ud83d\udd27r\ud83d\udd27e\ud83d\udd27n\ud83d\udd27t\ud83d\udd27 \ud83d\udd27p\ud83d\udd27r\ud83d\udd27o\ud83d\udd27b\ud83d\udd27a\ud83d\udd27b\ud83d\udd27i\ud83d\udd27l\ud83d\udd27i\ud83d\udd27t\ud83d\udd27y\ud83d\udd27 \ud83d\udd27r\ud83d\udd27e\ud83d\udd27g\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27s\ud83d\udd27\nplt.fill_between(x_prob, 0, y_prob, where=(x_prob &lt;= overall_mean - overall_std), \n                alpha=0.3, color='lightcoral', label='&lt; \u00bc-\u00c3 (16%)')\nplt.fill_between(x_prob, 0, y_prob, where=((x_prob &gt; overall_mean - overall_std) &amp; \n                (x_cap &lt;= overall_mean + overall_std)), \n                alpha=0.3, color='lightgreen', label='\u00bc\u00b1\u00c3 (68%)')\nplt.fill_between(x_prob, 0, y_prob, where=(x_prob &gt; overall_mean + overall_std), \n                alpha=0.3, color='lightcoral', label='&gt; \u00bc+\u00c3 (16%)')\n\nplt.xlabel('Length (mm)')\nplt.ylabel('Density')\nplt.title('Probability Regions (Empirical Rule)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd271\ud83d\udd271\ud83d\udd27-\ud83d\udd271\ud83d\udd272\ud83d\udd27:\ud83d\udd27 \ud83d\udd27A\ud83d\udd27d\ud83d\udd27d\ud83d\udd27i\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27e\ud83d\udd27s\ud83d\udd27\nplt.subplot(3, 4, 11)\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27B\ud83d\udd27o\ud83d\udd27x\ud83d\udd27 \ud83d\udd27p\ud83d\udd27l\ud83d\udd27o\ud83d\udd27t\ud83d\udd27 \ud83d\udd27b\ud83d\udd27y\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27y\ud83d\udd27 \ud83d\udd27(\ud83d\udd27s\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27 \ud83d\udd27o\ud83d\udd27f\ud83d\udd27 \ud83d\udd27d\ud83d\udd27a\ud83d\udd27y\ud83d\udd27s\ud83d\udd27)\ud83d\udd27\nsample_days = [1, 10, 20, 30]\nbox_data = [manufacturing_df[manufacturing_df['day'] == day]['length'].values \n           for day in sample_days]\nplt.boxplot(box_data, labels=[f'Day {d}' for d in sample_days])\nplt.ylabel('Length (mm)')\nplt.title('Distribution by Selected Days')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(3, 4, 12)\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27C\ud83d\udd27u\ud83d\udd27m\ud83d\udd27u\ud83d\udd27l\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27v\ud83d\udd27e\ud83d\udd27 \ud83d\udd27d\ud83d\udd27e\ud83d\udd27f\ud83d\udd27e\ud83d\udd27c\ud83d\udd27t\ud83d\udd27 \ud83d\udd27r\ud83d\udd27a\ud83d\udd27t\ud83d\udd27e\ud83d\udd27\nmanufacturing_df['cumulative_defects'] = (~manufacturing_df['within_spec']).cumsum()\nmanufacturing_df['cumulative_rate'] = manufacturing_df['cumulative_defects'] / np.arange(1, len(manufacturing_df) + 1)\nplt.plot(range(len(manufacturing_df)), manufacturing_df['cumulative_rate'] * 100, 'b-')\nplt.axhline(theoretical_defect_rate * 100, color='red', linestyle='--', label='Expected Rate')\nplt.xlabel('Sample Number')\nplt.ylabel('Cumulative Defect Rate (%)')\nplt.title('Cumulative Defect Rate Trend')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27S\ud83d\udd27t\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27i\ud83d\udd27c\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27s\ud83d\udd27u\ud83d\udd27m\ud83d\udd27m\ud83d\udd27a\ud83d\udd27r\ud83d\udd27y\ud83d\udd27\nprint(f\"\\nStatistical Analysis Summary:\")\nprint(f\"Shapiro-Wilk normality test: p = {stats.shapiro(manufacturing_df['length'])[1]:.6f}\")\nif stats.shapiro(manufacturing_df['length'])[1] &gt; 0.05:\n    print(\"  \u0092 Data appears to follow normal distribution (p &gt; 0.05)\")\nelse:\n    print(\"  \u0092 Data may not be perfectly normal (p d 0.05)\")\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27e\ud83d\udd27c\ud83d\udd27o\ud83d\udd27m\ud83d\udd27m\ud83d\udd27e\ud83d\udd27n\ud83d\udd27d\ud83d\udd27a\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\nprint(f\"\\nRecommendations:\")\nif cpk &gt;= 1.33:\n    print(\"\u0013 Process is operating excellently\")\nelif cpk &gt;= 1.0:\n    print(\"\u00a0 Process is adequate but could be improved\")\n    print(\"  - Consider reducing process variation\")\n    print(\"  - Check for process centering\")\nelse:\n    print(\"L Process needs immediate attention\")\n    print(\"  - High defect rate detected\")\n    print(\"  - Consider process adjustment or tighter control\")\n\nreturn manufacturing_df\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27n\ud83d\udd27 \ud83d\udd27t\ud83d\udd27h\ud83d\udd27e\ud83d\udd27 \ud83d\udd27m\ud83d\udd27a\ud83d\udd27n\ud83d\udd27u\ud83d\udd27f\ud83d\udd27a\ud83d\udd27c\ud83d\udd27t\ud83d\udd27u\ud83d\udd27r\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27\nmanufacturing_data = simulate_manufacturing_data()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27F\ud83d\udd27i\ud83d\udd27n\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27i\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27R\ud83d\udd27i\ud83d\udd27s\ud83d\udd27k\ud83d\udd27 \ud83d\udd27A\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27 \ud83d\udd27E\ud83d\udd27x\ud83d\udd27a\ud83d\udd27m\ud83d\udd27p\ud83d\udd27l\ud83d\udd27e\ud83d\udd27</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\nfrom datetime import datetime, timedelta\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27F\ud83d\udd27i\ud83d\udd27n\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27i\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27R\ud83d\udd27i\ud83d\udd27s\ud83d\udd27k\ud83d\udd27 \ud83d\udd27A\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27 \ud83d\udd27u\ud83d\udd27s\ud83d\udd27i\ud83d\udd27n\ud83d\udd27g\ud83d\udd27 \ud83d\udd27N\ud83d\udd27o\ud83d\udd27r\ud83d\udd27m\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27D\ud83d\udd27i\ud83d\udd27s\ud83d\udd27t\ud83d\udd27r\ud83d\udd27i\ud83d\udd27b\ud83d\udd27u\ud83d\udd27t\ud83d\udd27i\ud83d\udd27o\ud83d\udd27n\ud83d\udd27\ndef financial_risk_analysis():\n    \"\"\"\n    Analyze financial portfolio using normal distribution assumptions\n    \"\"\"\n    np.random.seed(42)\n\n    # Simulate daily returns for different assets\n    n_days = 252  # One trading year\n    assets = ['Stock_A', 'Stock_B', 'Bond', 'Commodity']\n\n    # Define asset characteristics (annual returns and volatility)\n    asset_params = {\n        'Stock_A': {'annual_return': 0.12, 'volatility': 0.20},\n        'Stock_B': {'annual_return': 0.08, 'volatility': 0.15},\n        'Bond': {'annual_return': 0.04, 'volatility': 0.05},\n        'Commodity': {'annual_return': 0.06, 'volatility': 0.25}\n    }\n\n    # Convert to daily parameters\n    daily_returns = {}\n    for asset, params in asset_params.items():\n        daily_mean = params['annual_return'] / 252\n        daily_std = params['volatility'] / np.sqrt(252)\n        daily_returns[asset] = np.random.normal(daily_mean, daily_std, n_days)\n\n    # Create DataFrame\n    returns_df = pd.DataFrame(daily_returns)\n\n    # Portfolio weights\n    weights = np.array([0.4, 0.3, 0.2, 0.1])  # 40% Stock_A, 30% Stock_B, 20% Bond, 10% Commodity\n\n    # Calculate portfolio returns\n    returns_df['Portfolio'] = np.dot(returns_df[assets], weights)\n\n    print(\"Financial Portfolio Risk Analysis\")\n    print(\"=\" * 50)\n    print(\"Asset Allocation:\")\n    for asset, weight in zip(assets, weights):\n        print(f\"  {asset}: {weight:.1%}\")\n\n    print(f\"\\nPortfolio Statistics (Annualized):\")\n    portfolio_annual_return = returns_df['Portfolio'].mean() * 252\n    portfolio_annual_volatility = returns_df['Portfolio'].std() * np.sqrt(252)\n    sharpe_ratio = portfolio_annual_return / portfolio_annual_volatility\n\n    print(f\"Expected Return: {portfolio_annual_return:.2%}\")\n    print(f\"Volatility (Risk): {portfolio_annual_volatility:.2%}\")\n    print(f\"Sharpe Ratio: {sharpe_ratio:.3f}\")\n\n    # Value at Risk (VaR) calculations using normal distribution\n    confidence_levels = [0.95, 0.99, 0.999]\n\n    print(f\"\\nValue at Risk (VaR) Analysis:\")\n    print(\"Confidence Level | 1-Day VaR | 10-Day VaR | Monthly VaR\")\n    print(\"-\" * 55)\n\n    portfolio_mean = returns_df['Portfolio'].mean()\n    portfolio_std = returns_df['Portfolio'].std()\n\n    for conf in confidence_levels:\n        # Z-score for confidence level\n        z_score = stats.norm.ppf(1 - conf)\n\n        # VaR calculations (negative because it's a loss)\n        var_1day = -(portfolio_mean + z_score * portfolio_std)\n        var_10day = -(portfolio_mean * 10 + z_score * portfolio_std * np.sqrt(10))\n        var_monthly = -(portfolio_mean * 21 + z_score * portfolio_std * np.sqrt(21))\n\n        print(f\"{conf:.1%}            | {var_1day:.2%}     | {var_10day:.2%}      | {var_monthly:.2%}\")\n\n    # Monte Carlo simulation for portfolio value\n    initial_value = 1000000  # $1M initial portfolio\n\n    # Simulate portfolio path\n    cumulative_returns = (1 + returns_df['Portfolio']).cumprod()\n    portfolio_values = initial_value * cumulative_returns\n\n    # Calculate maximum drawdown\n    running_max = portfolio_values.cummax()\n    drawdown = (portfolio_values - running_max) / running_max\n    max_drawdown = drawdown.min()\n\n    print(f\"\\nPortfolio Performance:\")\n    print(f\"Initial Value: ${initial_value:,.0f}\")\n    print(f\"Final Value: ${portfolio_values.iloc[-1]:,.0f}\")\n    print(f\"Total Return: {(portfolio_values.iloc[-1]/initial_value - 1):.2%}\")\n    print(f\"Maximum Drawdown: {max_drawdown:.2%}\")\n\n    # Risk metrics\n    downside_returns = returns_df['Portfolio'][returns_df['Portfolio'] &lt; 0]\n    downside_deviation = downside_returns.std() * np.sqrt(252)\n\n    print(f\"Downside Deviation: {downside_deviation:.2%}\")\n    print(f\"Probability of Loss (daily): {(returns_df['Portfolio'] &lt; 0).mean():.1%}\")\n\n    # Normal distribution tests for each asset\n    print(f\"\\nNormality Tests (Shapiro-Wilk p-values):\")\n    for asset in assets + ['Portfolio']:\n        _, p_value = stats.shapiro(returns_df[asset])\n        status = \"Normal\" if p_value &gt; 0.05 else \"Non-normal\"\n        print(f\"  {asset}: p = {p_value:.4f} ({status})\")\n\n    # Visualization\n    plt.figure(figsize=(20, 16))\n\n    # 1. Portfolio value over time\n    plt.subplot(4, 3, 1)\n    plt.plot(portfolio_values, linewidth=2, color='blue')\n    plt.title('Portfolio Value Over Time')\n    plt.xlabel('Trading Day')\n    plt.ylabel('Portfolio Value ($)')\n    plt.grid(True, alpha=0.3)\n\n    # 2. Daily returns distribution\n    plt.subplot(4, 3, 2)\n    plt.hist(returns_df['Portfolio'], bins=50, density=True, alpha=0.7, color='lightblue', edgecolor='black')\n\n    # Fit normal distribution\n    mu, sigma = stats.norm.fit(returns_df['Portfolio'])\n    x = np.linspace(returns_df['Portfolio'].min(), returns_df['Portfolio'].max(), 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Fitted Normal')\n\n    # Mark VaR levels\n    var_95 = stats.norm.ppf(0.05, mu, sigma)\n    var_99 = stats.norm.ppf(0.01, mu, sigma)\n    plt.axvline(var_95, color='orange', linestyle='--', label='95% VaR')\n    plt.axvline(var_99, color='red', linestyle='--', label='99% VaR')\n\n    plt.title('Portfolio Daily Returns Distribution')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 3. Q-Q plot for portfolio returns\n    plt.subplot(4, 3, 3)\n    stats.probplot(returns_df['Portfolio'], dist=\"norm\", plot=plt)\n    plt.title('Portfolio Returns Q-Q Plot')\n    plt.grid(True, alpha=0.3)\n\n    # 4. Individual asset returns\n    plt.subplot(4, 3, 4)\n    for i, asset in enumerate(assets):\n        plt.plot(np.cumsum(returns_df[asset]), label=asset, linewidth=2)\n    plt.title('Cumulative Returns by Asset')\n    plt.xlabel('Trading Day')\n    plt.ylabel('Cumulative Return')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 5. Correlation matrix\n    plt.subplot(4, 3, 5)\n    corr_matrix = returns_df[assets].corr()\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n                square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n    plt.title('Asset Correlation Matrix')\n\n    # 6. Rolling volatility\n    plt.subplot(4, 3, 6)\n    rolling_vol = returns_df['Portfolio'].rolling(window=21).std() * np.sqrt(252)\n    plt.plot(rolling_vol, linewidth=2, color='purple')\n    plt.axhline(portfolio_annual_volatility, color='red', linestyle='--', \n               label=f'Average: {portfolio_annual_volatility:.1%}')\n    plt.title('Rolling 21-Day Volatility (Annualized)')\n    plt.xlabel('Trading Day')\n    plt.ylabel('Volatility')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 7. Drawdown chart\n    plt.subplot(4, 3, 7)\n    plt.fill_between(range(len(drawdown)), drawdown, 0, color='red', alpha=0.3)\n    plt.plot(drawdown, color='red', linewidth=2)\n    plt.title('Portfolio Drawdown')\n    plt.xlabel('Trading Day')\n    plt.ylabel('Drawdown')\n    plt.grid(True, alpha=0.3)\n\n    # 8. VaR backtest\n    plt.subplot(4, 3, 8)\n    var_95_threshold = -(portfolio_mean + stats.norm.ppf(0.05) * portfolio_std)\n    breaches = returns_df['Portfolio'] &lt; -var_95_threshold\n\n    plt.plot(returns_df['Portfolio'], alpha=0.7, color='blue', linewidth=1)\n    plt.axhline(-var_95_threshold, color='red', linestyle='--', label='95% VaR Threshold')\n    plt.scatter(range(len(returns_df)), returns_df['Portfolio'], \n               c=breaches, cmap='RdYlGn', s=10, alpha=0.7)\n\n    breach_rate = breaches.mean()\n    plt.title(f'VaR Backtesting (Breach Rate: {breach_rate:.1%})')\n    plt.xlabel('Trading Day')\n    plt.ylabel('Daily Return')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 9. Risk-return scatter\n    plt.subplot(4, 3, 9)\n    annual_returns = returns_df[assets].mean() * 252\n    annual_volatilities = returns_df[assets].std() * np.sqrt(252)\n\n    plt.scatter(annual_volatilities, annual_returns, s=100, alpha=0.7)\n    for i, asset in enumerate(assets):\n        plt.annotate(asset, (annual_volatilities[i], annual_returns[i]), \n                    xytext=(5, 5), textcoords='offset points')\n\n    # Add portfolio point\n    plt.scatter(portfolio_annual_volatility, portfolio_annual_return, \n               s=200, color='red', marker='*', label='Portfolio')\n\n    plt.title('Risk-Return Profile')\n    plt.xlabel('Volatility (Risk)')\n    plt.ylabel('Expected Return')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 10. Monte Carlo simulation of future paths\n    plt.subplot(4, 3, 10)\n    n_simulations = 100\n    days_forward = 252  # 1 year forward\n\n    future_paths = []\n    current_value = portfolio_values.iloc[-1]\n\n    for _ in range(n_simulations):\n        future_returns = np.random.normal(portfolio_mean, portfolio_std, days_forward)\n        future_values = current_value * np.cumprod(1 + future_returns)\n        future_paths.append(future_values)\n        plt.plot(range(days_forward), future_values, alpha=0.1, color='blue')\n\n    # Plot percentiles\n    future_paths = np.array(future_paths)\n    percentiles = [5, 25, 50, 75, 95]\n    colors = ['red', 'orange', 'green', 'orange', 'red']\n\n    for p, color in zip(percentiles, colors):\n        plt.plot(range(days_forward), np.percentile(future_paths, p, axis=0), \n                color=color, linewidth=2, label=f'{p}th percentile')\n\n    plt.title('Monte Carlo Future Price Simulation')\n    plt.xlabel('Days Forward')\n    plt.ylabel('Portfolio Value ($)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # 11. Stress testing\n    plt.subplot(4, 3, 11)\n    stress_scenarios = {\n        'Normal': (0, 1),\n        'Mild Stress': (-0.02, 1.5),\n        'Severe Stress': (-0.05, 2.0),\n        'Extreme Stress': (-0.10, 3.0)\n    }\n\n    scenario_results = []\n    scenario_names = []\n\n    for name, (shock_mean, vol_multiplier) in stress_scenarios.items():\n        stressed_returns = np.random.normal(\n            portfolio_mean + shock_mean, \n            portfolio_std * vol_multiplier, \n            1000\n        )\n        var_99_stressed = -np.percentile(stressed_returns, 1)\n        scenario_results.append(var_99_stressed)\n        scenario_names.append(name)\n\n    bars = plt.bar(scenario_names, scenario_results, color=['green', 'yellow', 'orange', 'red'])\n    plt.title('Stress Testing - 99% VaR')\n    plt.ylabel('VaR (1%)')\n    plt.xticks(rotation=45)\n\n    # Add value labels\n    for bar, val in zip(bars, scenario_results):\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n                f'{val:.2%}', ha='center', va='bottom')\n    plt.grid(True, alpha=0.3)\n\n    # 12. Expected shortfall (Conditional VaR)\n    plt.subplot(4, 3, 12)\n    confidence_levels_es = np.linspace(0.90, 0.999, 50)\n    var_values = []\n    es_values = []\n\n    for conf in confidence_levels_es:\n        var_threshold = -stats.norm.ppf(1-conf, portfolio_mean, portfolio_std)\n        var_values.append(var_threshold)\n\n        # Expected Shortfall (average of losses beyond VaR)\n        tail_returns = returns_df['Portfolio'][returns_df['Portfolio'] &lt;= -var_threshold]\n        es = -tail_returns.mean() if len(tail_returns) &gt; 0 else var_threshold\n        es_values.append(es)\n\n    plt.plot(confidence_levels_es, var_values, label='VaR', linewidth=2)\n    plt.plot(confidence_levels_es, es_values, label='Expected Shortfall', linewidth=2)\n    plt.title('VaR vs Expected Shortfall')\n    plt.xlabel('Confidence Level')\n    plt.ylabel('Risk Measure')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return returns_df, portfolio_values\n\n\ud83d\udd27#\ud83d\udd27 \ud83d\udd27R\ud83d\udd27u\ud83d\udd27n\ud83d\udd27 \ud83d\udd27t\ud83d\udd27h\ud83d\udd27e\ud83d\udd27 \ud83d\udd27f\ud83d\udd27i\ud83d\udd27n\ud83d\udd27a\ud83d\udd27n\ud83d\udd27c\ud83d\udd27i\ud83d\udd27a\ud83d\udd27l\ud83d\udd27 \ud83d\udd27a\ud83d\udd27n\ud83d\udd27a\ud83d\udd27l\ud83d\udd27y\ud83d\udd27s\ud83d\udd27i\ud83d\udd27s\ud83d\udd27\nreturns_data, portfolio_data = financial_risk_analysis()\n</code></pre> <p>\ud83d\udd27#\ud83d\udd27#\ud83d\udd27 \ud83d\udd27=\ud83d\udd27\u00da\ud83d\udd27 \ud83d\udd27R\ud83d\udd27e\ud83d\udd27f\ud83d\udd27e\ud83d\udd27r\ud83d\udd27e\ud83d\udd27n\ud83d\udd27c\ud83d\udd27e\ud83d\udd27s\ud83d\udd27</p> <p>Foundational Texts: - Statistical Inference - Casella &amp; Berger - Introduction to Mathematical Statistics - Hogg, McKean, Craig - Probability and Statistics - Walpole, Myers, Myers, Ye</p> <p>Machine Learning Applications: - The Elements of Statistical Learning - Hastie, Tibshirani, Friedman - Pattern Recognition and Machine Learning - Christopher Bishop - Machine Learning: A Probabilistic Perspective - Kevin Murphy</p> <p>Classical Papers: - Central Limit Theorem - Strassen (1964) - Maximum Likelihood Estimation - Fisher (1922) - Box-Cox Transformations - Box &amp; Cox (1964)</p> <p>Statistical Computing: - SciPy Statistics Documentation - Statsmodels Documentation - NumPy Random Documentation</p> <p>Quality Control Applications: - Introduction to Statistical Quality Control - Douglas Montgomery - Statistical Process Control - John Oakland</p> <p>Financial Applications: - Risk Management and Financial Institutions - John Hull - Options, Futures, and Other Derivatives - John Hull - Value at Risk: The New Benchmark for Managing Financial Risk - Philippe Jorion</p> <p>Online Resources: - Khan Academy Statistics - StatQuest YouTube Channel - MIT OpenCourseWare - Probability and Statistics - Stanford CS229 Machine Learning Notes</p> <p>Specialized Topics: - Multivariate Normal Distribution - Gaussian Processes - Normalizing Flows - Central Limit Theorem Variations</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/","title":"\ud83d\udcca Normalization and Regularisation","text":"<p>Normalization and regularisation are fundamental techniques in machine learning: normalization ensures features are on similar scales for optimal algorithm performance, while regularisation prevents overfitting by constraining model complexity.</p> <p>Resources: Scikit-learn Preprocessing | Regularization in Deep Learning | Elements of Statistical Learning - Chapter 3</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#summary","title":"\ud83d\udcca Summary","text":"<p>Normalization (Feature Scaling) transforms features to similar scales, ensuring no single feature dominates due to its scale. Regularisation adds penalty terms to the loss function to prevent overfitting by constraining model complexity.</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#normalization","title":"Normalization","text":"<p>Feature scaling is crucial when features have different units, ranges, or variances. Without normalization, algorithms like gradient descent, SVM, and k-NN can be severely affected by scale differences.</p> <p>Common normalization techniques: - Standardization (Z-score): Mean = 0, Standard deviation = 1 - Min-Max scaling: Scale to [0,1] range - Robust scaling: Uses median and IQR, resistant to outliers - Unit vector scaling: Scale to unit norm - Quantile transformation: Map to uniform or normal distribution</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#regularisation","title":"Regularisation","text":"<p>Regularisation prevents overfitting by adding penalty terms that discourage complex models, leading to better generalization on unseen data.</p> <p>Common regularisation techniques: - L1 Regularization (Lasso): Promotes sparsity, feature selection - L2 Regularization (Ridge): Shrinks coefficients, handles multicollinearity - Elastic Net: Combines L1 and L2 penalties - Dropout: Randomly deactivates neurons (neural networks) - Early stopping: Stop training before overfitting occurs</p> <p>Applications: - Feature preprocessing for all ML algorithms - Linear models (Ridge, Lasso, Elastic Net) - Neural networks (dropout, batch normalization) - Tree-based models (pruning) - Computer vision and NLP pipelines</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#normalization-intuition","title":"Normalization Intuition","text":"<p>Imagine you're comparing houses using price (in hundreds of thousands) and square footage (in thousands). Without normalization, price variations (20-800) might overshadow square footage variations (1-5), causing algorithms to ignore the latter feature entirely.</p> <p>Example: In k-NN, Euclidean distance between houses: - Without normalization: Distance dominated by price differences - With normalization: Both features contribute meaningfully to distance</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#regularisation-intuition","title":"Regularisation Intuition","text":"<p>Think of regularisation like speed limits on roads. Without limits (regularisation), drivers (models) might go too fast (overfit) and crash. Regularisation enforces \"speed limits\" on model complexity, ensuring safer (more generalizable) performance.</p> <p>Analogy:  - No regularisation: Memorizing exam answers \u03bb fails on new questions - With regularisation: Understanding concepts \u03bb succeeds on new questions</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#1-normalization-techniques","title":"1. Normalization Techniques","text":"<p>Standardization (Z-score normalization): \\(\\(z = \\frac{x - \\mu}{\\sigma}\\)\\)</p> <p>Where \\(\\mu\\) is mean and \\(\\sigma\\) is standard deviation.</p> <p>Min-Max scaling: \\(\\(x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\)\\)</p> <p>Robust scaling: \\(\\(x_{robust} = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)}\\)\\)</p> <p>Where IQR is the interquartile range.</p> <p>Unit vector scaling: \\(\\(x_{unit} = \\frac{x}{||x||_2}\\)\\)</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#2-regularisation-mathematics","title":"2. Regularisation Mathematics","text":"<p>L1 Regularization (Lasso): \\(\\(\\text{Loss} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} |w_i|\\)\\)</p> <p>L2 Regularization (Ridge): \\(\\(\\text{Loss} = \\text{MSE} + \\lambda \\sum_{i=1}^{n} w_i^2\\)\\)</p> <p>Elastic Net: \\(\\(\\text{Loss} = \\text{MSE} + \\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2\\)\\)</p> <p>Where \\(\\lambda\\) controls regularisation strength.</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#3-effect-on-gradients","title":"3. Effect on Gradients","text":"<p>L1 gradient (creates sparsity): \\(\\(\\frac{\\partial}{\\partial w_i} \\lambda |w_i| = \\lambda \\cdot \\text{sign}(w_i)\\)\\)</p> <p>L2 gradient (shrinks coefficients): \\(\\(\\frac{\\partial}{\\partial w_i} \\lambda w_i^2 = 2\\lambda w_i\\)\\)</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#implementation-using-libraries","title":"\ud83d\udee0\ufe0f Implementation using Libraries","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#normalization-with-scikit-learn","title":"Normalization with Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, RobustScaler, \n    Normalizer, QuantileTransformer, PowerTransformer\n)\nfrom sklearn.datasets import make_classification, load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Generate sample dataset with different scales\nnp.random.seed(42)\nn_samples = 1000\n\n# Create features with vastly different scales\ndata = {\n    'income': np.random.normal(50000, 15000, n_samples),      # Mean ~50k\n    'age': np.random.normal(35, 10, n_samples),               # Mean ~35\n    'debt_ratio': np.random.uniform(0, 1, n_samples),         # Range [0,1]\n    'credit_score': np.random.normal(700, 100, n_samples),    # Mean ~700\n    'num_accounts': np.random.poisson(5, n_samples)           # Count data\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"Feature Scaling Demonstration\")\nprint(\"Original data statistics:\")\nprint(df.describe())\nprint(f\"\\nFeature ranges:\")\nfor col in df.columns:\n    print(f\"{col:15}: [{df[col].min():.2f}, {df[col].max():.2f}]\")\n\n# Visualize original distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\noriginal_data = df.values\n\nscalers = {\n    'Original': None,\n    'StandardScaler': StandardScaler(),\n    'MinMaxScaler': MinMaxScaler(),\n    'RobustScaler': RobustScaler(),\n    'Normalizer': Normalizer(),\n    'QuantileTransformer': QuantileTransformer(output_distribution='normal')\n}\n\nscaled_data = {}\n\nfor i, (name, scaler) in enumerate(scalers.items()):\n    if scaler is None:\n        data_transformed = original_data\n        title_stats = \"Original Data\"\n    else:\n        data_transformed = scaler.fit_transform(original_data)\n        title_stats = f\"Mean: {data_transformed.mean():.2f}, Std: {data_transformed.std():.2f}\"\n\n    scaled_data[name] = data_transformed\n\n    # Plot first feature (income) for each transformation\n    axes[i].hist(data_transformed[:, 0], bins=30, alpha=0.7, \n                density=True, edgecolor='black')\n    axes[i].set_title(f'{name}\\n{title_stats}')\n    axes[i].set_xlabel('Income (transformed)')\n    axes[i].set_ylabel('Density')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Compare performance impact on different algorithms\nX = df.values\ny = (df['income'] &gt; df['income'].median()).astype(int)  # Binary target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Test different scalers with k-NN (scale-sensitive algorithm)\nprint(f\"\\nImpact of normalization on k-NN classifier:\")\n\nresults = {}\nfor name, scaler in scalers.items():\n    if scaler is None:\n        X_train_scaled = X_train\n        X_test_scaled = X_test\n    else:\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n    # Train k-NN classifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train_scaled, y_train)\n\n    # Evaluate\n    y_pred = knn.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    results[name] = accuracy\n    print(f\"{name:20}: {accuracy:.3f}\")\n\n# Visualize results\nplt.figure(figsize=(10, 6))\nmethods = list(results.keys())\naccuracies = list(results.values())\n\nbars = plt.bar(methods, accuracies, color='skyblue', edgecolor='navy', alpha=0.7)\nplt.title('k-NN Performance with Different Scaling Methods')\nplt.ylabel('Accuracy')\nplt.xlabel('Scaling Method')\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor bar, acc in zip(bars, accuracies):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n             f'{acc:.3f}', ha='center', va='bottom')\n\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#advanced-normalization-techniques","title":"Advanced Normalization Techniques","text":"<pre><code># Outlier-robust scaling comparison\nnp.random.seed(42)\n\n# Create data with outliers\nnormal_data = np.random.normal(0, 1, 1000)\noutliers = np.random.normal(10, 1, 50)  # Extreme outliers\ndata_with_outliers = np.concatenate([normal_data, outliers])\n\n# Compare different scalers on data with outliers\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Original data\naxes[0,0].hist(data_with_outliers, bins=50, alpha=0.7, edgecolor='black')\naxes[0,0].set_title('Original Data (with outliers)')\naxes[0,0].set_ylabel('Frequency')\n\n# StandardScaler (sensitive to outliers)\nstandard_scaler = StandardScaler()\ndata_standard = standard_scaler.fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\naxes[0,1].hist(data_standard, bins=50, alpha=0.7, edgecolor='black')\naxes[0,1].set_title(f'StandardScaler\\nMean: {data_standard.mean():.2f}, Std: {data_standard.std():.2f}')\n\n# RobustScaler (resistant to outliers)\nrobust_scaler = RobustScaler()\ndata_robust = robust_scaler.fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\naxes[1,0].hist(data_robust, bins=50, alpha=0.7, edgecolor='black')\naxes[1,0].set_title(f'RobustScaler\\nMedian: {np.median(data_robust):.2f}, IQR: {np.percentile(data_robust, 75) - np.percentile(data_robust, 25):.2f}')\naxes[1,0].set_ylabel('Frequency')\naxes[1,0].set_xlabel('Scaled Values')\n\n# QuantileTransformer (maps to uniform distribution)\nquantile_transformer = QuantileTransformer(output_distribution='uniform')\ndata_quantile = quantile_transformer.fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\naxes[1,1].hist(data_quantile, bins=50, alpha=0.7, edgecolor='black')\naxes[1,1].set_title(f'QuantileTransformer (Uniform)\\nRange: [{data_quantile.min():.2f}, {data_quantile.max():.2f}]')\naxes[1,1].set_xlabel('Scaled Values')\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate PowerTransformer for non-normal data\nfrom scipy.stats import skew\n\n# Create skewed data\nskewed_data = np.random.exponential(2, 1000)\nprint(f\"Original skewness: {skew(skewed_data):.3f}\")\n\n# Apply different transformations\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original\naxes[0].hist(skewed_data, bins=30, alpha=0.7, edgecolor='black', density=True)\naxes[0].set_title(f'Original Data\\nSkewness: {skew(skewed_data):.3f}')\naxes[0].set_ylabel('Density')\n\n# Box-Cox transformation\npower_transformer_box = PowerTransformer(method='box-cox')\ndata_box_cox = power_transformer_box.fit_transform(skewed_data.reshape(-1, 1)).flatten()\naxes[1].hist(data_box_cox, bins=30, alpha=0.7, edgecolor='black', density=True)\naxes[1].set_title(f'Box-Cox Transform\\nSkewness: {skew(data_box_cox):.3f}')\n\n# Yeo-Johnson transformation (can handle negative values)\npower_transformer_yj = PowerTransformer(method='yeo-johnson')\ndata_yj = power_transformer_yj.fit_transform(skewed_data.reshape(-1, 1)).flatten()\naxes[2].hist(data_yj, bins=30, alpha=0.7, edgecolor='black', density=True)\naxes[2].set_title(f'Yeo-Johnson Transform\\nSkewness: {skew(data_yj):.3f}')\n\nfor ax in axes:\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Values')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#regularisation-implementation","title":"Regularisation Implementation","text":"<pre><code>from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import validation_curve\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate regression dataset with potential for overfitting\nnp.random.seed(42)\nn_samples = 100\nn_features = 50\n\nX_reg = np.random.randn(n_samples, n_features)\n# Create target with only few features actually relevant\ntrue_coef = np.zeros(n_features)\ntrue_coef[:5] = [2, -3, 1, 4, -2]  # Only first 5 features matter\ny_reg = X_reg @ true_coef + 0.1 * np.random.randn(n_samples)\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42\n)\n\nprint(\"Regularization Comparison\")\nprint(f\"Dataset: {n_samples} samples, {n_features} features\")\nprint(f\"True non-zero coefficients: {np.sum(true_coef != 0)}\")\n\n# Compare different regularization techniques\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Ridge (L2)': Ridge(alpha=1.0),\n    'Lasso (L1)': Lasso(alpha=0.1),\n    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5)\n}\n\nresults = {}\ncoefficients = {}\n\nfor name, model in models.items():\n    # Train model\n    model.fit(X_train_reg, y_train_reg)\n\n    # Predictions\n    y_pred_train = model.predict(X_train_reg)\n    y_pred_test = model.predict(X_test_reg)\n\n    # Evaluate\n    train_mse = mean_squared_error(y_train_reg, y_pred_train)\n    test_mse = mean_squared_error(y_test_reg, y_pred_test)\n    train_r2 = r2_score(y_train_reg, y_pred_train)\n    test_r2 = r2_score(y_test_reg, y_pred_test)\n\n    # Store results\n    results[name] = {\n        'train_mse': train_mse,\n        'test_mse': test_mse,\n        'train_r2': train_r2,\n        'test_r2': test_r2,\n        'overfitting': train_r2 - test_r2  # Measure of overfitting\n    }\n\n    # Store coefficients\n    if hasattr(model, 'coef_'):\n        coefficients[name] = model.coef_\n        non_zero = np.sum(np.abs(model.coef_) &gt; 1e-5)\n        results[name]['non_zero_coef'] = non_zero\n\n    print(f\"\\n{name}:\")\n    print(f\"  Train R\u00b2: {train_r2:.3f}, Test R\u00b2: {test_r2:.3f}\")\n    print(f\"  Train MSE: {train_mse:.3f}, Test MSE: {test_mse:.3f}\")\n    print(f\"  Overfitting gap: {train_r2 - test_r2:.3f}\")\n    if hasattr(model, 'coef_'):\n        print(f\"  Non-zero coefficients: {non_zero}/{n_features}\")\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Performance comparison\nmodel_names = list(results.keys())\ntrain_r2s = [results[name]['train_r2'] for name in model_names]\ntest_r2s = [results[name]['test_r2'] for name in model_names]\n\nx = np.arange(len(model_names))\nwidth = 0.35\n\naxes[0,0].bar(x - width/2, train_r2s, width, label='Train R\u00b2', alpha=0.7)\naxes[0,0].bar(x + width/2, test_r2s, width, label='Test R\u00b2', alpha=0.7)\naxes[0,0].set_xlabel('Model')\naxes[0,0].set_ylabel('R\u00b2 Score')\naxes[0,0].set_title('Train vs Test Performance')\naxes[0,0].set_xticks(x)\naxes[0,0].set_xticklabels(model_names, rotation=45)\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# Overfitting comparison\noverfitting_gaps = [results[name]['overfitting'] for name in model_names]\naxes[0,1].bar(model_names, overfitting_gaps, alpha=0.7)\naxes[0,1].set_ylabel('Overfitting Gap (Train R\u00b2 - Test R\u00b2)')\naxes[0,1].set_title('Overfitting Comparison')\naxes[0,1].tick_params(axis='x', rotation=45)\naxes[0,1].grid(True, alpha=0.3)\n\n# Coefficient plots\n# True coefficients vs estimated\naxes[1,0].plot(true_coef, 'ko-', label='True coefficients', linewidth=2, markersize=6)\nfor name in ['Ridge (L2)', 'Lasso (L1)', 'Elastic Net']:\n    if name in coefficients:\n        axes[1,0].plot(coefficients[name], 'o-', label=name, alpha=0.7)\naxes[1,0].set_xlabel('Feature Index')\naxes[1,0].set_ylabel('Coefficient Value')\naxes[1,0].set_title('Coefficient Comparison')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Sparsity comparison (number of non-zero coefficients)\nsparsity_names = [name for name in model_names if name != 'Linear Regression']\nnon_zero_coefs = [results[name]['non_zero_coef'] for name in sparsity_names]\naxes[1,1].bar(sparsity_names, non_zero_coefs, alpha=0.7)\naxes[1,1].axhline(y=5, color='red', linestyle='--', label='True non-zero features')\naxes[1,1].set_ylabel('Number of Non-zero Coefficients')\naxes[1,1].set_title('Feature Selection (Sparsity)')\naxes[1,1].tick_params(axis='x', rotation=45)\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#regularisation-path-analysis","title":"Regularisation Path Analysis","text":"<pre><code># Analyze how regularization strength affects coefficients\nalphas = np.logspace(-4, 2, 50)\n\n# Ridge regression path\nridge_coefs = []\nridge_scores = []\n\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_reg, y_train_reg)\n    ridge_coefs.append(ridge.coef_)\n    score = ridge.score(X_test_reg, y_test_reg)\n    ridge_scores.append(score)\n\nridge_coefs = np.array(ridge_coefs)\n\n# Lasso regression path\nlasso_coefs = []\nlasso_scores = []\n\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha, max_iter=2000)\n    lasso.fit(X_train_reg, y_train_reg)\n    lasso_coefs.append(lasso.coef_)\n    score = lasso.score(X_test_reg, y_test_reg)\n    lasso_scores.append(score)\n\nlasso_coefs = np.array(lasso_coefs)\n\n# Plot regularization paths\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Ridge coefficients path\nfor i in range(min(10, n_features)):  # Plot first 10 features\n    axes[0,0].plot(alphas, ridge_coefs[:, i], label=f'Feature {i}' if i &lt; 5 else \"\")\naxes[0,0].set_xscale('log')\naxes[0,0].set_xlabel('Regularization Strength (\u03bb)')\naxes[0,0].set_ylabel('Coefficient Value')\naxes[0,0].set_title('Ridge Regression Path')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# Lasso coefficients path\nfor i in range(min(10, n_features)):  # Plot first 10 features\n    axes[0,1].plot(alphas, lasso_coefs[:, i], label=f'Feature {i}' if i &lt; 5 else \"\")\naxes[0,1].set_xscale('log')\naxes[0,1].set_xlabel('Regularization Strength (\u03bb)')\naxes[0,1].set_ylabel('Coefficient Value')\naxes[0,1].set_title('Lasso Regression Path')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# Performance vs regularization strength\naxes[1,0].plot(alphas, ridge_scores, 'b-', label='Ridge', linewidth=2)\naxes[1,0].plot(alphas, lasso_scores, 'r-', label='Lasso', linewidth=2)\naxes[1,0].set_xscale('log')\naxes[1,0].set_xlabel('Regularization Strength (\u03bb)')\naxes[1,0].set_ylabel('R\u00b2 Score')\naxes[1,0].set_title('Performance vs Regularization')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Number of non-zero coefficients (sparsity)\nridge_sparsity = [np.sum(np.abs(coef) &gt; 1e-5) for coef in ridge_coefs]\nlasso_sparsity = [np.sum(np.abs(coef) &gt; 1e-5) for coef in lasso_coefs]\n\naxes[1,1].plot(alphas, ridge_sparsity, 'b-', label='Ridge', linewidth=2)\naxes[1,1].plot(alphas, lasso_sparsity, 'r-', label='Lasso', linewidth=2)\naxes[1,1].axhline(y=5, color='green', linestyle='--', label='True non-zero features')\naxes[1,1].set_xscale('log')\naxes[1,1].set_xlabel('Regularization Strength (\u03bb)')\naxes[1,1].set_ylabel('Number of Non-zero Coefficients')\naxes[1,1].set_title('Sparsity vs Regularization')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nKey Insights:\")\nprint(f\"- Ridge: Shrinks coefficients but rarely makes them exactly zero\")\nprint(f\"- Lasso: Creates sparse solutions by setting coefficients to exactly zero\")\nprint(f\"- Optimal \u03bb for Ridge: {alphas[np.argmax(ridge_scores)]:.4f}\")\nprint(f\"- Optimal \u03bb for Lasso: {alphas[np.argmax(lasso_scores)]:.4f}\")\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#from-scratch-implementation","title":"\u03bb\u000f From Scratch Implementation","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#custom-scalers-implementation","title":"Custom Scalers Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass StandardScalerFromScratch:\n    def __init__(self):\n        self.mean_ = None\n        self.scale_ = None\n        self.fitted = False\n\n    def fit(self, X):\n        \"\"\"Compute mean and standard deviation for later scaling\"\"\"\n        X = np.array(X)\n        self.mean_ = np.mean(X, axis=0)\n        self.scale_ = np.std(X, axis=0, ddof=1)  # Sample standard deviation\n\n        # Handle zero variance features\n        self.scale_[self.scale_ == 0] = 1.0\n\n        self.fitted = True\n        return self\n\n    def transform(self, X):\n        \"\"\"Scale features using computed statistics\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler has not been fitted yet.\")\n\n        X = np.array(X)\n        return (X - self.mean_) / self.scale_\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X).transform(X)\n\n    def inverse_transform(self, X_scaled):\n        \"\"\"Convert scaled features back to original scale\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler has not been fitted yet.\")\n\n        X_scaled = np.array(X_scaled)\n        return X_scaled * self.scale_ + self.mean_\n\nclass MinMaxScalerFromScratch:\n    def __init__(self, feature_range=(0, 1)):\n        self.feature_range = feature_range\n        self.min_ = None\n        self.scale_ = None\n        self.data_min_ = None\n        self.data_max_ = None\n        self.fitted = False\n\n    def fit(self, X):\n        \"\"\"Compute min and max for later scaling\"\"\"\n        X = np.array(X)\n        self.data_min_ = np.min(X, axis=0)\n        self.data_max_ = np.max(X, axis=0)\n\n        # Compute scaling parameters\n        data_range = self.data_max_ - self.data_min_\n        data_range[data_range == 0] = 1.0  # Handle constant features\n\n        self.scale_ = (self.feature_range[1] - self.feature_range[0]) / data_range\n        self.min_ = self.feature_range[0] - self.data_min_ * self.scale_\n\n        self.fitted = True\n        return self\n\n    def transform(self, X):\n        \"\"\"Scale features to specified range\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler has not been fitted yet.\")\n\n        X = np.array(X)\n        return X * self.scale_ + self.min_\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X).transform(X)\n\n    def inverse_transform(self, X_scaled):\n        \"\"\"Convert scaled features back to original scale\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler has not been fitted yet.\")\n\n        X_scaled = np.array(X_scaled)\n        return (X_scaled - self.min_) / self.scale_\n\nclass RobustScalerFromScratch:\n    def __init__(self):\n        self.center_ = None\n        self.scale_ = None\n        self.fitted = False\n\n    def fit(self, X):\n        \"\"\"Compute median and IQR for later scaling\"\"\"\n        X = np.array(X)\n        self.center_ = np.median(X, axis=0)\n\n        # Compute IQR (75th percentile - 25th percentile)\n        q75 = np.percentile(X, 75, axis=0)\n        q25 = np.percentile(X, 25, axis=0)\n        self.scale_ = q75 - q25\n\n        # Handle zero IQR\n        self.scale_[self.scale_ == 0] = 1.0\n\n        self.fitted = True\n        return self\n\n    def transform(self, X):\n        \"\"\"Scale features using median and IQR\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler has not been fitted yet.\")\n\n        X = np.array(X)\n        return (X - self.center_) / self.scale_\n\n    def fit_transform(self, X):\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X).transform(X)\n\n# Demonstration with synthetic data\nnp.random.seed(42)\n\n# Create data with outliers\nnormal_data = np.random.normal(10, 2, (100, 3))\noutliers = np.array([[50, 5, 15], [60, 8, 20], [-20, 1, 5]])  # Add outliers\ndata = np.vstack([normal_data, outliers])\n\nprint(\"Custom Scalers Demonstration\")\nprint(f\"Original data shape: {data.shape}\")\nprint(f\"Original data statistics:\")\nprint(f\"Mean: {np.mean(data, axis=0)}\")\nprint(f\"Std: {np.std(data, axis=0)}\")\nprint(f\"Min: {np.min(data, axis=0)}\")\nprint(f\"Max: {np.max(data, axis=0)}\")\n\n# Test custom scalers\nscalers_custom = {\n    'Custom StandardScaler': StandardScalerFromScratch(),\n    'Custom MinMaxScaler': MinMaxScalerFromScratch(),\n    'Custom RobustScaler': RobustScalerFromScratch()\n}\n\n# Compare with sklearn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nscalers_sklearn = {\n    'Sklearn StandardScaler': StandardScaler(),\n    'Sklearn MinMaxScaler': MinMaxScaler(),\n    'Sklearn RobustScaler': RobustScaler()\n}\n\n# Test and compare\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\nfor i, (name, scaler) in enumerate(scalers_custom.items()):\n    # Apply custom scaler\n    data_scaled_custom = scaler.fit_transform(data)\n\n    # Apply sklearn scaler for comparison\n    sklearn_name = name.replace('Custom', 'Sklearn')\n    sklearn_scaler = scalers_sklearn[sklearn_name]\n    data_scaled_sklearn = sklearn_scaler.fit_transform(data)\n\n    # Plot comparison for first feature\n    axes[0, i].hist(data_scaled_custom[:, 0], bins=20, alpha=0.7, \n                    label='Custom', color='blue', density=True)\n    axes[0, i].hist(data_scaled_sklearn[:, 0], bins=20, alpha=0.7, \n                    label='Sklearn', color='red', density=True)\n    axes[0, i].set_title(f'{name}\\n(Feature 1)')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n\n    # Check numerical differences\n    max_diff = np.max(np.abs(data_scaled_custom - data_scaled_sklearn))\n    axes[1, i].scatter(data_scaled_custom.flatten(), \n                      data_scaled_sklearn.flatten(), alpha=0.6)\n    axes[1, i].plot([data_scaled_custom.min(), data_scaled_custom.max()],\n                    [data_scaled_custom.min(), data_scaled_custom.max()], 'r--')\n    axes[1, i].set_xlabel('Custom Scaler')\n    axes[1, i].set_ylabel('Sklearn Scaler')\n    axes[1, i].set_title(f'Comparison\\nMax diff: {max_diff:.2e}')\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Test inverse transform\nprint(f\"\\nInverse Transform Test:\")\nscaler_test = StandardScalerFromScratch()\ndata_scaled = scaler_test.fit_transform(data)\ndata_reconstructed = scaler_test.inverse_transform(data_scaled)\n\nreconstruction_error = np.max(np.abs(data - data_reconstructed))\nprint(f\"Max reconstruction error: {reconstruction_error:.2e}\")\nprint(\"\u0013 Inverse transform working correctly\" if reconstruction_error &lt; 1e-10 else \"\u0017 Inverse transform failed\")\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#custom-regularised-regression","title":"Custom Regularised Regression","text":"<pre><code>class RidgeRegressionFromScratch:\n    def __init__(self, alpha=1.0, fit_intercept=True):\n        self.alpha = alpha\n        self.fit_intercept = fit_intercept\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        \"\"\"Fit Ridge regression using closed-form solution\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        if self.fit_intercept:\n            # Add intercept term\n            X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n        else:\n            X_with_intercept = X\n\n        n_features = X_with_intercept.shape[1]\n\n        # Ridge regression closed-form solution: (X'X + \u03bbI)^(-1)X'y\n        # Don't regularize the intercept term\n        I = np.eye(n_features)\n        if self.fit_intercept:\n            I[0, 0] = 0  # Don't regularize intercept\n\n        XTX_plus_alphaI = X_with_intercept.T @ X_with_intercept + self.alpha * I\n        XTy = X_with_intercept.T @ y\n\n        # Solve the system\n        params = np.linalg.solve(XTX_plus_alphaI, XTy)\n\n        if self.fit_intercept:\n            self.intercept_ = params[0]\n            self.coef_ = params[1:]\n        else:\n            self.intercept_ = 0\n            self.coef_ = params\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        X = np.array(X)\n        return X @ self.coef_ + self.intercept_\n\n    def score(self, X, y):\n        \"\"\"Calculate R\u00b2 score\"\"\"\n        y_pred = self.predict(X)\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        return 1 - (ss_res / ss_tot)\n\nclass LassoRegressionFromScratch:\n    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n        self.intercept_ = None\n\n    def _soft_threshold(self, x, alpha):\n        \"\"\"Soft thresholding operator for L1 regularization\"\"\"\n        return np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n\n    def fit(self, X, y):\n        \"\"\"Fit Lasso regression using coordinate descent\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n\n        # Center the data\n        X_mean = np.mean(X, axis=0)\n        y_mean = np.mean(y)\n        X_centered = X - X_mean\n        y_centered = y - y_mean\n\n        n_samples, n_features = X_centered.shape\n\n        # Initialize coefficients\n        self.coef_ = np.zeros(n_features)\n\n        # Precompute X'X diagonal for coordinate descent\n        XTX_diag = np.sum(X_centered ** 2, axis=0)\n\n        # Coordinate descent\n        for iteration in range(self.max_iter):\n            coef_old = self.coef_.copy()\n\n            for j in range(n_features):\n                if XTX_diag[j] == 0:\n                    continue\n\n                # Compute residual without j-th feature\n                residual = y_centered - X_centered @ self.coef_ + self.coef_[j] * X_centered[:, j]\n\n                # Update coefficient using soft thresholding\n                rho = X_centered[:, j] @ residual\n                self.coef_[j] = self._soft_threshold(rho, self.alpha * n_samples) / XTX_diag[j]\n\n            # Check convergence\n            if np.max(np.abs(self.coef_ - coef_old)) &lt; self.tol:\n                break\n\n        # Calculate intercept\n        self.intercept_ = y_mean - X_mean @ self.coef_\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        X = np.array(X)\n        return X @ self.coef_ + self.intercept_\n\n    def score(self, X, y):\n        \"\"\"Calculate R\u00b2 score\"\"\"\n        y_pred = self.predict(X)\n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        return 1 - (ss_res / ss_tot)\n\n# Test custom regularized regression\nnp.random.seed(42)\n\n# Generate test data\nn_samples, n_features = 50, 20\nX_test = np.random.randn(n_samples, n_features)\ntrue_coef = np.random.randn(n_features)\ntrue_coef[10:] = 0  # Make last 10 coefficients zero\ny_test = X_test @ true_coef + 0.1 * np.random.randn(n_samples)\n\n# Split data\nX_train_test, X_val_test, y_train_test, y_val_test = train_test_split(\n    X_test, y_test, test_size=0.3, random_state=42\n)\n\nprint(\"Custom Regularized Regression Test\")\n\n# Test Ridge regression\nridge_custom = RidgeRegressionFromScratch(alpha=1.0)\nridge_custom.fit(X_train_test, y_train_test)\n\nridge_sklearn = Ridge(alpha=1.0)\nridge_sklearn.fit(X_train_test, y_train_test)\n\nprint(f\"\\nRidge Regression Comparison:\")\nprint(f\"Custom Ridge R\u00b2: {ridge_custom.score(X_val_test, y_val_test):.4f}\")\nprint(f\"Sklearn Ridge R\u00b2: {ridge_sklearn.score(X_val_test, y_val_test):.4f}\")\n\ncoef_diff_ridge = np.max(np.abs(ridge_custom.coef_ - ridge_sklearn.coef_))\nprint(f\"Max coefficient difference: {coef_diff_ridge:.2e}\")\n\n# Test Lasso regression\nlasso_custom = LassoRegressionFromScratch(alpha=0.1, max_iter=2000)\nlasso_custom.fit(X_train_test, y_train_test)\n\nlasso_sklearn = Lasso(alpha=0.1, max_iter=2000)\nlasso_sklearn.fit(X_train_test, y_train_test)\n\nprint(f\"\\nLasso Regression Comparison:\")\nprint(f\"Custom Lasso R\u00b2: {lasso_custom.score(X_val_test, y_val_test):.4f}\")\nprint(f\"Sklearn Lasso R\u00b2: {lasso_sklearn.score(X_val_test, y_val_test):.4f}\")\n\n# Compare sparsity\ncustom_nonzero = np.sum(np.abs(lasso_custom.coef_) &gt; 1e-5)\nsklearn_nonzero = np.sum(np.abs(lasso_sklearn.coef_) &gt; 1e-5)\nprint(f\"Custom Lasso non-zero coefficients: {custom_nonzero}\")\nprint(f\"Sklearn Lasso non-zero coefficients: {sklearn_nonzero}\")\n\n# Visualize coefficient comparison\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Ridge coefficients\naxes[0].scatter(ridge_custom.coef_, ridge_sklearn.coef_, alpha=0.7)\naxes[0].plot([ridge_custom.coef_.min(), ridge_custom.coef_.max()],\n             [ridge_custom.coef_.min(), ridge_custom.coef_.max()], 'r--')\naxes[0].set_xlabel('Custom Ridge Coefficients')\naxes[0].set_ylabel('Sklearn Ridge Coefficients')\naxes[0].set_title('Ridge Regression Coefficients Comparison')\naxes[0].grid(True, alpha=0.3)\n\n# Lasso coefficients\naxes[1].scatter(lasso_custom.coef_, lasso_sklearn.coef_, alpha=0.7)\naxes[1].plot([lasso_custom.coef_.min(), lasso_custom.coef_.max()],\n             [lasso_custom.coef_.min(), lasso_custom.coef_.max()], 'r--')\naxes[1].set_xlabel('Custom Lasso Coefficients')\naxes[1].set_ylabel('Sklearn Lasso Coefficients')\naxes[1].set_title('Lasso Regression Coefficients Comparison')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#assumptions-and-limitations","title":"\u03bb\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#normalization-assumptions-and-limitations","title":"Normalization Assumptions and Limitations","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#key-assumptions","title":"Key Assumptions","text":"<ol> <li>Feature independence: Different scaling methods assume features are independent</li> <li>Distribution stability: Scaling parameters computed on training data apply to test data</li> <li>Outlier handling: StandardScaler assumes roughly normal distribution</li> <li>Missing values: Most scalers require handling missing values beforehand</li> </ol>"},{"location":"Machine-Learning/Normalization%20Regularisation/#limitations","title":"Limitations","text":"<ol> <li>StandardScaler limitations:</li> <li>Sensitive to outliers: Outliers heavily influence mean and standard deviation</li> <li>Assumes normal distribution: Works best with normally distributed data</li> <li> <p>Solution: Use RobustScaler for data with outliers</p> </li> <li> <p>MinMaxScaler limitations:</p> </li> <li>Very sensitive to outliers: Single outlier can compress all other values</li> <li>Fixed range assumption: Assumes test data falls within training range</li> <li> <p>Solution: Use robust scaling or outlier detection</p> </li> <li> <p>RobustScaler limitations:</p> </li> <li>Less efficient: May not utilize full feature range</li> <li>Assumes symmetric distribution around median</li> <li> <p>Assessment: Check if IQR-based scaling is appropriate</p> </li> <li> <p>Data leakage risk: Fitting scaler on entire dataset before train/test split</p> </li> <li>Critical error: Using test data to compute scaling parameters</li> <li>Solution: Always fit scaler only on training data</li> </ol>"},{"location":"Machine-Learning/Normalization%20Regularisation/#regularisation-assumptions-and-limitations","title":"Regularisation Assumptions and Limitations","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#key-assumptions_1","title":"Key Assumptions","text":"<ol> <li>Smooth coefficient penalty: Assumes large coefficients are undesirable</li> <li>Feature relevance: L1 assumes many features are irrelevant (sparsity assumption)</li> <li>Linear relationship: Regularization assumes linear model structure</li> <li>Homoscedastic errors: Assumes constant error variance</li> </ol>"},{"location":"Machine-Learning/Normalization%20Regularisation/#limitations_1","title":"Limitations","text":"<ol> <li>L1 (Lasso) limitations:</li> <li>Arbitrary feature selection: With correlated features, randomly picks one</li> <li>Bias introduction: Can be overly aggressive in shrinking coefficients</li> <li> <p>Solution: Use Elastic Net to combine L1 and L2</p> </li> <li> <p>L2 (Ridge) limitations:</p> </li> <li>No feature selection: Shrinks but doesn't eliminate features</li> <li>Multicollinearity: Distributes weight among correlated features</li> <li> <p>Alternative: Use Lasso for feature selection</p> </li> <li> <p>Hyperparameter sensitivity: Performance heavily depends on regularization strength</p> </li> <li>Challenge: Requires careful tuning using cross-validation</li> <li> <p>Solution: Use automated hyperparameter optimization</p> </li> <li> <p>Computational complexity: Some regularization methods scale poorly</p> </li> <li>Impact: Lasso coordinate descent can be slow on very high-dimensional data</li> <li>Solution: Use specialized libraries or approximate methods</li> </ol>"},{"location":"Machine-Learning/Normalization%20Regularisation/#comparison-of-techniques","title":"Comparison of Techniques","text":"Aspect StandardScaler MinMaxScaler RobustScaler L1 (Lasso) L2 (Ridge) Outlier Sensitivity High Very High Low Medium Medium Preserves Distribution Yes No Partially N/A N/A Computational Cost Low Low Medium High Low Feature Selection N/A N/A N/A Yes No Interpretability N/A N/A N/A High Medium <p>When to use each technique:</p> <p>Normalization: - StandardScaler: Normal distributions, no outliers, most ML algorithms - MinMaxScaler: Bounded features needed, neural networks - RobustScaler: Data with outliers, non-normal distributions - QuantileTransformer: Heavy outliers, need uniform distribution</p> <p>Regularisation: - Ridge: Multicollinearity, want to keep all features - Lasso: Feature selection needed, sparse solution desired - Elastic Net: Correlated features, balanced selection and shrinkage</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#interview-questions","title":"\u2753 Interview Questions","text":"Why is feature normalization important in machine learning, and when might you skip it? <p>Answer: Feature normalization is crucial for algorithms sensitive to feature scales:</p> <p>Why normalization matters: 1. Scale sensitivity: Algorithms like SVM, k-NN, neural networks use distance metrics 2. Convergence speed: Gradient descent converges faster with normalized features 3. Numerical stability: Prevents overflow/underflow in computations 4. Fair feature contribution: Ensures all features contribute meaningfully</p> <p>Example impact: <pre><code># Without normalization\nfeatures = [[50000, 25], [60000, 30]]  # [income, age]\n# Distance dominated by income differences\n\n# With normalization  \nfeatures_norm = [[0.1, 0.2], [0.6, 0.8]]\n# Both features contribute to distance\n</code></pre></p> <p>When to skip normalization: - Tree-based models: Decision trees, Random Forest, XGBoost (scale-invariant) - Naive Bayes: Works with original feature distributions - Linear regression with interpretability needs: Keep original coefficient meanings - Count data: When raw counts are meaningful (e.g., word frequencies)</p> <p>Algorithm sensitivity: - Requires normalization: SVM, k-NN, neural networks, PCA, clustering - Doesn't require: Tree-based models, Naive Bayes</p> Compare StandardScaler, MinMaxScaler, and RobustScaler. When would you use each? <p>Answer: Each scaler handles different data characteristics:</p> <p>StandardScaler (Z-score normalization): - Formula: <code>(x - mean) / std</code> - Result: Mean = 0, Std = 1 - Best for: Normally distributed data without outliers - Use cases: Most ML algorithms, when data follows Gaussian distribution</p> <p>MinMaxScaler: - Formula: <code>(x - min) / (max - min)</code> - Result: Range [0, 1] or custom range - Best for: Bounded output needed, neural networks - Use cases: Image processing, when you need specific value ranges</p> <p>RobustScaler: - Formula: <code>(x - median) / IQR</code> - Result: Median = 0, IQR-based scale - Best for: Data with outliers, non-normal distributions - Use cases: Financial data, medical data with extreme values</p> <p>Comparison with outliers: <pre><code>data = [1, 2, 3, 4, 5, 100]  # Last value is outlier\n\n# StandardScaler: All values affected by outlier\n# MinMaxScaler: Most values compressed near 0\n# RobustScaler: Outlier has minimal impact on scaling\n</code></pre></p> <p>Decision framework: 1. Check for outliers \u03bb If many, use RobustScaler 2. Check distribution \u03bb If normal, use StandardScaler 3. Check requirements \u03bb If bounded output needed, use MinMaxScaler 4. Algorithm requirements \u03bb Neural networks often prefer MinMax</p> Explain the difference between L1 and L2 regularization. When would you use each? <p>Answer: L1 and L2 regularization differ in penalty function and effects:</p> <p>L1 Regularization (Lasso): - Penalty: \\(\\lambda \\sum |w_i|\\) (sum of absolute values) - Effect: Creates sparse solutions (sets coefficients to exactly zero) - Gradient: Constant magnitude, doesn't shrink with coefficient size - Feature selection: Automatically selects relevant features</p> <p>L2 Regularization (Ridge): - Penalty: \\(\\lambda \\sum w_i^2\\) (sum of squared values) - Effect: Shrinks coefficients but rarely zeros them - Gradient: Proportional to coefficient size - Multicollinearity: Handles correlated features by distributing weights</p> <p>Mathematical intuition: <pre><code>L1 gradient: /w (\u03bb|w|) = \u03bb\u03bbsign(w)    # Constant push toward zero\nL2 gradient: /w (\u03bbw\u03bb) = 2\u03bbw           # Proportional shrinkage\n</code></pre></p> <p>Visual difference: - L1 constraint region: Diamond shape \u03bb creates sparsity at corners - L2 constraint region: Circle shape \u03bb shrinks uniformly</p> <p>When to use L1: - \u0005 Feature selection needed - \u0005 Interpretable sparse models - \u0005 High-dimensional data with irrelevant features - \u0005 Storage/computation constraints</p> <p>When to use L2: - \u0005 All features potentially relevant - \u0005 Multicollinearity present - \u0005 Stability over sparsity - \u0005 Better numerical properties</p> <p>Elastic Net combines both: \\(\\alpha \\rho ||w||_1 + \\alpha(1-\\rho)||w||_2^2\\)</p> How do you determine the optimal regularization strength (lambda/alpha)? <p>Answer: Several approaches for finding optimal regularization strength:</p> <p>1. Cross-Validation (Most common): <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\n\nalphas = [0.1, 1.0, 10.0, 100.0]\nbest_alpha = None\nbest_score = -np.inf\n\nfor alpha in alphas:\n    model = Ridge(alpha=alpha)\n    scores = cross_val_score(model, X, y, cv=5)\n    if scores.mean() &gt; best_score:\n        best_score = scores.mean()\n        best_alpha = alpha\n</code></pre></p> <p>2. Grid Search with Cross-Validation: <pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'alpha': np.logspace(-4, 4, 20)}\ngrid_search = GridSearchCV(Ridge(), param_grid, cv=5)\ngrid_search.fit(X, y)\noptimal_alpha = grid_search.best_params_['alpha']\n</code></pre></p> <p>3. Regularization Path Analysis: - Plot performance vs. regularization strength - Look for \"elbow\" in validation curve - Balance bias-variance tradeoff</p> <p>4. Information Criteria (AIC/BIC): <pre><code># For model selection without separate validation set\n# AIC = 2k - 2ln(L)  where k=parameters, L=likelihood\n</code></pre></p> <p>5. Early Stopping: - Monitor validation loss during training - Stop when validation loss stops improving - Implicit regularization through training time</p> <p>Search strategies: - Coarse to fine: Start with wide range, then narrow down - Logarithmic spacing: Use <code>np.logspace</code> for wide range exploration - Nested CV: Use inner CV for hyperparameter selection, outer CV for evaluation</p> <p>Practical tips: - Start with wide range: [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100] - Use stratified CV for classification - Consider computational budget vs. accuracy needs - Validate final choice on completely separate test set</p> What is the bias-variance tradeoff in the context of regularization? <p>Answer: Regularization directly addresses the bias-variance tradeoff:</p> <p>Bias-Variance Decomposition: \\(\\(E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}\\)\\)</p> <p>Without Regularization: - Low bias: Model can fit training data well - High variance: Model overfits, predictions vary greatly with training data - Risk: Poor generalization to new data</p> <p>With Regularization: - Higher bias: Model constrained, can't fit training data perfectly - Lower variance: More stable predictions across different training sets - Goal: Minimize total error = Bias\u03bb + Variance + Noise</p> <p>Regularization effects: <pre><code># No regularization (\u03bb = 0): High variance, low bias\n# Strong regularization (\u03bb &gt;&gt; 1): Low variance, high bias  \n# Optimal \u03bb: Minimizes bias\u03bb + variance\n</code></pre></p> <p>Visual intuition: - Underfit (too much regularization): High bias, predictions too simple - Overfit (too little regularization): High variance, predictions too complex - Just right: Balanced complexity, good generalization</p> <p>Practical example: <pre><code># Polynomial regression with different regularization\n\u03bb = 0:    Perfect training fit, poor test performance (overfit)\n\u03bb = 0.1:  Good training fit, good test performance (balanced)  \n\u03bb = 100:  Poor training fit, poor test performance (underfit)\n</code></pre></p> <p>How to detect: - High variance: Large gap between training and validation performance - High bias: Both training and validation performance are poor - Optimal point: Minimal validation error</p> <p>Regularization strength effects: - Increasing \u03bb: Reduces variance, increases bias - Decreasing \u03bb: Reduces bias, increases variance - Sweet spot: Cross-validation finds optimal balance</p> How does regularization help with multicollinearity, and what's the difference between Ridge and Lasso in handling it? <p>Answer: Regularization addresses multicollinearity differently depending on the type:</p> <p>Multicollinearity problem: - Issue: When features are highly correlated, ordinary least squares becomes unstable - Effect: Small changes in data cause large changes in coefficients - Math: \\((X^T X)\\) becomes nearly singular, leading to unstable \\((X^T X)^{-1}\\)</p> <p>Ridge Regression approach: - Solution: Adds \\(\\lambda I\\) to \\((X^T X)\\), making it invertible - Formula: \\((X^T X + \\lambda I)^{-1} X^T y\\) - Effect: Distributes coefficients among correlated features - Example: If features A and B are identical, Ridge gives both coefficient = 0.5</p> <p>Lasso Regression approach: - Solution: L1 penalty forces sparsity - Effect: Arbitrarily picks one feature from correlated group - Example: If features A and B are identical, Lasso gives one coefficient = 1, other = 0 - Limitation: Selection among correlated features is somewhat random</p> <p>Practical example: <pre><code># Highly correlated features: house size and number of rooms\nX = [[2000, 4], [2500, 5], [3000, 6]]  # [sqft, rooms]\n\n# Ridge: Both features get partial coefficients\n# Ridge coefficients: [0.7, 0.6] (both contribute)\n\n# Lasso: One feature dominates  \n# Lasso coefficients: [1.2, 0.0] (only sqft matters)\n</code></pre></p> <p>Elastic Net solution: - Combines both: \\(\\alpha \\rho ||w||_1 + \\alpha(1-\\rho)||w||_2^2\\) - Advantage: Groups correlated features together (like Ridge) but maintains sparsity (like Lasso) - Best of both: Handles multicollinearity while doing feature selection</p> <p>Comparison summary: | Aspect | Ridge | Lasso | Elastic Net | |--------|-------|-------|-------------| | Multicollinearity | Distributes weights | Random selection | Groups + selects | | Stability | High | Can be unstable | High | | Feature selection | No | Yes | Yes | | Interpretability | Medium | High | High |</p> <p>When to use each: - Ridge: When you believe all features are relevant - Lasso: When you need automatic feature selection - Elastic Net: When you have groups of correlated features</p> Explain data leakage in the context of feature scaling and how to prevent it. <p>Answer: Data leakage in feature scaling occurs when test set information influences the scaling parameters:</p> <p>What is scaling data leakage? - Problem: Using entire dataset (including test set) to compute scaling parameters - Effect: Model has indirect access to test set information - Result: Overly optimistic performance estimates</p> <p>Common mistakes: <pre><code># WRONG: Scaling before train/test split\nX_scaled = StandardScaler().fit_transform(X)  # Uses ALL data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n\n# WRONG: Fitting scaler on combined data\nscaler = StandardScaler().fit(np.vstack([X_train, X_test]))\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre></p> <p>Correct approach: <pre><code># CORRECT: Split first, then scale\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # Fit only on training\nX_test_scaled = scaler.transform(X_test)        # Transform using train params\n</code></pre></p> <p>Why this matters: - Statistical contamination: Test set statistics influence training - Optimistic bias: Model appears better than it actually is - Production problems: Real-world performance differs from validation</p> <p>Cross-validation considerations: <pre><code># CORRECT: Scaling inside CV loop\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression())\n])\n\n# Scaler fitted separately for each CV fold\nscores = cross_val_score(pipeline, X, y, cv=5)\n</code></pre></p> <p>Time series special case: - Problem: Future information leaking to past predictions - Solution: Use forward-chaining validation, fit scaler only on past data</p> <p>Impact magnitude: - Usually small but can be significant with small datasets - More problematic with MinMaxScaler (uses min/max) - Less problematic with RobustScaler (uses median/IQR)</p> <p>Detection methods: - Compare performance with/without proper scaling separation - Check if test performance seems unrealistically high - Validate scaling parameters make sense for training data only</p> How do you handle categorical features when applying normalization? <p>Answer: Categorical features require special handling as traditional scaling methods don't apply:</p> <p>Why traditional scaling fails: - No inherent order: Categories like [Red, Blue, Green] have no meaningful distance - Arbitrary encoding: Label encoding creates fake ordinal relationships - Scale meaningless: Normalizing [1, 2, 3] for categories is nonsensical</p> <p>Proper approaches:</p> <p>1. One-Hot Encoding (most common): <pre><code>from sklearn.preprocessing import OneHotEncoder\n\n# Original: ['Red', 'Blue', 'Red', 'Green']  \n# One-hot: [[1,0,0], [0,1,0], [1,0,0], [0,0,1]]\n\nohe = OneHotEncoder(drop='first', sparse=False)  # Avoid multicollinearity\nX_categorical_encoded = ohe.fit_transform(X_categorical)\n\n# Then apply scaling to numerical features only\n</code></pre></p> <p>2. Target Encoding: <pre><code># Replace category with mean target value for that category\ncategory_means = df.groupby('category')['target'].mean()\ndf['category_encoded'] = df['category'].map(category_means)\n\n# Can then apply scaling to encoded values\n</code></pre></p> <p>3. Mixed data pipeline: <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Specify which columns are categorical vs numerical\nnumerical_features = ['age', 'income', 'credit_score']\ncategorical_features = ['city', 'job_type', 'education']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ]\n)\n\nX_processed = preprocessor.fit_transform(X)\n</code></pre></p> <p>4. Ordinal encoding (only for ordinal categories): <pre><code># Only for inherently ordered categories\neducation_mapping = {\n    'High School': 1,\n    'Bachelor': 2, \n    'Master': 3,\n    'PhD': 4\n}\n\ndf['education_ordinal'] = df['education'].map(education_mapping)\n# Can then apply scaling\n</code></pre></p> <p>Best practices: - Separate preprocessing: Handle categorical and numerical features separately - Pipeline usage: Use sklearn pipelines to prevent data leakage - High cardinality: Consider target encoding or embedding for many categories - Rare categories: Group infrequent categories into \"Other\" before encoding - Validation: Ensure consistent categories between train/test sets</p> <p>Common pitfalls: - Scaling label-encoded categorical features - Forgetting to handle new categories in test set - Creating too many dummy variables (curse of dimensionality) - Data leakage in target encoding without proper CV</p> What are some advanced regularization techniques beyond L1/L2? <p>Answer: Several advanced regularization techniques beyond basic L1/L2:</p> <p>1. Elastic Net: - Formula: \\(\\alpha \\rho ||w||_1 + \\alpha(1-\\rho)||w||_2^2\\) - Advantage: Combines L1 sparsity with L2 stability - Use case: Correlated features where you want grouping + selection</p> <p>2. Group Lasso: - Concept: Regularizes groups of features together - Formula: \\(\\lambda \\sum_{g} ||w_g||_2\\) where \\(g\\) represents feature groups - Effect: Either selects entire group or zeros out entire group - Use case: Gene expression, image pixels, polynomial features</p> <p>3. Fused Lasso (Total Variation): - Formula: \\(\\lambda_1 ||w||_1 + \\lambda_2 \\sum_{i} |w_i - w_{i+1}|\\) - Effect: Promotes sparsity + smooth coefficient transitions - Use case: Time series, spatial data, signal processing</p> <p>4. Nuclear Norm (Matrix Regularization): - Formula: \\(\\lambda ||W||_*\\) (sum of singular values) - Effect: Promotes low-rank solutions - Use case: Matrix completion, collaborative filtering</p> <p>5. Dropout (Neural Networks): <pre><code># Randomly set neurons to zero during training\ndef dropout(x, rate=0.5, training=True):\n    if training:\n        mask = np.random.binomial(1, 1-rate, x.shape)\n        return x * mask / (1-rate)\n    return x\n</code></pre></p> <p>6. Batch Normalization: - Concept: Normalize layer inputs during training - Effect: Stabilizes training, acts as regularization - Formula: \\(\\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} * \\gamma + \\beta\\)</p> <p>7. Data Augmentation: <pre><code># Create additional training examples through transformations\n# Images: rotation, scaling, flipping\n# Text: synonym replacement, back-translation\n# Time series: jittering, warping\n</code></pre></p> <p>8. Early Stopping: - Method: Stop training when validation loss stops improving - Effect: Prevents overfitting through limited training time - Implementation: Monitor validation loss, stop after patience epochs</p> <p>9. Weight Decay: - Concept: Gradually reduce all weights during training - Formula: \\(w_{t+1} = (1-\\lambda)w_t - \\alpha \\nabla L\\) - Effect: Similar to L2 but applied during optimization</p> <p>10. Spectral Normalization: - Method: Constrain spectral norm of weight matrices - Effect: Stabilizes GAN training, improves generalization - Use case: Generative models, discriminator regularization</p> <p>Advanced combinations: <pre><code># Multi-task learning with shared regularization\nLoss = \u03bb TaskLoss_i + \u03bb\u03bb||W_shared||\u03bb\u03bb + \u03bb\u03bb||W_specific||\u03bb\n\n# Adaptive regularization (learning \u03bb)\n\u03bb = \u03bb\u03bb * exp(-decay * epoch)\n</code></pre></p> <p>Selection criteria: - Data structure: Spatial/temporal data \u03bb Fused Lasso - High dimensions: Group Lasso, Nuclear norm - Neural networks: Dropout, Batch norm, Weight decay - Interpretability needs: L1, Group Lasso - Stability needs: L2, Elastic Net</p>"},{"location":"Machine-Learning/Normalization%20Regularisation/#examples","title":"\ud83d\udcdd Examples","text":""},{"location":"Machine-Learning/Normalization%20Regularisation/#real-world-example-customer-churn-prediction","title":"Real-world Example: Customer Churn Prediction","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate realistic customer churn dataset\nnp.random.seed(42)\nn_customers = 5000\n\n# Create realistic customer features with different scales\ndata = {\n    'customer_id': range(1, n_customers + 1),\n    'age': np.random.normal(40, 15, n_customers).clip(18, 80),\n    'monthly_charges': np.random.normal(70, 25, n_customers).clip(20, 200),\n    'total_charges': np.random.normal(2500, 1500, n_customers).clip(100, 8000),\n    'contract_length': np.random.choice([1, 12, 24], n_customers, p=[0.3, 0.4, 0.3]),\n    'num_services': np.random.poisson(3, n_customers).clip(1, 8),\n    'support_calls': np.random.poisson(2, n_customers),\n    'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Cash', 'Check'], \n                                      n_customers, p=[0.4, 0.3, 0.2, 0.1]),\n    'internet_type': np.random.choice(['DSL', 'Fiber', 'Cable', 'None'], \n                                     n_customers, p=[0.3, 0.3, 0.3, 0.1]),\n    'senior_citizen': np.random.choice([0, 1], n_customers, p=[0.85, 0.15])\n}\n\n# Create target variable (churn) with realistic relationships\nchurn_prob = (\n    0.1 +  # Base churn rate\n    0.1 * (data['monthly_charges'] &gt; 100) +  # High charges increase churn\n    0.15 * (data['contract_length'] == 1) +  # Month-to-month increases churn\n    0.1 * (data['support_calls'] &gt; 3) +      # Many support calls indicate issues\n    0.05 * data['senior_citizen'] +          # Seniors slightly more likely to churn\n    -0.08 * (data['contract_length'] == 24)  # Long contracts reduce churn\n).clip(0, 1)\n\ndata['churn'] = np.random.binomial(1, churn_prob, n_customers)\n\n# Create DataFrame\ndf_churn = pd.DataFrame(data)\n\nprint(\"Customer Churn Prediction - Normalization &amp; Regularization Demo\")\nprint(f\"Dataset shape: {df_churn.shape}\")\nprint(f\"Churn rate: {df_churn['churn'].mean():.1%}\")\nprint(\"\\nDataset overview:\")\nprint(df_churn.describe())\n\n# Analyze feature distributions and scales\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nnumerical_features = ['age', 'monthly_charges', 'total_charges', 'contract_length', 'num_services', 'support_calls']\n\nfor i, feature in enumerate(numerical_features):\n    row, col = i // 3, i % 3\n\n    # Plot distribution\n    axes[row, col].hist(df_churn[feature], bins=30, alpha=0.7, edgecolor='black')\n    axes[row, col].set_title(f'{feature}\\nRange: [{df_churn[feature].min():.0f}, {df_churn[feature].max():.0f}]')\n    axes[row, col].set_ylabel('Frequency')\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.suptitle('Feature Distributions (Before Normalization)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Prepare features for modeling\nX = df_churn.drop(['customer_id', 'churn'], axis=1)\ny = df_churn['churn']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Define preprocessing for different column types\nnumerical_features = ['age', 'monthly_charges', 'total_charges', 'contract_length', 'num_services', 'support_calls']\ncategorical_features = ['payment_method', 'internet_type', 'senior_citizen']\n\nprint(f\"\\nFeature preprocessing:\")\nprint(f\"Numerical features: {numerical_features}\")\nprint(f\"Categorical features: {categorical_features}\")\n\n# Test different scaling approaches\nscalers_test = {\n    'No Scaling': None,\n    'StandardScaler': StandardScaler(),\n    'MinMaxScaler': MinMaxScaler(),\n    'RobustScaler': RobustScaler()\n}\n\npreprocessing_results = {}\n\nfor scaler_name, scaler in scalers_test.items():\n    print(f\"\\nTesting {scaler_name}:\")\n\n    # Create preprocessing pipeline\n    if scaler is None:\n        # No scaling - just handle categorical variables\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n            ],\n            remainder='passthrough'  # Keep numerical features as-is\n        )\n    else:\n        # Apply scaling to numerical features\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', scaler, numerical_features),\n                ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n            ]\n        )\n\n    # Create full pipeline with logistic regression\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n    ])\n\n    # Evaluate using cross-validation\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')\n\n    # Fit and predict for detailed metrics\n    pipeline.fit(X_train, y_train)\n    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n    test_auc = roc_auc_score(y_test, y_pred_proba)\n\n    preprocessing_results[scaler_name] = {\n        'cv_auc_mean': cv_scores.mean(),\n        'cv_auc_std': cv_scores.std(),\n        'test_auc': test_auc\n    }\n\n    print(f\"  CV AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n    print(f\"  Test AUC: {test_auc:.3f}\")\n\n# Compare preprocessing approaches\npreprocessing_df = pd.DataFrame(preprocessing_results).T\n\nplt.figure(figsize=(12, 6))\n\n# CV performance comparison\nplt.subplot(1, 2, 1)\nmethods = preprocessing_df.index\ncv_means = preprocessing_df['cv_auc_mean']\ncv_stds = preprocessing_df['cv_auc_std']\n\nbars = plt.bar(methods, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)\nplt.title('Cross-Validation Performance by Scaling Method')\nplt.ylabel('AUC Score')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\n\n# Add value labels\nfor bar, mean in zip(bars, cv_means):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n             f'{mean:.3f}', ha='center', va='bottom')\n\n# Test performance comparison\nplt.subplot(1, 2, 2)\ntest_aucs = preprocessing_df['test_auc']\nbars = plt.bar(methods, test_aucs, alpha=0.7, color='orange')\nplt.title('Test Set Performance by Scaling Method')\nplt.ylabel('AUC Score')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\n\n# Add value labels\nfor bar, auc in zip(bars, test_aucs):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n             f'{auc:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Now test different regularization techniques\nprint(f\"\\n\" + \"=\"*60)\nprint(\"REGULARIZATION COMPARISON\")\nprint(\"=\"*60)\n\n# Use best scaling method\nbest_scaler = StandardScaler()  # Typically works well\n\npreprocessor_final = ColumnTransformer(\n    transformers=[\n        ('num', best_scaler, numerical_features),\n        ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n    ]\n)\n\n# Apply preprocessing\nX_train_processed = preprocessor_final.fit_transform(X_train)\nX_test_processed = preprocessor_final.transform(X_test)\n\nprint(f\"Processed feature shape: {X_train_processed.shape}\")\n\n# Test different regularization techniques\nregularization_models = {\n    'Logistic Regression (No Reg)': LogisticRegression(penalty=None, max_iter=1000, random_state=42),\n    'Ridge (L2)': LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42),\n    'Lasso (L1)': LogisticRegression(penalty='l1', solver='liblinear', C=1.0, max_iter=1000, random_state=42),\n    'ElasticNet': LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga', C=1.0, max_iter=1000, random_state=42)\n}\n\nregularization_results = {}\n\nfor model_name, model in regularization_models.items():\n    print(f\"\\nTesting {model_name}:\")\n\n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=5, scoring='roc_auc')\n\n    # Fit model\n    model.fit(X_train_processed, y_train)\n\n    # Predictions\n    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n    test_auc = roc_auc_score(y_test, y_pred_proba)\n\n    # Count non-zero coefficients (sparsity)\n    if hasattr(model, 'coef_'):\n        non_zero_coefs = np.sum(np.abs(model.coef_) &gt; 1e-5)\n        total_coefs = model.coef_.shape[1]\n        sparsity = 1 - (non_zero_coefs / total_coefs)\n    else:\n        non_zero_coefs = \"N/A\"\n        sparsity = \"N/A\"\n\n    regularization_results[model_name] = {\n        'cv_auc_mean': cv_scores.mean(),\n        'cv_auc_std': cv_scores.std(),\n        'test_auc': test_auc,\n        'non_zero_coefs': non_zero_coefs,\n        'sparsity': sparsity,\n        'model': model\n    }\n\n    print(f\"  CV AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n    print(f\"  Test AUC: {test_auc:.3f}\")\n    print(f\"  Non-zero coefficients: {non_zero_coefs}\")\n    if sparsity != \"N/A\":\n        print(f\"  Sparsity: {sparsity:.1%}\")\n\n# Hyperparameter tuning for best model\nprint(f\"\\n\" + \"=\"*40)\nprint(\"HYPERPARAMETER TUNING\")\nprint(\"=\"*40)\n\n# Tune regularization strength for Ridge\nparam_grid = {'C': np.logspace(-3, 2, 20)}\n\ngrid_search = GridSearchCV(\n    LogisticRegression(penalty='l2', max_iter=1000, random_state=42),\n    param_grid,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train_processed, y_train)\n\nprint(f\"Best regularization strength (C): {grid_search.best_params_['C']:.4f}\")\nprint(f\"Best CV AUC: {grid_search.best_score_:.3f}\")\n\n# Final model evaluation\nbest_model = grid_search.best_estimator_\ny_pred_proba_final = best_model.predict_proba(X_test_processed)[:, 1]\ny_pred_final = best_model.predict(X_test_processed)\n\nfinal_auc = roc_auc_score(y_test, y_pred_proba_final)\nprint(f\"Final test AUC: {final_auc:.3f}\")\n\n# ROC curve comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\n# Plot ROC curves for different regularization methods\nfor model_name, results in regularization_results.items():\n    model = results['model']\n    y_proba = model.predict_proba(X_test_processed)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc_score = roc_auc_score(y_test, y_proba)\n    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Regularization Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Feature importance (coefficients) for Ridge regression\nplt.subplot(1, 2, 2)\nridge_model = regularization_results['Ridge (L2)']['model']\n\n# Get feature names after preprocessing\nfeature_names = (numerical_features + \n                list(preprocessor_final.named_transformers_['cat']\n                    .get_feature_names_out(categorical_features)))\n\ncoefficients = ridge_model.coef_[0]\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': coefficients,\n    'abs_coefficient': np.abs(coefficients)\n}).sort_values('abs_coefficient', ascending=False)\n\n# Plot top 10 most important features\ntop_features = feature_importance.head(10)\nbars = plt.barh(range(len(top_features)), top_features['coefficient'])\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Coefficient Value')\nplt.title('Top 10 Feature Importance (Ridge)')\nplt.grid(True, alpha=0.3)\n\n# Color bars by sign\nfor i, (bar, coef) in enumerate(zip(bars, top_features['coefficient'])):\n    bar.set_color('red' if coef &lt; 0 else 'blue')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTop 10 Most Important Features:\")\nfor i, (_, row) in enumerate(top_features.iterrows()):\n    direction = \"increases\" if row['coefficient'] &gt; 0 else \"decreases\"\n    print(f\"{i+1:2d}. {row['feature']:25} \u03bb {direction} churn risk (coef: {row['coefficient']:+.3f})\")\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#financial-risk-assessment-example","title":"Financial Risk Assessment Example","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\n\n# Generate realistic financial dataset with outliers\nnp.random.seed(42)\nn_loans = 2000\n\n# Create features with different scales and outlier patterns\ndata = {\n    'loan_amount': np.random.lognormal(10, 1, n_loans),  # Log-normal (right-skewed)\n    'annual_income': np.random.lognormal(10.5, 0.8, n_loans),  # Income distribution\n    'credit_score': np.random.beta(2, 1, n_loans) * 550 + 300,  # Credit scores 300-850\n    'debt_to_income': np.random.exponential(0.3, n_loans),  # Debt ratios\n    'employment_years': np.random.gamma(2, 2, n_loans),  # Employment history\n    'num_credit_lines': np.random.poisson(8, n_loans),  # Count of credit lines\n    'loan_to_value': np.random.uniform(0.5, 0.95, n_loans),  # LTV ratio\n    'market_volatility': np.random.normal(0.15, 0.05, n_loans).clip(0.05, 0.4)  # Market conditions\n}\n\n# Add some extreme outliers (data entry errors, unusual cases)\noutlier_indices = np.random.choice(n_loans, size=50, replace=False)\ndata['annual_income'][outlier_indices[:25]] *= 10  # Very high income outliers\ndata['debt_to_income'][outlier_indices[25:]] *= 5  # Very high debt outliers\n\n# Create target: default risk score (0-1, higher = more risky)\nrisk_score = (\n    0.1 * (data['debt_to_income'] / np.mean(data['debt_to_income'])) +\n    0.2 * (1 - (data['credit_score'] - 300) / 550) +\n    0.15 * (data['loan_to_value']) +\n    0.1 * (data['market_volatility'] / 0.4) +\n    -0.05 * np.log(data['annual_income'] / np.mean(data['annual_income'])) +\n    0.1 * np.random.normal(0, 1, n_loans)  # Random noise\n).clip(0, 1)\n\ndata['risk_score'] = risk_score\n\n# Create DataFrame\ndf_risk = pd.DataFrame(data)\n\nprint(\"Financial Risk Assessment - Robust Scaling &amp; Regularization\")\nprint(f\"Dataset shape: {df_risk.shape}\")\nprint(\"\\nDataset summary with outliers:\")\nprint(df_risk.describe())\n\n# Identify outliers using IQR method\ndef identify_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return (df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)\n\n# Check for outliers in key features\noutlier_analysis = {}\nfor col in ['loan_amount', 'annual_income', 'debt_to_income']:\n    outliers = identify_outliers(df_risk, col)\n    outlier_analysis[col] = {\n        'count': outliers.sum(),\n        'percentage': (outliers.sum() / len(df_risk)) * 100\n    }\n    print(f\"\\n{col}: {outliers.sum()} outliers ({(outliers.sum()/len(df_risk)*100):.1f}%)\")\n\n# Visualize distributions and outliers\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfeatures = list(df_risk.columns[:-1])  # Exclude target\n\nfor i, feature in enumerate(features):\n    row, col = i // 3, i % 3\n\n    # Histogram\n    axes[row, col].hist(df_risk[feature], bins=50, alpha=0.7, edgecolor='black')\n    axes[row, col].set_title(f'{feature}')\n    axes[row, col].set_ylabel('Frequency')\n\n    # Mark outliers if applicable\n    if feature in outlier_analysis:\n        outliers = identify_outliers(df_risk, feature)\n        if outliers.sum() &gt; 0:\n            outlier_values = df_risk.loc[outliers, feature]\n            axes[row, col].axvline(df_risk[feature].quantile(0.25) - 1.5*(df_risk[feature].quantile(0.75)-df_risk[feature].quantile(0.25)), \n                                  color='red', linestyle='--', alpha=0.7, label='Outlier bounds')\n            axes[row, col].axvline(df_risk[feature].quantile(0.75) + 1.5*(df_risk[feature].quantile(0.75)-df_risk[feature].quantile(0.25)), \n                                  color='red', linestyle='--', alpha=0.7)\n            axes[row, col].legend()\n\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.suptitle('Feature Distributions with Outliers Highlighted', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Prepare data\nX = df_risk.drop('risk_score', axis=1)\ny = df_risk['risk_score']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Compare different scaling approaches on data with outliers\nscalers_robust = {\n    'No Scaling': None,\n    'StandardScaler': StandardScaler(),\n    'RobustScaler': RobustScaler(),\n    'QuantileTransformer': QuantileTransformer(output_distribution='normal')\n}\n\nscaling_results = {}\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"SCALING COMPARISON ON DATA WITH OUTLIERS\")\nprint(\"=\"*60)\n\nfor scaler_name, scaler in scalers_robust.items():\n    print(f\"\\nTesting {scaler_name}:\")\n\n    if scaler is None:\n        X_train_scaled = X_train\n        X_test_scaled = X_test\n    else:\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n    # Train simple linear regression\n    lr = LinearRegression()\n    lr.fit(X_train_scaled, y_train)\n\n    # Evaluate\n    train_score = lr.score(X_train_scaled, y_train)\n    test_score = lr.score(X_test_scaled, y_test)\n    y_pred = lr.predict(X_test_scaled)\n\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    mae = mean_absolute_error(y_test, y_pred)\n\n    scaling_results[scaler_name] = {\n        'train_r2': train_score,\n        'test_r2': test_score,\n        'rmse': rmse,\n        'mae': mae\n    }\n\n    print(f\"  Train R\u00b2: {train_score:.3f}\")\n    print(f\"  Test R\u00b2: {test_score:.3f}\")\n    print(f\"  RMSE: {rmse:.3f}\")\n    print(f\"  MAE: {mae:.3f}\")\n\n# Visualize scaling effects on first few features\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nsample_features = ['loan_amount', 'annual_income', 'debt_to_income']\n\nfor i, scaler_name in enumerate(['StandardScaler', 'RobustScaler', 'QuantileTransformer']):\n    if i &gt;= 3:\n        break\n\n    scaler = scalers_robust[scaler_name]\n    X_scaled_sample = scaler.fit_transform(X_train[sample_features])\n\n    row, col = i // 2, i % 2\n\n    # Plot first feature\n    axes[row, col].hist(X_scaled_sample[:, 0], bins=30, alpha=0.7, edgecolor='black')\n    axes[row, col].set_title(f'{scaler_name}\\nTransformed: {sample_features[0]}')\n    axes[row, col].set_ylabel('Frequency')\n    axes[row, col].grid(True, alpha=0.3)\n\n    # Add statistics\n    mean_val = np.mean(X_scaled_sample[:, 0])\n    std_val = np.std(X_scaled_sample[:, 0])\n    axes[row, col].text(0.02, 0.95, f'Mean: {mean_val:.2f}\\nStd: {std_val:.2f}', \n                       transform=axes[row, col].transAxes, verticalalignment='top',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Performance comparison\naxes[1, 1].bar(scaling_results.keys(), [v['test_r2'] for v in scaling_results.values()], alpha=0.7)\naxes[1, 1].set_title('Test R\u00b2 by Scaling Method')\naxes[1, 1].set_ylabel('R\u00b2 Score')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Now test regularization with best scaler (RobustScaler typically best for outliers)\nprint(f\"\\n\" + \"=\"*60)\nprint(\"REGULARIZATION WITH ROBUST SCALING\")\nprint(\"=\"*60)\n\nrobust_scaler = RobustScaler()\nX_train_robust = robust_scaler.fit_transform(X_train)\nX_test_robust = robust_scaler.transform(X_test)\n\n# Test different regularization strengths\nalphas = np.logspace(-4, 2, 20)\n\n# Test Ridge, Lasso, and ElasticNet\nregularization_models = {\n    'Ridge': Ridge(),\n    'Lasso': Lasso(max_iter=2000),\n    'ElasticNet': ElasticNet(max_iter=2000, l1_ratio=0.5)\n}\n\nregularization_paths = {}\n\nfor model_name, base_model in regularization_models.items():\n    print(f\"\\nAnalyzing {model_name} regularization path:\")\n\n    train_scores = []\n    test_scores = []\n    coefficients = []\n    sparsity_levels = []\n\n    for alpha in alphas:\n        # Set regularization strength\n        if hasattr(base_model, 'alpha'):\n            model = base_model.__class__(alpha=alpha, max_iter=2000)\n            if model_name == 'ElasticNet':\n                model = base_model.__class__(alpha=alpha, l1_ratio=0.5, max_iter=2000)\n\n        # Fit model\n        model.fit(X_train_robust, y_train)\n\n        # Evaluate\n        train_score = model.score(X_train_robust, y_train)\n        test_score = model.score(X_test_robust, y_test)\n\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        coefficients.append(model.coef_.copy())\n\n        # Calculate sparsity (proportion of near-zero coefficients)\n        sparsity = np.sum(np.abs(model.coef_) &lt; 1e-5) / len(model.coef_)\n        sparsity_levels.append(sparsity)\n\n    regularization_paths[model_name] = {\n        'train_scores': train_scores,\n        'test_scores': test_scores,\n        'coefficients': np.array(coefficients),\n        'sparsity': sparsity_levels\n    }\n\n    # Find best alpha\n    best_idx = np.argmax(test_scores)\n    best_alpha = alphas[best_idx]\n    best_test_score = test_scores[best_idx]\n\n    print(f\"  Best alpha: {best_alpha:.4f}\")\n    print(f\"  Best test R\u00b2: {best_test_score:.3f}\")\n    print(f\"  Sparsity at best alpha: {sparsity_levels[best_idx]:.1%}\")\n\n# Visualize regularization paths\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\nfor i, (model_name, results) in enumerate(regularization_paths.items()):\n    # Performance vs regularization strength\n    axes[0, i].plot(alphas, results['train_scores'], 'b-', label='Train', linewidth=2)\n    axes[0, i].plot(alphas, results['test_scores'], 'r-', label='Test', linewidth=2)\n    axes[0, i].set_xscale('log')\n    axes[0, i].set_xlabel('Regularization Strength (\u03bb)')\n    axes[0, i].set_ylabel('R\u00b2 Score')\n    axes[0, i].set_title(f'{model_name}: Performance vs Regularization')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n\n    # Mark best alpha\n    best_idx = np.argmax(results['test_scores'])\n    axes[0, i].axvline(alphas[best_idx], color='green', linestyle='--', alpha=0.7, \n                      label=f'Best \u03bb={alphas[best_idx]:.4f}')\n\n    # Coefficient paths (show first 5 features)\n    for j in range(min(5, results['coefficients'].shape[1])):\n        axes[1, i].plot(alphas, results['coefficients'][:, j], \n                       label=f'Feature {j+1}' if i == 0 else \"\")\n\n    axes[1, i].set_xscale('log')\n    axes[1, i].set_xlabel('Regularization Strength (\u03bb)')\n    axes[1, i].set_ylabel('Coefficient Value')\n    axes[1, i].set_title(f'{model_name}: Coefficient Paths')\n    if i == 0:\n        axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Final model selection and feature importance\nprint(f\"\\n\" + \"=\"*40)\nprint(\"FINAL MODEL ANALYSIS\")\nprint(\"=\"*40)\n\n# Select best Ridge model (usually most stable)\nbest_alpha_ridge = alphas[np.argmax(regularization_paths['Ridge']['test_scores'])]\nfinal_model = Ridge(alpha=best_alpha_ridge)\nfinal_model.fit(X_train_robust, y_train)\n\n# Final evaluation\ny_pred_final = final_model.predict(X_test_robust)\nfinal_r2 = r2_score(y_test, y_pred_final)\nfinal_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\nfinal_mae = mean_absolute_error(y_test, y_pred_final)\n\nprint(f\"Final Ridge Model (\u03bb = {best_alpha_ridge:.4f}):\")\nprint(f\"  Test R\u00b2: {final_r2:.3f}\")\nprint(f\"  RMSE: {final_rmse:.3f}\")\nprint(f\"  MAE: {final_mae:.3f}\")\n\n# Feature importance analysis\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'coefficient': final_model.coef_,\n    'abs_coefficient': np.abs(final_model.coef_)\n}).sort_values('abs_coefficient', ascending=False)\n\nprint(f\"\\nFeature Importance Ranking:\")\nfor i, (_, row) in enumerate(feature_importance.iterrows()):\n    direction = \"increases\" if row['coefficient'] &gt; 0 else \"decreases\"\n    print(f\"{i+1:2d}. {row['feature']:20} \u03bb {direction} risk (coef: {row['coefficient']:+.4f})\")\n\n# Final visualization\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_final, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Risk Score')\nplt.ylabel('Predicted Risk Score')\nplt.title(f'Final Model Performance\\nR\u00b2 = {final_r2:.3f}')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nbars = plt.barh(range(len(feature_importance)), feature_importance['coefficient'])\nplt.yticks(range(len(feature_importance)), feature_importance['feature'])\nplt.xlabel('Coefficient Value')\nplt.title('Feature Importance (Ridge Coefficients)')\nplt.grid(True, alpha=0.3)\n\n# Color bars by sign\nfor bar, coef in zip(bars, feature_importance['coefficient']):\n    bar.set_color('red' if coef &lt; 0 else 'blue')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Normalization%20Regularisation/#references","title":"\ud83d\udcda References","text":"<ul> <li>Books:</li> <li>The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman - Chapters 3, 18</li> <li>Pattern Recognition and Machine Learning by Christopher Bishop - Chapter 1, 5</li> <li> <p>Deep Learning by Goodfellow, Bengio, and Courville - Chapter 7 (Regularization)</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn Preprocessing</li> <li>Scikit-learn Linear Models</li> <li> <p>Scikit-learn Feature Selection</p> </li> <li> <p>Research Papers:</p> </li> <li>Regularization and variable selection via the elastic net by Zou &amp; Hastie (2005)</li> <li>Regression Shrinkage and Selection via the Lasso by Tibshirani (1996)</li> <li> <p>Ridge Regression: Biased Estimation for Nonorthogonal Problems by Hoerl &amp; Kennard (1970)</p> </li> <li> <p>Tutorials and Guides:</p> </li> <li>Feature Scaling Techniques</li> <li>Regularization in Machine Learning</li> <li> <p>Understanding the Bias-Variance Tradeoff</p> </li> <li> <p>Advanced Topics:</p> </li> <li>Group Lasso by Yuan &amp; Lin (2006)</li> <li>The Fused Lasso by Tibshirani et al. (2005)</li> <li> <p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Srivastava et al. (2014)</p> </li> <li> <p>Online Courses:</p> </li> <li>Machine Learning Course - Stanford CS229</li> <li>Statistical Learning - Stanford Online</li> <li> <p>Regularization - Coursera Machine Learning</p> </li> <li> <p>Software and Tools:</p> </li> <li>scikit-learn (Python)</li> <li>glmnet (R package)</li> <li>TensorFlow/Keras (Deep learning regularization)</li> <li>PyTorch (Deep learning regularization)</li> </ul>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/","title":"\ud83c\udfaf Overfitting and Underfitting","text":"<p>Overfitting and Underfitting are fundamental concepts in machine learning that describe how well a model generalizes to unseen data - the central challenge in building reliable predictive models.</p> <p>Resources: Scikit-learn Model Selection | ESL Chapter 7 | Bias-Variance Tradeoff Paper</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#summary","title":"\ud83d\udcca Summary","text":"<p>Overfitting occurs when a model learns the training data too well, capturing noise and specific patterns that don't generalize to new data. Underfitting happens when a model is too simple to capture the underlying patterns in the data.</p> <p>Key Characteristics:</p> <p>Overfitting: - High training accuracy, low validation/test accuracy - Model memorizes training data instead of learning patterns - Complex models with too many parameters - Poor generalization to unseen data</p> <p>Underfitting: - Low training accuracy, low validation/test accuracy - Model is too simple to capture underlying patterns - High bias, unable to learn from training data - Consistent poor performance across all datasets</p> <p>Applications: - Model selection and hyperparameter tuning - Regularization technique selection - Architecture design for neural networks - Feature engineering decisions - Cross-validation strategy - Early stopping criteria</p> <p>Related Concepts: - Bias-Variance Tradeoff: Fundamental framework explaining overfitting/underfitting - Model Complexity: Key factor determining fitting behavior - Regularization: Primary technique to prevent overfitting - Cross-Validation: Method to detect and measure fitting issues</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#how-overfitting-and-underfitting-work","title":"How Overfitting and Underfitting Work","text":"<p>Imagine you're learning to recognize handwritten digits. An underfitted model might only look at basic features like \"has curves\" or \"has straight lines\" - too simple to distinguish between different digits. An overfitted model might memorize every tiny detail of each training example, including pen pressure variations and paper texture, making it fail on new handwriting styles.</p> <p>The ideal model finds the right balance - learning the essential patterns that generalize well without memorizing irrelevant details.</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#1-bias-variance-decomposition","title":"1. Bias-Variance Decomposition","text":"<p>The expected prediction error can be decomposed as: \\(\\(E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2\\)\\)</p> <p>Where: - Bias: Error from oversimplifying assumptions - Variance: Error from sensitivity to training data variations - Irreducible Error (\\(\\sigma^2\\)): Inherent noise in the problem</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#2-model-complexity-vs-error","title":"2. Model Complexity vs Error","text":"\\[\\text{Training Error} = \\frac{1}{n}\\sum_{i=1}^{n}L(y_i, \\hat{f}(x_i))\\] \\[\\text{Generalization Error} = E[L(y, \\hat{f}(x))]\\] <p>As model complexity increases: - Training error decreases monotonically - Generalization error follows a U-shaped curve - Optimal complexity minimizes generalization error</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#3-vc-dimension-and-generalization","title":"3. VC Dimension and Generalization","text":"<p>For a model class with VC dimension \\(d\\) and \\(n\\) training samples: \\(\\(\\text{Generalization Error} \\leq \\text{Training Error} + \\sqrt{\\frac{d\\log(n) - \\log(\\delta)}{n}}\\)\\)</p> <p>This bound shows that complex models (high \\(d\\)) need more data to generalize well.</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#4-learning-curves","title":"4. Learning Curves","text":"<p>Training and validation error as functions of: - Sample size: \\(\\text{Error}(n)\\) - Model complexity: \\(\\text{Error}(\\lambda)\\) where \\(\\lambda\\) controls complexity</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#implementation-using-libraries","title":"\ud83d\udee0\ufe0f Implementation using Libraries","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#scikit-learn-implementation","title":"Scikit-learn Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, validation_curve, learning_curve\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\n\n# Generate synthetic dataset\nnp.random.seed(42)\ndef generate_data(n_samples=100, noise=0.3):\n    X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n    y = 1.5 * X.ravel() + np.sin(1.5 * np.pi * X.ravel()) + np.random.normal(0, noise, n_samples)\n    return X, y\n\nX, y = generate_data(n_samples=100, noise=0.3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create models with different complexities\nmodels = {\n    'Underfitting (degree=1)': Pipeline([\n        ('poly', PolynomialFeatures(degree=1)),\n        ('ridge', Ridge(alpha=0.1))\n    ]),\n    'Good Fit (degree=3)': Pipeline([\n        ('poly', PolynomialFeatures(degree=3)),\n        ('ridge', Ridge(alpha=0.1))\n    ]),\n    'Overfitting (degree=15)': Pipeline([\n        ('poly', PolynomialFeatures(degree=15)),\n        ('ridge', Ridge(alpha=0.01))\n    ])\n}\n\n# Train and evaluate models\nresults = {}\nX_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n\nplt.figure(figsize=(15, 5))\nfor i, (name, model) in enumerate(models.items(), 1):\n    model.fit(X_train, y_train)\n\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n\n    results[name] = {\n        'train_score': train_score,\n        'test_score': test_score,\n        'predictions': model.predict(X_plot)\n    }\n\n    plt.subplot(1, 3, i)\n    plt.scatter(X_train, y_train, alpha=0.6, label='Training Data')\n    plt.scatter(X_test, y_test, alpha=0.6, label='Test Data')\n    plt.plot(X_plot, results[name]['predictions'], 'r-', linewidth=2)\n    plt.title(f'{name}\\nTrain R\u00b2: {train_score:.3f}, Test R\u00b2: {test_score:.3f}')\n    plt.legend()\n    plt.xlabel('X')\n    plt.ylabel('y')\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(\"Model Performance Comparison:\")\nprint(\"-\" * 50)\nfor name, result in results.items():\n    print(f\"{name:25s} | Train R\u00b2: {result['train_score']:.3f} | Test R\u00b2: {result['test_score']:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#learning-curves-analysis","title":"Learning Curves Analysis","text":"<pre><code>def plot_learning_curves(estimator, X, y, title):\n    \"\"\"Plot learning curves to diagnose overfitting/underfitting\"\"\"\n    train_sizes, train_scores, val_scores = learning_curve(\n        estimator, X, y, cv=5, n_jobs=-1, \n        train_sizes=np.linspace(0.1, 1.0, 10),\n        scoring='neg_mean_squared_error'\n    )\n\n    train_scores_mean = -train_scores.mean(axis=1)\n    train_scores_std = train_scores.std(axis=1)\n    val_scores_mean = -val_scores.mean(axis=1)\n    val_scores_std = val_scores.std(axis=1)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Error')\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color='r')\n\n    plt.plot(train_sizes, val_scores_mean, 'o-', color='g', label='Validation Error')\n    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n                     val_scores_mean + val_scores_std, alpha=0.1, color='g')\n\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Mean Squared Error')\n    plt.title(f'Learning Curves: {title}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Analyze different model complexities\nfor name, model in models.items():\n    plot_learning_curves(model, X, y, name)\n</code></pre>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#validation-curves-for-hyperparameter-tuning","title":"Validation Curves for Hyperparameter Tuning","text":"<pre><code>def plot_validation_curve(estimator, X, y, param_name, param_range, title):\n    \"\"\"Plot validation curve for hyperparameter tuning\"\"\"\n    train_scores, val_scores = validation_curve(\n        estimator, X, y, param_name=param_name, param_range=param_range,\n        cv=5, scoring='neg_mean_squared_error', n_jobs=-1\n    )\n\n    train_scores_mean = -train_scores.mean(axis=1)\n    val_scores_mean = -val_scores.mean(axis=1)\n\n    plt.figure(figsize=(8, 6))\n    plt.semilogx(param_range, train_scores_mean, 'o-', color='r', label='Training Error')\n    plt.semilogx(param_range, val_scores_mean, 'o-', color='g', label='Validation Error')\n    plt.xlabel(param_name)\n    plt.ylabel('Mean Squared Error')\n    plt.title(f'Validation Curve: {title}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Ridge regularization parameter tuning\nridge_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),\n    ('ridge', Ridge())\n])\n\nalpha_range = np.logspace(-4, 2, 20)\nplot_validation_curve(ridge_model, X, y, 'ridge__alpha', alpha_range, 'Ridge Alpha')\n</code></pre>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#from-scratch-implementation","title":"\ud83d\udd27 From Scratch Implementation","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#simple-overfitting-detection-framework","title":"Simple Overfitting Detection Framework","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Tuple, Callable\n\nclass FittingAnalyzer:\n    \"\"\"\n    A class to analyze and detect overfitting/underfitting patterns\n    \"\"\"\n\n    def __init__(self, random_state: int = 42):\n        self.random_state = random_state\n        np.random.seed(random_state)\n        self.history = {}\n\n    def generate_polynomial_data(self, n_samples: int = 100, \n                               noise: float = 0.3, \n                               true_degree: int = 3) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Generate synthetic polynomial data for testing\"\"\"\n        X = np.linspace(0, 1, n_samples).reshape(-1, 1)\n\n        # True function: polynomial of specified degree\n        if true_degree == 3:\n            y_true = 1.5 * X.ravel() + np.sin(1.5 * np.pi * X.ravel())\n        else:\n            # Generate random polynomial coefficients\n            coeffs = np.random.normal(0, 1, true_degree + 1)\n            y_true = sum(coeffs[i] * (X.ravel() ** i) for i in range(true_degree + 1))\n\n        # Add noise\n        y = y_true + np.random.normal(0, noise, n_samples)\n\n        return X, y, y_true\n\n    def polynomial_features(self, X: np.ndarray, degree: int) -&gt; np.ndarray:\n        \"\"\"Create polynomial features up to specified degree\"\"\"\n        n_samples = X.shape[0]\n        n_features = degree + 1\n\n        # Create polynomial feature matrix\n        X_poly = np.ones((n_samples, n_features))\n        for i in range(1, degree + 1):\n            X_poly[:, i] = (X[:, 0] ** i)\n\n        return X_poly\n\n    def ridge_regression_fit(self, X: np.ndarray, y: np.ndarray, \n                           alpha: float = 0.01) -&gt; np.ndarray:\n        \"\"\"Fit ridge regression with L2 regularization\"\"\"\n        # Add regularization to prevent singular matrix\n        I = np.eye(X.shape[1])\n        I[0, 0] = 0  # Don't regularize intercept\n\n        # Ridge regression solution: (X^T X + \u03bbI)^(-1) X^T y\n        coefficients = np.linalg.solve(X.T @ X + alpha * I, X.T @ y)\n\n        return coefficients\n\n    def predict(self, X: np.ndarray, coefficients: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Make predictions using fitted coefficients\"\"\"\n        return X @ coefficients\n\n    def mean_squared_error(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"Calculate mean squared error\"\"\"\n        return np.mean((y_true - y_pred) ** 2)\n\n    def r2_score(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; float:\n        \"\"\"Calculate R\u00b2 score\"\"\"\n        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n        ss_res = np.sum((y_true - y_pred) ** 2)\n        return 1 - (ss_res / ss_tot)\n\n    def train_test_split(self, X: np.ndarray, y: np.ndarray, \n                        test_size: float = 0.3) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"Split data into training and testing sets\"\"\"\n        n_samples = X.shape[0]\n        n_test = int(n_samples * test_size)\n\n        # Random indices for test set\n        test_indices = np.random.choice(n_samples, n_test, replace=False)\n        train_indices = np.setdiff1d(np.arange(n_samples), test_indices)\n\n        return (X[train_indices], X[test_indices], \n                y[train_indices], y[test_indices])\n\n    def analyze_model_complexity(self, X: np.ndarray, y: np.ndarray,\n                                max_degree: int = 15) -&gt; dict:\n        \"\"\"Analyze different polynomial degrees to show overfitting/underfitting\"\"\"\n        X_train, X_test, y_train, y_test = self.train_test_split(X, y)\n\n        degrees = range(1, max_degree + 1)\n        train_errors = []\n        test_errors = []\n        train_r2s = []\n        test_r2s = []\n\n        for degree in degrees:\n            # Create polynomial features\n            X_train_poly = self.polynomial_features(X_train, degree)\n            X_test_poly = self.polynomial_features(X_test, degree)\n\n            # Fit model\n            coeffs = self.ridge_regression_fit(X_train_poly, y_train)\n\n            # Make predictions\n            y_train_pred = self.predict(X_train_poly, coeffs)\n            y_test_pred = self.predict(X_test_poly, coeffs)\n\n            # Calculate metrics\n            train_mse = self.mean_squared_error(y_train, y_train_pred)\n            test_mse = self.mean_squared_error(y_test, y_test_pred)\n            train_r2 = self.r2_score(y_train, y_train_pred)\n            test_r2 = self.r2_score(y_test, y_test_pred)\n\n            train_errors.append(train_mse)\n            test_errors.append(test_mse)\n            train_r2s.append(train_r2)\n            test_r2s.append(test_r2)\n\n        results = {\n            'degrees': degrees,\n            'train_errors': train_errors,\n            'test_errors': test_errors,\n            'train_r2s': train_r2s,\n            'test_r2s': test_r2s\n        }\n\n        self.history['complexity_analysis'] = results\n        return results\n\n    def plot_complexity_analysis(self, results: dict = None):\n        \"\"\"Plot the complexity analysis results\"\"\"\n        if results is None:\n            results = self.history.get('complexity_analysis')\n            if results is None:\n                raise ValueError(\"No complexity analysis results found. Run analyze_model_complexity first.\")\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n        # Plot MSE vs complexity\n        ax1.plot(results['degrees'], results['train_errors'], 'o-', \n                label='Training Error', color='blue')\n        ax1.plot(results['degrees'], results['test_errors'], 'o-', \n                label='Validation Error', color='red')\n        ax1.set_xlabel('Polynomial Degree (Model Complexity)')\n        ax1.set_ylabel('Mean Squared Error')\n        ax1.set_title('Error vs Model Complexity')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n\n        # Plot R\u00b2 vs complexity\n        ax2.plot(results['degrees'], results['train_r2s'], 'o-', \n                label='Training R\u00b2', color='blue')\n        ax2.plot(results['degrees'], results['test_r2s'], 'o-', \n                label='Validation R\u00b2', color='red')\n        ax2.set_xlabel('Polynomial Degree (Model Complexity)')\n        ax2.set_ylabel('R\u00b2 Score')\n        ax2.set_title('R\u00b2 vs Model Complexity')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n    def detect_overfitting(self, train_score: float, test_score: float, \n                          threshold: float = 0.1) -&gt; str:\n        \"\"\"Detect overfitting based on train-test performance gap\"\"\"\n        gap = train_score - test_score\n\n        if gap &gt; threshold and test_score &lt; 0.7:\n            return \"Overfitting detected\"\n        elif train_score &lt; 0.6 and test_score &lt; 0.6:\n            return \"Underfitting detected\"\n        else:\n            return \"Good fit\"\n\n# Example usage\nif __name__ == \"__main__\":\n    # Initialize analyzer\n    analyzer = FittingAnalyzer(random_state=42)\n\n    # Generate synthetic data\n    X, y, y_true = analyzer.generate_polynomial_data(n_samples=100, noise=0.2)\n\n    # Analyze model complexity\n    print(\"Analyzing model complexity...\")\n    results = analyzer.analyze_model_complexity(X, y, max_degree=15)\n\n    # Plot results\n    analyzer.plot_complexity_analysis()\n\n    # Find optimal complexity\n    optimal_idx = np.argmin(results['test_errors'])\n    optimal_degree = results['degrees'][optimal_idx]\n\n    print(f\"\\nOptimal polynomial degree: {optimal_degree}\")\n    print(f\"Test R\u00b2 at optimal complexity: {results['test_r2s'][optimal_idx]:.3f}\")\n\n    # Detect fitting issues for different complexities\n    for i, degree in enumerate([1, optimal_degree, 15]):\n        if i &lt; len(results['train_r2s']):\n            status = analyzer.detect_overfitting(\n                results['train_r2s'][degree-1], \n                results['test_r2s'][degree-1]\n            )\n            print(f\"Degree {degree}: {status}\")\n</code></pre>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#overfitting-assumptions-and-limitations","title":"Overfitting Assumptions and Limitations","text":"<p>Assumptions: - Training data is representative of the target population - Test/validation sets are independent and identically distributed - The underlying function exists and is learnable - Sufficient data is available to assess generalization</p> <p>Limitations: - Data-dependent: Overfitting detection depends on data quality and quantity - Model-specific: Different models overfit in different ways - Metric sensitivity: Choice of evaluation metric affects overfitting detection - Temporal effects: Models may overfit to specific time periods in time series data</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#prevention-techniques-limitations","title":"Prevention Techniques Limitations","text":"<p>Regularization: - May underfit if regularization is too strong - Requires hyperparameter tuning - Different regularization types (L1, L2) have different effects</p> <p>Cross-validation: - Computationally expensive for large datasets - May not capture all generalization patterns - Assumes data is i.i.d. (problematic for time series)</p> <p>Early stopping: - Requires validation set, reducing training data - May stop too early or too late - Sensitive to learning rate and optimization dynamics</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#comparison-with-other-approaches","title":"Comparison with Other Approaches","text":"<p>vs. Statistical Model Selection: - Advantages: More flexible, works with complex models - Disadvantages: Less theoretical guarantees, more empirical</p> <p>vs. Bayesian Methods: - Advantages: Simpler implementation, faster computation - Disadvantages: Less principled uncertainty quantification</p> <p>vs. Ensemble Methods: - Advantages: Interpretable individual models - Disadvantages: May still overfit collectively</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#interview-questions","title":"\u2753 Interview Questions","text":"1. What is the fundamental difference between overfitting and underfitting? How do they relate to the bias-variance tradeoff? <p>Answer: - Overfitting: High variance, low bias - model memorizes training data, fails on new data - Underfitting: High bias, low variance - model too simple to capture underlying patterns - Bias-Variance Tradeoff:    - Overfitting: Low training error, high test error (high variance)   - Underfitting: High training error, high test error (high bias)   - Optimal model: Balance between bias and variance - Total Error = Bias\u00b2 + Variance + Irreducible Error - Goal: Find the sweet spot that minimizes total expected error</p> 2. How would you detect overfitting in a machine learning model? Provide multiple approaches. <p>Answer: - Training vs Validation Performance:   - Large gap between training and validation accuracy   - Training error decreases while validation error increases - Learning Curves:   - Training curve continues decreasing   - Validation curve plateaus or increases - Cross-Validation:   - High variance in cross-validation scores   - Mean CV score much lower than training score - Regularization Response:   - Model performance improves significantly with regularization   - Very sensitive to hyperparameter changes - Statistical Tests:   - Significant difference in performance metrics   - Bootstrap confidence intervals don't overlap</p> 3. What are the main techniques to prevent overfitting? Explain how each works. <p>Answer: - Regularization (L1/L2):   - Adds penalty term to loss function   - L1: Promotes sparsity, L2: Shrinks weights   - Controls model complexity - Cross-Validation:   - Better estimate of generalization performance   - Helps in hyperparameter tuning - Early Stopping:   - Stop training when validation error increases   - Prevents memorization of training data - Data Augmentation:   - Increases effective dataset size   - Reduces overfitting to specific training examples - Dropout (Neural Networks):   - Randomly deactivates neurons during training   - Prevents co-adaptation of features - Ensemble Methods:   - Combines multiple models   - Reduces variance through averaging</p> 4. In a neural network, you observe that training accuracy reaches 99% but validation accuracy is only 70%. What would you do? <p>Answer: - Immediate Actions:   - Add regularization (L2, dropout)   - Reduce model complexity (fewer layers/neurons)   - Implement early stopping - Data-Related Solutions:   - Collect more training data   - Implement data augmentation   - Check for data leakage - Architecture Changes:   - Use batch normalization   - Reduce learning rate   - Use different optimizer - Monitoring Strategy:   - Plot learning curves   - Monitor multiple metrics   - Use cross-validation for hyperparameter tuning - Validation:   - Ensure train/validation split is appropriate   - Check for distribution shift</p> 5. How does the amount of training data affect overfitting and underfitting? <p>Answer: - More Data Generally:   - Reduces overfitting (more examples to learn from)   - Allows for more complex models without overfitting   - Improves generalization capability - Overfitting with Limited Data:   - Models memorize small training sets easily   - High variance in model performance   - Need simpler models or regularization - Underfitting Scenarios:   - Even with more data, simple models may underfit   - Complex relationships require complex models regardless of data size - Learning Curves Analysis:   - Overfitting: Large gap between training/validation that persists   - Underfitting: Both curves plateau at poor performance   - Good fit: Curves converge to good performance</p> 6. Explain the concept of model complexity and how it relates to overfitting. How do you choose the right complexity? <p>Answer: - Model Complexity Definition:   - Number of parameters/features in the model   - Flexibility of the model to fit different patterns   - Measured by VC dimension, degrees of freedom, etc. - Relationship to Overfitting:   - Higher complexity \u2192 Higher risk of overfitting   - Lower complexity \u2192 Higher risk of underfitting   - Sweet spot depends on data size and problem complexity - Choosing Right Complexity:   - Validation curves: Plot performance vs complexity parameter   - Cross-validation: Use CV to select optimal hyperparameters   - Information criteria: AIC, BIC for statistical models   - Regularization path: Analyze performance across regularization strengths - Practical Guidelines:   - Start simple, increase complexity if needed   - Use domain knowledge to guide complexity choices   - Consider computational constraints</p> 7. What is the difference between training error, validation error, and test error? How do they help diagnose overfitting? <p>Answer: - Training Error:   - Error on data used to train the model   - Always optimistic estimate of true performance   - Decreases as model complexity increases - Validation Error:   - Error on held-out data during model development   - Used for hyperparameter tuning and model selection   - Estimates generalization performance - Test Error:   - Error on completely unseen data   - Final unbiased estimate of model performance   - Should only be used once at the end - Overfitting Diagnosis:   - Overfitting: Training error &lt;&lt; Validation error   - Underfitting: Training error H Validation error (both high)   - Good fit: Training error H Validation error H Test error (all reasonable) - Best Practices:   - Never tune based on test error   - Use validation error for all model development decisions   - Report test error as final performance estimate</p> 8. In time series forecasting, how does overfitting manifest differently than in traditional ML problems? <p>Answer: - Temporal Dependencies:   - Models can overfit to specific time patterns   - Random CV splits break temporal structure   - Need time-aware validation (walk-forward, time series CV) - Common Overfitting Patterns:   - Memorizing seasonal patterns that don't generalize   - Over-relying on recent data points   - Fitting noise in historical data - Detection Methods:   - Use time-series cross-validation   - Monitor performance on future time periods   - Check residual patterns for autocorrelation - Prevention Techniques:   - Use simpler models for shorter horizons   - Apply temporal regularization   - Implement proper feature engineering   - Use ensemble methods with different time windows - Validation Strategy:   - Split data chronologically   - Use expanding or sliding window validation   - Test on multiple future periods</p> 9. How do ensemble methods help with overfitting? What are their limitations? <p>Answer: - How Ensembles Help:   - Variance Reduction: Averaging reduces individual model variance   - Error Diversification: Different models make different mistakes   - Robustness: Less sensitive to outliers or noise   - Regularization Effect: Combining models acts as implicit regularization - Types of Ensembles:   - Bagging: Reduces variance (Random Forest)   - Boosting: Reduces bias (AdaBoost, Gradient Boosting)   - Stacking: Learns optimal combination of models - Limitations:   - Increased Complexity: Harder to interpret and debug   - Computational Cost: More expensive to train and predict   - Diminishing Returns: Adding more models may not help   - Can Still Overfit: Ensemble can collectively overfit - Best Practices:   - Use diverse base models   - Apply regularization to ensemble combination   - Monitor ensemble performance on validation data   - Consider ensemble size vs performance tradeoff</p> 10. You have a dataset with 1000 samples and are training a neural network with 1 million parameters. What issues might you face and how would you address them? <p>Answer: - Primary Issue: Severe overfitting due to parameter/sample ratio (1000:1) - Expected Problems:   - Model will memorize training data   - Very poor generalization performance   - High variance in predictions   - Unstable training dynamics - Solutions:   - Data: Collect more data, use data augmentation   - Architecture: Reduce network size, use simpler models   - Regularization: Heavy dropout, L2 regularization, batch normalization   - Training: Early stopping, lower learning rates   - Alternative Approaches: Transfer learning, pre-trained models - Monitoring Strategy:   - Use aggressive cross-validation   - Monitor training/validation gap closely   - Consider using simpler models as baselines - Rule of Thumb:   - Generally need 10x more samples than parameters   - For deep learning, often need much more   - Consider domain complexity when sizing models</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#examples","title":"\ud83d\udcdd Examples","text":""},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#real-world-example-house-price-prediction","title":"Real-World Example: House Price Prediction","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Generate realistic house price dataset\nnp.random.seed(42)\nX, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n\n# Add meaningful feature names\nfeature_names = ['Size_sqft', 'Bedrooms', 'Age_years', 'Location_score', 'Condition_score']\nX_df = pd.DataFrame(X, columns=feature_names)\n\n# Make target more realistic (house prices in thousands)\ny = np.abs(y) * 10 + 300  # Prices between $300K - $800K approximately\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=42)\n\nprint(\"House Price Prediction: Overfitting vs Underfitting Analysis\")\nprint(\"=\" * 60)\n\n# Model 1: Underfitting (too simple)\nprint(\"\\n1. UNDERFITTING EXAMPLE:\")\nprint(\"-\" * 30)\n\n# Use only one feature (house size)\nsimple_model = LinearRegression()\nsimple_model.fit(X_train[['Size_sqft']], y_train)\n\ntrain_score_simple = simple_model.score(X_train[['Size_sqft']], y_train)\ntest_score_simple = simple_model.score(X_test[['Size_sqft']], y_test)\n\nprint(f\"Simple Model (Size only):\")\nprint(f\"Training R\u00b2: {train_score_simple:.3f}\")\nprint(f\"Test R\u00b2: {test_score_simple:.3f}\")\nprint(f\"Performance Gap: {abs(train_score_simple - test_score_simple):.3f}\")\nprint(\"Analysis: Both scores are low \u2192 UNDERFITTING\")\n\n# Model 2: Good fit\nprint(\"\\n2. GOOD FIT EXAMPLE:\")\nprint(\"-\" * 30)\n\ngood_model = Ridge(alpha=1.0)\ngood_model.fit(X_train, y_train)\n\ntrain_score_good = good_model.score(X_train, y_train)\ntest_score_good = good_model.score(X_test, y_test)\n\nprint(f\"Ridge Model (All features):\")\nprint(f\"Training R\u00b2: {train_score_good:.3f}\")\nprint(f\"Test R\u00b2: {test_score_good:.3f}\")\nprint(f\"Performance Gap: {abs(train_score_good - test_score_good):.3f}\")\nprint(\"Analysis: Both scores reasonable, small gap \u2192 GOOD FIT\")\n\n# Model 3: Overfitting (too complex)\nprint(\"\\n3. OVERFITTING EXAMPLE:\")\nprint(\"-\" * 30)\n\n# Create high-degree polynomial features\noverfit_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=8, include_bias=False)),\n    ('linear', LinearRegression())\n])\n\noverfit_model.fit(X_train, y_train)\n\ntrain_score_overfit = overfit_model.score(X_train, y_train)\ntest_score_overfit = overfit_model.score(X_test, y_test)\n\nprint(f\"Polynomial Model (degree=8):\")\nprint(f\"Training R\u00b2: {train_score_overfit:.3f}\")\nprint(f\"Test R\u00b2: {test_score_overfit:.3f}\")\nprint(f\"Performance Gap: {abs(train_score_overfit - test_score_overfit):.3f}\")\nprint(\"Analysis: High training score, low test score \u2192 OVERFITTING\")\n\n# Cross-validation analysis\nprint(\"\\n4. CROSS-VALIDATION ANALYSIS:\")\nprint(\"-\" * 30)\n\nmodels = {\n    'Simple': Pipeline([('select', 'passthrough'), ('model', LinearRegression())]),\n    'Good Fit': Ridge(alpha=1.0),\n    'Complex': Pipeline([('poly', PolynomialFeatures(degree=8)), ('model', LinearRegression())])\n}\n\nfor name, model in models.items():\n    if name == 'Simple':\n        cv_scores = cross_val_score(LinearRegression(), X_train[['Size_sqft']], y_train, cv=5, scoring='r2')\n    else:\n        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n\n    print(f\"{name:12s}: Mean CV R\u00b2 = {cv_scores.mean():.3f} (\u00b1{cv_scores.std():.3f})\")\n\n# Learning curves visualization\ndef plot_learning_curve_example():\n    train_sizes = np.linspace(0.1, 1.0, 10)\n\n    models_to_plot = {\n        'Simple (Underfit)': LinearRegression(),\n        'Good Fit (Ridge)': Ridge(alpha=1.0),\n        'Complex (Overfit)': Pipeline([\n            ('poly', PolynomialFeatures(degree=8)),\n            ('model', LinearRegression())\n        ])\n    }\n\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    for i, (name, model) in enumerate(models_to_plot.items()):\n        train_errors = []\n        val_errors = []\n\n        for train_size in train_sizes:\n            n_train = int(train_size * len(X_train))\n            X_subset = X_train.iloc[:n_train] if name != 'Simple (Underfit)' else X_train[['Size_sqft']].iloc[:n_train]\n            y_subset = y_train[:n_train]\n\n            # Fit model\n            model.fit(X_subset, y_subset)\n\n            # Training error\n            train_pred = model.predict(X_subset)\n            train_mse = np.mean((y_subset - train_pred) ** 2)\n            train_errors.append(train_mse)\n\n            # Validation error (use a separate validation set)\n            X_val = X_test if name != 'Simple (Underfit)' else X_test[['Size_sqft']]\n            val_pred = model.predict(X_val)\n            val_mse = np.mean((y_test - val_pred) ** 2)\n            val_errors.append(val_mse)\n\n        axes[i].plot(train_sizes * len(X_train), train_errors, 'o-', label='Training Error', color='blue')\n        axes[i].plot(train_sizes * len(X_train), val_errors, 'o-', label='Validation Error', color='red')\n        axes[i].set_xlabel('Training Set Size')\n        axes[i].set_ylabel('Mean Squared Error')\n        axes[i].set_title(f'Learning Curve: {name}')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\\n5. LEARNING CURVES:\")\nprint(\"-\" * 30)\nprint(\"Plotting learning curves for visual analysis...\")\nplot_learning_curve_example()\n\n# Practical recommendations\nprint(\"\\n6. PRACTICAL RECOMMENDATIONS:\")\nprint(\"-\" * 30)\nprint(\"For this house price prediction problem:\")\nprint(\"\" Simple model: Add more features (bedrooms, age, location)\")\nprint(\"\" Good fit model: Current Ridge regression is appropriate\")  \nprint(\"\" Complex model: Reduce polynomial degree or increase regularization\")\nprint(\"\" Consider collecting more data if available\")\nprint(\"\" Feature engineering might help more than complex models\")\n</code></pre> <p>Output Analysis: - Underfitting: Simple model using only house size shows poor performance on both training and test data - Good Fit: Ridge regression with all features shows balanced performance - Overfitting: High-degree polynomial model shows perfect training performance but poor test performance - Learning Curves: Reveal the characteristic patterns of each fitting scenario</p>"},{"location":"Machine-Learning/Overfitting%2C%20Underfitting/#references","title":"\ud83d\udcda References","text":"<ol> <li>Books:</li> <li>The Elements of Statistical Learning - Hastie, Tibshirani, Friedman</li> <li>Pattern Recognition and Machine Learning - Bishop</li> <li> <p>Hands-On Machine Learning - Aur\u00e9lien G\u00e9ron</p> </li> <li> <p>Papers:</p> </li> <li>A Few Useful Things to Know About Machine Learning - Domingos</li> <li> <p>Understanding the Bias-Variance Tradeoff</p> </li> <li> <p>Online Resources:</p> </li> <li>Scikit-learn Model Evaluation</li> <li>Andrew Ng's Machine Learning Course - Stanford</li> <li> <p>Fast.ai Practical Deep Learning</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn Cross-Validation</li> <li>TensorFlow Regularization</li> <li>PyTorch Model Selection</li> </ol>"},{"location":"Machine-Learning/PCA/","title":"\ud83c\udfaf Principal Component Analysis (PCA)","text":"<p>PCA is a fundamental dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving maximum variance, making it invaluable for data visualization, noise reduction, and feature extraction.</p> <p>Resources: Scikit-learn PCA | Elements of Statistical Learning - Chapter 14 | Pattern Recognition and Machine Learning - Chapter 12</p>"},{"location":"Machine-Learning/PCA/#_1","title":"PCA (Principal Component Analysis)","text":"<p>\u000f Summary</p> <p>Principal Component Analysis (PCA) is an unsupervised linear dimensionality reduction technique that identifies the principal components (directions of maximum variance) in high-dimensional data. It projects the original data onto a lower-dimensional subspace defined by these components, effectively reducing the number of features while retaining as much information as possible.</p> <p>Key characteristics: - Dimensionality reduction: Reduces the number of features while preserving information - Variance maximization: Finds directions that capture maximum variance in data - Linear transformation: Uses linear combinations of original features - Orthogonal components: Principal components are orthogonal to each other - Data compression: Enables efficient storage and transmission of data - Noise reduction: Can filter out noise by discarding low-variance components</p> <p>Applications: - Data visualization (reducing to 2D/3D for plotting) - Image compression and processing - Feature extraction for machine learning - Exploratory data analysis - Noise reduction and signal processing - Face recognition systems - Stock market analysis - Gene expression analysis</p> <p>Types: - Standard PCA: Linear dimensionality reduction using covariance matrix - Kernel PCA: Non-linear extension using kernel methods - Sparse PCA: Incorporates sparsity constraints on components - Incremental PCA: For large datasets that don't fit in memory</p>"},{"location":"Machine-Learning/PCA/#intuition","title":"&gt;\ufffd Intuition","text":""},{"location":"Machine-Learning/PCA/#how-pca-works","title":"How PCA Works","text":"<p>Imagine you have a dataset of house prices with features like size, number of rooms, age, etc. Some features might be highly correlated (e.g., size and number of rooms). PCA finds new \"directions\" (principal components) that best capture the variation in your data. The first principal component captures the most variation, the second captures the next most variation (orthogonal to the first), and so on.</p> <p>Think of it like finding the best angle to photograph a 3D object on a 2D photo - you want the angle that preserves the most information about the object's shape.</p>"},{"location":"Machine-Learning/PCA/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/PCA/#1-covariance-matrix","title":"1. Covariance Matrix","text":"<p>For a dataset \\(X \\in \\mathbb{R}^{n \\times d}\\) (n samples, d features), first center the data: \\(\\(\\bar{X} = X - \\mathbf{1}\\mu^T\\)\\)</p> <p>where \\(\\mu = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\) is the mean vector.</p> <p>The covariance matrix is: \\(\\(C = \\frac{1}{n-1}\\bar{X}^T\\bar{X}\\)\\)</p>"},{"location":"Machine-Learning/PCA/#2-eigenvalue-decomposition","title":"2. Eigenvalue Decomposition","text":"<p>PCA finds the eigenvalues and eigenvectors of the covariance matrix: \\(\\(C\\mathbf{v} = \\lambda\\mathbf{v}\\)\\)</p> <p>Where: - \\(\\mathbf{v}\\) are the eigenvectors (principal components) - \\(\\lambda\\) are the eigenvalues (explained variance)</p>"},{"location":"Machine-Learning/PCA/#3-principal-components","title":"3. Principal Components","text":"<p>The eigenvectors \\(\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_d\\) ordered by decreasing eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d\\) are the principal components.</p>"},{"location":"Machine-Learning/PCA/#4-dimensionality-reduction","title":"4. Dimensionality Reduction","text":"<p>To reduce to \\(k\\) dimensions, select the first \\(k\\) eigenvectors: \\(\\(W = [\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k] \\in \\mathbb{R}^{d \\times k}\\)\\)</p> <p>Transform the data: \\(\\(Z = \\bar{X}W \\in \\mathbb{R}^{n \\times k}\\)\\)</p>"},{"location":"Machine-Learning/PCA/#5-reconstruction","title":"5. Reconstruction","text":"<p>The original data can be approximated as: \\(\\(\\hat{X} = ZW^T + \\mathbf{1}\\mu^T\\)\\)</p>"},{"location":"Machine-Learning/PCA/#6-explained-variance-ratio","title":"6. Explained Variance Ratio","text":"<p>The proportion of variance explained by the first \\(k\\) components: \\(\\(\\text{Explained Variance Ratio} = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i}\\)\\)</p>"},{"location":"Machine-Learning/PCA/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/PCA/#scikit-learn-implementation","title":"Scikit-learn Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, make_blobs\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\n\n# Load and prepare data\niris = load_iris()\nX, y = iris.data, iris.target\nfeature_names = iris.feature_names\n\n# Standardize the features (important for PCA)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Get explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\nprint(\"Explained Variance by Component:\")\nfor i, var in enumerate(explained_variance):\n    print(f\"PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n\nprint(f\"\\nCumulative Explained Variance:\")\nfor i, cum_var in enumerate(cumulative_variance):\n    print(f\"First {i+1} components: {cum_var:.3f} ({cum_var*100:.1f}%)\")\n\n# Visualize explained variance\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.bar(range(1, len(explained_variance) + 1), explained_variance)\nplt.title('Explained Variance by Principal Component')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 2D visualization using first 2 components\npca_2d = PCA(n_components=2)\nX_pca_2d = pca_2d.fit_transform(X_scaled)\n\nplt.figure(figsize=(10, 6))\ncolors = ['red', 'green', 'blue']\nfor i, color in enumerate(colors):\n    plt.scatter(X_pca_2d[y == i, 0], X_pca_2d[y == i, 1], \n                c=color, label=iris.target_names[i], alpha=0.6)\nplt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2f} variance)')\nplt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2f} variance)')\nplt.title('PCA: Iris Dataset in 2D')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Component interpretation\ncomponents_df = pd.DataFrame(\n    pca_2d.components_.T,\n    columns=['PC1', 'PC2'],\n    index=feature_names\n)\nprint(\"\\nPrincipal Component Loadings:\")\nprint(components_df)\n\n# Biplot (features and data points)\ndef biplot(X_pca, components, feature_names, y):\n    plt.figure(figsize=(12, 8))\n\n    # Plot data points\n    colors = ['red', 'green', 'blue']\n    for i, color in enumerate(colors):\n        plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], \n                   c=color, label=iris.target_names[i], alpha=0.6)\n\n    # Plot feature vectors\n    for i, feature in enumerate(feature_names):\n        plt.arrow(0, 0, components[i, 0]*3, components[i, 1]*3,\n                 head_width=0.1, head_length=0.1, fc='black', ec='black')\n        plt.text(components[i, 0]*3.2, components[i, 1]*3.2, feature,\n                fontsize=12, ha='center', va='center')\n\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.title('PCA Biplot - Iris Dataset')\n    plt.legend()\n    plt.grid(True)\n    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n    plt.show()\n\nbiplot(X_pca_2d, pca_2d.components_, feature_names, y)\n</code></pre>"},{"location":"Machine-Learning/PCA/#dimensionality-reduction-for-classification","title":"Dimensionality Reduction for Classification","text":"<pre><code># Compare classification performance with and without PCA\ndef compare_with_without_pca(X, y, n_components=2):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42, stratify=y)\n\n    # Standardize\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Without PCA\n    rf_original = RandomForestClassifier(random_state=42)\n    rf_original.fit(X_train_scaled, y_train)\n    y_pred_original = rf_original.predict(X_test_scaled)\n    accuracy_original = accuracy_score(y_test, y_pred_original)\n\n    # With PCA\n    pca = PCA(n_components=n_components)\n    X_train_pca = pca.fit_transform(X_train_scaled)\n    X_test_pca = pca.transform(X_test_scaled)\n\n    rf_pca = RandomForestClassifier(random_state=42)\n    rf_pca.fit(X_train_pca, y_train)\n    y_pred_pca = rf_pca.predict(X_test_pca)\n    accuracy_pca = accuracy_score(y_test, y_pred_pca)\n\n    print(f\"Results Comparison:\")\n    print(f\"Original features ({X.shape[1]}): {accuracy_original:.3f}\")\n    print(f\"PCA features ({n_components}): {accuracy_pca:.3f}\")\n    print(f\"Variance explained by PCA: {pca.explained_variance_ratio_.sum():.3f}\")\n    print(f\"Dimensionality reduction: {X.shape[1]} -&gt; {n_components} \" +\n          f\"({(1 - n_components/X.shape[1])*100:.1f}% reduction)\")\n\ncompare_with_without_pca(X_scaled, y, n_components=2)\n</code></pre>"},{"location":"Machine-Learning/PCA/#from-scratch-implementation","title":"\ufffd\u000f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass PCAFromScratch:\n    def __init__(self, n_components=None):\n        \"\"\"\n        Principal Component Analysis implementation from scratch\n\n        Parameters:\n        n_components: Number of components to keep (if None, keep all)\n        \"\"\"\n        self.n_components = n_components\n        self.components_ = None\n        self.explained_variance_ = None\n        self.explained_variance_ratio_ = None\n        self.mean_ = None\n        self.singular_values_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fit PCA on the training data\n\n        Parameters:\n        X: Training data of shape (n_samples, n_features)\n        \"\"\"\n        # Center the data\n        self.mean_ = np.mean(X, axis=0)\n        X_centered = X - self.mean_\n\n        # Compute covariance matrix\n        n_samples = X.shape[0]\n        covariance_matrix = np.dot(X_centered.T, X_centered) / (n_samples - 1)\n\n        # Compute eigenvalues and eigenvectors\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n\n        # Sort by decreasing eigenvalues\n        idx = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n\n        # Store results\n        if self.n_components is None:\n            self.n_components = len(eigenvalues)\n\n        self.components_ = eigenvectors[:, :self.n_components].T\n        self.explained_variance_ = eigenvalues[:self.n_components]\n        self.explained_variance_ratio_ = (\n            self.explained_variance_ / np.sum(eigenvalues)\n        )\n\n        # For compatibility with sklearn\n        self.singular_values_ = np.sqrt(self.explained_variance_ * (n_samples - 1))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transform the data to the principal component space\n\n        Parameters:\n        X: Data to transform of shape (n_samples, n_features)\n\n        Returns:\n        X_transformed: Transformed data of shape (n_samples, n_components)\n        \"\"\"\n        X_centered = X - self.mean_\n        return np.dot(X_centered, self.components_.T)\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit PCA and transform the data\n        \"\"\"\n        return self.fit(X).transform(X)\n\n    def inverse_transform(self, X_transformed):\n        \"\"\"\n        Transform the data back to original space (reconstruction)\n\n        Parameters:\n        X_transformed: Data in PC space of shape (n_samples, n_components)\n\n        Returns:\n        X_reconstructed: Reconstructed data in original space\n        \"\"\"\n        return np.dot(X_transformed, self.components_) + self.mean_\n\n    def get_covariance(self):\n        \"\"\"\n        Get the covariance matrix of the data in PC space\n        \"\"\"\n        return np.dot(self.components_ * self.explained_variance_,\n                     self.components_.T)\n\n# Demonstration with synthetic data\nnp.random.seed(42)\n\n# Create correlated 2D data\nmean = [0, 0]\ncov = [[3, 2.5], [2.5, 3]]\nX_synthetic = np.random.multivariate_normal(mean, cov, 300)\n\n# Apply custom PCA\npca_custom = PCAFromScratch(n_components=2)\nX_pca_custom = pca_custom.fit_transform(X_synthetic)\n\n# Compare with sklearn\nfrom sklearn.decomposition import PCA\npca_sklearn = PCA(n_components=2)\nX_pca_sklearn = pca_sklearn.fit_transform(X_synthetic)\n\nprint(\"Custom PCA Results:\")\nprint(\"Explained variance ratio:\", pca_custom.explained_variance_ratio_)\nprint(\"Components shape:\", pca_custom.components_.shape)\n\nprint(\"\\nSklearn PCA Results:\")\nprint(\"Explained variance ratio:\", pca_sklearn.explained_variance_ratio_)\nprint(\"Components shape:\", pca_sklearn.components_.shape)\n\nprint(\"\\nDifference in results (should be close to zero):\")\nprint(\"Explained variance ratio diff:\", \n      np.abs(pca_custom.explained_variance_ratio_ - \n             pca_sklearn.explained_variance_ratio_).max())\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original data\naxes[0].scatter(X_synthetic[:, 0], X_synthetic[:, 1], alpha=0.7)\naxes[0].set_title('Original Data')\naxes[0].set_xlabel('Feature 1')\naxes[0].set_ylabel('Feature 2')\naxes[0].grid(True)\n\n# PCA transformed data\naxes[1].scatter(X_pca_custom[:, 0], X_pca_custom[:, 1], alpha=0.7)\naxes[1].set_title('PCA Transformed Data')\naxes[1].set_xlabel('PC1')\naxes[1].set_ylabel('PC2')\naxes[1].grid(True)\n\n# Original data with principal components\naxes[2].scatter(X_synthetic[:, 0], X_synthetic[:, 1], alpha=0.7)\nmean_point = pca_custom.mean_\n\n# Plot principal component directions\nfor i in range(2):\n    direction = pca_custom.components_[i] * 3 * np.sqrt(pca_custom.explained_variance_[i])\n    axes[2].arrow(mean_point[0], mean_point[1], \n                  direction[0], direction[1],\n                  head_width=0.2, head_length=0.3, \n                  fc=f'C{i+1}', ec=f'C{i+1}', linewidth=2,\n                  label=f'PC{i+1}')\n\naxes[2].set_title('Original Data with Principal Components')\naxes[2].set_xlabel('Feature 1')\naxes[2].set_ylabel('Feature 2')\naxes[2].legend()\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/PCA/#advanced-features-implementation","title":"Advanced Features Implementation","text":"<pre><code>def reconstruction_error_analysis(X, max_components=None):\n    \"\"\"\n    Analyze reconstruction error vs number of components\n    \"\"\"\n    if max_components is None:\n        max_components = min(X.shape) - 1\n\n    errors = []\n    components_range = range(1, max_components + 1)\n\n    for n_comp in components_range:\n        pca = PCAFromScratch(n_components=n_comp)\n        X_transformed = pca.fit_transform(X)\n        X_reconstructed = pca.inverse_transform(X_transformed)\n\n        # Calculate reconstruction error (mean squared error)\n        error = np.mean((X - X_reconstructed) ** 2)\n        errors.append(error)\n\n    return components_range, errors\n\n# Example with iris dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_iris = iris.data\n\n# Standardize\nX_iris_scaled = (X_iris - np.mean(X_iris, axis=0)) / np.std(X_iris, axis=0)\n\n# Analyze reconstruction error\ncomponents, errors = reconstruction_error_analysis(X_iris_scaled, max_components=4)\n\nplt.figure(figsize=(10, 6))\nplt.plot(components, errors, 'bo-', linewidth=2, markersize=8)\nplt.title('Reconstruction Error vs Number of Components')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Mean Squared Reconstruction Error')\nplt.grid(True)\nplt.xticks(components)\nplt.show()\n\nprint(\"Reconstruction Errors:\")\nfor comp, error in zip(components, errors):\n    print(f\"{comp} components: {error:.6f}\")\n</code></pre>"},{"location":"Machine-Learning/PCA/#assumptions-and-limitations","title":"\ufffd\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/PCA/#key-assumptions","title":"Key Assumptions","text":"<ol> <li>Linear relationships: PCA assumes linear relationships between variables</li> <li>Variance equals importance: Higher variance directions are assumed to be more important</li> <li>Orthogonal components: Principal components are orthogonal (perpendicular)</li> <li>Gaussian distribution: Works best with normally distributed data</li> <li>Standardization: Features should be on similar scales (usually requires standardization)</li> </ol>"},{"location":"Machine-Learning/PCA/#limitations","title":"Limitations","text":"<ol> <li>Linear transformation only: Cannot capture non-linear relationships</li> <li> <p>Solution: Use Kernel PCA or other non-linear techniques</p> </li> <li> <p>Interpretability loss: Principal components are linear combinations of original features</p> </li> <li> <p>Solution: Use factor analysis or sparse PCA for more interpretable components</p> </li> <li> <p>Sensitive to scaling: Features with larger scales dominate the principal components</p> </li> <li> <p>Solution: Always standardize features before applying PCA</p> </li> <li> <p>Information loss: Dimensionality reduction inherently loses some information</p> </li> <li> <p>Assessment: Monitor explained variance ratio and reconstruction error</p> </li> <li> <p>Outlier sensitivity: Outliers can significantly affect principal components</p> </li> <li> <p>Solution: Use robust PCA variants or outlier detection/removal</p> </li> <li> <p>No guarantee of class separation: PCA maximizes variance, not class separability</p> </li> <li>Alternative: Use Linear Discriminant Analysis (LDA) for classification tasks</li> </ol>"},{"location":"Machine-Learning/PCA/#comparison-with-other-techniques","title":"Comparison with Other Techniques","text":"Method Linear Supervised Interpretable Non-linear PCA \u0013 \u0017 Partial \u0017 LDA \u0013 \u0013 Partial \u0017 t-SNE \u0017 \u0017 \u0017 \u0013 UMAP \u0017 \u0017 \u0017 \u0013 Factor Analysis \u0013 \u0017 \u0013 \u0017 ICA \u0013 \u0017 \u0013 \u0017 <p>When to avoid PCA: - When original features have clear business meaning that must be preserved - With categorical or ordinal data without proper encoding - When non-linear relationships are important - With very sparse data (consider specialized sparse PCA) - When you need exactly interpretable features for regulatory compliance</p>"},{"location":"Machine-Learning/PCA/#interview-questions","title":"\u2753 Interview Questions","text":"What is the mathematical intuition behind PCA and how does it work? <p>Answer: PCA finds the directions (principal components) in the data that capture the maximum variance. Mathematically, it performs eigenvalue decomposition on the covariance matrix:</p> <ol> <li>Center the data: Subtract the mean from each feature</li> <li>Compute covariance matrix: C = (X^T * X) / (n-1)</li> <li>Find eigenvalues and eigenvectors: C*v = \ufffd*v</li> <li>Sort by eigenvalues: Largest eigenvalues correspond to directions with most variance</li> <li>Project data: Transform original data onto selected eigenvectors</li> </ol> <p>The key insight is that eigenvectors of the covariance matrix are orthogonal directions of maximum variance, and eigenvalues represent the amount of variance explained by each direction.</p> Why do we need to standardize features before applying PCA? <p>Answer: Features must be standardized because PCA is sensitive to the scale of variables:</p> <ul> <li>Scale dominance: Features with larger scales (e.g., income in dollars vs age in years) will dominate the principal components</li> <li>Variance bias: PCA maximizes variance, so large-scale features appear to have more \"importance\"</li> <li>Covariance matrix distortion: The covariance matrix will be dominated by high-variance features</li> </ul> <p>Example: Without standardization, if you have height (cm, ~170) and weight (kg, ~70), height will dominate simply due to larger numerical values, not because it's more important.</p> <p>Solution: Use z-score standardization: (x - \ufffd) / \ufffd for each feature.</p> How do you choose the optimal number of principal components? <p>Answer: Several methods exist for selecting the number of components:</p> <ol> <li>Explained Variance Threshold: Keep components explaining 80-95% of variance</li> <li>Elbow Method: Plot explained variance vs components, look for \"elbow\" point</li> <li>Kaiser Rule: Keep components with eigenvalues &gt; 1 (for standardized data)</li> <li>Scree Plot: Visual inspection of eigenvalue decay</li> <li>Cross-validation: Use downstream task performance to select optimal number</li> <li>Business requirements: Based on computational constraints or interpretability needs</li> </ol> <p>Code example: <pre><code>cumsum_var = np.cumsum(pca.explained_variance_ratio_)\nn_components = np.argmax(cumsum_var &gt;= 0.95) + 1  # 95% variance\n</code></pre></p> What's the difference between PCA and Linear Discriminant Analysis (LDA)? <p>Answer: Key differences:</p> Aspect PCA LDA Type Unsupervised Supervised Objective Maximize variance Maximize class separation Input Features only Features + labels Components Up to min(n_features, n_samples) Up to (n_classes - 1) Use case Dimensionality reduction Classification preprocessing <p>When to use each: - PCA: Data exploration, compression, noise reduction, visualization - LDA: Classification tasks, when you want to maximize class separability</p> <p>Example: For 3-class iris dataset, LDA can find at most 2 components, while PCA can find up to 4.</p> How do you interpret the principal components and their loadings? <p>Answer: Principal components and loadings provide insights into data structure:</p> <p>Loadings (Component coefficients): - Show contribution of each original feature to each PC - Values range typically from -1 to 1 - Large absolute values indicate strong influence</p> <p>Interpretation steps: 1. Examine loading values: Which features contribute most to each PC? 2. Look for patterns: Do related features load together? 3. Name components: Based on dominant features (e.g., \"size factor\", \"ratio factor\")</p> <p>Example interpretation: <pre><code>PC1 loadings: [0.8 height, 0.7 weight, 0.1 age] \ufffd \"Physical size factor\"\nPC2 loadings: [0.2 height, -0.1 weight, 0.9 age] \ufffd \"Age factor\"\n</code></pre></p> What are the limitations of PCA and when should you not use it? <p>Answer: Major limitations and alternatives:</p> <p>Limitations: 1. Linear only: Cannot capture non-linear relationships \ufffd Use Kernel PCA, t-SNE 2. Variance ` Importance: High variance doesn't always mean importance \ufffd Use domain knowledge 3. Loss of interpretability: PCs are combinations of original features \ufffd Use Sparse PCA, Factor Analysis 4. Outlier sensitive: Outliers can skew components \ufffd Use Robust PCA 5. No class consideration: Doesn't consider target variable \ufffd Use LDA for classification</p> <p>When NOT to use PCA: - Categorical data without proper encoding - When original features must be preserved (regulatory requirements) - Very sparse data (many zeros) - Non-linear relationships are crucial - Small datasets (overfitting risk)</p> How do you handle missing values when applying PCA? <p>Answer: Several strategies for missing data in PCA:</p> <p>1. Complete Case Analysis: <pre><code># Remove rows with any missing values\nX_complete = X.dropna()\n</code></pre></p> <p>2. Imputation before PCA: <pre><code>from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\n</code></pre></p> <p>3. Iterative Imputation: <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimputer = IterativeImputer()\nX_imputed = imputer.fit_transform(X)\n</code></pre></p> <p>4. PCA with missing values (specialized methods): - Use algorithms like NIPALS (Nonlinear Iterative Partial Least Squares) - Probabilistic PCA that handles missing values directly</p> <p>Best practice: Analyze missing data patterns first, then choose appropriate strategy based on data characteristics.</p> Explain the relationship between PCA and Singular Value Decomposition (SVD). <p>Answer: PCA and SVD are mathematically related:</p> <p>SVD decomposition of centered data matrix X: <pre><code>X = U * \ufffd * V^T\n</code></pre> Where: - U: Left singular vectors - \ufffd: Singular values (diagonal matrix) - V: Right singular vectors</p> <p>Connection to PCA: - Principal components = columns of V - Explained variance = (singular values)\ufffd / (n-1) - Transformed data = U * \ufffd</p> <p>Advantages of SVD approach: 1. More numerically stable 2. Computationally efficient for tall matrices 3. Doesn't require computing covariance matrix explicitly 4. Better for sparse data</p> <p>Implementation: <pre><code># Using SVD for PCA\nU, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\ncomponents = Vt  # Principal components\nexplained_variance = (s ** 2) / (n - 1)\n</code></pre></p> How do you evaluate the quality of PCA results? <p>Answer: Multiple metrics assess PCA quality:</p> <p>1. Explained Variance Ratio: <pre><code>total_variance_explained = sum(pca.explained_variance_ratio_)\nprint(f\"Total variance explained: {total_variance_explained:.3f}\")\n</code></pre></p> <p>2. Reconstruction Error: <pre><code>X_reconstructed = pca.inverse_transform(X_pca)\nmse = np.mean((X_original - X_reconstructed) ** 2)\n</code></pre></p> <p>3. Silhouette Score (if labels available): <pre><code>from sklearn.metrics import silhouette_score\nscore = silhouette_score(X_pca, labels)\n</code></pre></p> <p>4. Downstream Task Performance: - Compare classifier accuracy before/after PCA - Monitor if important patterns are preserved</p> <p>5. Visual Assessment: - Scree plots for eigenvalue decay - Biplots for feature relationships - 2D/3D scatter plots for cluster visualization</p> <p>Quality indicators: - \u0005 First few PCs explain &gt;80% variance - \u0005 Smooth eigenvalue decay (no sudden drops) - \u0005 Components are interpretable - \u0005 Downstream performance maintained</p>"},{"location":"Machine-Learning/PCA/#examples","title":"&gt;\ufffd Examples","text":""},{"location":"Machine-Learning/PCA/#real-world-example-image-compression-with-pca","title":"Real-world Example: Image Compression with PCA","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.decomposition import PCA\n\n# Load face images dataset\nfaces = fetch_olivetti_faces(shuffle=True, random_state=42)\nX_faces = faces.data\ny_faces = faces.target\n\nprint(f\"Dataset shape: {X_faces.shape}\")\nprint(f\"Original image dimensions: {int(np.sqrt(X_faces.shape[1]))}x{int(np.sqrt(X_faces.shape[1]))}\")\n\n# Apply PCA with different numbers of components\nn_components_list = [10, 50, 100, 200, 400]\nfig, axes = plt.subplots(2, len(n_components_list) + 1, figsize=(18, 6))\n\n# Original image\nsample_idx = 0\noriginal_image = X_faces[sample_idx].reshape(64, 64)\naxes[0, 0].imshow(original_image, cmap='gray')\naxes[0, 0].set_title('Original')\naxes[0, 0].axis('off')\n\n# Show compression ratios\ncompression_ratios = []\nreconstruction_errors = []\n\nfor i, n_comp in enumerate(n_components_list):\n    # Apply PCA\n    pca = PCA(n_components=n_comp)\n    X_pca = pca.fit_transform(X_faces)\n    X_reconstructed = pca.inverse_transform(X_pca)\n\n    # Calculate compression ratio and error\n    original_size = X_faces.shape[1]  # 4096 pixels\n    compressed_size = n_comp + original_size * n_comp  # components + loadings\n    compression_ratio = original_size / compressed_size\n\n    mse = np.mean((X_faces - X_reconstructed) ** 2)\n\n    compression_ratios.append(compression_ratio)\n    reconstruction_errors.append(mse)\n\n    # Display reconstructed image\n    reconstructed_image = X_reconstructed[sample_idx].reshape(64, 64)\n    axes[0, i + 1].imshow(reconstructed_image, cmap='gray')\n    axes[0, i + 1].set_title(f'{n_comp} PCs\\n({pca.explained_variance_ratio_.sum():.2f} var)')\n    axes[0, i + 1].axis('off')\n\n# Plot metrics\naxes[1, 0].remove()  # Remove empty subplot\naxes[1, 1].bar(range(len(n_components_list)), compression_ratios)\naxes[1, 1].set_title('Compression Ratio')\naxes[1, 1].set_xlabel('Number of Components')\naxes[1, 1].set_xticks(range(len(n_components_list)))\naxes[1, 1].set_xticklabels(n_components_list)\n\naxes[1, 2].plot(n_components_list, reconstruction_errors, 'ro-')\naxes[1, 2].set_title('Reconstruction Error')\naxes[1, 2].set_xlabel('Number of Components')\naxes[1, 2].set_ylabel('MSE')\n\n# Explained variance\naxes[1, 3].remove()\naxes[1, 4].remove()\nax_combined = plt.subplot(2, len(n_components_list) + 1, (2*len(n_components_list) + 4, 2*len(n_components_list) + 6))\n\n# Plot cumulative explained variance\npca_full = PCA()\npca_full.fit(X_faces)\ncumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n\nax_combined.plot(range(1, min(201, len(cumsum_var) + 1)), \n                cumsum_var[:200], 'b-', linewidth=2)\nax_combined.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\nax_combined.set_title('Cumulative Explained Variance')\nax_combined.set_xlabel('Number of Components')\nax_combined.set_ylabel('Cumulative Variance')\nax_combined.legend()\nax_combined.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"\\nImage Compression Results:\")\nfor n_comp, ratio, error in zip(n_components_list, compression_ratios, reconstruction_errors):\n    print(f\"{n_comp:3d} components: {ratio:4.1f}x compression, MSE: {error:.6f}\")\n</code></pre>"},{"location":"Machine-Learning/PCA/#market-analysis-example","title":"Market Analysis Example","text":"<pre><code>import pandas as pd\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\n# Download stock data for tech companies\ntickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX']\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*2)  # 2 years of data\n\n# Download stock returns\nstock_data = {}\nfor ticker in tickers:\n    try:\n        stock = yf.download(ticker, start=start_date, end=end_date)\n        stock_data[ticker] = stock['Adj Close'].pct_change().dropna()\n    except:\n        print(f\"Could not download data for {ticker}\")\n\n# Create returns dataframe\nreturns_df = pd.DataFrame(stock_data)\nreturns_df = returns_df.dropna()\n\nprint(f\"Stock returns data shape: {returns_df.shape}\")\nprint(\"\\nBasic statistics:\")\nprint(returns_df.describe())\n\n# Apply PCA to stock returns\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize returns\nscaler = StandardScaler()\nreturns_scaled = scaler.fit_transform(returns_df)\n\n# Fit PCA\npca_stocks = PCA()\nreturns_pca = pca_stocks.fit_transform(returns_scaled)\n\n# Analyze results\nexplained_var = pca_stocks.explained_variance_ratio_\ncumulative_var = np.cumsum(explained_var)\n\nprint(f\"\\nPCA Results for Stock Returns:\")\nprint(\"Explained Variance by Component:\")\nfor i, var in enumerate(explained_var[:5]):\n    print(f\"PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n\nprint(f\"\\nFirst 3 components explain {cumulative_var[2]:.3f} ({cumulative_var[2]*100:.1f}%) of variance\")\n\n# Component interpretation\ncomponents_df = pd.DataFrame(\n    pca_stocks.components_[:3].T,  # First 3 components\n    columns=['PC1 (Market)', 'PC2 (Tech vs Value)', 'PC3 (Volatility)'],\n    index=returns_df.columns\n)\n\nprint(\"\\nComponent Loadings (Stock Exposure to Factors):\")\nprint(components_df.round(3))\n\n# Visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Explained variance\naxes[0, 0].bar(range(1, len(explained_var) + 1), explained_var)\naxes[0, 0].set_title('Explained Variance by Component')\naxes[0, 0].set_xlabel('Principal Component')\naxes[0, 0].set_ylabel('Explained Variance Ratio')\n\n# Cumulative variance\naxes[0, 1].plot(range(1, len(cumulative_var) + 1), cumulative_var, 'bo-')\naxes[0, 1].axhline(y=0.95, color='r', linestyle='--', label='95%')\naxes[0, 1].set_title('Cumulative Explained Variance')\naxes[0, 1].set_xlabel('Number of Components')\naxes[0, 1].legend()\n\n# Component loadings heatmap\nimport seaborn as sns\nsns.heatmap(components_df.T, annot=True, cmap='coolwarm', center=0,\n            ax=axes[1, 0], cbar_kws={'label': 'Loading'})\naxes[1, 0].set_title('Component Loadings Heatmap')\n\n# Factor scores over time\nfactor_scores = pd.DataFrame(returns_pca[:, :3], \n                           index=returns_df.index,\n                           columns=['Market Factor', 'Style Factor', 'Volatility Factor'])\n\naxes[1, 1].plot(factor_scores.index, factor_scores['Market Factor'], \n                label='Market Factor', alpha=0.7)\naxes[1, 1].plot(factor_scores.index, factor_scores['Style Factor'], \n                label='Style Factor', alpha=0.7)\naxes[1, 1].set_title('Principal Component Scores Over Time')\naxes[1, 1].set_ylabel('Factor Score')\naxes[1, 1].legend()\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# Portfolio analysis using PCA\nprint(\"\\nPortfolio Risk Analysis using PCA:\")\n\n# Risk contribution by component\nportfolio_weights = np.ones(len(tickers)) / len(tickers)  # Equal weights\nportfolio_return_std = np.dot(portfolio_weights, returns_df.std())\n\nprint(f\"Portfolio return volatility: {portfolio_return_std:.4f}\")\n\n# Factor exposures\nfactor_exposures = np.dot(portfolio_weights, components_df.values)\nprint(\"Portfolio factor exposures:\")\nfor i, exposure in enumerate(factor_exposures):\n    print(f\"  PC{i+1}: {exposure:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/PCA/#references","title":"\ud83d\udcda References","text":"<ul> <li>Books:</li> <li>The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman - Chapter 14</li> <li>Pattern Recognition and Machine Learning by Christopher Bishop - Chapter 12</li> <li> <p>Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani - Chapter 10</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn PCA</li> <li>Scikit-learn Decomposition Guide</li> <li> <p>NumPy Linear Algebra</p> </li> <li> <p>Tutorials:</p> </li> <li>PCA Explained Visually</li> <li>A Tutorial on Principal Component Analysis</li> <li> <p>PCA with Python Tutorial</p> </li> <li> <p>Research Papers:</p> </li> <li>Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space</li> <li>Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components</li> <li> <p>Jolliffe, I.T. (2002). Principal Component Analysis, Second Edition</p> </li> <li> <p>Advanced Topics:</p> </li> <li>Kernel PCA</li> <li>Sparse PCA</li> <li> <p>Incremental PCA</p> </li> <li> <p>Online Courses:</p> </li> <li>Machine Learning Course - Stanford CS229</li> <li>Dimensionality Reduction - Coursera</li> <li>Advanced Machine Learning - edX</li> </ul>"},{"location":"Machine-Learning/Random%20Forest/","title":"\ud83c\udf33 Random Forest","text":"<p>Random Forest is a powerful ensemble machine learning algorithm that builds multiple decision trees and combines their predictions to create a more robust and accurate model, reducing overfitting while maintaining interpretability.</p> <p>Resources: Scikit-learn Random Forest | Random Forests Paper - Leo Breiman | Elements of Statistical Learning - Chapter 15</p>"},{"location":"Machine-Learning/Random%20Forest/#_1","title":"Random Forest","text":"<p>\u000f Summary</p> <p>Random Forest is an ensemble learning method that combines multiple decision trees using two key techniques: bagging (bootstrap aggregating) and random feature selection. Each tree in the forest is trained on a bootstrap sample of the data and considers only a random subset of features at each split, reducing correlation between trees and improving generalization.</p> <p>Key characteristics: - Ensemble method: Combines multiple decision trees for better performance - Bootstrap sampling: Each tree trained on different subset of data - Random feature selection: Each split considers random subset of features - Reduces overfitting: Averaging multiple trees reduces variance - Handles missing values: Can work with datasets containing missing values - Feature importance: Provides built-in feature importance metrics</p> <p>Applications: - Classification tasks (fraud detection, medical diagnosis) - Regression problems (house price prediction, stock returns) - Feature selection and ranking - Outlier detection - Missing value imputation - Bioinformatics and genomics - Natural language processing - Computer vision</p> <p>Types: - Random Forest Classifier: For classification tasks - Random Forest Regressor: For regression tasks - Extremely Randomized Trees (Extra Trees): Uses random thresholds for splits - Isolation Forest: Specialized variant for anomaly detection</p>"},{"location":"Machine-Learning/Random%20Forest/#intuition","title":"&gt;\ufffd Intuition","text":""},{"location":"Machine-Learning/Random%20Forest/#how-random-forest-works","title":"How Random Forest Works","text":"<p>Imagine you're making an important decision and want multiple expert opinions. Instead of asking one expert (single decision tree), you ask 100 experts (trees), where each expert: 1. Has seen different training examples (bootstrap sampling) 2. Considers different aspects of the problem (random features) 3. Makes their own prediction</p> <p>The final decision is made by majority vote (classification) or averaging (regression). This \"wisdom of crowds\" approach often performs better than any individual expert.</p>"},{"location":"Machine-Learning/Random%20Forest/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Random%20Forest/#1-bootstrap-aggregating-bagging","title":"1. Bootstrap Aggregating (Bagging)","text":"<p>For a training dataset \\(D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\):</p> <ol> <li>Bootstrap sampling: Create \\(B\\) bootstrap samples \\(D_1, D_2, ..., D_B\\)</li> <li>Each \\(D_b\\) contains \\(n\\) samples drawn with replacement from \\(D\\)</li> <li> <p>Typically ~63.2% unique samples per bootstrap</p> </li> <li> <p>Train individual trees: For each \\(D_b\\), train tree \\(T_b\\)</p> </li> <li> <p>Aggregate predictions:</p> </li> <li>Classification: \\(\\hat{y} = \\text{mode}(T_1(x), T_2(x), ..., T_B(x))\\)</li> <li>Regression: \\(\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B} T_b(x)\\)</li> </ol>"},{"location":"Machine-Learning/Random%20Forest/#2-random-feature-selection","title":"2. Random Feature Selection","text":"<p>At each node split, instead of considering all \\(p\\) features, randomly select \\(m\\) features where: - Classification: \\(m = \\sqrt{p}\\) (typical default) - Regression: \\(m = \\frac{p}{3}\\) (typical default) - Custom: Can be tuned as hyperparameter</p>"},{"location":"Machine-Learning/Random%20Forest/#3-out-of-bag-oob-error","title":"3. Out-of-Bag (OOB) Error","text":"<p>For each sample not in bootstrap sample \\(D_b\\) (out-of-bag samples): - Use trees trained on \\(D_b\\) to predict - OOB error provides unbiased estimate of generalization error</p> \\[\\text{OOB Error} = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\hat{y}_{i}^{OOB})\\] <p>where \\(\\hat{y}_{i}^{OOB}\\) is the prediction for \\(x_i\\) using only trees where \\(x_i\\) was out-of-bag.</p>"},{"location":"Machine-Learning/Random%20Forest/#4-feature-importance","title":"4. Feature Importance","text":"<p>Gini Importance (Mean Decrease Impurity): \\(\\(\\text{Importance}(f) = \\frac{1}{B}\\sum_{b=1}^{B} \\sum_{t \\in T_b} p(t) \\cdot \\Delta I(t)\\)\\)</p> <p>where \\(p(t)\\) is the proportion of samples reaching node \\(t\\), and \\(\\Delta I(t)\\) is the impurity decrease at node \\(t\\) when splitting on feature \\(f\\).</p> <p>Permutation Importance (Mean Decrease Accuracy): 1. Calculate OOB error for original data 2. Randomly permute values of feature \\(f\\) in OOB samples 3. Calculate new OOB error with permuted feature 4. Importance = increase in OOB error</p>"},{"location":"Machine-Learning/Random%20Forest/#5-variance-reduction","title":"5. Variance Reduction","text":"<p>For \\(B\\) independent trees with variance \\(\\sigma^2\\), the ensemble variance is: \\(\\(\\text{Var}(\\text{ensemble}) = \\frac{\\sigma^2}{B}\\)\\)</p> <p>However, trees are correlated with correlation \\(\\rho\\): \\(\\(\\text{Var}(\\text{ensemble}) = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2\\)\\)</p> <p>Random feature selection reduces \\(\\rho\\), improving variance reduction.</p>"},{"location":"Machine-Learning/Random%20Forest/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/Random%20Forest/#scikit-learn-implementation","title":"Scikit-learn Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, load_boston, make_classification\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Classification Example with Iris Dataset\niris = load_iris()\nX_iris, y_iris = iris.data, iris.target\nfeature_names = iris.feature_names\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris)\n\n# Create and train Random Forest classifier\nrf_classifier = RandomForestClassifier(\n    n_estimators=100,          # Number of trees\n    max_depth=None,            # No limit on tree depth\n    min_samples_split=2,       # Min samples to split internal node\n    min_samples_leaf=1,        # Min samples at leaf node\n    max_features='sqrt',       # Features to consider at each split\n    bootstrap=True,            # Use bootstrap sampling\n    oob_score=True,           # Calculate out-of-bag score\n    random_state=42\n)\n\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_classifier.predict(X_test)\ny_prob = rf_classifier.predict_proba(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\noob_score = rf_classifier.oob_score_\n\nprint(f\"Random Forest Classification Results:\")\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Out-of-bag Score: {oob_score:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': rf_classifier.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(f\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Visualize feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\nplt.title('Random Forest Feature Importance')\nplt.xlabel('Importance Score')\nplt.tight_layout()\nplt.show()\n\n# Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=iris.target_names, \n            yticklabels=iris.target_names)\nplt.title('Confusion Matrix - Random Forest')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#regression-example","title":"Regression Example","text":"<pre><code># Generate synthetic regression data\nX_reg, y_reg = make_classification(\n    n_samples=1000, n_features=20, n_informative=15, \n    n_redundant=5, noise=0.1, random_state=42\n)\n\n# Convert to regression problem\ny_reg = y_reg.astype(float) + np.random.normal(0, 0.1, len(y_reg))\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42)\n\n# Random Forest Regressor\nrf_regressor = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    max_features='sqrt',\n    bootstrap=True,\n    oob_score=True,\n    random_state=42\n)\n\nrf_regressor.fit(X_train_reg, y_train_reg)\n\n# Predictions\ny_pred_reg = rf_regressor.predict(X_test_reg)\n\n# Evaluate\nmse = mean_squared_error(y_test_reg, y_pred_reg)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_reg, y_pred_reg)\noob_score_reg = rf_regressor.oob_score_\n\nprint(f\"\\nRandom Forest Regression Results:\")\nprint(f\"RMSE: {rmse:.3f}\")\nprint(f\"R\ufffd Score: {r2:.3f}\")\nprint(f\"Out-of-bag Score: {oob_score_reg:.3f}\")\n\n# Plot predictions vs actual\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\nplt.plot([y_test_reg.min(), y_test_reg.max()], \n         [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title(f'Random Forest Regression: Actual vs Predicted (R\ufffd = {r2:.3f})')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Hyperparameter tuning with GridSearchCV\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid, \n    cv=5, \n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"\\nBest parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n\n# Best model performance\nbest_rf = grid_search.best_estimator_\ny_pred_best = best_rf.predict(X_test)\nbest_accuracy = accuracy_score(y_test, y_pred_best)\n\nprint(f\"Test accuracy with best parameters: {best_accuracy:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#learning-curves-analysis","title":"Learning Curves Analysis","text":"<pre><code>from sklearn.model_selection import learning_curve\n\ndef plot_learning_curves(estimator, X, y, cv=5):\n    train_sizes, train_scores, val_scores = learning_curve(\n        estimator, X, y, cv=cv, \n        train_sizes=np.linspace(0.1, 1.0, 10),\n        scoring='accuracy', random_state=42)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n                     alpha=0.1, color='blue')\n\n    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n                     alpha=0.1, color='red')\n\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Accuracy Score')\n    plt.title('Random Forest Learning Curves')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Plot learning curves for default Random Forest\nplot_learning_curves(RandomForestClassifier(random_state=42), X_iris, y_iris)\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#from-scratch-implementation","title":"\ufffd\u000f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nclass DecisionTreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature      # Feature index for split\n        self.threshold = threshold  # Threshold value for split\n        self.left = left           # Left child node\n        self.right = right         # Right child node\n        self.value = value         # Prediction value (for leaf nodes)\n\nclass DecisionTreeFromScratch:\n    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n                 max_features=None, task='classification'):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        self.task = task\n        self.tree = None\n\n    def fit(self, X, y):\n        self.n_features = X.shape[1]\n        if self.max_features is None:\n            self.max_features = self.n_features\n        elif isinstance(self.max_features, str):\n            if self.max_features == 'sqrt':\n                self.max_features = int(np.sqrt(self.n_features))\n            elif self.max_features == 'log2':\n                self.max_features = int(np.log2(self.n_features))\n\n        self.tree = self._build_tree(X, y, depth=0)\n\n    def _build_tree(self, X, y, depth):\n        n_samples, n_features = X.shape\n\n        # Stopping criteria\n        if (self.max_depth is not None and depth &gt;= self.max_depth) or \\\n           n_samples &lt; self.min_samples_split or \\\n           len(np.unique(y)) == 1:\n            return DecisionTreeNode(value=self._leaf_value(y))\n\n        # Random feature selection\n        feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n\n        # Find best split\n        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n\n        if best_feature is None:\n            return DecisionTreeNode(value=self._leaf_value(y))\n\n        # Create child nodes\n        left_indices = X[:, best_feature] &lt;= best_threshold\n        right_indices = ~left_indices\n\n        # Check minimum samples per leaf\n        if np.sum(left_indices) &lt; self.min_samples_leaf or \\\n           np.sum(right_indices) &lt; self.min_samples_leaf:\n            return DecisionTreeNode(value=self._leaf_value(y))\n\n        left_child = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n        right_child = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n\n        return DecisionTreeNode(best_feature, best_threshold, left_child, right_child)\n\n    def _best_split(self, X, y, feature_indices):\n        best_gini = float('inf')\n        best_feature, best_threshold = None, None\n\n        for feature_idx in feature_indices:\n            thresholds = np.unique(X[:, feature_idx])\n\n            for threshold in thresholds:\n                left_indices = X[:, feature_idx] &lt;= threshold\n                right_indices = ~left_indices\n\n                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n                    continue\n\n                # Calculate weighted impurity\n                n_left, n_right = len(y[left_indices]), len(y[right_indices])\n                n_total = n_left + n_right\n\n                if self.task == 'classification':\n                    gini_left = self._gini_impurity(y[left_indices])\n                    gini_right = self._gini_impurity(y[right_indices])\n                    weighted_gini = (n_left/n_total) * gini_left + (n_right/n_total) * gini_right\n                else:  # regression\n                    mse_left = self._mse(y[left_indices])\n                    mse_right = self._mse(y[right_indices])\n                    weighted_gini = (n_left/n_total) * mse_left + (n_right/n_total) * mse_right\n\n                if weighted_gini &lt; best_gini:\n                    best_gini = weighted_gini\n                    best_feature = feature_idx\n                    best_threshold = threshold\n\n        return best_feature, best_threshold\n\n    def _gini_impurity(self, y):\n        if len(y) == 0:\n            return 0\n        proportions = np.bincount(y) / len(y)\n        return 1 - np.sum(proportions ** 2)\n\n    def _mse(self, y):\n        if len(y) == 0:\n            return 0\n        return np.mean((y - np.mean(y)) ** 2)\n\n    def _leaf_value(self, y):\n        if self.task == 'classification':\n            return stats.mode(y)[0][0]\n        else:\n            return np.mean(y)\n\n    def predict(self, X):\n        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n\n    def _predict_sample(self, x, node):\n        if node.value is not None:\n            return node.value\n\n        if x[node.feature] &lt;= node.threshold:\n            return self._predict_sample(x, node.left)\n        else:\n            return self._predict_sample(x, node.right)\n\nclass RandomForestFromScratch:\n    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n                 min_samples_leaf=1, max_features='sqrt', bootstrap=True,\n                 task='classification', random_state=None):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        self.bootstrap = bootstrap\n        self.task = task\n        self.random_state = random_state\n        self.trees = []\n        self.feature_importances_ = None\n\n    def fit(self, X, y):\n        if self.random_state:\n            np.random.seed(self.random_state)\n\n        self.n_samples, self.n_features = X.shape\n        self.trees = []\n\n        # Feature importance tracking\n        feature_importance_sum = np.zeros(self.n_features)\n\n        for i in range(self.n_estimators):\n            # Bootstrap sampling\n            if self.bootstrap:\n                indices = np.random.choice(self.n_samples, self.n_samples, replace=True)\n                X_bootstrap = X[indices]\n                y_bootstrap = y[indices]\n            else:\n                X_bootstrap, y_bootstrap = X, y\n\n            # Train tree\n            tree = DecisionTreeFromScratch(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf,\n                max_features=self.max_features,\n                task=self.task\n            )\n            tree.fit(X_bootstrap, y_bootstrap)\n            self.trees.append(tree)\n\n        # Calculate feature importances (simplified version)\n        self._calculate_feature_importance(X, y)\n\n    def _calculate_feature_importance(self, X, y):\n        \"\"\"Simplified feature importance using permutation importance\"\"\"\n        self.feature_importances_ = np.zeros(self.n_features)\n        baseline_score = self._score(X, y)\n\n        for feature_idx in range(self.n_features):\n            # Permute feature\n            X_permuted = X.copy()\n            np.random.shuffle(X_permuted[:, feature_idx])\n\n            # Calculate score with permuted feature\n            permuted_score = self._score(X_permuted, y)\n\n            # Feature importance = decrease in performance\n            self.feature_importances_[feature_idx] = baseline_score - permuted_score\n\n        # Normalize\n        if np.sum(self.feature_importances_) &gt; 0:\n            self.feature_importances_ /= np.sum(self.feature_importances_)\n\n    def _score(self, X, y):\n        predictions = self.predict(X)\n        if self.task == 'classification':\n            return np.mean(predictions == y)\n        else:\n            return -np.mean((predictions - y) ** 2)  # Negative MSE\n\n    def predict(self, X):\n        # Collect predictions from all trees\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n\n        if self.task == 'classification':\n            # Majority vote\n            return np.array([stats.mode(tree_predictions[:, i])[0][0] \n                           for i in range(X.shape[0])])\n        else:\n            # Average for regression\n            return np.mean(tree_predictions, axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"For classification tasks, return class probabilities\"\"\"\n        if self.task != 'classification':\n            raise ValueError(\"predict_proba only available for classification\")\n\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n        n_samples = X.shape[0]\n        n_classes = len(np.unique(tree_predictions))\n\n        probabilities = np.zeros((n_samples, n_classes))\n\n        for i in range(n_samples):\n            class_counts = Counter(tree_predictions[:, i])\n            for class_val, count in class_counts.items():\n                probabilities[i, int(class_val)] = count / self.n_estimators\n\n        return probabilities\n\n# Demonstration\nnp.random.seed(42)\n\n# Create synthetic dataset\nX_demo, y_demo = make_classification(\n    n_samples=500, n_features=10, n_informative=8, \n    n_redundant=2, n_clusters_per_class=1, random_state=42\n)\n\n# Split data\nX_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n    X_demo, y_demo, test_size=0.3, random_state=42, stratify=y_demo\n)\n\n# Train custom Random Forest\nrf_custom = RandomForestFromScratch(\n    n_estimators=50,\n    max_depth=10,\n    max_features='sqrt',\n    task='classification',\n    random_state=42\n)\n\nrf_custom.fit(X_train_demo, y_train_demo)\n\n# Predictions\ny_pred_custom = rf_custom.predict(X_test_demo)\ny_proba_custom = rf_custom.predict_proba(X_test_demo)\n\n# Compare with sklearn\nrf_sklearn = RandomForestClassifier(\n    n_estimators=50, max_depth=10, max_features='sqrt', random_state=42\n)\nrf_sklearn.fit(X_train_demo, y_train_demo)\ny_pred_sklearn = rf_sklearn.predict(X_test_demo)\n\n# Evaluate\naccuracy_custom = np.mean(y_pred_custom == y_test_demo)\naccuracy_sklearn = np.mean(y_pred_sklearn == y_test_demo)\n\nprint(f\"From Scratch Random Forest Results:\")\nprint(f\"Custom RF Accuracy: {accuracy_custom:.3f}\")\nprint(f\"Sklearn RF Accuracy: {accuracy_sklearn:.3f}\")\nprint(f\"Difference: {abs(accuracy_custom - accuracy_sklearn):.3f}\")\n\n# Feature importance comparison\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.bar(range(len(rf_custom.feature_importances_)), rf_custom.feature_importances_)\nplt.title('Custom Random Forest - Feature Importance')\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\n\nplt.subplot(1, 2, 2)\nplt.bar(range(len(rf_sklearn.feature_importances_)), rf_sklearn.feature_importances_)\nplt.title('Sklearn Random Forest - Feature Importance')\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#assumptions-and-limitations","title":"\ufffd\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Random%20Forest/#key-assumptions","title":"Key Assumptions","text":"<ol> <li>Independence of errors: Assumes that prediction errors are independent</li> <li>Representative training data: Training data should represent the population</li> <li>Stable relationships: Feature-target relationships remain consistent over time</li> <li>No data leakage: Future information is not used to predict past events</li> <li>Appropriate tree depth: Trees should be complex enough to capture patterns but not overfit</li> </ol>"},{"location":"Machine-Learning/Random%20Forest/#limitations","title":"Limitations","text":"<ol> <li>Can overfit with very deep trees</li> <li> <p>Solution: Limit max_depth, increase min_samples_split/leaf</p> </li> <li> <p>Biased toward features with more levels</p> </li> <li> <p>Solution: Use balanced datasets, consider feature scaling</p> </li> <li> <p>Difficulty with linear relationships</p> </li> <li> <p>Alternative: Consider linear models or feature engineering</p> </li> <li> <p>Less interpretable than single decision tree</p> </li> <li> <p>Solution: Use feature importance plots, partial dependence plots</p> </li> <li> <p>Memory intensive for large forests</p> </li> <li> <p>Solution: Use smaller n_estimators, consider online learning alternatives</p> </li> <li> <p>Slower prediction than single tree</p> </li> <li>Assessment: Trade-off between accuracy and prediction speed</li> </ol>"},{"location":"Machine-Learning/Random%20Forest/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Algorithm Interpretability Overfitting Risk Training Time Prediction Speed Performance Decision Tree High High Fast Very Fast Moderate Random Forest Medium Low Medium Medium High Gradient Boosting Low Medium Slow Fast Very High SVM Low Medium Slow Fast High Logistic Regression High Low Fast Very Fast Moderate Neural Networks Very Low High Very Slow Medium Very High <p>When to use Random Forest: - \u0005 Mixed data types (numerical and categorical) - \u0005 Non-linear relationships - \u0005 Need feature importance insights - \u0005 Robust performance without much tuning - \u0005 Medium-sized datasets</p> <p>When to avoid Random Forest: - L Very large datasets (consider XGBoost, LightGBM) - L Real-time prediction requirements - L Linear relationships dominate - L High interpretability requirements - L Memory constraints</p>"},{"location":"Machine-Learning/Random%20Forest/#interview-questions","title":"\u2753 Interview Questions","text":"How does Random Forest reduce overfitting compared to a single decision tree? <p>Answer: Random Forest reduces overfitting through several mechanisms:</p> <ol> <li>Bootstrap Aggregating (Bagging): Each tree sees different training samples, reducing variance</li> <li>Random Feature Selection: Each split considers random features, decorrelating trees</li> <li>Ensemble Averaging: Averaging multiple models reduces overall variance</li> <li>Bias-Variance Tradeoff: Trades slight bias increase for large variance reduction</li> </ol> <p>Mathematical intuition: If individual trees have variance \u00f2, ensemble variance is \u00f2/B for independent trees, or \ufffd\u00f2 + (1-\ufffd)\u00f2/B for correlated trees. Random features reduce correlation \ufffd.</p> <p>Practical impact: Single tree might achieve 85% accuracy with high variance, while Random Forest with 100 trees achieves 92% accuracy with much lower variance.</p> What is the difference between Random Forest and Gradient Boosting? <p>Answer: Key differences:</p> Aspect Random Forest Gradient Boosting Training Parallel (independent trees) Sequential (corrects previous errors) Tree depth Usually deep trees Usually shallow trees Overfitting Resistant Prone to overfitting Speed Faster training/prediction Slower Hyperparameters Fewer to tune More complex tuning Performance Good out-of-box Often higher with tuning <p>Use cases: - Random Forest: When you want robust performance without much tuning - Gradient Boosting: When you can invest time in hyperparameter tuning for maximum performance</p> How do you handle categorical features in Random Forest? <p>Answer: Several approaches for categorical features:</p> <p>1. Label Encoding (simple but problematic): <pre><code># Creates artificial ordering\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX_encoded = le.fit_transform(categorical_column)\n</code></pre></p> <p>2. One-Hot Encoding (recommended): <pre><code>from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(drop='first')  # Avoid multicollinearity\nX_encoded = ohe.fit_transform(categorical_column)\n</code></pre></p> <p>3. Target Encoding (for high cardinality): <pre><code># Replace category with mean target value\nmean_values = df.groupby('category')['target'].mean()\ndf['category_encoded'] = df['category'].map(mean_values)\n</code></pre></p> <p>4. Native handling (some implementations): - R's randomForest package handles categorical features natively - Use Gini impurity for categorical splits</p> <p>Best practice: Use one-hot encoding for low cardinality (&lt;10 categories), target encoding for high cardinality.</p> Explain the concept of Out-of-Bag (OOB) error and its advantages. <p>Answer: Out-of-Bag error provides unbiased performance estimation without separate validation set:</p> <p>Process: 1. Bootstrap sampling leaves ~37% of data \"out-of-bag\" for each tree 2. For each sample, use trees that didn't see it during training 3. Make prediction using only those trees 4. Calculate error across all OOB predictions</p> <p>Advantages: - No data splitting needed: Use full dataset for training - Unbiased estimate: Similar to k-fold cross-validation - Computational efficiency: No separate validation runs - Early stopping: Monitor OOB error during training</p> <p>Code example: <pre><code>rf = RandomForestClassifier(oob_score=True)\nrf.fit(X, y)\nprint(f\"OOB Score: {rf.oob_score_}\")  # Unbiased performance estimate\n</code></pre></p> <p>Limitation: OOB error might be pessimistic for small datasets.</p> How do you interpret feature importance in Random Forest and what are its limitations? <p>Answer: Random Forest provides two types of feature importance:</p> <p>1. Gini Importance (Mean Decrease Impurity): - Measures average impurity decrease when splitting on each feature - Fast to compute but can be biased toward high-cardinality features</p> <p>2. Permutation Importance (Mean Decrease Accuracy): - Measures accuracy drop when feature values are randomly permuted - More reliable but computationally expensive</p> <p>Interpretation example: <pre><code># Gini importance\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Permutation importance\nfrom sklearn.inspection import permutation_importance\nperm_importance = permutation_importance(rf, X, y, n_repeats=10)\n</code></pre></p> <p>Limitations: - Biased toward numerical and high-cardinality categorical features - Doesn't capture feature interactions - Can be misleading with correlated features - Different from causal importance</p> <p>Best practices: Use both types, validate with domain knowledge, consider SHAP values for individual predictions.</p> How do hyperparameters affect Random Forest performance and how do you tune them? <p>Answer: Key hyperparameters and their effects:</p> <p>Tree-level parameters: - n_estimators: More trees \ufffd better performance, diminishing returns after ~100-500 - max_depth: Controls overfitting vs underfitting - min_samples_split/leaf: Higher values prevent overfitting - max_features: 'sqrt' (classification), 'log2', or fraction of features</p> <p>Bootstrap parameters: - bootstrap: True for bagging, False for pasting - oob_score: Enable for validation without holdout set</p> <p>Tuning strategies:</p> <p>1. Grid Search (systematic but slow): <pre><code>param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5, 10]\n}\ngrid_search = GridSearchCV(rf, param_grid, cv=5)\n</code></pre></p> <p>2. Random Search (more efficient): <pre><code>from sklearn.model_selection import RandomizedSearchCV\nparam_dist = {\n    'n_estimators': [50, 100, 200, 500],\n    'max_depth': [3, 5, 10, 20, None]\n}\nrandom_search = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=5)\n</code></pre></p> <p>3. Progressive tuning: Start with n_estimators, then tree parameters, then bootstrap parameters.</p> What is the computational complexity of Random Forest training and prediction? <p>Answer: Computational complexity analysis:</p> <p>Training complexity: - Single tree: O(n \ufffd m \ufffd log n) where n=samples, m=features - Random Forest: O(B \ufffd n \ufffd m \ufffd log n) where B=number of trees - With random features: O(B \ufffd n \ufffd \u001am \ufffd log n) for classification - Parallelizable: Trees can be trained independently</p> <p>Prediction complexity: - Single tree: O(log n) for balanced tree - Random Forest: O(B \ufffd log n) - Space complexity: O(B \ufffd tree_size)</p> <p>Memory usage: <pre><code># Approximate memory for Random Forest\nmemory_mb = (n_estimators * max_depth * n_features * 8) / (1024**2)\n</code></pre></p> <p>Optimization strategies: - Parallel training: Use n_jobs=-1 - Feature subsampling: Reduces computation per split - Early stopping: Monitor OOB score - Tree pruning: Remove unnecessary nodes post-training</p> <p>Scalability considerations: - Linear scaling with number of trees (parallelizable) - Memory can be limiting factor for very large forests - Consider ensemble methods like ExtraTrees for speed</p> How does Random Forest handle missing values and what are the best practices? <p>Answer: Random Forest cannot directly handle missing values, requiring preprocessing:</p> <p>Preprocessing strategies:</p> <p>1. Simple imputation: <pre><code>from sklearn.impute import SimpleImputer\n# Mean/median for numerical, mode for categorical\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n</code></pre></p> <p>2. Advanced imputation: <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# Uses other features to predict missing values\nimputer = IterativeImputer(random_state=42)\nX_imputed = imputer.fit_transform(X)\n</code></pre></p> <p>3. Missing indicator: <pre><code>from sklearn.impute import MissingIndicator\n# Add binary features indicating missingness\nindicator = MissingIndicator()\nmissing_mask = indicator.fit_transform(X)\nX_augmented = np.hstack([X_imputed, missing_mask])\n</code></pre></p> <p>4. Multiple imputation: - Create multiple imputed datasets - Train separate models - Combine predictions</p> <p>Native approaches (limited implementations): - Surrogate splits: Use alternative features when primary feature is missing - Missing as category: Treat missing as separate category for categorical features</p> <p>Best practices: - Analyze missingness patterns (MCAR, MAR, MNAR) - Consider domain knowledge for imputation strategy - Validate imputation quality - Report missing value handling in model documentation</p> Compare Random Forest with other ensemble methods like AdaBoost and XGBoost. <p>Answer: Comprehensive comparison of ensemble methods:</p> Aspect Random Forest AdaBoost XGBoost Algorithm type Bagging Boosting Gradient Boosting Tree training Parallel Sequential Sequential Error focus Reduces variance Reduces bias Reduces both Overfitting Resistant Can overfit Regularized Speed Fast Medium Medium-Fast Hyperparameters Few, robust Few Many, needs tuning Performance Good baseline Good for weak learners Often best Interpretability Medium Low Low <p>When to use each:</p> <p>Random Forest: - \u0005 Quick baseline model - \u0005 Mixed data types - \u0005 Interpretability needed - \u0005 Robust performance without tuning</p> <p>AdaBoost: - \u0005 Weak learners available - \u0005 Binary classification - \u0005 Less prone to outliers than other boosting - L Sensitive to noise and outliers</p> <p>XGBoost/LightGBM: - \u0005 Maximum predictive performance - \u0005 Kaggle competitions - \u0005 Large datasets - \u0005 Advanced regularization needed - L Requires careful hyperparameter tuning</p> <p>Performance hierarchy (generally): XGBoost &gt; Random Forest &gt; AdaBoost &gt; Single Tree</p> <p>Complexity hierarchy: XGBoost &gt; AdaBoost &gt; Random Forest &gt; Single Tree</p> How do you handle class imbalance in Random Forest? <p>Answer: Several strategies for handling imbalanced datasets:</p> <p>1. Built-in class balancing: <pre><code>rf = RandomForestClassifier(\n    class_weight='balanced',  # Automatically adjusts weights\n    random_state=42\n)\n# Or custom weights\nrf = RandomForestClassifier(\n    class_weight={0: 1, 1: 10},  # More weight to minority class\n    random_state=42\n)\n</code></pre></p> <p>2. Sampling strategies: <pre><code>from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Oversampling minority class\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Undersampling majority class\nundersampler = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = undersampler.fit_resample(X, y)\n</code></pre></p> <p>3. Stratified sampling: <pre><code># Ensure each bootstrap sample maintains class proportions\nrf = RandomForestClassifier(\n    bootstrap=True,\n    # Manual stratified sampling in custom implementation\n    random_state=42\n)\n</code></pre></p> <p>4. Threshold tuning: <pre><code># Adjust decision threshold based on precision-recall tradeoff\nfrom sklearn.metrics import precision_recall_curve\n\ny_proba = rf.predict_proba(X_test)[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n\n# Find optimal threshold\nf1_scores = 2 * (precision * recall) / (precision + recall)\noptimal_threshold = thresholds[np.argmax(f1_scores)]\n</code></pre></p> <p>5. Evaluation metrics: - Use precision, recall, F1-score instead of accuracy - ROC-AUC and Precision-Recall curves - Stratified cross-validation</p> <p>Best approach: Combine multiple strategies and validate with appropriate metrics.</p>"},{"location":"Machine-Learning/Random%20Forest/#examples","title":"&gt;\ufffd Examples","text":""},{"location":"Machine-Learning/Random%20Forest/#real-world-example-credit-risk-assessment","title":"Real-world Example: Credit Risk Assessment","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Generate synthetic credit risk dataset\nnp.random.seed(42)\nn_samples = 10000\n\n# Create base features\nX_base, y = make_classification(\n    n_samples=n_samples, \n    n_features=15, \n    n_informative=12,\n    n_redundant=3,\n    n_clusters_per_class=1,\n    weights=[0.85, 0.15],  # Imbalanced dataset (15% default rate)\n    flip_y=0.02,  # Small amount of label noise\n    random_state=42\n)\n\n# Create meaningful feature names\nfeature_names = [\n    'income', 'debt_to_income', 'credit_score', 'employment_years',\n    'loan_amount', 'loan_to_value', 'payment_history', 'age',\n    'education_level', 'num_accounts', 'total_credit_limit',\n    'credit_utilization', 'num_inquiries', 'delinquencies', 'bankruptcies'\n]\n\n# Create DataFrame\ndf_credit = pd.DataFrame(X_base, columns=feature_names)\ndf_credit['default'] = y\n\n# Add some realistic transformations\ndf_credit['income'] = np.exp(df_credit['income']) * 1000  # Log-normal income\ndf_credit['credit_score'] = (df_credit['credit_score'] * 100 + 700).clip(300, 850)\ndf_credit['age'] = (df_credit['age'] * 15 + 35).clip(18, 80)\n\nprint(\"Credit Risk Dataset Overview:\")\nprint(f\"Dataset shape: {df_credit.shape}\")\nprint(f\"Default rate: {df_credit['default'].mean():.3f}\")\nprint(f\"\\nFeature statistics:\")\nprint(df_credit.describe())\n\n# Prepare features and target\nX = df_credit[feature_names].values\ny = df_credit['default'].values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"\\nTrain set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Train default rate: {y_train.mean():.3f}\")\nprint(f\"Test default rate: {y_test.mean():.3f}\")\n\n# Train Random Forest with class balancing\nrf_credit = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=15,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    max_features='sqrt',\n    class_weight='balanced',  # Handle class imbalance\n    bootstrap=True,\n    oob_score=True,\n    random_state=42,\n    n_jobs=-1\n)\n\nrf_credit.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf_credit.predict(X_test)\ny_pred_proba = rf_credit.predict_proba(X_test)[:, 1]\n\nprint(f\"\\nModel Performance:\")\nprint(f\"Out-of-bag Score: {rf_credit.oob_score_:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))\n\n# Feature importance analysis\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': rf_credit.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(f\"\\nTop 10 Most Important Features:\")\nprint(feature_importance.head(10))\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Feature importance\nsns.barplot(data=feature_importance.head(10), y='feature', x='importance', ax=axes[0, 0])\naxes[0, 0].set_title('Top 10 Feature Importances')\naxes[0, 0].set_xlabel('Importance Score')\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\naxes[0, 1].set_title('Confusion Matrix')\naxes[0, 1].set_ylabel('True Label')\naxes[0, 1].set_xlabel('Predicted Label')\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\naxes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, \n                label=f'ROC curve (AUC = {roc_auc:.3f})')\naxes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1, 0].set_xlim([0.0, 1.0])\naxes[1, 0].set_ylim([0.0, 1.05])\naxes[1, 0].set_xlabel('False Positive Rate')\naxes[1, 0].set_ylabel('True Positive Rate')\naxes[1, 0].set_title('ROC Curve')\naxes[1, 0].legend(loc=\"lower right\")\n\n# Prediction distribution\naxes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Default', density=True)\naxes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Default', density=True)\naxes[1, 1].set_xlabel('Predicted Probability of Default')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].set_title('Prediction Distribution by True Class')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Business impact analysis\ndef calculate_profit(y_true, y_pred_proba, threshold=0.5, \n                    profit_approved_good=200, loss_approved_bad=-1000,\n                    profit_rejected_good=0, profit_rejected_bad=0):\n    \"\"\"Calculate business profit based on loan decisions\"\"\"\n\n    decisions = (y_pred_proba &gt;= threshold).astype(int)  # 1: Approve, 0: Reject\n\n    # Calculate profit for each scenario\n    approved_good = ((decisions == 1) &amp; (y_true == 0)).sum() * profit_approved_good\n    approved_bad = ((decisions == 1) &amp; (y_true == 1)).sum() * loss_approved_bad\n    rejected_good = ((decisions == 0) &amp; (y_true == 0)).sum() * profit_rejected_good\n    rejected_bad = ((decisions == 0) &amp; (y_true == 1)).sum() * profit_rejected_bad\n\n    total_profit = approved_good + approved_bad + rejected_good + rejected_bad\n\n    return {\n        'total_profit': total_profit,\n        'approved_good': approved_good,\n        'approved_bad': approved_bad,\n        'rejected_good': rejected_good,\n        'rejected_bad': rejected_bad,\n        'approval_rate': decisions.mean(),\n        'threshold': threshold\n    }\n\n# Analyze profit across different thresholds\nthresholds = np.linspace(0.1, 0.9, 17)\nprofits = []\n\nfor threshold in thresholds:\n    profit_info = calculate_profit(y_test, y_pred_proba, threshold)\n    profits.append(profit_info)\n\nprofits_df = pd.DataFrame(profits)\n\n# Find optimal threshold\noptimal_idx = profits_df['total_profit'].idxmax()\noptimal_threshold = profits_df.loc[optimal_idx, 'threshold']\noptimal_profit = profits_df.loc[optimal_idx, 'total_profit']\n\nprint(f\"\\nBusiness Impact Analysis:\")\nprint(f\"Optimal threshold: {optimal_threshold:.2f}\")\nprint(f\"Maximum profit: ${optimal_profit:,.0f}\")\nprint(f\"Approval rate at optimal threshold: {profits_df.loc[optimal_idx, 'approval_rate']:.1%}\")\n\n# Plot profit vs threshold\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(profits_df['threshold'], profits_df['total_profit'], 'bo-', linewidth=2)\nplt.axvline(optimal_threshold, color='red', linestyle='--', \n            label=f'Optimal: {optimal_threshold:.2f}')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Total Profit ($)')\nplt.title('Profit vs Decision Threshold')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(profits_df['threshold'], profits_df['approval_rate'], 'go-', linewidth=2)\nplt.axvline(optimal_threshold, color='red', linestyle='--', \n            label=f'Optimal: {optimal_threshold:.2f}')\nplt.xlabel('Decision Threshold')\nplt.ylabel('Loan Approval Rate')\nplt.title('Approval Rate vs Decision Threshold')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#house-price-prediction-example","title":"House Price Prediction Example","text":"<pre><code>from sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.inspection import plot_partial_dependence\n\n# Generate synthetic house price dataset\nnp.random.seed(42)\nn_samples = 5000\n\nX_house, y_house = make_regression(\n    n_samples=n_samples,\n    n_features=12,\n    n_informative=10,\n    noise=0.1,\n    random_state=42\n)\n\n# Create meaningful feature names\nhouse_features = [\n    'square_feet', 'bedrooms', 'bathrooms', 'age_years',\n    'lot_size', 'garage_spaces', 'school_rating', 'crime_rate',\n    'distance_downtown', 'property_tax', 'neighborhood_income', 'walk_score'\n]\n\n# Transform features to realistic scales\nX_house[:, 0] = (X_house[:, 0] * 800 + 2000).clip(800, 5000)  # Square feet\nX_house[:, 1] = (X_house[:, 1] * 2 + 3).clip(1, 6).round()  # Bedrooms\nX_house[:, 2] = (X_house[:, 2] * 1.5 + 2).clip(1, 4).round()  # Bathrooms\nX_house[:, 3] = (X_house[:, 3] * 20 + 25).clip(0, 100)  # Age\nX_house[:, 4] = (X_house[:, 4] * 0.3 + 0.25).clip(0.1, 2.0)  # Lot size (acres)\n\n# Transform target to realistic house prices ($)\ny_house = (y_house * 100000 + 400000).clip(150000, 1500000)\n\n# Create DataFrame\ndf_houses = pd.DataFrame(X_house, columns=house_features)\ndf_houses['price'] = y_house\n\nprint(\"House Price Dataset Overview:\")\nprint(f\"Dataset shape: {df_houses.shape}\")\nprint(f\"Price statistics:\")\nprint(df_houses['price'].describe())\n\n# Split data\nX_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n    X_house, y_house, test_size=0.2, random_state=42\n)\n\n# Train Random Forest Regressor\nrf_houses = RandomForestRegressor(\n    n_estimators=200,\n    max_depth=20,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    max_features='sqrt',\n    bootstrap=True,\n    oob_score=True,\n    random_state=42,\n    n_jobs=-1\n)\n\nrf_houses.fit(X_train_h, y_train_h)\n\n# Predictions\ny_pred_h = rf_houses.predict(X_test_h)\n\n# Evaluate\nmse = mean_squared_error(y_test_h, y_pred_h)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test_h, y_pred_h)\nr2 = r2_score(y_test_h, y_pred_h)\n\nprint(f\"\\nHouse Price Prediction Results:\")\nprint(f\"R\ufffd Score: {r2:.3f}\")\nprint(f\"RMSE: ${rmse:,.0f}\")\nprint(f\"MAE: ${mae:,.0f}\")\nprint(f\"MAPE: {np.mean(np.abs((y_test_h - y_pred_h) / y_test_h)) * 100:.1f}%\")\nprint(f\"Out-of-bag Score: {rf_houses.oob_score_:.3f}\")\n\n# Feature importance for house prices\nhouse_importance = pd.DataFrame({\n    'feature': house_features,\n    'importance': rf_houses.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(f\"\\nFeature Importance for House Prices:\")\nprint(house_importance)\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Feature importance\nsns.barplot(data=house_importance, y='feature', x='importance', ax=axes[0, 0])\naxes[0, 0].set_title('Feature Importance - House Prices')\naxes[0, 0].set_xlabel('Importance Score')\n\n# Actual vs Predicted\naxes[0, 1].scatter(y_test_h, y_pred_h, alpha=0.6)\naxes[0, 1].plot([y_test_h.min(), y_test_h.max()], \n                [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\naxes[0, 1].set_xlabel('Actual Price ($)')\naxes[0, 1].set_ylabel('Predicted Price ($)')\naxes[0, 1].set_title(f'Actual vs Predicted (R\ufffd = {r2:.3f})')\n\n# Residuals\nresiduals = y_test_h - y_pred_h\naxes[1, 0].scatter(y_pred_h, residuals, alpha=0.6)\naxes[1, 0].axhline(y=0, color='r', linestyle='--')\naxes[1, 0].set_xlabel('Predicted Price ($)')\naxes[1, 0].set_ylabel('Residuals ($)')\naxes[1, 0].set_title('Residual Plot')\n\n# Residual distribution\naxes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\naxes[1, 1].set_xlabel('Residuals ($)')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Residual Distribution')\naxes[1, 1].axvline(x=0, color='red', linestyle='--')\n\nplt.tight_layout()\nplt.show()\n\n# Partial dependence plots\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\ntop_features = house_importance.head(6)['feature'].tolist()\nfeature_indices = [house_features.index(f) for f in top_features]\n\nfor i, (feature_idx, feature_name) in enumerate(zip(feature_indices, top_features)):\n    plot_partial_dependence(\n        rf_houses, X_train_h, [feature_idx], \n        ax=axes[i], feature_names=[feature_name]\n    )\n    axes[i].set_title(f'Partial Dependence: {feature_name}')\n\nplt.tight_layout()\nplt.show()\n\n# Model interpretation: Feature interactions\nprint(f\"\\nModel Interpretation:\")\nprint(f\"The model shows that {house_importance.iloc[0]['feature']} is the most important factor,\")\nprint(f\"explaining {house_importance.iloc[0]['importance']:.1%} of the house price variation.\")\n\n# Prediction confidence intervals (approximate)\ndef prediction_intervals(rf, X, confidence=0.95):\n    \"\"\"Calculate prediction intervals using tree-level predictions\"\"\"\n    # Get predictions from all trees\n    tree_predictions = np.array([tree.predict(X) for tree in rf.estimators_])\n\n    # Calculate percentiles\n    alpha = 1 - confidence\n    lower_percentile = (alpha / 2) * 100\n    upper_percentile = (1 - alpha / 2) * 100\n\n    lower_bound = np.percentile(tree_predictions, lower_percentile, axis=0)\n    upper_bound = np.percentile(tree_predictions, upper_percentile, axis=0)\n\n    return lower_bound, upper_bound\n\n# Calculate 95% prediction intervals for test set\nlower_bounds, upper_bounds = prediction_intervals(rf_houses, X_test_h)\n\n# Coverage analysis\ncoverage = ((y_test_h &gt;= lower_bounds) &amp; (y_test_h &lt;= upper_bounds)).mean()\navg_interval_width = np.mean(upper_bounds - lower_bounds)\n\nprint(f\"\\nPrediction Intervals (95% confidence):\")\nprint(f\"Coverage: {coverage:.1%}\")\nprint(f\"Average interval width: ${avg_interval_width:,.0f}\")\n\n# Show some examples\nexamples = pd.DataFrame({\n    'Actual': y_test_h[:10],\n    'Predicted': y_pred_h[:10],\n    'Lower_95%': lower_bounds[:10],\n    'Upper_95%': upper_bounds[:10]\n})\nexamples['Within_Interval'] = (\n    (examples['Actual'] &gt;= examples['Lower_95%']) &amp; \n    (examples['Actual'] &lt;= examples['Upper_95%'])\n)\n\nprint(f\"\\nSample Predictions with Intervals:\")\nprint(examples.round(0))\n</code></pre>"},{"location":"Machine-Learning/Random%20Forest/#references","title":"\ud83d\udcda References","text":"<ul> <li>Original Paper:</li> <li>Random Forests by Leo Breiman (2001)</li> <li> <p>Bagging Predictors by Leo Breiman (1996)</p> </li> <li> <p>Books:</p> </li> <li>The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman - Chapter 15</li> <li>Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani - Chapter 8</li> <li> <p>Hands-On Machine Learning by Aur\ufffdlien G\ufffdron - Chapter 7</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn Random Forest</li> <li>Scikit-learn RandomForestClassifier</li> <li> <p>Scikit-learn RandomForestRegressor</p> </li> <li> <p>Tutorials and Guides:</p> </li> <li>Random Forest Algorithm Guide</li> <li>Random Forest Feature Importance</li> <li> <p>Hyperparameter Tuning for Random Forest</p> </li> <li> <p>Advanced Topics:</p> </li> <li>Extremely Randomized Trees</li> <li>Isolation Forest</li> <li> <p>Random Forest Feature Selection</p> </li> <li> <p>Research Papers:</p> </li> <li>Geurts, P., Ernst, D., &amp; Wehenkel, L. (2006). Extremely randomized trees</li> <li>Strobl, C., Boulesteix, A. L., Kneib, T., Augustin, T., &amp; Zeileis, A. (2008). Conditional variable importance for random forests</li> <li> <p>Louppe, G. (2014). Understanding Random Forests: From Theory to Practice</p> </li> <li> <p>Online Courses:</p> </li> <li>Machine Learning Course - Stanford CS229</li> <li>Random Forest in Machine Learning - Coursera</li> <li> <p>Applied Machine Learning - edX</p> </li> <li> <p>Implementations:</p> </li> <li>scikit-learn</li> <li>R randomForest package</li> <li>XGBoost (gradient boosting alternative)</li> </ul>"},{"location":"Machine-Learning/Support%20Vector%20Machines/","title":"\u2694\ufe0f Support Vector Machines (SVM)","text":"<p>Support Vector Machines are powerful supervised learning algorithms that find the optimal decision boundary by maximizing the margin between classes, capable of handling both linear and non-linear classification and regression problems through kernel methods.</p> <p>Resources: Scikit-learn SVM | Support Vector Networks Paper | Elements of Statistical Learning - Chapter 12</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#_1","title":"Support Vector Machines (SVM)","text":"<p>\u000f Summary</p> <p>Support Vector Machine (SVM) is a discriminative classifier that finds the optimal hyperplane to separate different classes by maximizing the margin (distance) between the closest points of each class. The algorithm focuses on the most informative data points (support vectors) rather than using all training data, making it efficient and robust.</p> <p>Key characteristics: - Maximum margin classifier: Finds the hyperplane with largest margin - Support vector focus: Only depends on support vectors, not all training data - Kernel trick: Can handle non-linear decision boundaries using kernel functions - Regularization: Built-in regularization through the C parameter - Versatile: Works for classification, regression, and outlier detection - Memory efficient: Stores only support vectors, not entire dataset</p> <p>Applications: - Text classification and sentiment analysis - Image classification and computer vision - Bioinformatics and gene classification - Handwriting recognition - Face detection and recognition - Document classification - Spam email filtering - Medical diagnosis - Financial market analysis</p> <p>Types: - Linear SVM: For linearly separable data - Soft Margin SVM: Handles non-separable data with slack variables - Kernel SVM: Non-linear classification using kernel methods - SVR (Support Vector Regression): For regression tasks - One-Class SVM: For anomaly detection and novelty detection</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#intuition","title":"&gt;\ufffd Intuition","text":""},{"location":"Machine-Learning/Support%20Vector%20Machines/#how-svm-works","title":"How SVM Works","text":"<p>Imagine you're trying to separate two groups of people in a room. Instead of just drawing any line between them, SVM finds the \"widest corridor\" that separates the groups. The people standing closest to this corridor (support vectors) determine where the boundary should be. Everyone else could leave the room, and the boundary would stay the same.</p> <p>For non-linearly separable data, SVM uses the \"kernel trick\" - it projects the data into a higher-dimensional space where a linear separator can be found, then maps the decision boundary back to the original space.</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Support%20Vector%20Machines/#1-linear-svm-hard-margin","title":"1. Linear SVM - Hard Margin","text":"<p>For a binary classification problem with training data \\(\\{(x_i, y_i)\\}_{i=1}^n\\) where \\(y_i \\in \\{-1, +1\\}\\):</p> <p>Decision boundary: \\(w^T x + b = 0\\)</p> <p>Classification rule: \\(f(x) = \\text{sign}(w^T x + b)\\)</p> <p>Margin: The distance from the hyperplane to the nearest data point is \\(\\frac{1}{||w||}\\)</p> <p>Optimization problem (Hard Margin): \\(\\(\\min_{w,b} \\frac{1}{2}||w||^2\\)\\)</p> <p>Subject to: \\(y_i(w^T x_i + b) \\geq 1, \\quad \\forall i = 1,...,n\\)</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#2-soft-margin-svm","title":"2. Soft Margin SVM","text":"<p>For non-separable data, introduce slack variables \\(\\xi_i \\geq 0\\):</p> <p>Optimization problem (Soft Margin): \\(\\(\\min_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^n \\xi_i\\)\\)</p> <p>Subject to:  - \\(y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i\\) - \\(\\xi_i \\geq 0, \\quad \\forall i\\)</p> <p>Where \\(C\\) is the regularization parameter controlling the trade-off between margin maximization and training error minimization.</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#3-dual-formulation-lagrangian","title":"3. Dual Formulation (Lagrangian)","text":"<p>The primal problem is converted to dual form using Lagrange multipliers \\(\\alpha_i\\):</p> <p>Dual optimization problem: \\(\\(\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\)\\)</p> <p>Subject to: - \\(0 \\leq \\alpha_i \\leq C, \\quad \\forall i\\) - \\(\\sum_{i=1}^n \\alpha_i y_i = 0\\)</p> <p>Decision function: \\(\\(f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i x_i^T x + b\\right)\\)\\)</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#4-kernel-trick","title":"4. Kernel Trick","text":"<p>Replace the dot product \\(x_i^T x_j\\) with a kernel function \\(K(x_i, x_j)\\):</p> <p>Decision function with kernels: \\(\\(f(x) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\\right)\\)\\)</p> <p>Common kernel functions:</p> <p>Linear: \\(K(x, z) = x^T z\\)</p> <p>Polynomial: \\(K(x, z) = (x^T z + c)^d\\)</p> <p>RBF (Radial Basis Function): \\(K(x, z) = \\exp\\left(-\\gamma ||x - z||^2\\right)\\)</p> <p>Sigmoid: \\(K(x, z) = \\tanh(\\gamma x^T z + c)\\)</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#5-support-vector-regression-svr","title":"5. Support Vector Regression (SVR)","text":"<p>For regression, use \\(\\varepsilon\\)-insensitive loss:</p> <p>Optimization problem: \\(\\(\\min_{w,b,\\xi,\\xi^*} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^n (\\xi_i + \\xi_i^*)\\)\\)</p> <p>Subject to: - \\(y_i - w^T x_i - b \\leq \\varepsilon + \\xi_i\\) - \\(w^T x_i + b - y_i \\leq \\varepsilon + \\xi_i^*\\) - \\(\\xi_i, \\xi_i^* \\geq 0\\)</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/Support%20Vector%20Machines/#scikit-learn-implementation","title":"Scikit-learn Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, make_regression\n\n# Classification Example with Iris Dataset\niris = datasets.load_iris()\nX_iris = iris.data[:, :2]  # Use only first 2 features for visualization\ny_iris = iris.target\n\n# Binary classification (setosa vs non-setosa)\ny_binary = (y_iris != 0).astype(int)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_iris, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n)\n\n# Standardize features (important for SVM)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"SVM Classification Example:\")\nprint(f\"Training data shape: {X_train_scaled.shape}\")\nprint(f\"Test data shape: {X_test_scaled.shape}\")\n\n# Train different SVM models\nsvm_models = {\n    'Linear SVM': SVC(kernel='linear', C=1.0, random_state=42),\n    'RBF SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n    'Polynomial SVM': SVC(kernel='poly', degree=3, C=1.0, random_state=42),\n    'Sigmoid SVM': SVC(kernel='sigmoid', C=1.0, random_state=42)\n}\n\nresults = {}\nfor name, model in svm_models.items():\n    # Train model\n    model.fit(X_train_scaled, y_train)\n\n    # Predictions\n    y_pred = model.predict(X_test_scaled)\n\n    # Evaluate\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = {\n        'model': model,\n        'accuracy': accuracy,\n        'n_support': len(model.support_),\n        'support_vectors': model.support_vectors_\n    }\n\n    print(f\"\\n{name} Results:\")\n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Number of support vectors: {len(model.support_)}\")\n\n# Detailed analysis of best model (RBF SVM)\nbest_model = results['RBF SVM']['model']\ny_pred_best = best_model.predict(X_test_scaled)\n\nprint(f\"\\nDetailed Results for RBF SVM:\")\nprint(f\"Classification Report:\")\nprint(classification_report(y_test, y_pred_best))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred_best)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix - RBF SVM')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#decision-boundary-visualization","title":"Decision Boundary Visualization","text":"<pre><code>def plot_svm_decision_boundary(X, y, model, title, scaler=None):\n    \"\"\"Plot SVM decision boundary with support vectors\"\"\"\n    if scaler:\n        X = scaler.transform(X)\n\n    plt.figure(figsize=(10, 8))\n\n    # Create mesh for decision boundary\n    h = 0.01  # Step size in mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\n    # Predict on mesh\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(mesh_points)\n    Z = Z.reshape(xx.shape)\n\n    # Plot decision boundary\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n    plt.contour(xx, yy, Z, colors='black', linewidths=0.5)\n\n    # Plot data points\n    colors = ['red', 'blue']\n    for i, color in enumerate(colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, \n                   marker='o', label=f'Class {i}', alpha=0.7)\n\n    # Highlight support vectors\n    if hasattr(model, 'support_vectors_'):\n        plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2,\n                   label='Support Vectors')\n\n    plt.xlabel('Feature 1 (standardized)')\n    plt.ylabel('Feature 2 (standardized)')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Plot decision boundaries for different kernels\nfor name, result in results.items():\n    plot_svm_decision_boundary(X_train, y_train, result['model'], \n                              f'{name} - Decision Boundary', scaler)\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Comprehensive hyperparameter tuning for RBF SVM\nparam_grid = {\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n    'kernel': ['rbf']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    SVC(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(f\"\\nHyperparameter Tuning Results:\")\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n\n# Evaluate best model\nbest_svm = grid_search.best_estimator_\ny_pred_tuned = best_svm.predict(X_test_scaled)\ntuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n\nprint(f\"Test accuracy with best parameters: {tuned_accuracy:.3f}\")\nprint(f\"Number of support vectors: {len(best_svm.support_)}\")\n\n# Visualize hyperparameter effects\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Heatmap of C vs gamma performance\npivot_table = results_df.pivot_table(\n    values='mean_test_score',\n    index='param_gamma', \n    columns='param_C'\n)\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis')\nplt.title('SVM Performance: C vs Gamma')\nplt.xlabel('C (Regularization)')\nplt.ylabel('Gamma (Kernel coefficient)')\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#support-vector-regression-svr","title":"Support Vector Regression (SVR)","text":"<pre><code># Generate regression dataset\nX_reg, y_reg = make_regression(\n    n_samples=500, n_features=1, noise=0.1, \n    random_state=42\n)\n\n# Split data\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42\n)\n\n# Standardize\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n\n# Train SVR models\nsvr_models = {\n    'Linear SVR': SVR(kernel='linear', C=100, epsilon=0.1),\n    'RBF SVR': SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1),\n    'Polynomial SVR': SVR(kernel='poly', degree=3, C=100, epsilon=0.1)\n}\n\nprint(f\"\\nSupport Vector Regression Results:\")\n\nplt.figure(figsize=(15, 5))\n\nfor i, (name, model) in enumerate(svr_models.items()):\n    # Train model\n    model.fit(X_train_reg_scaled, y_train_reg)\n\n    # Predictions\n    y_pred_reg = model.predict(X_test_reg_scaled)\n\n    # Evaluate\n    mse = mean_squared_error(y_test_reg, y_pred_reg)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test_reg, y_pred_reg)\n\n    print(f\"{name}: RMSE = {rmse:.3f}, R\ufffd = {r2:.3f}, Support Vectors = {len(model.support_)}\")\n\n    # Plot results\n    plt.subplot(1, 3, i+1)\n\n    # Sort data for smooth line plotting\n    X_plot = X_test_reg_scaled\n    sort_idx = np.argsort(X_plot[:, 0])\n\n    plt.scatter(X_test_reg_scaled, y_test_reg, alpha=0.6, label='Actual', color='blue')\n    plt.scatter(X_test_reg_scaled, y_pred_reg, alpha=0.6, label='Predicted', color='red')\n    plt.plot(X_plot[sort_idx], y_pred_reg[sort_idx], color='red', linewidth=2)\n\n    # Highlight support vectors\n    if len(model.support_) &gt; 0:\n        support_X = X_train_reg_scaled[model.support_]\n        support_y = y_train_reg[model.support_]\n        plt.scatter(support_X, support_y, s=100, facecolors='none', \n                   edgecolors='black', linewidth=2, label='Support Vectors')\n\n    plt.xlabel('Feature (standardized)')\n    plt.ylabel('Target')\n    plt.title(f'{name}\\nR\ufffd = {r2:.3f}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#multi-class-classification","title":"Multi-class Classification","text":"<pre><code># Multi-class classification with full iris dataset\nX_multi = iris.data\ny_multi = iris.target\n\nX_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n    X_multi, y_multi, test_size=0.3, random_state=42, stratify=y_multi\n)\n\n# Standardize\nscaler_multi = StandardScaler()\nX_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)\nX_test_multi_scaled = scaler_multi.transform(X_test_multi)\n\n# Train multi-class SVM\nsvm_multi = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\nsvm_multi.fit(X_train_multi_scaled, y_train_multi)\n\n# Predictions\ny_pred_multi = svm_multi.predict(X_test_multi_scaled)\n\n# Evaluate\naccuracy_multi = accuracy_score(y_test_multi, y_pred_multi)\n\nprint(f\"\\nMulti-class Classification Results:\")\nprint(f\"Accuracy: {accuracy_multi:.3f}\")\nprint(f\"Total support vectors: {len(svm_multi.support_)}\")\nprint(f\"Support vectors per class: {svm_multi.n_support_}\")\n\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test_multi, y_pred_multi, \n                          target_names=iris.target_names))\n\n# Confusion matrix\nplt.figure(figsize=(8, 6))\ncm_multi = confusion_matrix(y_test_multi, y_pred_multi)\nsns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues',\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\nplt.title('Multi-class SVM - Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#from-scratch-implementation","title":"\ufffd\u000f From Scratch Implementation","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\n\nclass SVMFromScratch:\n    def __init__(self, C=1.0, kernel='linear', gamma='scale', degree=3, max_iter=1000):\n        \"\"\"\n        Support Vector Machine implementation from scratch\n\n        Parameters:\n        C: Regularization parameter\n        kernel: Kernel function ('linear', 'rbf', 'poly')\n        gamma: Kernel coefficient for RBF and polynomial kernels\n        degree: Degree for polynomial kernel\n        max_iter: Maximum number of iterations\n        \"\"\"\n        self.C = C\n        self.kernel = kernel\n        self.gamma = gamma\n        self.degree = degree\n        self.max_iter = max_iter\n\n        # Model parameters\n        self.alpha = None\n        self.support_vectors = None\n        self.support_vector_labels = None\n        self.support_vector_alpha = None\n        self.b = None\n        self.X_train = None\n        self.y_train = None\n\n    def _kernel_function(self, x1, x2):\n        \"\"\"Compute kernel function between two vectors\"\"\"\n        if self.kernel == 'linear':\n            return np.dot(x1, x2)\n\n        elif self.kernel == 'rbf':\n            if self.gamma == 'scale':\n                gamma = 1.0 / (x1.shape[0] * np.var(x1))\n            else:\n                gamma = self.gamma\n            return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n\n        elif self.kernel == 'poly':\n            if self.gamma == 'scale':\n                gamma = 1.0 / (x1.shape[0] * np.var(x1))\n            else:\n                gamma = self.gamma\n            return (gamma * np.dot(x1, x2) + 1) ** self.degree\n\n        else:\n            raise ValueError(\"Unsupported kernel type\")\n\n    def _compute_kernel_matrix(self, X1, X2):\n        \"\"\"Compute kernel matrix between two sets of points\"\"\"\n        n1, n2 = X1.shape[0], X2.shape[0]\n        kernel_matrix = np.zeros((n1, n2))\n\n        for i in range(n1):\n            for j in range(n2):\n                kernel_matrix[i, j] = self._kernel_function(X1[i], X2[j])\n\n        return kernel_matrix\n\n    def fit(self, X, y):\n        \"\"\"\n        Train SVM using SMO (Sequential Minimal Optimization) algorithm\n        Simplified implementation\n        \"\"\"\n        n_samples, n_features = X.shape\n        self.X_train = X\n        self.y_train = y\n\n        # Convert labels to -1 and 1\n        y = np.where(y &lt;= 0, -1, 1)\n\n        # Initialize alpha\n        self.alpha = np.zeros(n_samples)\n        self.b = 0\n\n        # Compute kernel matrix\n        K = self._compute_kernel_matrix(X, X)\n\n        # SMO algorithm (simplified)\n        for iteration in range(self.max_iter):\n            alpha_prev = np.copy(self.alpha)\n\n            for j in range(n_samples):\n                # Calculate prediction for point j\n                prediction_j = np.sum(self.alpha * y * K[:, j]) + self.b\n\n                # Calculate error\n                E_j = prediction_j - y[j]\n\n                # Check KKT conditions\n                if (y[j] * E_j &lt; -1e-3 and self.alpha[j] &lt; self.C) or \\\n                   (y[j] * E_j &gt; 1e-3 and self.alpha[j] &gt; 0):\n\n                    # Select second alpha randomly\n                    i = j\n                    while i == j:\n                        i = np.random.randint(0, n_samples)\n\n                    # Calculate prediction and error for point i\n                    prediction_i = np.sum(self.alpha * y * K[:, i]) + self.b\n                    E_i = prediction_i - y[i]\n\n                    # Save old alphas\n                    alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n\n                    # Compute bounds\n                    if y[i] != y[j]:\n                        L = max(0, self.alpha[j] - self.alpha[i])\n                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n                    else:\n                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n                        H = min(self.C, self.alpha[i] + self.alpha[j])\n\n                    if L == H:\n                        continue\n\n                    # Compute eta\n                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n                    if eta &gt;= 0:\n                        continue\n\n                    # Update alpha_j\n                    self.alpha[j] = self.alpha[j] - (y[j] * (E_i - E_j)) / eta\n\n                    # Clip alpha_j\n                    self.alpha[j] = max(L, min(H, self.alpha[j]))\n\n                    if abs(self.alpha[j] - alpha_j_old) &lt; 1e-5:\n                        continue\n\n                    # Update alpha_i\n                    self.alpha[i] = self.alpha[i] + y[i] * y[j] * (alpha_j_old - self.alpha[j])\n\n                    # Update bias\n                    b1 = self.b - E_i - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - \\\n                         y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n\n                    b2 = self.b - E_j - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - \\\n                         y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n\n                    if 0 &lt; self.alpha[i] &lt; self.C:\n                        self.b = b1\n                    elif 0 &lt; self.alpha[j] &lt; self.C:\n                        self.b = b2\n                    else:\n                        self.b = (b1 + b2) / 2\n\n            # Check convergence\n            if np.allclose(self.alpha, alpha_prev, atol=1e-5):\n                break\n\n        # Identify support vectors\n        support_vector_indices = np.where(self.alpha &gt; 1e-5)[0]\n        self.support_vectors = X[support_vector_indices]\n        self.support_vector_labels = y[support_vector_indices]\n        self.support_vector_alpha = self.alpha[support_vector_indices]\n\n        print(f\"Training completed in {iteration + 1} iterations\")\n        print(f\"Number of support vectors: {len(support_vector_indices)}\")\n\n    def predict(self, X):\n        \"\"\"Make predictions on new data\"\"\"\n        if self.support_vectors is None:\n            raise ValueError(\"Model has not been trained yet\")\n\n        n_samples = X.shape[0]\n        predictions = np.zeros(n_samples)\n\n        for i in range(n_samples):\n            prediction = 0\n            for j in range(len(self.support_vectors)):\n                prediction += (self.support_vector_alpha[j] * \n                             self.support_vector_labels[j] * \n                             self._kernel_function(X[i], self.support_vectors[j]))\n            predictions[i] = prediction + self.b\n\n        return np.sign(predictions).astype(int)\n\n    def decision_function(self, X):\n        \"\"\"Return decision function values\"\"\"\n        if self.support_vectors is None:\n            raise ValueError(\"Model has not been trained yet\")\n\n        n_samples = X.shape[0]\n        decisions = np.zeros(n_samples)\n\n        for i in range(n_samples):\n            decision = 0\n            for j in range(len(self.support_vectors)):\n                decision += (self.support_vector_alpha[j] * \n                           self.support_vector_labels[j] * \n                           self._kernel_function(X[i], self.support_vectors[j]))\n            decisions[i] = decision + self.b\n\n        return decisions\n\n# Demonstration with synthetic dataset\nnp.random.seed(42)\n\n# Generate synthetic dataset\nX_demo, y_demo = make_classification(\n    n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n    random_state=42, n_clusters_per_class=1\n)\n\n# Convert to binary classification\ny_demo = np.where(y_demo == 0, -1, 1)\n\n# Split data\nX_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n    X_demo, y_demo, test_size=0.3, random_state=42\n)\n\n# Standardize\nscaler_demo = StandardScaler()\nX_train_demo_scaled = scaler_demo.fit_transform(X_train_demo)\nX_test_demo_scaled = scaler_demo.transform(X_test_demo)\n\n# Train custom SVM\nprint(\"Training Custom SVM:\")\nsvm_custom = SVMFromScratch(C=1.0, kernel='rbf', gamma=1.0)\nsvm_custom.fit(X_train_demo_scaled, y_train_demo)\n\n# Predictions\ny_pred_custom = svm_custom.predict(X_test_demo_scaled)\n\n# Compare with sklearn\nfrom sklearn.svm import SVC\nsvm_sklearn = SVC(kernel='rbf', C=1.0, gamma=1.0)\nsvm_sklearn.fit(X_train_demo_scaled, y_train_demo)\ny_pred_sklearn = svm_sklearn.predict(X_test_demo_scaled)\n\n# Evaluate\naccuracy_custom = np.mean(y_pred_custom == y_test_demo)\naccuracy_sklearn = np.mean(y_pred_sklearn == y_test_demo)\n\nprint(f\"\\nComparison Results:\")\nprint(f\"Custom SVM accuracy: {accuracy_custom:.3f}\")\nprint(f\"Sklearn SVM accuracy: {accuracy_sklearn:.3f}\")\nprint(f\"Difference: {abs(accuracy_custom - accuracy_sklearn):.3f}\")\n\n# Visualize results\ndef plot_svm_comparison(X, y, svm_custom, svm_sklearn, title_custom, title_sklearn):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Create mesh\n    h = 0.01\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n\n    # Plot custom SVM\n    Z_custom = svm_custom.decision_function(mesh_points)\n    Z_custom = Z_custom.reshape(xx.shape)\n\n    ax1.contourf(xx, yy, Z_custom, levels=50, alpha=0.3, cmap=plt.cm.RdYlBu)\n    ax1.contour(xx, yy, Z_custom, levels=[0], colors='black', linewidths=2)\n\n    # Plot data points\n    colors = ['red', 'blue']\n    for i, color in enumerate([-1, 1]):\n        idx = np.where(y == color)[0]\n        ax1.scatter(X[idx, 0], X[idx, 1], c=colors[i], marker='o', \n                   label=f'Class {color}', alpha=0.7)\n\n    # Highlight support vectors\n    if svm_custom.support_vectors is not None:\n        ax1.scatter(svm_custom.support_vectors[:, 0], svm_custom.support_vectors[:, 1],\n                   s=100, facecolors='none', edgecolors='black', linewidth=2,\n                   label='Support Vectors')\n\n    ax1.set_title(title_custom)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot sklearn SVM\n    Z_sklearn = svm_sklearn.decision_function(mesh_points)\n    Z_sklearn = Z_sklearn.reshape(xx.shape)\n\n    ax2.contourf(xx, yy, Z_sklearn, levels=50, alpha=0.3, cmap=plt.cm.RdYlBu)\n    ax2.contour(xx, yy, Z_sklearn, levels=[0], colors='black', linewidths=2)\n\n    for i, color in enumerate([-1, 1]):\n        idx = np.where(y == color)[0]\n        ax2.scatter(X[idx, 0], X[idx, 1], c=colors[i], marker='o', \n                   label=f'Class {color}', alpha=0.7)\n\n    # Highlight support vectors\n    ax2.scatter(svm_sklearn.support_vectors_[:, 0], svm_sklearn.support_vectors_[:, 1],\n               s=100, facecolors='none', edgecolors='black', linewidth=2,\n               label='Support Vectors')\n\n    ax2.set_title(title_sklearn)\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_svm_comparison(X_train_demo_scaled, y_train_demo, svm_custom, svm_sklearn,\n                   f'Custom SVM (Acc: {accuracy_custom:.3f})',\n                   f'Sklearn SVM (Acc: {accuracy_sklearn:.3f})')\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#assumptions-and-limitations","title":"\ufffd\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Support%20Vector%20Machines/#key-assumptions","title":"Key Assumptions","text":"<ol> <li>Margin maximization is optimal: Assumes that maximizing margin leads to better generalization</li> <li>Support vector sufficiency: Only support vectors matter for the decision boundary</li> <li>Kernel validity: Chosen kernel should satisfy Mercer's conditions</li> <li>Feature scaling: SVM is sensitive to feature scales (assumes standardized features)</li> <li>Data quality: Assumes training data is representative of test distribution</li> </ol>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#limitations","title":"Limitations","text":"<ol> <li>Computational complexity: O(n\ufffd) training complexity for SMO algorithm</li> <li>Impact: Slow on large datasets (&gt;10,000 samples)</li> <li> <p>Solution: Use approximate methods, sub-sampling, or linear SVM</p> </li> <li> <p>Memory requirements: Stores support vectors and kernel matrix</p> </li> <li>Impact: Memory issues with large datasets or complex kernels</li> <li> <p>Solution: Use linear kernels, feature selection, or incremental learning</p> </li> <li> <p>No probabilistic output: Standard SVM provides only class predictions</p> </li> <li> <p>Solution: Use Platt scaling or cross-validation for probability estimates</p> </li> <li> <p>Sensitive to feature scaling: Different scales can dominate the kernel</p> </li> <li> <p>Solution: Always standardize features before training</p> </li> <li> <p>Hyperparameter sensitivity: Performance heavily depends on C and kernel parameters</p> </li> <li> <p>Solution: Use cross-validation for hyperparameter tuning</p> </li> <li> <p>Limited interpretability: Kernel SVMs create complex decision boundaries</p> </li> <li>Alternative: Use linear SVM or other interpretable models when needed</li> </ol>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Algorithm Training Speed Prediction Speed Memory Usage Interpretability Non-linear Capability SVM Slow (O(n\ufffd)) Fast High Low (kernel) High Logistic Regression Fast Very Fast Low High Low Random Forest Medium Medium Medium Medium High Neural Networks Slow Fast High Very Low Very High k-NN Very Fast Slow Medium High High Naive Bayes Very Fast Very Fast Low High Low <p>When to use SVM: - \u0005 High-dimensional data - \u0005 Clear margin of separation exists - \u0005 More features than samples - \u0005 Non-linear relationships (with kernels) - \u0005 Robust to outliers needed</p> <p>When to avoid SVM: - L Very large datasets (&gt;100k samples) - L Noisy data with overlapping classes - L Need probability estimates - L Real-time prediction requirements - L Interpretability is crucial</p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#interview-questions","title":"\u2753 Interview Questions","text":"Explain the mathematical intuition behind SVM and the concept of margin maximization. <p>Answer: SVM finds the hyperplane that separates classes with maximum margin:</p> <p>Mathematical foundation: 1. Decision boundary: \\(w^T x + b = 0\\) 2. Margin: Distance from hyperplane to nearest points = \\(\\frac{1}{||w||}\\) 3. Optimization: Maximize margin = Minimize \\(\\frac{1}{2}||w||^2\\) 4. Constraints: Ensure correct classification: \\(y_i(w^T x_i + b) \\geq 1\\)</p> <p>Intuition:  - Larger margins \ufffd better generalization (statistical learning theory) - Only support vectors (points on margin) determine decision boundary - All other points could be removed without changing the model</p> <p>Why maximize margin? - Provides robustness against small perturbations - Reduces VC dimension \ufffd better generalization bounds - Unique solution (convex optimization problem)</p> What is the kernel trick and how does it enable SVM to handle non-linear data? <p>Answer: The kernel trick allows SVM to handle non-linear data without explicitly computing high-dimensional transformations:</p> <p>The trick: 1. Replace dot products in dual formulation with kernel function: \\(x_i^T x_j \ufffd K(x_i, x_j)\\) 2. Implicit mapping: \\(K(x_i, x_j) = \ufffd(x_i)^T \ufffd(x_j)\\) where \ufffd maps to higher dimension 3. No explicit computation of \ufffd(x) needed</p> <p>Popular kernels: <pre><code># Linear: K(x,z) = x^T z\n# Polynomial: K(x,z) = (x^T z + c)^d\n# RBF: K(x,z) = exp(-\ufffd||x-z||\ufffd)\n# Sigmoid: K(x,z) = tanh(\ufffdx^T z + c)\n</code></pre></p> <p>Example: RBF kernel maps data to infinite-dimensional space, allowing separation of any finite dataset</p> <p>Advantages: - Computational efficiency (no explicit mapping) - Handles complex non-linear relationships - Mathematical elegance through Mercer's theorem</p> <p>Limitations:  - Kernel choice is crucial - Interpretability decreases - Hyperparameter tuning becomes more complex</p> How do you choose appropriate hyperparameters (C, gamma, kernel) for SVM? <p>Answer: Systematic approach to SVM hyperparameter tuning:</p> <p>Key hyperparameters:</p> <p>1. Regularization parameter C: - Small C: Soft margin, more misclassifications allowed, prevents overfitting - Large C: Hard margin, fewer misclassifications, risk of overfitting - Typical range: [0.1, 1, 10, 100, 1000]</p> <p>2. Kernel parameter gamma (for RBF/poly): - Small gamma: Far-reaching influence, smoother boundaries - Large gamma: Close influence, complex boundaries, overfitting risk - Typical values: ['scale', 'auto', 0.001, 0.01, 0.1, 1]</p> <p>3. Kernel selection: <pre><code># Linear: Good for high-dimensional, linearly separable data\n# RBF: Default choice, good for most non-linear problems\n# Polynomial: Specific polynomial relationships\n# Sigmoid: Neural network-like behavior\n</code></pre></p> <p>Tuning strategy: <pre><code># Grid search with cross-validation\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 0.001, 0.01, 0.1, 1],\n    'kernel': ['rbf', 'poly', 'linear']\n}\nGridSearchCV(SVC(), param_grid, cv=5)\n</code></pre></p> <p>Best practices: - Start with default parameters - Use cross-validation for unbiased estimates - Consider computational constraints - Validate on separate test set</p> What's the difference between hard margin and soft margin SVM? <p>Answer: Key differences in handling non-separable data:</p> <p>Hard Margin SVM: - Assumption: Data is linearly separable - Constraint: All points correctly classified: \\(y_i(w^T x_i + b) \\geq 1\\) - Objective: \\(\\min \\frac{1}{2}||w||^2\\) - Problem: No solution exists if data isn't separable - Use case: Clean, separable data</p> <p>Soft Margin SVM: - Assumption: Data may have noise/overlap - Slack variables: \\(\ufffd_i e 0\\) allow constraint violations - Modified constraints: \\(y_i(w^T x_i + b) \\geq 1 - \ufffd_i\\) - Objective: \\(\\min \\frac{1}{2}||w||^2 + C\\sum \ufffd_i\\) - Trade-off: Margin maximization vs. training error</p> <p>C parameter controls: - $C \ufffd \u001e$: Approaches hard margin (no violations) - \\(C \ufffd 0\\): Allows many violations (maximum margin)</p> <p>Practical impact: <pre><code># Hard margin equivalent\nSVC(C=1e6)  # Very large C\n\n# Soft margin\nSVC(C=1.0)   # Balanced trade-off\n</code></pre></p> <p>When to use: - Hard margin: Perfect data, small datasets - Soft margin: Real-world data (recommended)</p> How does SVM handle multi-class classification? <p>Answer: SVM is inherently binary, but extends to multi-class using two main strategies:</p> <p>1. One-vs-Rest (OvR): - Train K binary classifiers (K = number of classes) - Each classifier: \"Class i vs All other classes\" - Prediction: Class with highest decision function score - Pros: Simple, efficient - Cons: Imbalanced datasets per classifier</p> <pre><code># Automatic in sklearn\nSVC()  # Uses OvR by default\n\n# Explicit\nfrom sklearn.multiclass import OneVsRestClassifier\nOneVsRestClassifier(SVC())\n</code></pre> <p>2. One-vs-One (OvO): - Train K(K-1)/2 binary classifiers - Each classifier: \"Class i vs Class j\" - Prediction: Majority voting among all classifiers - Pros: Balanced datasets, often more accurate - Cons: More classifiers to train</p> <pre><code># In sklearn\nSVC(decision_function_shape='ovo')\n\n# Explicit\nfrom sklearn.multiclass import OneVsOneClassifier\nOneVsOneClassifier(SVC())\n</code></pre> <p>Comparison: | Aspect | OvR | OvO | |--------|-----|-----| | Classifiers | K | K(K-1)/2 | | Training time | Faster | Slower | | Prediction time | Faster | Slower | | Accuracy | Good | Often better | | Memory | Less | More |</p> <p>Decision function: - OvR: Use raw scores from each classifier - OvO: Aggregate pairwise comparisons</p> What are the advantages and disadvantages of different SVM kernels? <p>Answer: Comprehensive comparison of SVM kernels:</p> <p>Linear Kernel: \\(K(x,z) = x^T z\\)</p> <p>Advantages: - \u0005 Fast training and prediction - \u0005 Interpretable (weights have meaning) - \u0005 Good for high-dimensional data - \u0005 Less prone to overfitting - \u0005 No hyperparameters to tune</p> <p>Disadvantages: - L Only linear decision boundaries - L Poor for complex non-linear relationships</p> <p>Use when: Text classification, high-dimensional data, linear relationships</p> <p>RBF (Gaussian) Kernel: \\(K(x,z) = \\exp(-\\gamma||x-z||^2)\\)</p> <p>Advantages: - \u0005 Handles non-linear relationships - \u0005 Universal approximator - \u0005 Works well as default choice - \u0005 Smooth decision boundaries</p> <p>Disadvantages: - L Requires hyperparameter tuning (\ufffd) - L Can overfit with large \ufffd - L Less interpretable - L Slower than linear</p> <p>Use when: Non-linear data, default choice for most problems</p> <p>Polynomial Kernel: \\(K(x,z) = (x^T z + c)^d\\)</p> <p>Advantages: - \u0005 Good for specific polynomial relationships - \u0005 Interpretable degree parameter - \u0005 Can capture interactions</p> <p>Disadvantages: - L Computationally expensive for high degrees - L Numerical instability - L Less general than RBF - L Multiple hyperparameters</p> <p>Use when: Known polynomial relationships in data</p> <p>Sigmoid Kernel: \\(K(x,z) = \\tanh(\\gamma x^T z + c)\\)</p> <p>Advantages: - \u0005 Neural network-like behavior - \u0005 S-shaped decision boundaries</p> <p>Disadvantages: - L Not positive semi-definite (violates Mercer's condition) - L Can be unstable - L Often outperformed by RBF - L Limited practical use</p> <p>Selection guidelines: 1. Start with RBF (default choice) 2. Try linear if high-dimensional 3. Use polynomial for specific domain knowledge 4. Avoid sigmoid unless specific need</p> How do you handle imbalanced datasets with SVM? <p>Answer: Several strategies for handling class imbalance in SVM:</p> <p>1. Class weight balancing: <pre><code># Automatic balancing\nSVC(class_weight='balanced')\n\n# Manual weights\nSVC(class_weight={0: 1, 1: 10})  # 10x weight for minority class\n\n# Effect: Increases penalty for misclassifying minority class\n</code></pre></p> <p>2. Resampling techniques: <pre><code>from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Oversample minority class\nsmote = SMOTE()\nX_balanced, y_balanced = smote.fit_resample(X, y)\n\n# Undersample majority class  \nundersampler = RandomUnderSampler()\nX_balanced, y_balanced = undersampler.fit_resample(X, y)\n</code></pre></p> <p>3. Threshold adjustment: <pre><code># Use decision function for custom thresholds\nscores = svm.decision_function(X_test)\n# Instead of scores &gt; 0, use scores &gt; custom_threshold\npredictions = (scores &gt; optimal_threshold).astype(int)\n</code></pre></p> <p>4. Cost-sensitive learning: - Modify C parameter per class - Different misclassification costs <pre><code># Higher C for minority class\nSVC(C=100, class_weight={0: 1, 1: 5})\n</code></pre></p> <p>5. Evaluation metrics: <pre><code># Don't use accuracy for imbalanced data\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score\n\n# Use precision, recall, F1-score, AUC\nprecision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred)\nauc = roc_auc_score(y_true, decision_scores)\n</code></pre></p> <p>Best practices: - Combine multiple techniques - Use stratified cross-validation - Focus on minority class performance - Consider ensemble methods as alternative</p> Explain the computational complexity of SVM training and prediction. <p>Answer: Detailed complexity analysis:</p> <p>Training Complexity:</p> <p>SMO Algorithm (most common): - Time: O(n\ufffd) to O(n\ufffd) depending on data - Average case: O(n\ufffd\ufffd\ufffd) for most datasets - Worst case: O(n\ufffd) for very difficult datasets - Space: O(n\ufffd) for kernel matrix storage</p> <p>Factors affecting training time: <pre><code># Dataset size (most important)\nn_samples = 1000    # Fast\nn_samples = 100000  # Very slow\n\n# Kernel complexity\nkernel='linear'     # Fastest\nkernel='rbf'        # Medium  \nkernel='poly'       # Slower\n\n# Hyperparameters\nC=0.1              # Faster (more violations allowed)\nC=1000             # Slower (strict constraints)\n</code></pre></p> <p>Prediction Complexity: - Time: O(n_support_vectors \ufffd n_features) - Typical: Much faster than training - Linear kernel: O(n_features) - very fast - Non-linear: O(n_sv \ufffd n_features) - depends on support vectors</p> <p>Memory Requirements: <pre><code># Kernel matrix: n \ufffd n \ufffd 8 bytes (for RBF/poly)\nmemory_gb = (n_samples ** 2 * 8) / (1024**3)\n\n# For 10,000 samples: ~0.75 GB\n# For 100,000 samples: ~75 GB (impractical)\n</code></pre></p> <p>Scalability solutions: 1. Linear SVM: Use for n &gt; 10,000 2. Sampling: Train on subset of data 3. Online SVM: Incremental learning algorithms 4. Approximate methods: Nystr\ufffdm approximation 5. Alternative algorithms: Random Forest, XGBoost for large data</p> <p>Practical guidelines: - n &lt; 1,000: Any kernel works - 1,000 &lt; n &lt; 10,000: RBF with tuning - n &gt; 10,000: Consider linear SVM or alternatives - n &gt; 100,000: Use other algorithms</p> How do you interpret and visualize SVM results? <p>Answer: Multiple approaches for SVM interpretation:</p> <p>1. Decision boundaries (2D visualization): <pre><code>def plot_svm_boundary(X, y, model):\n    # Create mesh\n    h = 0.01\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\n    # Predict on mesh\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot boundary and margins\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n\n    # Highlight support vectors\n    plt.scatter(model.support_vectors_[:, 0], \n               model.support_vectors_[:, 1],\n               s=100, facecolors='none', edgecolors='black')\n</code></pre></p> <p>2. Support vector analysis: <pre><code>print(f\"Number of support vectors: {len(model.support_)}\")\nprint(f\"Support vector ratio: {len(model.support_)/len(X_train):.2%}\")\nprint(f\"Support vectors per class: {model.n_support_}\")\n\n# High ratio might indicate:\n# - Complex decision boundary\n# - Noisy data  \n# - Need for different kernel/parameters\n</code></pre></p> <p>3. Feature importance (linear kernel only): <pre><code>if model.kernel == 'linear':\n    # Coefficients indicate feature importance\n    feature_importance = abs(model.coef_[0])\n\n    plt.barh(feature_names, feature_importance)\n    plt.title('Linear SVM Feature Importance')\n</code></pre></p> <p>4. Decision function analysis: <pre><code># Distance from hyperplane\ndecision_scores = model.decision_function(X_test)\n\n# Confidence interpretation\n# |score| &gt; 1: High confidence\n# |score| &lt; 1: Low confidence (near boundary)\n\nplt.hist(decision_scores, bins=30)\nplt.axvline(x=0, color='red', linestyle='--', label='Decision boundary')\nplt.axvline(x=1, color='orange', linestyle='--', label='Margin')\nplt.axvline(x=-1, color='orange', linestyle='--')\n</code></pre></p> <p>5. Hyperparameter sensitivity analysis: <pre><code># Plot performance vs hyperparameters\nC_values = [0.1, 1, 10, 100]\nscores = []\n\nfor C in C_values:\n    model = SVC(C=C, kernel='rbf')\n    score = cross_val_score(model, X, y, cv=5).mean()\n    scores.append(score)\n\nplt.plot(C_values, scores)\nplt.xlabel('C (log scale)')\nplt.xscale('log')\nplt.ylabel('Cross-validation accuracy')\n</code></pre></p> <p>6. Error analysis: <pre><code># Analyze misclassified points\ny_pred = model.predict(X_test)\nmisclassified = X_test[y_test != y_pred]\n\n# Are they near the decision boundary?\ndecision_scores_errors = model.decision_function(misclassified)\nprint(f\"Average distance from boundary: {np.mean(abs(decision_scores_errors))}\")\n</code></pre></p>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#examples","title":"&gt;\ufffd Examples","text":""},{"location":"Machine-Learning/Support%20Vector%20Machines/#real-world-example-text-classification","title":"Real-world Example: Text Classification","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load text dataset (subset of 20 newsgroups)\ncategories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\nnewsgroups = fetch_20newsgroups(\n    subset='all',\n    categories=categories, \n    shuffle=True,\n    random_state=42,\n    remove=('headers', 'footers', 'quotes')\n)\n\nprint(\"Text Classification with SVM\")\nprint(f\"Dataset shape: {len(newsgroups.data)} documents\")\nprint(f\"Categories: {newsgroups.target_names}\")\nprint(f\"Class distribution:\")\nfor i, name in enumerate(newsgroups.target_names):\n    count = sum(newsgroups.target == i)\n    print(f\"  {name}: {count} documents\")\n\n# Split data\nX_text, X_test_text, y_text, y_test_text = train_test_split(\n    newsgroups.data, newsgroups.target, \n    test_size=0.2, random_state=42, stratify=newsgroups.target\n)\n\n# Create pipeline with TF-IDF and SVM\ntext_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(\n        max_features=10000,      # Limit vocabulary\n        min_df=2,                # Ignore rare words  \n        max_df=0.95,             # Ignore too common words\n        stop_words='english',     # Remove stop words\n        ngram_range=(1, 2)       # Use unigrams and bigrams\n    )),\n    ('svm', SVC(\n        kernel='linear',         # Linear works well for text\n        C=1.0,\n        random_state=42\n    ))\n])\n\n# Train model\nprint(\"\\nTraining SVM text classifier...\")\ntext_pipeline.fit(X_text, y_text)\n\n# Predictions\ny_pred_text = text_pipeline.predict(X_test_text)\n\n# Evaluate\naccuracy_text = np.mean(y_pred_text == y_test_text)\nprint(f\"Test accuracy: {accuracy_text:.3f}\")\n\n# Detailed classification report\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test_text, y_pred_text, \n                          target_names=newsgroups.target_names))\n\n# Confusion matrix\nplt.figure(figsize=(8, 6))\ncm_text = confusion_matrix(y_test_text, y_pred_text)\nsns.heatmap(cm_text, annot=True, fmt='d', cmap='Blues',\n            xticklabels=newsgroups.target_names,\n            yticklabels=newsgroups.target_names)\nplt.title('Text Classification - Confusion Matrix')\nplt.ylabel('True Category')\nplt.xlabel('Predicted Category')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n# Feature importance analysis (most important words)\nfeature_names = text_pipeline.named_steps['tfidf'].get_feature_names_out()\nsvm_model = text_pipeline.named_steps['svm']\n\n# For each class, show most important features\nn_features = 10\nfor i, category in enumerate(newsgroups.target_names):\n    if hasattr(svm_model, 'coef_'):\n        # Get coefficients for this class (one-vs-rest)\n        if len(svm_model.classes_) == 2:\n            coef = svm_model.coef_[0] if i == 1 else -svm_model.coef_[0]\n        else:\n            coef = svm_model.coef_[i]\n\n        # Get top features\n        top_positive_indices = coef.argsort()[-n_features:][::-1]\n        top_negative_indices = coef.argsort()[:n_features]\n\n        print(f\"\\nMost important features for '{category}':\")\n        print(\"Positive indicators:\")\n        for idx in top_positive_indices:\n            print(f\"  {feature_names[idx]}: {coef[idx]:.3f}\")\n\n        print(\"Negative indicators:\")\n        for idx in top_negative_indices:\n            print(f\"  {feature_names[idx]}: {coef[idx]:.3f}\")\n\n# Cross-validation performance\ncv_scores = cross_val_score(text_pipeline, X_text, y_text, cv=5)\nprint(f\"\\nCross-validation scores: {cv_scores}\")\nprint(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n\n# Example predictions with confidence\nsample_texts = [\n    \"I believe in God and Jesus Christ\",\n    \"The graphics card is not working properly\",\n    \"This medical treatment showed promising results\",\n    \"There is no scientific evidence for the existence of God\"\n]\n\nprint(f\"\\nExample Predictions:\")\nfor text in sample_texts:\n    prediction = text_pipeline.predict([text])[0]\n    decision_score = text_pipeline.decision_function([text])\n    predicted_category = newsgroups.target_names[prediction]\n\n    print(f\"\\nText: '{text[:50]}...'\")\n    print(f\"Predicted: {predicted_category}\")\n    print(f\"Decision scores: {decision_score[0]}\")\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#image-classification-example","title":"Image Classification Example","text":"<pre><code>from sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\n# Load face recognition dataset\nfaces = fetch_olivetti_faces(shuffle=True, random_state=42)\nX_faces = faces.data\ny_faces = faces.target\n\nprint(\"Face Recognition with SVM\")\nprint(f\"Dataset shape: {X_faces.shape}\")\nprint(f\"Number of people: {len(np.unique(y_faces))}\")\nprint(f\"Image dimensions: 64x64 pixels\")\n\n# Visualize some sample faces\nfig, axes = plt.subplots(2, 5, figsize=(12, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_faces[i].reshape(64, 64), cmap='gray')\n    ax.set_title(f'Person {y_faces[i]}')\n    ax.axis('off')\nplt.suptitle('Sample Face Images')\nplt.tight_layout()\nplt.show()\n\n# Split data\nX_train_faces, X_test_faces, y_train_faces, y_test_faces = train_test_split(\n    X_faces, y_faces, test_size=0.25, random_state=42, stratify=y_faces\n)\n\n# Apply PCA for dimensionality reduction (faces are high-dimensional)\nn_components = 150  # Reduce from 4096 to 150 dimensions\npca_faces = PCA(n_components=n_components, whiten=True, random_state=42)\nX_train_pca = pca_faces.fit_transform(X_train_faces)\nX_test_pca = pca_faces.transform(X_test_faces)\n\nprint(f\"\\nDimensionality reduction:\")\nprint(f\"Original dimensions: {X_train_faces.shape[1]}\")\nprint(f\"Reduced dimensions: {X_train_pca.shape[1]}\")\nprint(f\"Variance explained: {pca_faces.explained_variance_ratio_.sum():.3f}\")\n\n# Train SVM classifier\nsvm_faces = SVC(kernel='rbf', C=1000, gamma=0.005, random_state=42)\nsvm_faces.fit(X_train_pca, y_train_faces)\n\n# Predictions\ny_pred_faces = svm_faces.predict(X_test_pca)\n\n# Evaluate\naccuracy_faces = accuracy_score(y_test_faces, y_pred_faces)\nprint(f\"\\nFace Recognition Results:\")\nprint(f\"Accuracy: {accuracy_faces:.3f}\")\nprint(f\"Number of support vectors: {len(svm_faces.support_)}\")\nprint(f\"Support vector ratio: {len(svm_faces.support_)/len(X_train_pca):.2%}\")\n\n# Visualize some predictions\nfig, axes = plt.subplots(3, 6, figsize=(15, 9))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(X_test_faces):\n        # Show original image\n        ax.imshow(X_test_faces[i].reshape(64, 64), cmap='gray')\n\n        # Get prediction and confidence\n        true_label = y_test_faces[i] \n        pred_label = y_pred_faces[i]\n        decision_score = svm_faces.decision_function([X_test_pca[i]])\n        confidence = np.max(decision_score)\n\n        # Color border based on correctness\n        color = 'green' if true_label == pred_label else 'red'\n        ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n                    color=color, fontsize=8)\n        ax.axis('off')\n\n        # Add border\n        for spine in ax.spines.values():\n            spine.set_color(color)\n            spine.set_linewidth(3)\n    else:\n        ax.axis('off')\n\nplt.suptitle('Face Recognition Predictions (Green=Correct, Red=Incorrect)')\nplt.tight_layout()\nplt.show()\n\n# Analyze errors\nincorrect_indices = np.where(y_test_faces != y_pred_faces)[0]\nprint(f\"\\nError Analysis:\")\nprint(f\"Total errors: {len(incorrect_indices)}\")\n\nif len(incorrect_indices) &gt; 0:\n    # Show decision scores for incorrect predictions\n    incorrect_scores = svm_faces.decision_function(X_test_pca[incorrect_indices])\n    avg_incorrect_confidence = np.mean(np.max(incorrect_scores, axis=1))\n\n    correct_indices = np.where(y_test_faces == y_pred_faces)[0]\n    correct_scores = svm_faces.decision_function(X_test_pca[correct_indices])\n    avg_correct_confidence = np.mean(np.max(correct_scores, axis=1))\n\n    print(f\"Average confidence for correct predictions: {avg_correct_confidence:.3f}\")\n    print(f\"Average confidence for incorrect predictions: {avg_incorrect_confidence:.3f}\")\n\n    # Plot confidence distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(np.max(correct_scores, axis=1), bins=20, alpha=0.7, \n             label='Correct predictions', color='green')\n    plt.hist(np.max(incorrect_scores, axis=1), bins=20, alpha=0.7, \n             label='Incorrect predictions', color='red')\n    plt.xlabel('Maximum Decision Score (Confidence)')\n    plt.ylabel('Frequency')\n    plt.title('Confidence Distribution: Correct vs Incorrect Predictions')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Hyperparameter sensitivity analysis\nC_values = [1, 10, 100, 1000]\ngamma_values = [0.001, 0.005, 0.01, 0.05]\n\nresults_grid = np.zeros((len(C_values), len(gamma_values)))\n\nprint(f\"\\nHyperparameter sensitivity analysis:\")\nfor i, C in enumerate(C_values):\n    for j, gamma in enumerate(gamma_values):\n        svm_temp = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n        svm_temp.fit(X_train_pca, y_train_faces)\n        score = svm_temp.score(X_test_pca, y_test_faces)\n        results_grid[i, j] = score\n        print(f\"C={C:4}, gamma={gamma:.3f}: {score:.3f}\")\n\n# Visualize hyperparameter effects\nplt.figure(figsize=(8, 6))\nsns.heatmap(results_grid, \n           xticklabels=[f'{g:.3f}' for g in gamma_values],\n           yticklabels=C_values,\n           annot=True, fmt='.3f', cmap='viridis')\nplt.title('Face Recognition Accuracy: C vs Gamma')\nplt.xlabel('Gamma')\nplt.ylabel('C')\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#regression-example-with-support-vector-regression-svr","title":"Regression Example with Support Vector Regression (SVR)","text":"<pre><code>from sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Generate synthetic regression dataset with noise\nnp.random.seed(42)\nX_reg, y_reg = make_regression(\n    n_samples=300, \n    n_features=1, \n    noise=15,\n    random_state=42\n)\n\n# Add some outliers\noutlier_indices = np.random.choice(len(X_reg), size=20, replace=False)\ny_reg[outlier_indices] += np.random.normal(0, 50, size=20)\n\n# Sort for plotting\nsort_indices = np.argsort(X_reg[:, 0])\nX_reg_sorted = X_reg[sort_indices]\ny_reg_sorted = y_reg[sort_indices]\n\nprint(\"Support Vector Regression Example\")\nprint(f\"Dataset shape: {X_reg.shape}\")\nprint(f\"Target range: [{y_reg.min():.1f}, {y_reg.max():.1f}]\")\n\n# Split data\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42\n)\n\n# Standardize features\nscaler_svr = StandardScaler()\nX_train_reg_scaled = scaler_svr.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_svr.transform(X_test_reg)\n\n# Train different SVR models\nsvr_models = {\n    'Linear SVR': SVR(kernel='linear', C=100, epsilon=0.1),\n    'RBF SVR': SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1),\n    'Polynomial SVR': SVR(kernel='poly', degree=3, C=100, epsilon=0.1)\n}\n\nplt.figure(figsize=(15, 10))\n\nfor i, (name, model) in enumerate(svr_models.items()):\n    # Train model\n    model.fit(X_train_reg_scaled, y_train_reg)\n\n    # Predictions\n    y_pred_train = model.predict(X_train_reg_scaled)\n    y_pred_test = model.predict(X_test_reg_scaled)\n\n    # Evaluate\n    train_r2 = r2_score(y_train_reg, y_pred_train)\n    test_r2 = r2_score(y_test_reg, y_pred_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test))\n    test_mae = mean_absolute_error(y_test_reg, y_pred_test)\n\n    print(f\"\\n{name} Results:\")\n    print(f\"Train R\ufffd: {train_r2:.3f}\")\n    print(f\"Test R\ufffd: {test_r2:.3f}\")\n    print(f\"Test RMSE: {test_rmse:.3f}\")\n    print(f\"Test MAE: {test_mae:.3f}\")\n    print(f\"Support vectors: {len(model.support_)} ({len(model.support_)/len(X_train_reg_scaled)*100:.1f}%)\")\n\n    # Plot results\n    plt.subplot(2, 3, i+1)\n\n    # Create smooth line for predictions\n    X_plot = scaler_svr.transform(X_reg_sorted.reshape(-1, 1))\n    y_plot = model.predict(X_plot)\n\n    # Plot data points\n    plt.scatter(X_train_reg_scaled, y_train_reg, alpha=0.6, color='blue', \n                label='Training data', s=30)\n    plt.scatter(X_test_reg_scaled, y_test_reg, alpha=0.6, color='red', \n                label='Test data', s=30)\n\n    # Plot prediction line\n    plt.plot(X_plot, y_plot, color='green', linewidth=2, label='SVR prediction')\n\n    # Highlight support vectors\n    if len(model.support_) &gt; 0:\n        support_X = X_train_reg_scaled[model.support_]\n        support_y = y_train_reg[model.support_]\n        plt.scatter(support_X, support_y, s=100, facecolors='none', \n                   edgecolors='black', linewidth=2, label='Support vectors')\n\n    plt.xlabel('Feature (standardized)')\n    plt.ylabel('Target')\n    plt.title(f'{name}\\nR\ufffd = {test_r2:.3f}, RMSE = {test_rmse:.1f}')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n# Plot residuals analysis\nplt.subplot(2, 3, 4)\nbest_model = svr_models['RBF SVR']  # Use RBF as best model\ny_pred_best = best_model.predict(X_test_reg_scaled)\nresiduals = y_test_reg - y_pred_best\n\nplt.scatter(y_pred_best, residuals, alpha=0.6)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot (RBF SVR)')\nplt.grid(True, alpha=0.3)\n\n# Plot actual vs predicted\nplt.subplot(2, 3, 5)\nplt.scatter(y_test_reg, y_pred_best, alpha=0.6)\nplt.plot([y_test_reg.min(), y_test_reg.max()], \n         [y_test_reg.min(), y_test_reg.max()], 'r--', linewidth=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs Predicted (RBF SVR)')\nplt.grid(True, alpha=0.3)\n\n# Plot epsilon-tube visualization\nplt.subplot(2, 3, 6)\nepsilon = best_model.epsilon\n\n# Sort data for smooth plotting\nsort_idx = np.argsort(X_test_reg_scaled[:, 0])\nX_sorted = X_test_reg_scaled[sort_idx]\ny_pred_sorted = y_pred_best[sort_idx]\n\nplt.scatter(X_test_reg_scaled, y_test_reg, alpha=0.6, color='blue', \n           label='Test data')\nplt.plot(X_sorted, y_pred_sorted, color='green', linewidth=2, \n         label='SVR prediction')\nplt.fill_between(X_sorted[:, 0], y_pred_sorted - epsilon, y_pred_sorted + epsilon,\n                alpha=0.3, color='yellow', label=f'\ufffd-tube (\ufffd={epsilon})')\n\nplt.xlabel('Feature (standardized)')\nplt.ylabel('Target')\nplt.title('SVR with \ufffd-insensitive Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Hyperparameter tuning for SVR\nprint(f\"\\nSVR Hyperparameter Analysis:\")\n\n# Test different epsilon values\nepsilon_values = [0.01, 0.1, 0.5, 1.0, 2.0]\nC_values = [1, 10, 100, 1000]\n\nbest_score = -np.inf\nbest_params = {}\n\nfor epsilon in epsilon_values:\n    for C in C_values:\n        svr_temp = SVR(kernel='rbf', C=C, epsilon=epsilon, gamma='scale')\n        svr_temp.fit(X_train_reg_scaled, y_train_reg)\n        score = svr_temp.score(X_test_reg_scaled, y_test_reg)\n\n        if score &gt; best_score:\n            best_score = score\n            best_params = {'C': C, 'epsilon': epsilon}\n\n        print(f\"C={C:4}, \ufffd={epsilon:4.2f}: R\ufffd = {score:.3f}, \"\n              f\"Support vectors: {len(svr_temp.support_):3d}\")\n\nprint(f\"\\nBest parameters: {best_params}\")\nprint(f\"Best R\ufffd score: {best_score:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/Support%20Vector%20Machines/#references","title":"\ud83d\udcda References","text":"<ul> <li>Original Papers:</li> <li>Support-Vector Networks by Cortes &amp; Vapnik (1995)</li> <li>The Nature of Statistical Learning Theory by Vladimir Vapnik (1995)</li> <li> <p>SMO Algorithm by John Platt (1998)</p> </li> <li> <p>Books:</p> </li> <li>The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman - Chapter 12</li> <li>Pattern Recognition and Machine Learning by Christopher Bishop - Chapter 7</li> <li> <p>Learning with Kernels by Sch\ufffdlkopf and Smola</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn SVM Guide</li> <li>Scikit-learn SVC</li> <li> <p>Scikit-learn SVR</p> </li> <li> <p>Tutorials and Guides:</p> </li> <li>SVM Tutorial - Andrew Ng</li> <li>Understanding SVM</li> <li> <p>Kernel Methods Tutorial</p> </li> <li> <p>Advanced Topics:</p> </li> <li>One-Class SVM for anomaly detection</li> <li>Nu-SVM alternative parameterization</li> <li> <p>Linear SVM for large datasets</p> </li> <li> <p>Research Papers:</p> </li> <li>Sch\ufffdlkopf, B., &amp; Smola, A. J. (2002). Learning with kernels: Support vector machines</li> <li>Chang, C. C., &amp; Lin, C. J. (2011). LIBSVM: A library for support vector machines</li> <li> <p>Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., &amp; Lin, C. J. (2008). LIBLINEAR: A library for large linear classification</p> </li> <li> <p>Online Courses:</p> </li> <li>Machine Learning Course - Stanford CS229</li> <li>SVM in Machine Learning - Coursera</li> <li> <p>Statistical Learning - edX</p> </li> <li> <p>Implementations:</p> </li> <li>scikit-learn (Python)</li> <li>LIBSVM (C++, multiple language bindings)</li> <li>e1071 (R package)</li> </ul>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/","title":"\u2696\ufe0f Unbalanced and Skewed Data","text":"<p>Unbalanced and Skewed Data are common challenges in machine learning where the distribution of classes or feature values is highly imbalanced, leading to biased models that favor majority classes or specific value ranges.</p> <p>Resources: Imbalanced-learn Library | SMOTE Paper | Cost-Sensitive Learning Survey</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#_1","title":"Unbalanced and Skewed Data","text":"<p>\u000f Summary</p> <p>Unbalanced Data (Class Imbalance) occurs when the distribution of target classes is significantly unequal. Skewed Data refers to non-normal distributions in features where most values are concentrated at one end of the range.</p> <p>Key Characteristics:</p> <p>Unbalanced Data: - Minority classes have significantly fewer samples than majority classes - Common in fraud detection, medical diagnosis, rare event prediction - Standard algorithms tend to favor majority class - Accuracy can be misleading as a performance metric</p> <p>Skewed Data: - Feature distributions are asymmetric (left-skewed or right-skewed) - Mean and median differ significantly - Can cause issues with algorithms assuming normal distributions - May contain outliers that affect model performance</p> <p>Applications: - Fraud detection (few fraud cases vs. many normal transactions) - Medical diagnosis (rare diseases vs. healthy patients) - Email spam detection (spam vs. legitimate emails) - Quality control (defective vs. normal products) - Customer churn prediction (churned vs. retained customers) - Anomaly detection in cybersecurity</p> <p>Related Concepts: - Sampling Techniques: Methods to balance class distributions - Cost-Sensitive Learning: Assigning different costs to classification errors - Ensemble Methods: Combining multiple models to improve minority class performance - Evaluation Metrics: Precision, Recall, F1-score, AUC-ROC for imbalanced datasets</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#intuition","title":"&gt;\ufffd Intuition","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#how-imbalanced-data-affects-learning","title":"How Imbalanced Data Affects Learning","text":"<p>Imagine training a model to detect rare diseases where only 1% of patients have the disease. A naive model could achieve 99% accuracy by always predicting \"no disease\" - but this would be completely useless for actually identifying sick patients. The model learns to favor the majority class because:</p> <ol> <li>Training Bias: More examples of majority class dominate the learning process</li> <li>Decision Boundary: Gets pushed toward minority class regions</li> <li>Loss Function: Optimizes for overall accuracy, not class-specific performance</li> <li>Gradient Updates: Majority class errors have more influence on weight updates</li> </ol>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#1-class-imbalance-ratio","title":"1. Class Imbalance Ratio","text":"<p>For binary classification with classes 0 and 1: \\(\\(\\text{Imbalance Ratio} = \\frac{\\text{Number of samples in minority class}}{\\text{Number of samples in majority class}}\\)\\)</p> <p>Severe imbalance: IR &lt; 0.1, Moderate imbalance: 0.1 d IR d 0.5</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#2-skewness-measure","title":"2. Skewness Measure","text":"<p>For a distribution with values \\(x_1, x_2, ..., x_n\\): \\(\\(\\text{Skewness} = \\frac{E[(X - \\mu)^3]}{\\sigma^3} = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^3}{s^3}\\)\\)</p> <p>Where: - Skewness &gt; 0: Right-skewed (long tail on right) - Skewness &lt; 0: Left-skewed (long tail on left) - Skewness H 0: Approximately symmetric</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#3-cost-sensitive-learning","title":"3. Cost-Sensitive Learning","text":"<p>Modify the loss function to penalize minority class errors more heavily: \\(\\(\\text{Cost-Sensitive Loss} = \\sum_{i=1}^{n} C(y_i) \\cdot L(y_i, \\hat{y_i})\\)\\)</p> <p>Where \\(C(y_i)\\) is the cost matrix assigning higher costs to minority class misclassifications.</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#4-smote-algorithm","title":"4. SMOTE Algorithm","text":"<p>Synthetic Minority Oversampling Technique creates new minority samples: \\(\\(x_{new} = x_i + \\lambda \\cdot (x_{neighbor} - x_i)\\)\\)</p> <p>Where \\(\\lambda \\in [0,1]\\) is a random number and \\(x_{neighbor}\\) is a randomly chosen k-nearest neighbor.</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#implementation-using-libraries","title":"=\" Implementation using Libraries","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#scikit-learn-and-imbalanced-learn-implementation","title":"Scikit-learn and Imbalanced-learn Implementation","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                           precision_recall_curve, roc_auc_score, roc_curve,\n                           precision_score, recall_score, f1_score)\nfrom imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Generate imbalanced dataset\ndef create_imbalanced_dataset(n_samples=10000, weights=[0.99, 0.01], random_state=42):\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=20,\n        n_informative=10,\n        n_redundant=5,\n        n_clusters_per_class=1,\n        weights=weights,\n        random_state=random_state\n    )\n    return X, y\n\n# Create dataset\nX, y = create_imbalanced_dataset()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    stratify=y, random_state=42)\n\nprint(\"Original Dataset Distribution:\")\nprint(f\"Total samples: {len(y)}\")\nprint(f\"Class distribution: {Counter(y)}\")\nprint(f\"Imbalance ratio: {Counter(y)[1] / Counter(y)[0]:.3f}\")\n\n# Helper function for evaluation\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n    \"\"\"Comprehensive model evaluation for imbalanced datasets\"\"\"\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n\n    print(f\"\\n{model_name} Results:\")\n    print(\"-\" * 50)\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    if y_proba is not None:\n        auc_score = roc_auc_score(y_test, y_proba)\n        print(f\"AUC-ROC Score: {auc_score:.3f}\")\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n    print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n\n    return {\n        'precision': precision_score(y_test, y_pred),\n        'recall': recall_score(y_test, y_pred),\n        'f1': f1_score(y_test, y_pred),\n        'auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None\n    }\n\n# 1. Baseline model without handling imbalance\nprint(\"=\"*60)\nprint(\"1. BASELINE MODEL (No Imbalance Handling)\")\nprint(\"=\"*60)\n\nbaseline_model = LogisticRegression(random_state=42)\nbaseline_results = evaluate_model(baseline_model, X_train, X_test, y_train, y_test, \n                                \"Baseline Logistic Regression\")\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#sampling-techniques","title":"Sampling Techniques","text":"<pre><code>print(\"\\n\" + \"=\"*60)\nprint(\"2. SAMPLING TECHNIQUES\")\nprint(\"=\"*60)\n\n# Dictionary to store all resampling techniques\nsampling_techniques = {\n    'Random Over-sampling': RandomOverSampler(random_state=42),\n    'SMOTE': SMOTE(random_state=42),\n    'ADASYN': ADASYN(random_state=42),\n    'Random Under-sampling': RandomUnderSampler(random_state=42),\n    'Edited Nearest Neighbours': EditedNearestNeighbours(),\n    'SMOTE + ENN': SMOTEENN(random_state=42),\n    'SMOTE + Tomek': SMOTETomek(random_state=42)\n}\n\nsampling_results = {}\n\nfor name, sampler in sampling_techniques.items():\n    print(f\"\\n{name}:\")\n    print(\"-\" * 40)\n\n    try:\n        # Apply sampling\n        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n\n        print(f\"Original distribution: {Counter(y_train)}\")\n        print(f\"Resampled distribution: {Counter(y_resampled)}\")\n        print(f\"Resampling ratio: {len(y_resampled) / len(y_train):.2f}\")\n\n        # Train and evaluate model\n        model = LogisticRegression(random_state=42)\n        results = evaluate_model(model, X_resampled, X_test, y_resampled, y_test, name)\n        sampling_results[name] = results\n\n    except Exception as e:\n        print(f\"Error with {name}: {str(e)}\")\n        sampling_results[name] = {'precision': 0, 'recall': 0, 'f1': 0, 'auc': 0}\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#cost-sensitive-learning","title":"Cost-Sensitive Learning","text":"<pre><code>print(\"\\n\" + \"=\"*60)\nprint(\"3. COST-SENSITIVE LEARNING\")\nprint(\"=\"*60)\n\n# Calculate class weights\nfrom sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n\nprint(f\"Computed class weights: {class_weight_dict}\")\n\n# Models with different class weights\ncost_sensitive_models = {\n    'Balanced Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42),\n    'Balanced Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n    'Balanced RF (imblearn)': BalancedRandomForestClassifier(random_state=42)\n}\n\ncost_sensitive_results = {}\n\nfor name, model in cost_sensitive_models.items():\n    results = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n    cost_sensitive_results[name] = results\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#advanced-ensemble-methods","title":"Advanced Ensemble Methods","text":"<pre><code>print(\"\\n\" + \"=\"*60)\nprint(\"4. ENSEMBLE METHODS FOR IMBALANCED DATA\")\nprint(\"=\"*60)\n\nfrom imblearn.ensemble import BalancedBaggingClassifier, EasyEnsembleClassifier, RUSBoostClassifier\n\nensemble_models = {\n    'Balanced Bagging': BalancedBaggingClassifier(random_state=42),\n    'Easy Ensemble': EasyEnsembleClassifier(random_state=42),\n    'RUSBoost': RUSBoostClassifier(random_state=42)\n}\n\nensemble_results = {}\n\nfor name, model in ensemble_models.items():\n    results = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n    ensemble_results[name] = results\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#comprehensive-results-comparison","title":"Comprehensive Results Comparison","text":"<pre><code>print(\"\\n\" + \"=\"*60)\nprint(\"5. COMPREHENSIVE RESULTS COMPARISON\")\nprint(\"=\"*60)\n\n# Combine all results\nall_results = {\n    'Baseline': baseline_results,\n    **sampling_results,\n    **cost_sensitive_results,\n    **ensemble_results\n}\n\n# Create comparison DataFrame\nresults_df = pd.DataFrame(all_results).T\nresults_df = results_df.round(3)\n\nprint(\"Performance Comparison:\")\nprint(results_df.sort_values('f1', ascending=False))\n\n# Visualize results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\nmetrics = ['precision', 'recall', 'f1', 'auc']\ntitles = ['Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n\nfor i, (metric, title) in enumerate(zip(metrics, titles)):\n    ax = axes[i//2, i%2]\n\n    # Filter out None values for AUC\n    plot_data = results_df[metric].dropna()\n\n    plot_data.plot(kind='bar', ax=ax, color='skyblue')\n    ax.set_title(f'{title} Comparison')\n    ax.set_ylabel(title)\n    ax.tick_params(axis='x', rotation=45)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find best performing models\nprint(\"\\nBest Models by Metric:\")\nprint(\"-\" * 30)\nfor metric in ['precision', 'recall', 'f1', 'auc']:\n    if metric in results_df.columns:\n        best_model = results_df[metric].idxmax()\n        best_score = results_df[metric].max()\n        print(f\"{metric.upper():10s}: {best_model} ({best_score:.3f})\")\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#from-scratch-implementation","title":"\ufffd\u000f From Scratch Implementation","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#smote-implementation-from-scratch","title":"SMOTE Implementation from Scratch","text":"<pre><code>import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\n\nclass SMOTEFromScratch:\n    \"\"\"\n    Synthetic Minority Oversampling Technique (SMOTE) implementation from scratch\n    \"\"\"\n\n    def __init__(self, k_neighbors=5, random_state=42):\n        self.k_neighbors = k_neighbors\n        self.random_state = random_state\n        np.random.seed(random_state)\n\n    def fit_resample(self, X, y):\n        \"\"\"\n        Apply SMOTE to balance the dataset\n        \"\"\"\n        # Separate majority and minority classes\n        unique_classes = np.unique(y)\n        if len(unique_classes) != 2:\n            raise ValueError(\"SMOTE currently supports only binary classification\")\n\n        class_counts = [(cls, np.sum(y == cls)) for cls in unique_classes]\n        class_counts.sort(key=lambda x: x[1])  # Sort by count\n\n        minority_class, minority_count = class_counts[0]\n        majority_class, majority_count = class_counts[1]\n\n        print(f\"Original distribution - Minority class {minority_class}: {minority_count}, \"\n              f\"Majority class {majority_class}: {majority_count}\")\n\n        # Extract minority class samples\n        minority_mask = (y == minority_class)\n        X_minority = X[minority_mask]\n\n        # Calculate number of synthetic samples needed\n        n_synthetic = majority_count - minority_count\n\n        # Generate synthetic samples\n        X_synthetic = self._generate_synthetic_samples(X_minority, n_synthetic)\n        y_synthetic = np.full(n_synthetic, minority_class)\n\n        # Combine original and synthetic data\n        X_resampled = np.vstack([X, X_synthetic])\n        y_resampled = np.hstack([y, y_synthetic])\n\n        print(f\"Generated {n_synthetic} synthetic samples\")\n        print(f\"New distribution - Class {minority_class}: {majority_count}, \"\n              f\"Class {majority_class}: {majority_count}\")\n\n        return X_resampled, y_resampled\n\n    def _generate_synthetic_samples(self, X_minority, n_synthetic):\n        \"\"\"\n        Generate synthetic samples using SMOTE algorithm\n        \"\"\"\n        n_minority = X_minority.shape[0]\n        n_features = X_minority.shape[1]\n\n        # Find k-nearest neighbors for each minority sample\n        nn = NearestNeighbors(n_neighbors=self.k_neighbors + 1)  # +1 to exclude self\n        nn.fit(X_minority)\n\n        synthetic_samples = []\n\n        for _ in range(n_synthetic):\n            # Randomly select a minority sample\n            random_idx = np.random.randint(0, n_minority)\n            sample = X_minority[random_idx]\n\n            # Find k-nearest neighbors\n            distances, indices = nn.kneighbors([sample])\n            neighbor_indices = indices[0][1:]  # Exclude the sample itself\n\n            # Randomly select one of the k-nearest neighbors\n            neighbor_idx = np.random.choice(neighbor_indices)\n            neighbor = X_minority[neighbor_idx]\n\n            # Generate synthetic sample along the line between sample and neighbor\n            lambda_val = np.random.random()  # Random value between 0 and 1\n            synthetic_sample = sample + lambda_val * (neighbor - sample)\n\n            synthetic_samples.append(synthetic_sample)\n\n        return np.array(synthetic_samples)\n\n    def plot_samples(self, X_original, y_original, X_resampled, y_resampled, \n                    feature_idx=[0, 1]):\n        \"\"\"\n        Visualize original vs resampled data (for 2D visualization)\n        \"\"\"\n        if X_original.shape[1] &lt; 2:\n            print(\"Need at least 2 features for 2D visualization\")\n            return\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n        # Plot original data\n        for class_val in np.unique(y_original):\n            mask = y_original == class_val\n            ax1.scatter(X_original[mask, feature_idx[0]], \n                       X_original[mask, feature_idx[1]], \n                       label=f'Class {class_val}', alpha=0.7)\n        ax1.set_title('Original Data')\n        ax1.set_xlabel(f'Feature {feature_idx[0]}')\n        ax1.set_ylabel(f'Feature {feature_idx[1]}')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n\n        # Plot resampled data\n        for class_val in np.unique(y_resampled):\n            mask = y_resampled == class_val\n            ax2.scatter(X_resampled[mask, feature_idx[0]], \n                       X_resampled[mask, feature_idx[1]], \n                       label=f'Class {class_val}', alpha=0.7)\n        ax2.set_title('After SMOTE')\n        ax2.set_xlabel(f'Feature {feature_idx[0]}')\n        ax2.set_ylabel(f'Feature {feature_idx[1]}')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n# Custom Cost-Sensitive Logistic Regression\nclass CostSensitiveLogisticRegression:\n    \"\"\"\n    Logistic Regression with custom cost-sensitive learning\n    \"\"\"\n\n    def __init__(self, learning_rate=0.01, max_iterations=1000, cost_ratio=1.0):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.cost_ratio = cost_ratio  # Cost ratio for minority class\n        self.weights = None\n        self.bias = None\n        self.cost_history = []\n\n    def _sigmoid(self, z):\n        \"\"\"Sigmoid activation function with numerical stability\"\"\"\n        z = np.clip(z, -250, 250)  # Prevent overflow\n        return 1 / (1 + np.exp(-z))\n\n    def _compute_cost(self, y_true, y_pred):\n        \"\"\"Compute cost-sensitive logistic loss\"\"\"\n        # Avoid log(0) by adding small epsilon\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # Standard logistic loss\n        standard_loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n        # Apply cost weighting\n        cost_weights = np.where(y_true == 1, self.cost_ratio, 1.0)\n        weighted_loss = cost_weights * standard_loss\n\n        return np.mean(weighted_loss)\n\n    def fit(self, X, y):\n        \"\"\"Train the cost-sensitive logistic regression model\"\"\"\n        n_samples, n_features = X.shape\n\n        # Initialize weights and bias\n        self.weights = np.random.normal(0, 0.01, n_features)\n        self.bias = 0\n\n        # Gradient descent\n        for i in range(self.max_iterations):\n            # Forward pass\n            z = X @ self.weights + self.bias\n            y_pred = self._sigmoid(z)\n\n            # Compute cost\n            cost = self._compute_cost(y, y_pred)\n            self.cost_history.append(cost)\n\n            # Compute gradients with cost weighting\n            cost_weights = np.where(y == 1, self.cost_ratio, 1.0)\n            weighted_errors = cost_weights * (y_pred - y)\n\n            dw = (1/n_samples) * X.T @ weighted_errors\n            db = (1/n_samples) * np.sum(weighted_errors)\n\n            # Update parameters\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n            # Print progress\n            if i % 100 == 0:\n                print(f\"Iteration {i}, Cost: {cost:.4f}\")\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities\"\"\"\n        z = X @ self.weights + self.bias\n        return self._sigmoid(z)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"Make binary predictions\"\"\"\n        probabilities = self.predict_proba(X)\n        return (probabilities &gt;= threshold).astype(int)\n\n    def plot_cost_history(self):\n        \"\"\"Plot the cost function over iterations\"\"\"\n        plt.figure(figsize=(10, 6))\n        plt.plot(self.cost_history)\n        plt.title('Cost-Sensitive Logistic Regression - Training Cost')\n        plt.xlabel('Iterations')\n        plt.ylabel('Cost')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n# Example usage of custom implementations\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*60)\n    print(\"CUSTOM IMPLEMENTATIONS EXAMPLE\")\n    print(\"=\"*60)\n\n    # Generate imbalanced dataset for testing\n    from sklearn.datasets import make_classification\n    X_demo, y_demo = make_classification(n_samples=1000, n_features=2, \n                                        n_redundant=0, n_informative=2,\n                                        n_clusters_per_class=1, \n                                        weights=[0.9, 0.1], random_state=42)\n\n    X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n        X_demo, y_demo, test_size=0.2, stratify=y_demo, random_state=42\n    )\n\n    print(f\"Demo dataset - Training distribution: {Counter(y_train_demo)}\")\n\n    # Test custom SMOTE\n    print(\"\\n1. Testing Custom SMOTE Implementation:\")\n    print(\"-\" * 40)\n\n    smote_custom = SMOTEFromScratch(k_neighbors=5, random_state=42)\n    X_smote, y_smote = smote_custom.fit_resample(X_train_demo, y_train_demo)\n\n    # Visualize SMOTE results\n    smote_custom.plot_samples(X_train_demo, y_train_demo, X_smote, y_smote)\n\n    # Test custom cost-sensitive logistic regression\n    print(\"\\n2. Testing Custom Cost-Sensitive Logistic Regression:\")\n    print(\"-\" * 50)\n\n    # Calculate appropriate cost ratio\n    minority_count = np.sum(y_train_demo == 1)\n    majority_count = np.sum(y_train_demo == 0)\n    cost_ratio = majority_count / minority_count\n\n    print(f\"Calculated cost ratio: {cost_ratio:.2f}\")\n\n    # Train custom model\n    custom_model = CostSensitiveLogisticRegression(\n        learning_rate=0.1, \n        max_iterations=1000, \n        cost_ratio=cost_ratio\n    )\n\n    custom_model.fit(X_train_demo, y_train_demo)\n\n    # Make predictions\n    y_pred_custom = custom_model.predict(X_test_demo)\n    y_proba_custom = custom_model.predict_proba(X_test_demo)\n\n    # Evaluate custom model\n    print(\"\\nCustom Model Results:\")\n    print(\"Classification Report:\")\n    from sklearn.metrics import classification_report, roc_auc_score\n    print(classification_report(y_test_demo, y_pred_custom))\n    print(f\"AUC-ROC: {roc_auc_score(y_test_demo, y_proba_custom):.3f}\")\n\n    # Plot cost history\n    custom_model.plot_cost_history()\n</code></pre>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#assumptions-and-limitations","title":"\ufffd\u000f Assumptions and Limitations","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#assumptions-for-imbalanced-data-techniques","title":"Assumptions for Imbalanced Data Techniques","text":"<p>SMOTE Assumptions: - Minority class samples form meaningful clusters - Linear interpolation between samples creates realistic examples - Local neighborhood structure is preserved - Features are continuous (not categorical)</p> <p>Cost-Sensitive Learning Assumptions: - Misclassification costs can be accurately estimated - Cost ratios remain constant across different regions of feature space - Business/domain costs can be translated to algorithmic costs</p> <p>Undersampling Assumptions: - Removed majority samples are truly redundant - Information loss is acceptable for balance - Remaining samples are representative of the full distribution</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#limitations-and-challenges","title":"Limitations and Challenges","text":"<p>SMOTE Limitations: - Overgeneralization: May create synthetic samples in inappropriate regions - Curse of dimensionality: Less effective in high-dimensional spaces - Categorical features: Not directly applicable to categorical variables - Noise amplification: May amplify noise in minority class data</p> <p>Cost-Sensitive Limitations: - Cost estimation: Difficult to determine appropriate cost ratios - Class overlap: May not work well when classes have significant overlap - Imbalanced validation: Standard cross-validation may not be appropriate</p> <p>General Limitations: - Evaluation challenges: Standard metrics can be misleading - Model selection: Need specialized techniques for hyperparameter tuning - Real-world deployment: Performance may degrade in production - Temporal drift: Class distributions may change over time</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#comparison-with-alternative-approaches","title":"Comparison with Alternative Approaches","text":"<p>Sampling vs. Algorithmic Solutions: - Sampling: Modifies data distribution, works with any algorithm - Algorithmic: Modifies algorithm behavior, preserves original data</p> <p>Ensemble vs. Single Model: - Ensemble: More robust but complex and harder to interpret - Single Model: Simpler but may be more sensitive to imbalance</p> <p>Threshold Moving vs. Data Modification: - Threshold: Simple post-processing approach - Data Modification: Changes training process but may introduce artifacts</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#interview-questions","title":"=\ufffd Interview Questions","text":"1. What is the difference between imbalanced and skewed data? How do you detect each? <p>Answer: - Imbalanced Data: Unequal class distribution in target variable   - Detection: Check class counts, calculate imbalance ratio   - Example: 95% normal transactions, 5% fraud - Skewed Data: Non-normal distribution in features   - Detection: Histogram analysis, skewness coefficient   - Example: Income distribution (most people earn moderate amounts, few earn very high) - Key Differences:   - Imbalanced affects target variable, skewed affects features   - Different solutions: sampling for imbalance, transformation for skewness   - Different evaluation challenges - Detection Methods:   - Imbalanced: <code>Counter(y)</code>, class distribution plots   - Skewed: <code>scipy.stats.skew()</code>, Q-Q plots, histograms</p> 2. Why is accuracy a poor metric for imbalanced datasets? What metrics should you use instead? <p>Answer: - Why Accuracy Fails:   - Can achieve high accuracy by always predicting majority class   - Example: 99% accuracy on 99:1 dataset by predicting majority class   - Doesn't reflect performance on minority class - Better Metrics:   - Precision: TP/(TP+FP) - How many predicted positives are actually positive   - Recall/Sensitivity: TP/(TP+FN) - How many actual positives were found   - F1-Score: Harmonic mean of precision and recall   - AUC-ROC: Area under ROC curve, threshold-independent   - AUC-PR: Area under Precision-Recall curve, better for severe imbalance - Confusion Matrix Analysis:   - Focus on True Positives and False Negatives for minority class   - Consider business cost of different error types - Stratified Evaluation:   - Use stratified cross-validation   - Report per-class metrics separately</p> 3. Explain SMOTE algorithm. What are its advantages and disadvantages? <p>Answer: - SMOTE Algorithm:   - Finds k-nearest neighbors of minority samples   - Creates synthetic samples along lines between samples and neighbors   - Formula: new_sample = sample + \ufffd \ufffd (neighbor - sample), \ufffd \b [0,1] - Advantages:   - Increases minority class size without exact duplication   - Considers local neighborhood structure   - Works well with continuous features   - Reduces overfitting compared to simple oversampling - Disadvantages:   - Can create synthetic samples in majority class regions   - Assumes linear relationships between features   - Doesn't work well with categorical features   - May amplify noise in the data   - Can lead to overgeneralization - Variants:   - Borderline-SMOTE: Focuses on borderline samples   - ADASYN: Adaptive density-based approach   - SMOTE-ENN/Tomek: Combines oversampling with undersampling</p> 4. When would you use undersampling vs oversampling? What are the tradeoffs? <p>Answer: - Undersampling When:   - Large dataset with sufficient majority samples   - Computational resources are limited   - Majority class has redundant/noisy samples   - Training time is a major constraint - Oversampling When:   - Small dataset where information loss is critical   - Minority class is very small   - Sufficient computational resources available   - Want to preserve all original information - Tradeoffs:   - Undersampling: Faster training, information loss, potential underfitting   - Oversampling: Preserves information, longer training, potential overfitting - Hybrid Approaches:   - SMOTE + Tomek: Oversample then clean borderline samples   - SMOTE + ENN: Oversample then remove noisy samples - Decision Framework:   - Consider data size, computational budget, domain expertise   - Try both approaches and compare validation performance</p> 5. How do you evaluate a model on imbalanced data? What cross-validation strategy should you use? <p>Answer: - Evaluation Strategy:   - Stratified Cross-Validation: Maintains class distribution in each fold   - Multiple Metrics: Use precision, recall, F1, AUC-ROC, AUC-PR   - Business Metrics: Consider actual costs of different error types   - Threshold Analysis: Plot precision-recall curves, find optimal threshold - Cross-Validation Considerations:   - Always use stratified splits to maintain class balance   - Consider time-series splits for temporal data   - Be careful with very small minority classes (may have zero samples in some folds) - Reporting Guidelines:   - Report confidence intervals for metrics   - Show confusion matrices for each fold   - Analyze per-class performance separately   - Consider statistical significance tests - Validation Pitfalls:   - Don't use random splits without stratification   - Don't rely solely on accuracy   - Don't ignore class-specific performance</p> 6. What is cost-sensitive learning and how does it help with imbalanced data? <p>Answer: - Cost-Sensitive Learning:   - Assigns different costs to different types of misclassifications   - Modifies loss function to penalize minority class errors more heavily   - Can be applied at algorithm level or through class weights - Implementation Methods:   - Class Weights: Multiply loss by class-specific weights   - Cost Matrix: Define explicit costs for each error type   - Threshold Adjustment: Move decision boundary based on costs - Benefits:   - Directly incorporates business/domain costs   - Can be applied to most algorithms   - More principled than arbitrary sampling - Challenges:   - Difficult to estimate appropriate costs   - May not work well with overlapping classes   - Requires domain expertise for cost determination - Example: In medical diagnosis, false negative (missing disease) might cost 100x more than false positive (unnecessary test)</p> 7. How would you handle a dataset with 99.9% majority class and 0.1% minority class? <p>Answer: - Severe Imbalance Strategies:   - Anomaly Detection: Treat as one-class problem instead of classification   - Ensemble Methods: Use specialized ensemble techniques (BalancedBagging)   - Threshold Optimization: Move decision boundary toward minority class   - Cost-Sensitive Learning: High penalty for minority class errors - Sampling Approaches:   - Conservative Oversampling: Moderate SMOTE to avoid overfitting   - Informed Undersampling: Remove only clearly redundant majority samples   - Hybrid Methods: Combine multiple techniques carefully - Evaluation Considerations:   - Focus on AUC-PR rather than AUC-ROC   - Use stratified sampling with large number of folds   - Consider using bootstrap validation - Alternative Formulations:   - Treat as ranking problem   - Use one-class SVM or isolation forest   - Consider active learning to find more minority examples - Business Considerations:   - Understand the cost of false negatives vs false positives   - Consider if problem needs to be solved as classification or detection</p> 8. What are the challenges in deploying models trained on imbalanced data in production? <p>Answer: - Distribution Drift:   - Class distributions may change over time   - Need monitoring systems to detect drift   - May require model retraining or threshold adjustment - Performance Degradation:   - Synthetic samples may not reflect real-world complexity   - Overfitted models may perform poorly on new data   - Need robust validation strategies - Threshold Selection:   - Optimal threshold may change in production   - Need business-driven threshold selection   - Consider implementing dynamic thresholds - Monitoring Challenges:   - Standard accuracy metrics are misleading   - Need to monitor precision, recall separately   - Set up alerts for minority class performance drops - Mitigation Strategies:   - Implement A/B testing for model updates   - Use ensemble models for robustness   - Maintain feedback loops for continuous learning   - Regular model retraining with fresh data</p> 9. How do you choose between different sampling techniques (SMOTE, ADASYN, Random sampling, etc.)? <p>Answer: - Selection Criteria:   - Data characteristics: Size, dimensionality, noise level   - Computational constraints: Training time, memory requirements   - Domain knowledge: Understanding of feature relationships - Technique Guidelines:   - Random Oversampling: Quick baseline, risk of overfitting   - SMOTE: Good for continuous features, assumes linear relationships   - ADASYN: Better for varying density distributions   - Borderline-SMOTE: When minority class has clear boundaries   - Random Undersampling: Large datasets, computational constraints - Experimental Approach:   - Try multiple techniques with cross-validation   - Compare using appropriate metrics (F1, AUC-PR)   - Consider ensemble of different sampling approaches - Validation Strategy:   - Use stratified CV to compare techniques   - Test on holdout set with original distribution   - Consider robustness across different random seeds - Practical Considerations:   - Implementation complexity and maintenance   - Interpretability requirements   - Integration with existing ML pipelines</p> 10. Describe a real-world scenario where you had to deal with severely imbalanced data and explain your approach. <p>Answer: This question expects a detailed walkthrough of a practical solution. Here's an example framework:</p> <ul> <li>Problem Description:</li> <li>\"Fraud detection with 0.1% fraud rate in credit card transactions\"</li> <li>Business impact: $1M loss per undetected fraud, $50 cost per false alarm</li> <li>Initial Analysis:</li> <li>Analyzed data distribution, feature importance</li> <li>Identified temporal patterns, seasonal effects</li> <li>Explored feature engineering opportunities</li> <li>Solution Approach:</li> <li>Phase 1: Established baseline with cost-sensitive logistic regression</li> <li>Phase 2: Applied SMOTE with careful validation</li> <li>Phase 3: Implemented ensemble with threshold optimization</li> <li>Evaluation Strategy:</li> <li>Time-based validation splits (no data leakage)</li> <li>Focused on precision-recall curves</li> <li>Business-driven threshold selection ($50 vs $1M cost)</li> <li>Production Considerations:</li> <li>Real-time scoring requirements</li> <li>Model monitoring and drift detection</li> <li>A/B testing framework for improvements</li> <li>Results and Learning:</li> <li>Achieved 95% recall at 2% precision (acceptable business tradeoff)</li> <li>Learned importance of domain expertise in feature engineering</li> <li>Ongoing monitoring revealed seasonal drift patterns</li> </ul>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#examples","title":"&gt;\ufffd Examples","text":""},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#real-world-example-credit-fraud-detection","title":"Real-World Example: Credit Fraud Detection","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nimport seaborn as sns\n\n# Simulate credit fraud dataset\ndef create_fraud_dataset():\n    \"\"\"Create a realistic fraud detection dataset\"\"\"\n    np.random.seed(42)\n\n    # Create imbalanced dataset\n    X, y = make_classification(\n        n_samples=50000,\n        n_features=30,\n        n_informative=20,\n        n_redundant=5,\n        n_clusters_per_class=2,\n        weights=[0.999, 0.001],  # 0.1% fraud rate\n        random_state=42\n    )\n\n    # Create meaningful feature names\n    feature_names = [\n        'transaction_amount', 'account_age_days', 'num_transactions_day',\n        'avg_transaction_amount', 'time_since_last_transaction', 'merchant_risk_score',\n        'geographic_risk', 'device_risk_score', 'velocity_1hr', 'velocity_24hr'\n    ] + [f'feature_{i}' for i in range(10, 30)]\n\n    return pd.DataFrame(X, columns=feature_names), y\n\n# Create the fraud dataset\nprint(\"Creating Fraud Detection Dataset...\")\nX_fraud, y_fraud = create_fraud_dataset()\n\nprint(f\"Dataset shape: {X_fraud.shape}\")\nprint(f\"Fraud cases: {np.sum(y_fraud)} ({np.sum(y_fraud)/len(y_fraud)*100:.3f}%)\")\nprint(f\"Normal cases: {np.sum(y_fraud == 0)} ({np.sum(y_fraud == 0)/len(y_fraud)*100:.3f}%)\")\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_fraud, y_fraud, test_size=0.2, stratify=y_fraud, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"\\nFraud Detection: Comprehensive Analysis\")\nprint(\"=\"*60)\n\n# 1. Baseline Model\nprint(\"\\n1. BASELINE MODEL (No Imbalance Handling)\")\nprint(\"-\"*50)\n\nbaseline_model = LogisticRegression(random_state=42)\nbaseline_model.fit(X_train_scaled, y_train)\n\ny_pred_baseline = baseline_model.predict(X_test_scaled)\ny_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"Baseline Results:\")\nprint(classification_report(y_test, y_pred_baseline))\n\n# Calculate business metrics\ndef calculate_business_metrics(y_true, y_pred, cost_fn=1000000, cost_fp=50):\n    \"\"\"Calculate business-relevant metrics for fraud detection\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n\n    total_cost = fn * cost_fn + fp * cost_fp  # Cost of false negatives + false positives\n    savings = tp * cost_fn  # Money saved by catching fraud\n    net_savings = savings - total_cost\n\n    return {\n        'total_cost': total_cost,\n        'savings': savings,\n        'net_savings': net_savings,\n        'cost_per_transaction': total_cost / len(y_true)\n    }\n\nbaseline_business = calculate_business_metrics(y_test, y_pred_baseline)\nprint(f\"\\nBaseline Business Metrics:\")\nprint(f\"Total Cost: ${baseline_business['total_cost']:,.2f}\")\nprint(f\"Savings: ${baseline_business['savings']:,.2f}\")\nprint(f\"Net Savings: ${baseline_business['net_savings']:,.2f}\")\n\n# 2. SMOTE Approach\nprint(\"\\n2. SMOTE APPROACH\")\nprint(\"-\"*30)\n\nsmote_pipeline = ImbPipeline([\n    ('sampling', SMOTE(random_state=42)),\n    ('classifier', LogisticRegression(random_state=42))\n])\n\nsmote_pipeline.fit(X_train_scaled, y_train)\ny_pred_smote = smote_pipeline.predict(X_test_scaled)\ny_proba_smote = smote_pipeline.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"SMOTE Results:\")\nprint(classification_report(y_test, y_pred_smote))\n\nsmote_business = calculate_business_metrics(y_test, y_pred_smote)\nprint(f\"\\nSMOTE Business Metrics:\")\nprint(f\"Total Cost: ${smote_business['total_cost']:,.2f}\")\nprint(f\"Savings: ${smote_business['savings']:,.2f}\")\nprint(f\"Net Savings: ${smote_business['net_savings']:,.2f}\")\n\n# 3. Cost-Sensitive Approach\nprint(\"\\n3. COST-SENSITIVE APPROACH\")\nprint(\"-\"*35)\n\n# Calculate class weights based on business costs\nfraud_cases = np.sum(y_train == 1)\nnormal_cases = np.sum(y_train == 0)\ncost_ratio = (cost_fn / cost_fp) * (normal_cases / fraud_cases)\n\ncost_sensitive_model = LogisticRegression(\n    class_weight={0: 1, 1: cost_ratio}, \n    random_state=42\n)\ncost_sensitive_model.fit(X_train_scaled, y_train)\n\ny_pred_cost = cost_sensitive_model.predict(X_test_scaled)\ny_proba_cost = cost_sensitive_model.predict_proba(X_test_scaled)[:, 1]\n\nprint(\"Cost-Sensitive Results:\")\nprint(classification_report(y_test, y_pred_cost))\n\ncost_business = calculate_business_metrics(y_test, y_pred_cost)\nprint(f\"\\nCost-Sensitive Business Metrics:\")\nprint(f\"Total Cost: ${cost_business['total_cost']:,.2f}\")\nprint(f\"Savings: ${cost_business['savings']:,.2f}\")\nprint(f\"Net Savings: ${cost_business['net_savings']:,.2f}\")\n\n# 4. Threshold Optimization\nprint(\"\\n4. THRESHOLD OPTIMIZATION\")\nprint(\"-\"*30)\n\ndef find_optimal_threshold(y_true, y_proba, cost_fn=1000000, cost_fp=50):\n    \"\"\"Find optimal threshold based on business costs\"\"\"\n    thresholds = np.arange(0.01, 1.0, 0.01)\n    best_threshold = 0.5\n    best_net_savings = float('-inf')\n\n    results = []\n\n    for threshold in thresholds:\n        y_pred_thresh = (y_proba &gt;= threshold).astype(int)\n        business_metrics = calculate_business_metrics(y_true, y_pred_thresh, cost_fn, cost_fp)\n\n        results.append({\n            'threshold': threshold,\n            'net_savings': business_metrics['net_savings'],\n            'total_cost': business_metrics['total_cost']\n        })\n\n        if business_metrics['net_savings'] &gt; best_net_savings:\n            best_net_savings = business_metrics['net_savings']\n            best_threshold = threshold\n\n    return best_threshold, best_net_savings, pd.DataFrame(results)\n\n# Find optimal threshold for cost-sensitive model\noptimal_threshold, optimal_savings, threshold_results = find_optimal_threshold(\n    y_test, y_proba_cost\n)\n\nprint(f\"Optimal Threshold: {optimal_threshold:.3f}\")\nprint(f\"Optimal Net Savings: ${optimal_savings:,.2f}\")\n\n# Apply optimal threshold\ny_pred_optimal = (y_proba_cost &gt;= optimal_threshold).astype(int)\nprint(\"\\nOptimal Threshold Results:\")\nprint(classification_report(y_test, y_pred_optimal))\n\n# 5. Comprehensive Visualization\nprint(\"\\n5. COMPREHENSIVE VISUALIZATION\")\nprint(\"-\"*35)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Plot 1: Class Distribution\nax1 = axes[0, 0]\nclass_counts = [np.sum(y_fraud == 0), np.sum(y_fraud == 1)]\nax1.bar(['Normal', 'Fraud'], class_counts, color=['lightblue', 'red'], alpha=0.7)\nax1.set_title('Original Class Distribution')\nax1.set_ylabel('Number of Samples')\nax1.set_yscale('log')  # Log scale to show both classes clearly\n\n# Plot 2: ROC Curves\nax2 = axes[0, 1]\nmodels = {\n    'Baseline': y_proba_baseline,\n    'SMOTE': y_proba_smote,\n    'Cost-Sensitive': y_proba_cost\n}\n\nfor name, y_proba in models.items():\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc_score = roc_auc_score(y_test, y_proba)\n    ax2.plot(fpr, tpr, label=f'{name} (AUC={auc_score:.3f})')\n\nax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nax2.set_xlabel('False Positive Rate')\nax2.set_ylabel('True Positive Rate')\nax2.set_title('ROC Curves Comparison')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Precision-Recall Curves\nax3 = axes[0, 2]\nfor name, y_proba in models.items():\n    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n    auc_pr = auc(recall, precision)\n    ax3.plot(recall, precision, label=f'{name} (AUC-PR={auc_pr:.3f})')\n\nax3.set_xlabel('Recall')\nax3.set_ylabel('Precision')\nax3.set_title('Precision-Recall Curves')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Business Metrics Comparison\nax4 = axes[1, 0]\nmethods = ['Baseline', 'SMOTE', 'Cost-Sensitive', 'Optimal Threshold']\nnet_savings = [\n    baseline_business['net_savings'],\n    smote_business['net_savings'], \n    cost_business['net_savings'],\n    optimal_savings\n]\n\ncolors = ['red' if x &lt; 0 else 'green' for x in net_savings]\nbars = ax4.bar(methods, net_savings, color=colors, alpha=0.7)\nax4.set_title('Net Savings Comparison')\nax4.set_ylabel('Net Savings ($)')\nax4.tick_params(axis='x', rotation=45)\n\n# Add value labels on bars\nfor bar, value in zip(bars, net_savings):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'${value:,.0f}', ha='center', va='bottom' if value &gt; 0 else 'top')\n\n# Plot 5: Threshold Analysis\nax5 = axes[1, 1]\nax5.plot(threshold_results['threshold'], threshold_results['net_savings'])\nax5.axvline(x=optimal_threshold, color='red', linestyle='--', \n           label=f'Optimal: {optimal_threshold:.3f}')\nax5.set_xlabel('Classification Threshold')\nax5.set_ylabel('Net Savings ($)')\nax5.set_title('Threshold vs Net Savings')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# Plot 6: Confusion Matrix for Optimal Model\nax6 = axes[1, 2]\ncm_optimal = confusion_matrix(y_test, y_pred_optimal)\nsns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Blues', ax=ax6)\nax6.set_title('Optimal Model Confusion Matrix')\nax6.set_xlabel('Predicted')\nax6.set_ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n# Final Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL SUMMARY - FRAUD DETECTION CASE STUDY\")\nprint(\"=\"*60)\n\nsummary_data = {\n    'Method': ['Baseline', 'SMOTE', 'Cost-Sensitive', 'Optimal Threshold'],\n    'Precision': [\n        precision_score(y_test, y_pred_baseline),\n        precision_score(y_test, y_pred_smote),\n        precision_score(y_test, y_pred_cost),\n        precision_score(y_test, y_pred_optimal)\n    ],\n    'Recall': [\n        recall_score(y_test, y_pred_baseline),\n        recall_score(y_test, y_pred_smote),\n        recall_score(y_test, y_pred_cost),\n        recall_score(y_test, y_pred_optimal)\n    ],\n    'F1-Score': [\n        f1_score(y_test, y_pred_baseline),\n        f1_score(y_test, y_pred_smote),\n        f1_score(y_test, y_pred_cost),\n        f1_score(y_test, y_pred_optimal)\n    ],\n    'Net Savings': net_savings\n}\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\nPerformance Summary:\")\nprint(summary_df.round(3).to_string(index=False))\n\nprint(f\"\\nKey Insights:\")\nprint(f\"\" Baseline model had high precision but very low recall\")\nprint(f\"\" SMOTE improved recall but at the cost of precision\")\nprint(f\"\" Cost-sensitive approach balanced precision and recall better\")\nprint(f\"\" Optimal threshold maximized business value\")\nprint(f\"\" Best approach achieved ${optimal_savings:,.0f} net savings\")\n\nprint(f\"\\nBusiness Recommendations:\")\nprint(f\"\" Deploy cost-sensitive model with optimal threshold ({optimal_threshold:.3f})\")\nprint(f\"\" Monitor precision-recall tradeoff in production\")\nprint(f\"\" Implement real-time threshold adjustment based on costs\")\nprint(f\"\" Regular model retraining as fraud patterns evolve\")\n</code></pre> <p>Key Takeaways from the Example: - Business Context Matters: The optimal approach depends on the actual costs of different error types - Multiple Techniques: Often combining approaches (cost-sensitive + threshold optimization) works best - Evaluation is Critical: Standard metrics can be misleading; business metrics are essential - Threshold Optimization: Can significantly improve business outcomes without changing the model</p>"},{"location":"Machine-Learning/Unbalanced%2C%20Skewed%20data/#references","title":"=\ufffd References","text":"<ol> <li>Books:</li> <li>Learning from Imbalanced Data Sets - Alberto Fern\ufffdndez</li> <li>Imbalanced Learning: Foundations, Algorithms, and Applications - He &amp; Ma</li> <li> <p>The Elements of Statistical Learning - Hastie, Tibshirani, Friedman</p> </li> <li> <p>Research Papers:</p> </li> <li>SMOTE: Synthetic Minority Over-sampling Technique - Chawla et al.</li> <li>Learning from Imbalanced Data - He &amp; Garcia</li> <li> <p>Cost-Sensitive Learning - Elkan</p> </li> <li> <p>Libraries and Documentation:</p> </li> <li>Imbalanced-learn Documentation</li> <li>Scikit-learn Imbalanced Datasets</li> <li> <p>XGBoost Imbalanced Classification</p> </li> <li> <p>Online Resources:</p> </li> <li>Google's Rules of Machine Learning - Dealing with Imbalanced Data</li> <li>Towards Data Science - Imbalanced Data Articles</li> <li> <p>Kaggle Learn - Intermediate Machine Learning</p> </li> <li> <p>Datasets for Practice:</p> </li> <li>Credit Card Fraud Detection - Kaggle</li> <li>Adult Income Dataset - UCI</li> <li>Mammographic Mass Dataset - UCI</li> </ol>"},{"location":"Machine-Learning/kNN/","title":"\ud83d\udcd8 k-Nearest Neighbors (kNN)","text":"<p>k-Nearest Neighbors (kNN) is a simple, versatile, non-parametric algorithm used for both classification and regression tasks that makes predictions based on the majority class or average value of its k closest training examples.</p> <p>Resources: Scikit-learn kNN | Elements of Statistical Learning - Chapter 13</p>"},{"location":"Machine-Learning/kNN/#summary","title":"\u270d\ufe0f Summary","text":"<p>k-Nearest Neighbors (kNN) is an instance-based, lazy learning algorithm that delays all computation until prediction time. The core idea is simple: similar instances tend to have similar outputs. For classification, kNN predicts a class by finding the most common class among the k-closest neighbors. For regression, it predicts a value by averaging the values of its k-nearest neighbors.</p> <p>Key characteristics: - Non-parametric: Makes no assumptions about data distribution - Lazy learner: No explicit training phase - Instance-based: Stores all training examples for prediction - Intuitive: Easy to understand and implement - Versatile: Works for both classification and regression</p> <p>Applications: - Recommendation systems - Credit scoring - Medical diagnosis - Anomaly detection - Image classification - Pattern recognition - Gene expression analysis</p>"},{"location":"Machine-Learning/kNN/#intuition","title":"\ud83e\udde0 Intuition","text":""},{"location":"Machine-Learning/kNN/#how-knn-works","title":"How kNN Works","text":"<ol> <li>Store: Remember all training examples</li> <li>Distance: Calculate distances between new example and all stored examples</li> <li>Neighbors: Find k nearest neighbors based on distance</li> <li>Decision: Make prediction based on neighbors (majority vote or average)</li> </ol>"},{"location":"Machine-Learning/kNN/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"Machine-Learning/kNN/#1-distance-metrics","title":"1. Distance Metrics","text":"<p>Euclidean Distance (most common): \\(\\(d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\)\\)</p> <p>Manhattan Distance: \\(\\(d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\\)\\)</p> <p>Minkowski Distance (generalization): \\(\\(d(x, y) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}\\)\\) - p = 1: Manhattan distance - p = 2: Euclidean distance</p> <p>Hamming Distance (for categorical features): \\(\\(d(x, y) = \\sum_{i=1}^{n} \\mathbb{1}(x_i \\neq y_i)\\)\\)</p>"},{"location":"Machine-Learning/kNN/#2-decision-rules","title":"2. Decision Rules","text":"<p>For Classification: \\(\\(\\hat{y} = \\text{mode}(y_i), \\text{ where } i \\in \\text{top-}k \\text{ nearest neighbors}\\)\\)</p> <p>For Regression: \\(\\(\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i, \\text{ where } i \\in \\text{top-}k \\text{ nearest neighbors}\\)\\)</p> <p>Weighted kNN: \\(\\(\\hat{y} = \\frac{\\sum_{i=1}^{k} w_i y_i}{\\sum_{i=1}^{k} w_i}, \\text{ where } w_i = \\frac{1}{d(x, x_i)^2}\\)\\)</p>"},{"location":"Machine-Learning/kNN/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Choose k: Determine appropriate number of neighbors</li> <li>Calculate distances: Measure distance between query point and all training samples</li> <li>Find neighbors: Identify k closest points</li> <li>Make prediction: Classify by majority vote or predict by average</li> </ol>"},{"location":"Machine-Learning/kNN/#implementation-using-libraries","title":"\ud83d\udd22 Implementation using Libraries","text":""},{"location":"Machine-Learning/kNN/#using-scikit-learn","title":"Using Scikit-learn","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, load_boston, make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# ---- Classification Example ----\n# Load Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale features (important for distance-based algorithms)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train classifier\nknn_clf = KNeighborsClassifier(\n    n_neighbors=5,            # number of neighbors\n    weights='uniform',        # or 'distance' for weighted voting\n    algorithm='auto',         # or 'ball_tree', 'kd_tree', 'brute'\n    leaf_size=30,             # affects speed of tree algorithms\n    p=2                       # power parameter for Minkowski distance\n)\n\nknn_clf.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = knn_clf.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\n\n# Confusion matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Visualize decision boundaries (for 2 features)\ndef plot_decision_boundary(X, y, model, scaler, feature_indices=[0, 1]):\n    h = 0.02  # step size in the mesh\n\n    # Select two features\n    X_selected = X[:, feature_indices]\n    X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(\n        X_selected, y, test_size=0.2, random_state=42\n    )\n\n    # Scale features\n    X_train_sel_scaled = scaler.fit_transform(X_train_sel)\n    X_test_sel_scaled = scaler.transform(X_test_sel)\n\n    # Train model on selected features\n    model.fit(X_train_sel_scaled, y_train_sel)\n\n    # Create a mesh grid\n    x_min, x_max = X_selected[:, 0].min() - 1, X_selected[:, 0].max() + 1\n    y_min, y_max = X_selected[:, 1].min() - 1, X_selected[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Scale mesh grid\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    mesh_points_scaled = scaler.transform(mesh_points)\n\n    # Predict\n    Z = model.predict(mesh_points_scaled)\n    Z = Z.reshape(xx.shape)\n\n    # Plot decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n\n    # Plot training points\n    scatter = plt.scatter(X_selected[:, 0], X_selected[:, 1], c=y, \n                 edgecolors='k', cmap=plt.cm.Paired)\n\n    plt.xlabel(f'Feature {feature_indices[0]} ({iris.feature_names[feature_indices[0]]})')\n    plt.ylabel(f'Feature {feature_indices[1]} ({iris.feature_names[feature_indices[1]]})')\n    plt.title(f'Decision Boundary with k={model.n_neighbors}')\n    plt.legend(*scatter.legend_elements(), title=\"Classes\")\n    plt.show()\n\n# Plot decision boundary\nplot_decision_boundary(X, y, KNeighborsClassifier(n_neighbors=5), \n                      StandardScaler(), [0, 1])\n\n# Finding optimal k value\nk_values = list(range(1, 31))\ntrain_accuracies = []\ntest_accuracies = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n\n    # Training accuracy\n    y_train_pred = knn.predict(X_train_scaled)\n    train_acc = accuracy_score(y_train, y_train_pred)\n    train_accuracies.append(train_acc)\n\n    # Testing accuracy\n    y_test_pred = knn.predict(X_test_scaled)\n    test_acc = accuracy_score(y_test, y_test_pred)\n    test_accuracies.append(test_acc)\n\n# Plot accuracy vs k\nplt.figure(figsize=(12, 6))\nplt.plot(k_values, train_accuracies, label='Training Accuracy', marker='o')\nplt.plot(k_values, test_accuracies, label='Testing Accuracy', marker='s')\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs k for kNN Classifier')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"Best k: {k_values[np.argmax(test_accuracies)]}\")\n\n# ---- Regression Example ----\n\n# Create synthetic regression data\nnp.random.seed(42)\nX_reg = np.random.rand(100, 1) * 10\ny_reg = 2 * X_reg.squeeze() + 3 + np.random.randn(100) * 2\n\n# Split regression data\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\n# Create and train regression model\nknn_reg = KNeighborsRegressor(n_neighbors=3)\nknn_reg.fit(X_train_reg, y_train_reg)\n\n# Make predictions\ny_pred_reg = knn_reg.predict(X_test_reg)\nmse = mean_squared_error(y_test_reg, y_pred_reg)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error: {rmse:.3f}\")\n\n# Plot regression results\nplt.figure(figsize=(10, 6))\nplt.scatter(X_reg, y_reg, c='b', label='Data')\nplt.scatter(X_test_reg, y_test_reg, c='g', marker='s', label='Test Data')\nplt.scatter(X_test_reg, y_pred_reg, c='r', marker='^', label='Predictions')\n\n# Plot predictions for a fine-grained range\nX_range = np.linspace(0, 10, 1000).reshape(-1, 1)\ny_range_pred = knn_reg.predict(X_range)\nplt.plot(X_range, y_range_pred, c='orange', label='kNN Predictions')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('kNN Regression (k=3)')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/kNN/#from-scratch-implementation","title":"\u2699\ufe0f From Scratch Implementation","text":"<pre><code>import numpy as np\nfrom collections import Counter\nfrom scipy.spatial.distance import cdist\n\nclass KNNFromScratch:\n    def __init__(self, k=5, distance_metric='euclidean', weights='uniform', algorithm_type='classification'):\n        \"\"\"\n        k-Nearest Neighbors algorithm implementation from scratch\n\n        Parameters:\n        k (int): Number of neighbors to use\n        distance_metric (str): 'euclidean', 'manhattan', or 'minkowski'\n        weights (str): 'uniform' or 'distance'\n        algorithm_type (str): 'classification' or 'regression'\n        \"\"\"\n        self.k = k\n        self.distance_metric = distance_metric\n        self.weights = weights\n        self.algorithm_type = algorithm_type\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"Store training data (lazy learning)\"\"\"\n        self.X_train = np.array(X)\n        self.y_train = np.array(y)\n        return self\n\n    def _calculate_distances(self, X):\n        \"\"\"Calculate distances between test points and all training points\"\"\"\n        return cdist(X, self.X_train, metric=self.distance_metric)\n\n    def _get_neighbors(self, distances):\n        \"\"\"Get indices of k-nearest neighbors\"\"\"\n        return np.argsort(distances, axis=1)[:, :self.k]\n\n    def _get_weights(self, distances, neighbor_indices):\n        \"\"\"Get weights for neighbors\"\"\"\n        if self.weights == 'uniform':\n            # All neighbors have equal weight\n            return np.ones((distances.shape[0], self.k))\n        elif self.weights == 'distance':\n            # Weights are inverse of distances\n            neighbor_distances = np.take_along_axis(\n                distances, neighbor_indices, axis=1)\n\n            # Avoid division by zero\n            neighbor_distances = np.maximum(neighbor_distances, 1e-10)\n\n            # Weights = 1/distance\n            weights = 1.0 / neighbor_distances\n\n            # Normalize weights\n            row_sums = weights.sum(axis=1, keepdims=True)\n            return weights / row_sums\n\n    def predict(self, X):\n        \"\"\"Make predictions for test data\"\"\"\n        X = np.array(X)\n\n        # Calculate distances\n        distances = self._calculate_distances(X)\n\n        # Get indices of k-nearest neighbors\n        neighbor_indices = self._get_neighbors(distances)\n\n        # Get weights\n        weights = self._get_weights(distances, neighbor_indices)\n\n        # Get labels of neighbors\n        neighbor_labels = self.y_train[neighbor_indices]\n\n        if self.algorithm_type == 'classification':\n            return self._predict_classification(neighbor_labels, weights)\n        else:  # regression\n            return self._predict_regression(neighbor_labels, weights)\n\n    def _predict_classification(self, neighbor_labels, weights):\n        \"\"\"Predict class labels using weighted voting\"\"\"\n        predictions = []\n\n        for i in range(neighbor_labels.shape[0]):\n            if self.weights == 'uniform':\n                # Majority vote\n                most_common = Counter(neighbor_labels[i]).most_common(1)\n                predictions.append(most_common[0][0])\n            else:  # 'distance'\n                # Weighted vote\n                class_weights = {}\n                for j in range(self.k):\n                    label = neighbor_labels[i, j]\n                    weight = weights[i, j]\n                    class_weights[label] = class_weights.get(label, 0) + weight\n\n                # Get class with highest weight\n                predictions.append(max(class_weights, key=class_weights.get))\n\n        return np.array(predictions)\n\n    def _predict_regression(self, neighbor_labels, weights):\n        \"\"\"Predict values using weighted average\"\"\"\n        # Weighted average: sum(weights * values) / sum(weights)\n        return np.sum(neighbor_labels * weights, axis=1)\n\n    def score(self, X, y):\n        \"\"\"Calculate accuracy (classification) or R\u00b2 (regression)\"\"\"\n        y_pred = self.predict(X)\n\n        if self.algorithm_type == 'classification':\n            # Classification accuracy\n            return np.mean(y_pred == y)\n        else:  # regression\n            # R\u00b2 score\n            y_mean = np.mean(y)\n            ss_total = np.sum((y - y_mean) ** 2)\n            ss_residual = np.sum((y - y_pred) ** 2)\n            return 1 - (ss_residual / ss_total)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample classification data\n    np.random.seed(42)\n    X = np.random.rand(100, 2)\n    y = (X[:, 0] + X[:, 1] &gt; 1).astype(int)\n\n    # Split data\n    indices = np.random.permutation(len(X))\n    train_size = int(len(X) * 0.8)\n    X_train, X_test = X[indices[:train_size]], X[indices[train_size:]]\n    y_train, y_test = y[indices[:train_size]], y[indices[train_size:]]\n\n    # Train custom kNN classifier\n    knn = KNNFromScratch(k=3, algorithm_type='classification')\n    knn.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = knn.predict(X_test)\n    accuracy = np.mean(y_pred == y_test)\n    print(f\"Classification Accuracy: {accuracy:.3f}\")\n\n    # Generate sample regression data\n    X_reg = np.random.rand(100, 1) * 10\n    y_reg = 2 * X_reg.squeeze() + 3 + np.random.randn(100)\n\n    # Split regression data\n    indices_reg = np.random.permutation(len(X_reg))\n    X_train_reg = X_reg[indices_reg[:train_size]]\n    X_test_reg = X_reg[indices_reg[train_size:]]\n    y_train_reg = y_reg[indices_reg[:train_size]]\n    y_test_reg = y_reg[indices_reg[train_size:]]\n\n    # Train custom kNN regressor\n    knn_reg = KNNFromScratch(k=3, algorithm_type='regression', weights='distance')\n    knn_reg.fit(X_train_reg, y_train_reg)\n\n    # Evaluate\n    y_pred_reg = knn_reg.predict(X_test_reg)\n    mse = np.mean((y_pred_reg - y_test_reg) ** 2)\n    rmse = np.sqrt(mse)\n    r2 = knn_reg.score(X_test_reg, y_test_reg)\n    print(f\"Regression RMSE: {rmse:.3f}\")\n    print(f\"Regression R\u00b2: {r2:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/kNN/#assumptions-and-limitations","title":"\u26a0\ufe0f Assumptions and Limitations","text":""},{"location":"Machine-Learning/kNN/#assumptions","title":"Assumptions","text":"<ol> <li>Similarity-proximity assumption: Similar instances are located close to each other in feature space</li> <li>Equal feature importance: All features contribute equally to distance calculations</li> <li>Locally constant function: Target function is assumed to be locally constant</li> </ol>"},{"location":"Machine-Learning/kNN/#limitations","title":"Limitations","text":"<ol> <li>Curse of dimensionality: Performance degrades in high-dimensional spaces</li> <li>Computational cost: Calculating distances to all training samples is expensive</li> <li>Memory intensive: Stores all training data</li> <li>Sensitive to irrelevant features: All features contribute to distance</li> <li>Sensitive to scale: Features with larger scales dominate distance calculations</li> <li>Imbalanced data: Majority class dominates in classification</li> <li>Parameter sensitivity: Results highly dependent on choice of k</li> </ol>"},{"location":"Machine-Learning/kNN/#comparison-with-other-models","title":"Comparison with Other Models","text":"Algorithm Advantages vs kNN Disadvantages vs kNN Decision Trees Handles irrelevant features, fast prediction Less accurate for complex boundaries SVM Works well in high dimensions, handles non-linear patterns Complex tuning, black box model Naive Bayes Very fast, works well with high dimensions Assumes feature independence Linear Regression Simple, interpretable Only captures linear relationships Neural Networks Captures complex patterns, automatic feature learning Needs more data, complex tuning"},{"location":"Machine-Learning/kNN/#interview-questions","title":"\ud83d\udca1 Interview Questions","text":"1. What's the difference between a lazy and eager learning algorithm, and where does kNN fit? <p>Answer:</p> <p>Lazy learning algorithms delay all computation until prediction time: - No explicit training phase - Store all training examples in memory - Computation happens at prediction time - Examples: k-Nearest Neighbors, Case-Based Reasoning</p> <p>Eager learning algorithms create a model during training: - Generalize from training data during training phase - Discard training data after model is built - Fast predictions using the pre-built model - Examples: Decision Trees, Neural Networks, SVM</p> <p>kNN is a lazy learner because: - It doesn't build a model during training - It simply stores all training examples - Computations (distance calculations, neighbor finding) happen during prediction - Each prediction requires scanning the entire training set</p> <p>This gives kNN certain characteristics: - Slow predictions (especially with large training sets) - Fast training (just stores data) - Adapts naturally to new training data - No information loss from generalization</p> 2. How do you choose the optimal value of k in kNN? <p>Answer:</p> <p>Methods for choosing k:</p> <p>1. Cross-validation: - Most common approach - Split data into training and validation sets - Train models with different k values - Choose k with best validation performance - Example: k-fold cross-validation</p> <p>2. Square root heuristic: - Rule of thumb: k \u2248 \u221an, where n is training set size - Quick starting point for experimentation</p> <p>3. Elbow method: - Plot error rate against different k values - Look for \"elbow\" where error rate stabilizes</p> <p>Considerations when choosing k:</p> <ul> <li>Small k values:</li> <li>More flexible decision boundaries</li> <li>Can lead to overfitting</li> <li> <p>More sensitive to noise</p> </li> <li> <p>Large k values:</p> </li> <li>Smoother decision boundaries</li> <li>Can lead to underfitting</li> <li> <p>Computationally more expensive</p> </li> <li> <p>Odd vs. even:</p> </li> <li> <p>For binary classification, use odd k to avoid ties</p> </li> <li> <p>Domain knowledge:</p> </li> <li>Consider problem characteristics</li> <li>Some domains benefit from specific k ranges</li> </ul> <p>Implementation example: <pre><code>from sklearn.model_selection import cross_val_score\n\n# Find optimal k\nk_range = range(1, 31)\nk_scores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n    k_scores.append(scores.mean())\n\nbest_k = k_range[np.argmax(k_scores)]\n</code></pre></p> 3. Why is feature scaling important for kNN, and how would you implement it? <p>Answer:</p> <p>Importance of feature scaling:</p> <ul> <li>Distance domination: Features with larger scales will dominate the distance calculation</li> <li>Equal contribution: Scaling ensures all features contribute equally</li> <li>Improved accuracy: Properly scaled features generally lead to better performance</li> </ul> <p>Example: Consider two features: age (0-100) and income (0-1,000,000) - Without scaling, income differences will completely overwhelm age differences - After scaling, both contribute proportionally to their importance</p> <p>Common scaling methods:</p> <p>1. Min-Max Scaling (Normalization): \\(\\(X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\)\\) - Scales features to range [0,1] - Good when distribution is not Gaussian</p> <p>2. Z-score Standardization: \\(\\(X_{scaled} = \\frac{X - \\mu}{\\sigma}\\)\\) - Transforms to mean=0, std=1 - Good for normally distributed features</p> <p>3. Robust Scaling: \\(\\(X_{scaled} = \\frac{X - median}{IQR}\\)\\) - Uses median and interquartile range - Robust to outliers</p> <p>Implementation: <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# Z-score standardization\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Min-Max scaling\nminmax = MinMaxScaler()\nX_scaled = minmax.fit_transform(X)\n\n# Robust scaling\nrobust = RobustScaler()\nX_scaled = robust.fit_transform(X)\n</code></pre></p> <p>Important considerations: - Always fit scaler on training data only - Apply same transformation to test data - Different scaling methods may be appropriate for different features - Categorical features may need special handling (e.g., one-hot encoding)</p> 4. How can kNN handle categorical features? <p>Answer:</p> <p>Approaches for handling categorical features in kNN:</p> <p>1. One-Hot Encoding: - Convert categorical variables to binary columns - Each category becomes its own binary feature - Increases dimensionality - Example: Color (Red, Blue, Green) \u2192 [1,0,0], [0,1,0], [0,0,1]</p> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse=False)\nX_cat_encoded = encoder.fit_transform(X_categorical)\n</code></pre> <p>2. Distance metrics for mixed data: - Use specialized metrics that handle mixed data types - Gower distance: combines different metrics for different types   - Euclidean for numeric features   - Hamming for categorical features</p> <pre><code>def gower_distance(x, y, categorical_features):\n    numeric_dist = np.sqrt(np.sum(((x[~categorical_features] - \n                                   y[~categorical_features]) ** 2)))\n    categ_dist = np.sum(x[categorical_features] != y[categorical_features])\n    return (numeric_dist + categ_dist) / len(x)\n</code></pre> <p>3. Ordinal Encoding: - Assign integers to categories - Only appropriate when categories have natural ordering - Example: Size (Small, Medium, Large) \u2192 1, 2, 3</p> <pre><code>from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder()\nX_cat_encoded = encoder.fit_transform(X_categorical)\n</code></pre> <p>4. Feature hashing: - Hash categorical values to fixed-length vectors - Useful for high-cardinality features</p> <p>5. Custom distance functions: - Implement specialized distance measures - Can apply different distance metrics to different features</p> <p>Best practices: - Consider feature relevance when choosing encoding - For small categorical sets, one-hot encoding often works best - For high cardinality, consider embeddings or hashing - Test different approaches with cross-validation</p> 5. What happens when kNN is applied to high-dimensional data, and how can you address these issues? <p>Answer:</p> <p>The Curse of Dimensionality in kNN:</p> <ul> <li>Distance concentration: As dimensions increase, distances between points become more similar</li> <li>Sparsity: Points become farther apart, requiring more data to maintain density</li> <li>Computational cost: Calculating distances in high dimensions is expensive</li> <li>Irrelevant features: More dimensions increase the chance of including irrelevant features</li> </ul> <p>In high dimensions: - All points tend to be equidistant from each other - Concept of \"nearest neighbor\" becomes less meaningful - Distance calculations become computationally expensive - Model accuracy degrades significantly</p> <p>Solutions to address high-dimensionality:</p> <p>1. Dimensionality reduction: <pre><code>from sklearn.decomposition import PCA\n\n# Reduce to 10 dimensions\npca = PCA(n_components=10)\nX_reduced = pca.fit_transform(X)\n</code></pre></p> <p>2. Feature selection: <pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\n# Select top 10 features\nselector = SelectKBest(f_classif, k=10)\nX_selected = selector.fit_transform(X, y)\n</code></pre></p> <p>3. Use approximate nearest neighbors: <pre><code>from sklearn.neighbors import NearestNeighbors\n\n# Use ball tree algorithm\nnn = NearestNeighbors(algorithm='ball_tree')\n</code></pre></p> <p>4. Locality Sensitive Hashing (LSH): - Maps similar items to same buckets with high probability - Allows approximate nearest neighbor search</p> <p>5. Feature weighting: - Assign different weights to features based on importance - Can use feature importance from other models</p> <p>6. Use distance metrics suited for high dimensions: - Cosine similarity for text data - Manhattan distance often works better than Euclidean in high dimensions</p> <p>7. Increase training data: - More data helps combat sparsity issues</p> 6. How does weighted kNN differ from standard kNN, and when would you use it? <p>Answer:</p> <p>Standard kNN vs. Weighted kNN:</p> <p>Standard kNN: - All k neighbors contribute equally to prediction - Classification: simple majority vote - Regression: simple average of neighbor values</p> <p>Weighted kNN: - Neighbors contribute based on their distance - Closer neighbors have greater influence - Weight typically inversely proportional to distance</p> <p>Mathematical formulation:</p> <p>For standard kNN: \\(\\(\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i\\)\\)</p> <p>For weighted kNN: \\(\\(\\hat{y} = \\frac{\\sum_{i=1}^{k} w_i y_i}{\\sum_{i=1}^{k} w_i}\\)\\)</p> <p>Where common weight functions include: - Inverse distance: \\(w_i = \\frac{1}{d(x, x_i)}\\) - Inverse squared distance: \\(w_i = \\frac{1}{d(x, x_i)^2}\\) - Exponential: \\(w_i = e^{-d(x, x_i)}\\)</p> <p>When to use weighted kNN:</p> <ul> <li>Uneven distribution: When training data is unevenly distributed</li> <li>Varying importance: When certain neighbors should have more influence</li> <li>Noisy data: Reduces impact of outliers</li> <li>Class imbalance: Helps with imbalanced classification problems</li> <li>Boundary regions: Improves accuracy near decision boundaries</li> </ul> <p>Implementation in scikit-learn: <pre><code># Standard kNN\nknn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n\n# Weighted kNN\nknn_weighted = KNeighborsClassifier(n_neighbors=5, weights='distance')\n</code></pre></p> <p>Custom weight functions: <pre><code>def custom_weight(distances):\n    return np.exp(-distances)\n\nknn_custom = KNeighborsClassifier(\n    n_neighbors=5, \n    weights=custom_weight\n)\n</code></pre></p> 7. Compare and contrast kNN with other classification algorithms like Decision Trees and SVM. <p>Answer:</p> Aspect kNN Decision Trees SVM Learning Type Lazy (instance-based) Eager Eager Training Speed Very fast (just stores data) Moderate Slow (especially with non-linear kernels) Prediction Speed Slow (distance calculation) Fast Fast (except with many support vectors) Memory Requirements High (stores all training data) Low Moderate (stores support vectors) Interpretability Moderate High (can visualize tree) Low (especially with kernels) Handling Non-linearity Naturally handles non-linear boundaries Steps (hierarchical) Kernels transform to linearly separable space Feature Scaling Critical Not needed Important Outlier Sensitivity High Low Low (with proper C value) Missing Value Handling Poor Good Poor High Dimensions Poor (curse of dimensionality) Good (feature selection) Good (regularization) Imbalanced Data Poor Moderate Good (with class weights) Categorical Features Needs encoding Handles naturally Needs encoding Overfitting Risk High with small k High with deep trees Low with proper regularization Hyperparameter Tuning Simple (mainly k) Moderate Complex <p>Key Contrasts:</p> <p>kNN vs. Decision Trees: - kNN: Instance-based, creates complex non-linear boundaries - Trees: Rule-based, creates axis-parallel decision boundaries - kNN relies on distance; Trees use feature thresholds - Trees automatically handle feature importance; kNN treats all equally</p> <p>kNN vs. SVM: - kNN: Local patterns based on neighbors - SVM: Global patterns based on support vectors - kNN: Simple but computationally expensive at prediction time - SVM: Complex optimization but efficient predictions</p> <p>When to choose each:</p> <p>Choose kNN when: - Small to medium dataset - Low dimensionality - Complex non-linear decision boundaries - Quick implementation needed - Prediction speed not critical</p> <p>Choose Decision Trees when: - Feature importance needed - Interpretability is critical - Mixed feature types - Fast predictions required</p> <p>Choose SVM when: - High-dimensional data - Complex boundaries - Memory efficiency needed - Strong theoretical guarantees required</p> 8. Explain how to implement kNN for large datasets where the data doesn't fit in memory. <p>Answer:</p> <p>Strategies for large-scale kNN:</p> <p>1. Data sampling techniques: - Use a representative subset of training data - Condensed nearest neighbors: only keep points that affect decision boundaries - Edited nearest neighbors: remove noisy samples</p> <pre><code>from imblearn.under_sampling import CondensedNearestNeighbour\n\ncnn = CondensedNearestNeighbour(n_neighbors=5)\nX_reduced, y_reduced = cnn.fit_resample(X, y)\n</code></pre> <p>2. Approximate nearest neighbor methods: - Locality-Sensitive Hashing (LSH) - Random projection trees - Product quantization</p> <pre><code># Using Annoy library for approximate nearest neighbors\nfrom annoy import AnnoyIndex\n\n# Build index\nf = X.shape[1]  # dimensionality\nt = AnnoyIndex(f, 'euclidean')\nfor i, x in enumerate(X):\n    t.add_item(i, x)\nt.build(10)  # 10 trees\n\n# Query\nindices = t.get_nns_by_vector(query_vector, 5)  # find 5 nearest neighbors\n</code></pre> <p>3. KD-Trees and Ball Trees: - Space-partitioning data structures - Log(n) query time for low dimensions - Still struggle in high dimensions</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', \n                           leaf_size=30, n_jobs=-1)\n</code></pre> <p>4. Distributed computing: - Parallelize distance computations - Map-reduce framework for kNN</p> <p>5. GPU acceleration: - Leverage GPU for parallel distance calculations - Libraries like FAISS from Facebook</p> <pre><code># Using FAISS for GPU-accelerated nearest neighbor search\nimport faiss\n\n# Convert to float32\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\n# Build index\nindex = faiss.IndexFlatL2(X_train.shape[1])\nindex.add(X_train)\n\n# Search\nk = 5\ndistances, indices = index.search(X_test, k)\n</code></pre> <p>6. Incremental learning: - Process data in batches - Update model with new chunks of data</p> <p>7. Database-backed implementations: - Store data in database - Use database's indexing capabilities - SQL or NoSQL solutions</p> <p>8. Feature reduction: - Apply dimensionality reduction first - PCA, t-SNE, or UMAP to reduce dimensions</p> <p>Best practices: - Consider problem requirements (accuracy vs. speed) - Benchmark different approaches - Combine multiple strategies - Use specialized libraries for large-scale kNN</p>"},{"location":"Machine-Learning/kNN/#examples","title":"\ud83e\udde0 Examples","text":""},{"location":"Machine-Learning/kNN/#example-1-handwritten-digit-recognition","title":"Example 1: Handwritten Digit Recognition","text":"<pre><code>from sklearn.datasets import load_digits\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load digit dataset\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train kNN classifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Predict\ny_pred = knn.predict(X_test)\n\n# Evaluate\naccuracy = knn.score(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=range(10),\n            yticklabels=range(10))\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Visualize some predictions\nfig, axes = plt.subplots(4, 5, figsize=(12, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(X_test):\n        ax.imshow(X_test[i].reshape(8, 8), cmap='binary')\n        pred = knn.predict(X_test[i].reshape(1, -1))[0]\n        true = y_test[i]\n        color = 'green' if pred == true else 'red'\n        ax.set_title(f'Pred: {pred}, True: {true}', color=color)\n        ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Test with different k values\nk_values = list(range(1, 21))\ntrain_scores = []\ntest_scores = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    train_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_scores, 'o-', label='Training Accuracy')\nplt.plot(k_values, test_scores, 's-', label='Testing Accuracy')\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('Accuracy')\nplt.title('kNN Accuracy vs k for Digit Recognition')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Machine-Learning/kNN/#example-2-anomaly-detection","title":"Example 2: Anomaly Detection","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n# Generate sample data with outliers\nX_inliers, _ = make_blobs(n_samples=300, centers=1, cluster_std=2.0, random_state=42)\nX_outliers = np.random.uniform(low=-15, high=15, size=(15, 2))\nX = np.vstack([X_inliers, X_outliers])\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Fit nearest neighbors model\nk = 5\nnn = NearestNeighbors(n_neighbors=k)\nnn.fit(X_scaled)\n\n# Calculate distance to k-th nearest neighbor\ndistances, indices = nn.kneighbors(X_scaled)\nk_distance = distances[:, k-1]\n\n# Set threshold for anomaly detection\nthreshold = np.percentile(k_distance, 95)  # 95th percentile\nanomalies = k_distance &gt; threshold\n\n# Plot results\nplt.figure(figsize=(12, 8))\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='blue', label='Normal points')\nplt.scatter(X_scaled[anomalies, 0], X_scaled[anomalies, 1], \n            c='red', s=100, marker='x', label='Detected Anomalies')\n\n# Plot true outliers\ntrue_outliers = np.arange(len(X_inliers), len(X))\nplt.scatter(X_scaled[true_outliers, 0], X_scaled[true_outliers, 1], \n            edgecolors='green', s=140, facecolors='none', \n            linewidth=2, marker='o', label='True Outliers')\n\nplt.title('kNN-based Anomaly Detection')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Evaluation\ntrue_anomalies = np.zeros(len(X), dtype=bool)\ntrue_anomalies[true_outliers] = True\n\n# True Positives, False Positives, etc.\nTP = np.sum(np.logical_and(anomalies, true_anomalies))\nFP = np.sum(np.logical_and(anomalies, ~true_anomalies))\nTN = np.sum(np.logical_and(~anomalies, ~true_anomalies))\nFN = np.sum(np.logical_and(~anomalies, true_anomalies))\n\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nf1 = 2 * precision * recall / (precision + recall)\n\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1 Score: {f1:.3f}\")\n</code></pre>"},{"location":"Machine-Learning/kNN/#references","title":"\ud83d\udcda References","text":"<ul> <li>Books:</li> <li>Pattern Recognition and Machine Learning by Christopher Bishop</li> <li>The Elements of Statistical Learning by Hastie, Tibshirani, &amp; Friedman</li> <li> <p>Machine Learning: A Probabilistic Perspective by Kevin Murphy</p> </li> <li> <p>Documentation:</p> </li> <li>Scikit-learn Nearest Neighbors</li> <li> <p>SciPy Spatial Distance Functions</p> </li> <li> <p>Tutorials:</p> </li> <li>KNN Algorithm - How KNN Algorithm Works</li> <li>Complete Guide to kNN in Python</li> <li> <p>KNN from Scratch in Python</p> </li> <li> <p>Research Papers:</p> </li> <li>Fix, E., &amp; Hodges, J. L. (1951). Discriminatory analysis, nonparametric discrimination: Consistency properties.</li> <li>Cover, T., &amp; Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21-27.</li> <li> <p>Weinberger, K. Q., &amp; Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10, 207-244.</p> </li> <li> <p>Online Courses:</p> </li> <li>Machine Learning by Andrew Ng (Coursera)</li> <li>KNN Algorithm - StatQuest with Josh Starmer</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/","title":"\ud83d\udcda Online Study Material","text":"<p>The most comprehensive collection of online resources, courses, tutorials, learning platforms, and career development materials for data science, machine learning, artificial intelligence, and related fields. This guide covers everything from beginner fundamentals to cutting-edge research and industry applications.</p>"},{"location":"Online-Material/Online-Material-for-Learning/#online-courses-moocs","title":"\ud83c\udf93 Online Courses &amp; MOOCs","text":""},{"location":"Online-Material/Online-Material-for-Learning/#machine-learning-fundamentals","title":"Machine Learning Fundamentals","text":""},{"location":"Online-Material/Online-Material-for-Learning/#coursera-comprehensive-ml-programs","title":"Coursera - Comprehensive ML Programs","text":"<ul> <li>Machine Learning Specialization by Andrew Ng (Stanford/DeepLearning.AI)</li> <li>Duration: 3 months, 10 hours/week</li> <li>Prerequisites: Basic Python, high school math</li> <li>Outcome: Complete understanding of supervised/unsupervised learning</li> <li>Projects: Linear regression, logistic regression, neural networks, recommender systems</li> <li>Certificate: Professional certificate recognized by employers</li> <li> <p>Cost: $39-79/month with financial aid available</p> </li> <li> <p>Deep Learning Specialization by Andrew Ng</p> </li> <li>Duration: 5 months, 5 courses, 7 hours/week</li> <li>Prerequisites: Python programming, basic linear algebra</li> <li>Courses: Neural Networks, Hyperparameter Tuning, CNN, RNN, Transformers</li> <li>Projects: Image classification, face recognition, machine translation, trigger word detection</li> <li>Tools: TensorFlow, Keras, NumPy, Matplotlib</li> <li> <p>Industry Focus: Production-ready deep learning applications</p> </li> <li> <p>Machine Learning by Stanford (Andrew Ng)</p> </li> <li>Duration: 11 weeks, 8-10 hours/week</li> <li>Language: MATLAB/Octave (classic version)</li> <li>Theory Focus: Mathematical foundations, algorithm derivations</li> <li>Assignments: Implement algorithms from scratch</li> <li> <p>Legacy: Original ML course that launched modern AI education</p> </li> <li> <p>TensorFlow Developer Professional Certificate</p> </li> <li>Duration: 4 months, 5 hours/week</li> <li>Instructor: Laurence Moroney (Google AI)</li> <li>Focus: Production TensorFlow development</li> <li>Specializations: Computer vision, NLP, time series, deployment</li> <li> <p>Certification: Google-recognized professional certificate</p> </li> <li> <p>IBM Data Science Professional Certificate</p> </li> <li>Duration: 11 months, 5 hours/week</li> <li>Courses: 10 courses covering full data science pipeline</li> <li>Tools: Python, R, SQL, Jupyter, GitHub, Watson Studio</li> <li>Capstone: Real-world data science project</li> <li>Career Services: Job placement assistance, resume building</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#edx-academic-excellence","title":"edX - Academic Excellence","text":"<ul> <li>MIT: Introduction to Machine Learning (6.036)</li> <li>Institution: Massachusetts Institute of Technology</li> <li>Duration: 13 weeks, 8-10 hours/week</li> <li>Prerequisites: Multivariable calculus, linear algebra, probability</li> <li>Focus: Mathematical rigor, theoretical foundations</li> <li>Assignments: Problem sets with mathematical proofs</li> <li> <p>Language: Python with mathematical analysis</p> </li> <li> <p>Harvard CS109: Data Science</p> </li> <li>Institution: Harvard University</li> <li>Duration: 12 weeks, 6-8 hours/week</li> <li>Focus: Statistical thinking, data visualization, prediction</li> <li>Projects: Real datasets from various domains</li> <li> <p>Tools: Python, pandas, scikit-learn, matplotlib</p> </li> <li> <p>Microsoft: Machine Learning for Data Science and Analytics</p> </li> <li>Duration: 6 weeks, 4-6 hours/week</li> <li>Focus: Business applications, practical implementations</li> <li>Platform: Azure Machine Learning Studio</li> <li> <p>Case Studies: Real-world business problems</p> </li> <li> <p>UC Berkeley: Foundations of Data Science</p> </li> <li>Duration: 8 weeks, 3-5 hours/week</li> <li>Language: Python with Jupyter notebooks</li> <li>Focus: Statistical inference, prediction, machine learning</li> <li>Unique: Non-technical students friendly approach</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#udacity-industry-focused-nanodegrees","title":"Udacity - Industry-Focused Nanodegrees","text":"<ul> <li>Machine Learning Engineer Nanodegree</li> <li>Duration: 3 months, 15 hours/week</li> <li>Prerequisites: Intermediate Python, statistics, linear algebra</li> <li>Focus: Production ML systems, deployment, monitoring</li> <li>Projects: Model deployment, A/B testing, pipeline optimization</li> <li>Mentorship: 1-on-1 technical mentoring</li> <li> <p>Career Services: Portfolio review, interview preparation</p> </li> <li> <p>Deep Learning Nanodegree</p> </li> <li>Duration: 4 months, 12 hours/week</li> <li>Focus: Advanced neural architectures, GANs, reinforcement learning</li> <li>Projects: Neural style transfer, face generation, quadcopter control</li> <li> <p>Frameworks: PyTorch, TensorFlow, OpenAI Gym</p> </li> <li> <p>AI for Trading Nanodegree</p> </li> <li>Duration: 6 months, 10 hours/week</li> <li>Prerequisites: Python, linear algebra, statistics, finance basics</li> <li>Focus: Quantitative trading, portfolio optimization, risk management</li> <li> <p>Projects: Alpha research, risk model development, sentiment analysis</p> </li> <li> <p>Natural Language Processing Nanodegree</p> </li> <li>Duration: 3 months, 10 hours/week</li> <li>Focus: Text processing, sentiment analysis, chatbots, machine translation</li> <li>Technologies: spaCy, NLTK, Transformers, TensorFlow</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#fastai-top-down-practical-approach","title":"Fast.ai - Top-Down Practical Approach","text":"<ul> <li>Practical Deep Learning for Coders</li> <li>Philosophy: Code first, theory later</li> <li>Duration: 7 weeks, self-paced</li> <li>Prerequisites: 1 year coding experience (any language)</li> <li>Unique: State-of-the-art results from lesson 1</li> <li>Framework: FastAI library built on PyTorch</li> <li> <p>Projects: Image classification, NLP, tabular data, recommendation systems</p> </li> <li> <p>Deep Learning from the Foundations</p> </li> <li>Level: Advanced (Part 2)</li> <li>Focus: Implementing deep learning from scratch</li> <li>Prerequisites: Part 1 completion</li> <li> <p>Topics: Backpropagation, optimization, architectures from ground up</p> </li> <li> <p>A Code-First Introduction to Natural Language Processing</p> </li> <li>Duration: 18 lessons, self-paced</li> <li>Focus: Modern NLP techniques, transformers, BERT, GPT</li> <li>Practical: Build real applications, not just toy examples</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#specialized-domain-courses","title":"Specialized Domain Courses","text":""},{"location":"Online-Material/Online-Material-for-Learning/#computer-vision","title":"Computer Vision","text":"<ul> <li>CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)</li> <li>Instructor: Andrej Karpathy, Fei-Fei Li</li> <li>Duration: 16 weeks, 10+ hours/week</li> <li>Prerequisites: Linear algebra, Python, basic machine learning</li> <li>Assignments: Implement CNN, RNN, GAN from scratch</li> <li> <p>Final Project: Original research-level project</p> </li> <li> <p>Computer Vision Basics by University at Buffalo</p> </li> <li>Duration: 4 weeks, 3-5 hours/week</li> <li>Focus: Image processing fundamentals, feature detection</li> <li> <p>Tools: OpenCV, Python, mathematical foundations</p> </li> <li> <p>Deep Learning for Computer Vision by University of Michigan</p> </li> <li>Duration: 4 weeks, 4-6 hours/week</li> <li>Focus: CNN architectures, object detection, image segmentation</li> <li>Frameworks: TensorFlow, Keras</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#natural-language-processing","title":"Natural Language Processing","text":"<ul> <li>CS224n: Natural Language Processing with Deep Learning (Stanford)</li> <li>Instructor: Christopher Manning</li> <li>Duration: 10 weeks, 12+ hours/week</li> <li>Prerequisites: Python, linear algebra, probability, machine learning</li> <li>Focus: Word vectors, attention, transformers, BERT, GPT</li> <li> <p>Assignments: Neural dependency parsing, machine translation, question answering</p> </li> <li> <p>Natural Language Processing Specialization (DeepLearning.ai)</p> </li> <li>Duration: 4 months, 6 hours/week</li> <li>Courses: Classification, probabilistic models, sequence models, attention</li> <li> <p>Projects: Sentiment analysis, autocomplete, chatbots, summarization</p> </li> <li> <p>Applied Text Mining in Python (University of Michigan)</p> </li> <li>Duration: 4 weeks, 5-7 hours/week</li> <li>Focus: Text processing, information extraction, topic modeling</li> <li>Tools: NLTK, spaCy, Gensim, scikit-learn</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>CS285: Deep Reinforcement Learning (UC Berkeley)</li> <li>Instructor: Sergey Levine</li> <li>Duration: 16 weeks, 8-12 hours/week</li> <li>Prerequisites: Machine learning, linear algebra, probability</li> <li>Focus: Policy gradients, Q-learning, actor-critic methods</li> <li> <p>Implementations: From scratch implementations of major RL algorithms</p> </li> <li> <p>Reinforcement Learning Specialization (University of Alberta)</p> </li> <li>Duration: 6 months, 6 hours/week</li> <li>Instructors: Martha White, Adam White</li> <li>Focus: Fundamentals to advanced topics</li> <li>Capstone: Complete RL project</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#mlops-and-production-systems","title":"MLOps and Production Systems","text":"<ul> <li>Machine Learning Engineering for Production (MLOps) Specialization</li> <li>Duration: 4 months, 5 hours/week</li> <li>Instructor: Andrew Ng, Robert Crowe</li> <li>Focus: ML lifecycle, deployment, monitoring, infrastructure</li> <li> <p>Tools: TensorFlow Extended (TFX), Kubernetes, Docker</p> </li> <li> <p>Full Stack Deep Learning</p> </li> <li>Format: Intensive bootcamp-style course</li> <li>Focus: End-to-end ML project development</li> <li>Topics: Infrastructure, testing, deployment, monitoring</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#mathematics-for-mlds-comprehensive-foundation","title":"\ud83e\uddee Mathematics for ML/DS - Comprehensive Foundation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#linear-algebra-essential-foundation","title":"Linear Algebra - Essential Foundation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#khan-academy-mathematics","title":"Khan Academy Mathematics","text":"<ul> <li>Linear Algebra</li> <li>Topics: Vectors, matrices, transformations, eigenvalues</li> <li>Duration: 40+ hours of video content</li> <li>Interactive: Practice problems with instant feedback</li> <li>Prerequisites: High school algebra</li> <li> <p>Strength: Visual intuition, step-by-step explanations</p> </li> <li> <p>Multivariable Calculus</p> </li> <li>Topics: Partial derivatives, gradients, optimization, integrals</li> <li>Essential for: Understanding gradient descent, backpropagation</li> <li>Duration: 60+ hours comprehensive coverage</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#3blue1brown-visual-mathematics","title":"3Blue1Brown - Visual Mathematics","text":"<ul> <li>Essence of Linear Algebra</li> <li>Episodes: 16 videos, ~15 minutes each</li> <li>Unique: Geometric intuition, visual animations</li> <li>Topics: Vectors, linear transformations, determinants, eigenvectors</li> <li> <p>Impact: Transforms abstract concepts into visual understanding</p> </li> <li> <p>Essence of Calculus</p> </li> <li>Episodes: 12 videos covering calculus fundamentals</li> <li>Focus: Chain rule (essential for backpropagation)</li> <li>Visualization: Makes derivatives and integrals intuitive</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#mit-opencourseware","title":"MIT OpenCourseWare","text":"<ul> <li>18.06: Linear Algebra</li> <li>Instructor: Gilbert Strang (legendary MIT professor)</li> <li>Duration: Full semester course, 35 lectures</li> <li>Textbook: \"Introduction to Linear Algebra\" by Gilbert Strang</li> <li>Level: Rigorous mathematical treatment</li> <li> <p>Applications: Includes applications to differential equations, statistics</p> </li> <li> <p>18.02: Multivariable Calculus</p> </li> <li>Coverage: Partial derivatives, multiple integrals, vector calculus</li> <li>Duration: Full semester, comprehensive treatment</li> <li>Applications: Essential for understanding optimization in ML</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#statistics-and-probability-data-science-core","title":"Statistics and Probability - Data Science Core","text":""},{"location":"Online-Material/Online-Material-for-Learning/#comprehensive-statistical-learning","title":"Comprehensive Statistical Learning","text":"<ul> <li>Statistics and Probability (Khan Academy)</li> <li>Topics: Descriptive statistics, probability, inference, regression</li> <li>Duration: 80+ hours of content</li> <li>Interactive: Built-in practice and assessment</li> <li> <p>Practical: Real-world examples and applications</p> </li> <li> <p>Introduction to Probability - Harvard Stat 110</p> </li> <li>Instructor: Joe Blitzstein</li> <li>Format: Video lectures, problem sets, textbook</li> <li>Duration: Full semester course</li> <li>Reputation: One of the best probability courses globally</li> <li>Resources: Free textbook, extensive problem sets</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#bayesian-statistics","title":"Bayesian Statistics","text":"<ul> <li>Bayesian Methods for Machine Learning</li> <li>Institution: HSE University</li> <li>Duration: 6 weeks, 6-8 hours/week</li> <li>Prerequisites: Probability, linear algebra, Python</li> <li>Topics: Bayesian inference, MCMC, variational methods</li> <li> <p>Applications: Bayesian neural networks, Gaussian processes</p> </li> <li> <p>Think Bayes</p> </li> <li>Format: Free online book with Python implementations</li> <li>Author: Allen B. Downey</li> <li>Approach: Computational methods for Bayesian statistics</li> <li>Code: All examples in Python with clear explanations</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#advanced-mathematical-topics","title":"Advanced Mathematical Topics","text":""},{"location":"Online-Material/Online-Material-for-Learning/#information-theory","title":"Information Theory","text":"<ul> <li>Information Theory for Machine Learning</li> <li>Topics: Entropy, mutual information, KL divergence</li> <li>Applications: Understanding loss functions, generative models</li> <li>Duration: Self-paced study with multiple resources</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#optimization-theory","title":"Optimization Theory","text":"<ul> <li>Convex Optimization (Stanford CS364A)</li> <li>Instructor: Stephen Boyd</li> <li>Textbook: \"Convex Optimization\" (freely available)</li> <li>Prerequisites: Linear algebra, multivariable calculus</li> <li>Applications: Understanding SVM, neural network training</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#graph-theory-for-ml","title":"Graph Theory for ML","text":"<ul> <li>Graph Neural Networks</li> <li>Platform: Distill.pub (visual explanations)</li> <li>Applications: Social networks, molecular structures, knowledge graphs</li> <li>Tools: PyTorch Geometric, DGL</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#programming-languages-tools","title":"\ud83d\udcbb Programming Languages &amp; Tools","text":""},{"location":"Online-Material/Online-Material-for-Learning/#python-the-ml-standard","title":"Python - The ML Standard","text":""},{"location":"Online-Material/Online-Material-for-Learning/#core-python-mastery","title":"Core Python Mastery","text":"<ul> <li>Python.org Official Tutorial</li> <li>Comprehensive: Official documentation with examples</li> <li>Updates: Always current with latest Python version</li> <li> <p>Depth: From basics to advanced features</p> </li> <li> <p>Real Python</p> </li> <li>Content: 500+ tutorials, articles, and courses</li> <li>Quality: Professional-grade Python education</li> <li>Topics: Web development, data science, DevOps, testing</li> <li> <p>Membership: Premium content with video courses</p> </li> <li> <p>Automate the Boring Stuff with Python</p> </li> <li>Author: Al Sweigart</li> <li>Approach: Practical automation projects</li> <li>Free: Complete book available online</li> <li>Projects: Web scraping, Excel automation, email handling</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#scientific-python-ecosystem","title":"Scientific Python Ecosystem","text":"<ul> <li>Python Data Science Handbook</li> <li>Author: Jake VanderPlas (Google Research)</li> <li>Coverage: NumPy, Pandas, Matplotlib, Scikit-learn</li> <li>Format: Jupyter notebooks with executable code</li> <li> <p>Depth: From basics to advanced techniques</p> </li> <li> <p>Effective Python</p> </li> <li>Author: Brett Slatkin (Google)</li> <li>Focus: Writing better, more Pythonic code</li> <li>Items: 90 specific ways to improve Python programming</li> <li>Level: Intermediate to advanced</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#advanced-python-for-data-science","title":"Advanced Python for Data Science","text":"<ul> <li>Python for Data Analysis</li> <li>Author: Wes McKinney (creator of Pandas)</li> <li>Edition: 3<sup>rd</sup> edition (2022)</li> <li>Focus: Data wrangling, cleaning, analysis with Pandas</li> <li> <p>Companion: Official Pandas documentation</p> </li> <li> <p>High Performance Python</p> </li> <li>Authors: Micha Gorelick, Ian Ozsvald</li> <li>Focus: Optimizing Python code for speed</li> <li>Topics: Profiling, NumPy optimization, parallel processing</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#r-statistical-computing-excellence","title":"R - Statistical Computing Excellence","text":""},{"location":"Online-Material/Online-Material-for-Learning/#comprehensive-r-learning","title":"Comprehensive R Learning","text":"<ul> <li>R for Data Science</li> <li>Authors: Hadley Wickham, Garrett Grolemund</li> <li>Framework: Tidyverse ecosystem</li> <li>Coverage: Data import, tidy, transform, visualize, model, communicate</li> <li> <p>Updates: 2<sup>nd</sup> edition covers latest R developments</p> </li> <li> <p>Advanced R</p> </li> <li>Author: Hadley Wickham</li> <li>Level: Deep dive into R internals</li> <li>Topics: Object-oriented programming, functional programming, metaprogramming</li> <li> <p>Audience: Experienced programmers wanting R mastery</p> </li> <li> <p>R Packages</p> </li> <li>Authors: Hadley Wickham, Jenny Bryan</li> <li>Focus: Creating and maintaining R packages</li> <li>Tools: Modern development workflow with usethis, devtools</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#statistical-learning-with-r","title":"Statistical Learning with R","text":"<ul> <li>An Introduction to Statistical Learning with R</li> <li>Authors: James, Witten, Hastie, Tibshirani</li> <li>Companion: To \"Elements of Statistical Learning\"</li> <li>Level: Accessible to undergraduates</li> <li> <p>Labs: Hands-on R implementations of all methods</p> </li> <li> <p>Hands-On Machine Learning with R</p> </li> <li>Authors: Bradley Boehmke, Brandon Greenwell</li> <li>Focus: Practical implementation of ML algorithms in R</li> <li>Coverage: From linear regression to deep learning</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#julia-high-performance-scientific-computing","title":"Julia - High-Performance Scientific Computing","text":""},{"location":"Online-Material/Online-Material-for-Learning/#julia-for-machine-learning","title":"Julia for Machine Learning","text":"<ul> <li>Julia Academy</li> <li>Courses: Introduction to Julia, Data Science, Machine Learning</li> <li>Free: Comprehensive courses at no cost</li> <li> <p>Instructors: Julia core developers and experts</p> </li> <li> <p>Julia Data Science</p> </li> <li>Authors: Jose Storopoli, Rik Huijzer, Lazaro Alonso</li> <li>Coverage: Complete data science workflow in Julia</li> <li> <p>Modern: Uses latest Julia 1.x features and packages</p> </li> <li> <p>Machine Learning with Julia</p> </li> <li>Framework: MLJ.jl - unified machine learning interface</li> <li>Performance: Native Julia speed for large datasets</li> <li>Integration: Can call Python/R libraries when needed</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#scala-big-data-and-functional-programming","title":"Scala - Big Data and Functional Programming","text":""},{"location":"Online-Material/Online-Material-for-Learning/#scala-for-data-science","title":"Scala for Data Science","text":"<ul> <li>Scala for Data Science</li> <li>Focus: Functional programming approach to data science</li> <li>Tools: Spark, Akka, Play framework</li> <li> <p>Level: Intermediate programming experience required</p> </li> <li> <p>Apache Spark with Scala</p> </li> <li>Official: Apache Spark documentation</li> <li>Focus: Distributed computing for big data</li> <li>APIs: RDD, DataFrame, Dataset APIs</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#sql-data-foundation","title":"SQL - Data Foundation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#comprehensive-sql-mastery","title":"Comprehensive SQL Mastery","text":"<ul> <li>SQLBolt</li> <li>Format: Interactive lessons with immediate feedback</li> <li>Progression: From basic queries to advanced joins</li> <li>Duration: 18 lessons, self-paced</li> <li> <p>Practice: Built-in exercises with real datasets</p> </li> <li> <p>W3Schools SQL Tutorial</p> </li> <li>Comprehensive: Complete SQL reference and tutorial</li> <li>Interactive: Try-it-yourself editor</li> <li> <p>Coverage: All SQL dialects and advanced features</p> </li> <li> <p>Mode Analytics SQL Tutorial</p> </li> <li>Business Focus: Real business scenarios and datasets</li> <li>Advanced: Window functions, CTEs, complex joins</li> <li>Tools: Practice with actual business intelligence tools</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#advanced-sql-for-data-science","title":"Advanced SQL for Data Science","text":"<ul> <li>Advanced SQL for Data Scientists</li> <li>Platform: DataCamp</li> <li>Focus: Statistical functions, time series analysis</li> <li> <p>Level: Intermediate to advanced</p> </li> <li> <p>SQL Performance Tuning</p> </li> <li>Author: Markus Winand</li> <li>Focus: Database performance optimization</li> <li>Depth: Understanding indexes, query optimization</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#free-online-books-comprehensive-literature","title":"\ud83d\udcd6 Free Online Books &amp; Comprehensive Literature","text":""},{"location":"Online-Material/Online-Material-for-Learning/#machine-learning-classics","title":"Machine Learning Classics","text":""},{"location":"Online-Material/Online-Material-for-Learning/#the-trinity-of-statistical-learning","title":"The Trinity of Statistical Learning","text":"<ul> <li>The Elements of Statistical Learning</li> <li>Authors: Hastie, Tibshirani, Friedman</li> <li>Level: Graduate-level mathematical treatment</li> <li>Coverage: Complete statistical learning theory</li> <li>Length: 745 pages of comprehensive content</li> <li>Applications: Theory with practical insights</li> <li> <p>PDF: Freely available from Stanford</p> </li> <li> <p>An Introduction to Statistical Learning</p> </li> <li>Authors: James, Witten, Hastie, Tibshirani</li> <li>Level: Undergraduate-friendly version of ESL</li> <li>Code: R and Python implementations available</li> <li>Videos: Complete lecture series on YouTube</li> <li> <p>Length: 426 pages with clear explanations</p> </li> <li> <p>Pattern Recognition and Machine Learning</p> </li> <li>Author: Christopher Bishop</li> <li>Approach: Bayesian perspective on machine learning</li> <li>Depth: Mathematical rigor with intuitive explanations</li> <li>Topics: Bayesian networks, neural networks, kernel methods</li> <li>Exercises: Extensive problem sets for each chapter</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#modern-machine-learning","title":"Modern Machine Learning","text":"<ul> <li>Machine Learning Yearning</li> <li>Author: Andrew Ng</li> <li>Focus: Practical strategies for ML projects</li> <li>Length: 118 pages of actionable advice</li> <li>Topics: Error analysis, debugging ML systems, data strategy</li> <li> <p>Audience: Practitioners working on real ML systems</p> </li> <li> <p>Understanding Machine Learning: From Theory to Algorithms</p> </li> <li>Authors: Shai Shalev-Shwartz, Shai Ben-David</li> <li>Focus: Theoretical foundations with algorithmic perspective</li> <li>Mathematical: Formal treatment of learning theory</li> <li>Length: 410 pages of rigorous content</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#deep-learning-literature","title":"Deep Learning Literature","text":""},{"location":"Online-Material/Online-Material-for-Learning/#foundational-deep-learning","title":"Foundational Deep Learning","text":"<ul> <li>Deep Learning</li> <li>Authors: Ian Goodfellow, Yoshua Bengio, Aaron Courville</li> <li>Status: The definitive deep learning textbook</li> <li>Structure: Mathematics \u2192 Foundations \u2192 Research</li> <li>Length: 800+ pages comprehensive coverage</li> <li> <p>Sections: Linear algebra, optimization, regularization, CNNs, RNNs, attention</p> </li> <li> <p>Neural Networks and Deep Learning</p> </li> <li>Author: Michael Nielsen</li> <li>Approach: Intuitive explanations with mathematical depth</li> <li>Interactive: Code examples and visualizations</li> <li> <p>Focus: Understanding rather than just implementation</p> </li> <li> <p>Dive into Deep Learning</p> </li> <li>Authors: Aston Zhang, Zachary Lipton, Mu Li, Alexander Smola</li> <li>Interactive: Jupyter notebooks with runnable code</li> <li>Frameworks: TensorFlow, PyTorch, MXNet implementations</li> <li>Updates: Continuously updated with latest research</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#specialized-deep-learning-topics","title":"Specialized Deep Learning Topics","text":"<ul> <li>Deep Learning with Python</li> <li>Author: Fran\u00e7ois Chollet (creator of Keras)</li> <li>Practical: Hands-on approach with Keras/TensorFlow</li> <li>Second Edition: Updated for TensorFlow 2.x</li> <li> <p>Projects: Real-world applications and case studies</p> </li> <li> <p>Generative Deep Learning</p> </li> <li>Author: David Foster</li> <li>Focus: VAEs, GANs, transformers, diffusion models</li> <li>Code: Complete implementations in TensorFlow/Keras</li> <li>Applications: Art, music, and creative AI</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#natural-language-processing_1","title":"Natural Language Processing","text":""},{"location":"Online-Material/Online-Material-for-Learning/#comprehensive-nlp-resources","title":"Comprehensive NLP Resources","text":"<ul> <li>Speech and Language Processing</li> <li>Authors: Dan Jurafsky, James Martin</li> <li>Edition: Third edition (draft freely available)</li> <li>Coverage: Traditional NLP to modern neural methods</li> <li> <p>Depth: From linguistics to deep learning applications</p> </li> <li> <p>Natural Language Processing with Python</p> </li> <li>Authors: Steven Bird, Ewan Klein, Edward Loper</li> <li>Framework: NLTK library focus</li> <li>Practical: Hands-on approach with real text data</li> <li>Topics: Tokenization, parsing, semantic analysis</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#computer-vision_1","title":"Computer Vision","text":""},{"location":"Online-Material/Online-Material-for-Learning/#computer-vision-foundations","title":"Computer Vision Foundations","text":"<ul> <li>Computer Vision: Algorithms and Applications</li> <li>Author: Richard Szeliski (Microsoft Research)</li> <li>Coverage: Classical to modern computer vision</li> <li>Length: 950+ pages comprehensive treatment</li> <li> <p>Applications: Image processing to 3D reconstruction</p> </li> <li> <p>Multiple View Geometry</p> </li> <li>Authors: Hartley and Zisserman</li> <li>Focus: 3D computer vision and reconstruction</li> <li>Mathematical: Rigorous geometric approach</li> <li>Applications: Structure from motion, stereo vision</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#video-content-educational-channels","title":"\ud83c\udfa5 Video Content &amp; Educational Channels","text":""},{"location":"Online-Material/Online-Material-for-Learning/#mathematical-intuition-channels","title":"Mathematical Intuition Channels","text":""},{"location":"Online-Material/Online-Material-for-Learning/#3blue1brown-grant-sanderson","title":"3Blue1Brown - Grant Sanderson","text":"<ul> <li>Neural Networks Series</li> <li>Episodes: 4-part series on neural network fundamentals</li> <li>Visualizations: Stunning mathematical animations</li> <li>Topics: Gradient descent, backpropagation, network topology</li> <li> <p>Impact: Makes complex concepts visually intuitive</p> </li> <li> <p>Essence of Calculus</p> </li> <li>Essential for: Understanding optimization and backpropagation</li> <li>Visualization: Geometric interpretation of derivatives</li> <li>Chain Rule: Critical for understanding neural networks</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#statquest-with-josh-starmer","title":"StatQuest with Josh Starmer","text":"<ul> <li>Machine Learning Playlist</li> <li>Content: 100+ videos covering ML algorithms</li> <li>Style: Clear explanations with humor and music</li> <li>Coverage: Linear regression to advanced ensemble methods</li> <li> <p>Strength: Makes statistics accessible and memorable</p> </li> <li> <p>Statistics Fundamentals</p> </li> <li>Topics: P-values, confidence intervals, hypothesis testing</li> <li>Approach: Intuitive explanations without heavy math</li> <li>Duration: Short, focused videos (10-15 minutes)</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#research-and-paper-discussions","title":"Research and Paper Discussions","text":""},{"location":"Online-Material/Online-Material-for-Learning/#two-minute-papers-karoly-zsolnai-feher","title":"Two Minute Papers - K\u00e1roly Zsolnai-Feh\u00e9r","text":"<ul> <li>AI Research Summaries</li> <li>Frequency: 2-3 videos per week</li> <li>Format: Latest AI research in accessible format</li> <li>Coverage: Computer graphics, deep learning, generative AI</li> <li>Catchphrase: \"What a time to be alive!\"</li> <li>Value: Stay current with cutting-edge research</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#yannic-kilcher","title":"Yannic Kilcher","text":"<ul> <li>Paper Reviews</li> <li>Depth: Detailed technical analysis of papers</li> <li>Duration: 30-60 minute deep dives</li> <li>Coverage: Latest ML/AI research papers</li> <li>Style: Line-by-line paper reading with explanations</li> <li>Audience: Advanced students and researchers</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#machine-learning-street-talk","title":"Machine Learning Street Talk","text":"<ul> <li>Research Discussions</li> <li>Format: Panel discussions with researchers</li> <li>Guests: Leading AI researchers and practitioners</li> <li>Topics: AGI, consciousness, AI safety, technical deep-dives</li> <li>Duration: 1-3 hour conversations</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#implementation-and-coding","title":"Implementation and Coding","text":""},{"location":"Online-Material/Online-Material-for-Learning/#sentdex-harrison-kinsley","title":"Sentdex - Harrison Kinsley","text":"<ul> <li>Machine Learning with Python</li> <li>Style: Code-along tutorials</li> <li>Coverage: Scikit-learn, TensorFlow, financial applications</li> <li>Projects: Real-world implementations</li> <li>Audience: Beginner to intermediate programmers</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#deeplearningai-youtube-channel","title":"DeepLearningAI YouTube Channel","text":"<ul> <li>Course Supplements</li> <li>Content: Supplementary material to Coursera courses</li> <li>Instructors: Andrew Ng and team</li> <li>Topics: Latest in AI research and applications</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#university-lecture-series","title":"University Lecture Series","text":""},{"location":"Online-Material/Online-Material-for-Learning/#stanford-university-courses","title":"Stanford University Courses","text":"<ul> <li>CS229: Machine Learning</li> <li>Instructor: Andrew Ng</li> <li>Duration: Complete semester (20 lectures)</li> <li>Level: Graduate-level mathematical treatment</li> <li> <p>Prerequisites: Linear algebra, probability, programming</p> </li> <li> <p>CS231n: CNNs for Visual Recognition</p> </li> <li>Instructors: Andrej Karpathy, Fei-Fei Li</li> <li>Focus: Deep learning for computer vision</li> <li>Assignments: Available online with solutions</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#mit-opencourseware_1","title":"MIT OpenCourseWare","text":"<ul> <li>6.034 Artificial Intelligence</li> <li>Instructor: Patrick Winston</li> <li>Style: Engaging storytelling approach</li> <li>Coverage: Classical AI to machine learning</li> <li>Duration: Full semester course</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#interactive-learning-platforms-hands-on-practice","title":"\ud83d\udcbb Interactive Learning Platforms &amp; Hands-On Practice","text":""},{"location":"Online-Material/Online-Material-for-Learning/#comprehensive-learning-platforms","title":"Comprehensive Learning Platforms","text":""},{"location":"Online-Material/Online-Material-for-Learning/#kaggle-learn","title":"Kaggle Learn","text":"<ul> <li>Free Micro-Courses</li> <li>Python: 7 lessons, 5 hours</li> <li>Machine Learning: 7 lessons, 3 hours</li> <li>Deep Learning: 5 lessons, 4 hours</li> <li>Feature Engineering: 5 lessons, 5 hours</li> <li>Data Visualization: 4 lessons, 4 hours</li> <li>Pandas: 4 lessons, 4 hours</li> <li>SQL: 4 lessons, 2 hours</li> <li>Natural Language Processing: 5 lessons, 4 hours</li> <li>Computer Vision: 4 lessons, 3 hours</li> <li>Time Series: 5 lessons, 5 hours</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#datacamp","title":"DataCamp","text":"<ul> <li>Career Tracks</li> <li>Data Scientist with Python: 25 courses, 96 hours</li> <li>Data Analyst with Python: 16 courses, 62 hours</li> <li>Machine Learning Scientist with Python: 23 courses, 98 hours</li> <li>Data Scientist with R: 23 courses, 98 hours</li> <li> <p>Statistician with R: 26 courses, 112 hours</p> </li> <li> <p>Skill Tracks</p> </li> <li>Machine Learning with scikit-learn: 4 courses, 16 hours</li> <li>Deep Learning in Python: 4 courses, 16 hours</li> <li>Natural Language Processing in Python: 4 courses, 16 hours</li> <li>Image Processing in Python: 4 courses, 16 hours</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#coursera-hands-on-projects","title":"Coursera Hands-On Projects","text":"<ul> <li>Guided Projects</li> <li>Duration: 1-2 hours each</li> <li>Format: Split-screen with instructor guidance</li> <li>Topics: Specific skills and tools</li> <li>Examples: Build a chatbot, create a neural network, analyze stock data</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#coding-challenge-platforms","title":"Coding Challenge Platforms","text":""},{"location":"Online-Material/Online-Material-for-Learning/#algorithm-and-data-structure-practice","title":"Algorithm and Data Structure Practice","text":"<ul> <li>LeetCode</li> <li>Problems: 2000+ algorithmic challenges</li> <li>Categories: Easy, Medium, Hard difficulties</li> <li>Topics: Arrays, trees, graphs, dynamic programming</li> <li>Interview Prep: Company-specific problem sets</li> <li> <p>Premium: Solutions, company tags, frequency data</p> </li> <li> <p>HackerRank</p> </li> <li>AI Domain: Machine learning specific challenges</li> <li>Categories: Bot Building, Machine Learning, Probability</li> <li>Certifications: Skills verification badges</li> <li>Languages: Multiple programming language support</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#data-science-competitions","title":"Data Science Competitions","text":"<ul> <li>Kaggle Competitions</li> <li>Categories: Featured, research, getting started, playground</li> <li>Prize Money: Up to $100,000+ for major competitions</li> <li>Learning: Public kernels with winning solutions</li> <li>Networking: Global community of data scientists</li> <li> <p>Progression: Novice to Grandmaster ranking system</p> </li> <li> <p>DrivenData</p> </li> <li>Focus: Social impact competitions</li> <li>Partners: Non-profits, governments, research institutions</li> <li> <p>Topics: Healthcare, education, environment, humanitarian</p> </li> <li> <p>Analytics Vidhya DataHack</p> </li> <li>Frequency: Regular competitions and hackathons</li> <li>Community: Active discussion forums</li> <li>Learning: Detailed solution explanations</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#project-based-learning","title":"Project-Based Learning","text":""},{"location":"Online-Material/Online-Material-for-Learning/#google-colab-notebooks","title":"Google Colab Notebooks","text":"<ul> <li>Seedbank</li> <li>Collection: Curated machine learning examples</li> <li>Categories: Vision, language, music, art</li> <li>Interactive: Run in browser with free GPU</li> <li>Educational: Well-documented example projects</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#github-learning-repositories","title":"GitHub Learning Repositories","text":"<ul> <li>ML-For-Beginners (Microsoft)</li> <li>Duration: 12-week curriculum</li> <li>Languages: Python and R options</li> <li>Projects: Real-world applications</li> <li> <p>Quizzes: Built-in assessment tools</p> </li> <li> <p>Data Science Handbook</p> </li> <li>Author: Jake VanderPlas</li> <li>Content: Complete book as Jupyter notebooks</li> <li>Libraries: NumPy, Pandas, Matplotlib, Scikit-learn</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#research-resources-academic-papers","title":"\ud83d\udd2c Research Resources &amp; Academic Papers","text":""},{"location":"Online-Material/Online-Material-for-Learning/#paper-repositories-and-search","title":"Paper Repositories and Search","text":""},{"location":"Online-Material/Online-Material-for-Learning/#primary-research-sources","title":"Primary Research Sources","text":"<ul> <li>arXiv.org</li> <li>Sections: cs.LG (Learning), cs.AI (AI), cs.CV (Vision), cs.CL (NLP)</li> <li>Daily Updates: Latest research posted daily</li> <li>Access: Free, open-access pre-prints</li> <li> <p>Search Tips: Use specific categories and date ranges</p> </li> <li> <p>Papers With Code</p> </li> <li>Innovation: Links papers with implementation code</li> <li>Benchmarks: Tracks state-of-the-art results</li> <li>Datasets: Comprehensive dataset collection</li> <li>Trends: Popular papers and emerging topics</li> <li> <p>Reproducibility: Focus on reproducible research</p> </li> <li> <p>Google Scholar</p> </li> <li>Coverage: Academic papers across all disciplines</li> <li>Citations: Citation counts and academic impact</li> <li>Alerts: Set up alerts for specific topics or authors</li> <li>Profiles: Follow leading researchers</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#academic-conferences","title":"Academic Conferences","text":""},{"location":"Online-Material/Online-Material-for-Learning/#top-tier-machine-learning-conferences","title":"Top-Tier Machine Learning Conferences","text":"<ul> <li>NeurIPS (Neural Information Processing Systems)</li> <li>Status: Premier ML conference</li> <li>Acceptance Rate: ~20% (highly selective)</li> <li>Proceedings: All papers freely available</li> <li>Videos: Conference talks on YouTube</li> <li> <p>Workshops: Specialized topic workshops</p> </li> <li> <p>ICML (International Conference on Machine Learning)</p> </li> <li>Focus: Theoretical and practical ML research</li> <li>Proceedings: Available through PMLR</li> <li>Tutorials: Educational sessions for researchers</li> <li> <p>Best Papers: Annual awards for outstanding research</p> </li> <li> <p>ICLR (International Conference on Learning Representations)</p> </li> <li>Innovation: Open review process</li> <li>Focus: Representation learning and deep learning</li> <li>OpenReview: All submissions and reviews public</li> <li>Workshops: Cutting-edge research areas</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#specialized-conference-areas","title":"Specialized Conference Areas","text":"<ul> <li>CVPR (Computer Vision and Pattern Recognition)</li> <li>Computer Vision: Premier CV conference</li> <li>Industry: Strong industry participation</li> <li> <p>Applications: Practical CV applications</p> </li> <li> <p>ACL (Association for Computational Linguistics)</p> </li> <li>NLP Focus: Natural language processing</li> <li>Findings: Additional venue for quality papers</li> <li>Workshops: Specialized NLP topics</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#research-paper-reading-guides","title":"Research Paper Reading Guides","text":""},{"location":"Online-Material/Online-Material-for-Learning/#how-to-read-research-papers","title":"How to Read Research Papers","text":"<ul> <li>Efficient Reading of Papers in Science and Technology</li> <li>Author: S. Keshav (University of Waterloo)</li> <li>Method: Three-pass approach</li> <li> <p>Structure: Strategic reading for comprehension and critique</p> </li> <li> <p>How to Read a Paper (Andrew Ng)</p> </li> <li>Approach: Systematic method for paper comprehension</li> <li>Tables: Focus on results and methodology</li> <li>Code: Importance of implementation details</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#paper-implementation-practice","title":"Paper Implementation Practice","text":"<ul> <li>Paperspace Gradient</li> <li>Environment: Cloud-based ML development</li> <li>Tutorials: Step-by-step paper implementations</li> <li> <p>GPUs: Access to high-performance computing</p> </li> <li> <p>Replicate</p> </li> <li>Repository: Pre-trained model implementations</li> <li>API: Easy access to state-of-the-art models</li> <li>Community: Share and discover model implementations</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#competitions-practical-application","title":"\ud83c\udfc6 Competitions &amp; Practical Application","text":""},{"location":"Online-Material/Online-Material-for-Learning/#machine-learning-competitions","title":"Machine Learning Competitions","text":""},{"location":"Online-Material/Online-Material-for-Learning/#kaggle-competition-strategy","title":"Kaggle Competition Strategy","text":"<ul> <li>Getting Started Competitions</li> <li>Titanic: Classic binary classification problem</li> <li>House Prices: Regression with feature engineering</li> <li>Digit Recognizer: MNIST digit classification</li> <li> <p>Purpose: Learn competition format and evaluation</p> </li> <li> <p>Featured Competitions</p> </li> <li>Prize Money: Significant cash prizes (\\(10K-\\)100K+)</li> <li>Industry Partners: Real business problems</li> <li>Timeline: 2-4 months typically</li> <li>Teams: Collaboration opportunities</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#competition-learning-resources","title":"Competition Learning Resources","text":"<ul> <li>Kaggle Learn Competitions Course</li> <li>Duration: 3 hours</li> <li>Focus: Competition-specific techniques</li> <li> <p>Tools: Kaggle notebook environment</p> </li> <li> <p>Winning Solution Analysis</p> </li> <li>Post-Competition: Winners share detailed solutions</li> <li>Code: Complete implementations available</li> <li>Insights: Feature engineering and model ensemble techniques</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#specialized-competition-platforms","title":"Specialized Competition Platforms","text":""},{"location":"Online-Material/Online-Material-for-Learning/#computer-vision-challenges","title":"Computer Vision Challenges","text":"<ul> <li>ImageNet Challenge</li> <li>Historical: Launched the deep learning revolution</li> <li>Legacy: Annual results show progress in CV</li> <li> <p>Datasets: Large-scale image classification</p> </li> <li> <p>COCO Challenge</p> </li> <li>Tasks: Object detection, segmentation, captioning</li> <li>Metrics: Standardized evaluation protocols</li> <li>Leaderboards: Compare against state-of-the-art</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#natural-language-processing_2","title":"Natural Language Processing","text":"<ul> <li>GLUE Benchmark</li> <li>Tasks: 9 English sentence understanding tasks</li> <li>Leaderboard: Track progress on language understanding</li> <li> <p>SuperGLUE: More challenging follow-up benchmark</p> </li> <li> <p>SQuAD (Stanford Question Answering Dataset)</p> </li> <li>Task: Reading comprehension</li> <li>Versions: SQuAD 1.1 and 2.0 with different challenges</li> <li>Leaderboard: Human performance comparison</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#specialized-learning-paths","title":"\ud83c\udfaf Specialized Learning Paths","text":""},{"location":"Online-Material/Online-Material-for-Learning/#domain-specific-career-tracks","title":"Domain-Specific Career Tracks","text":""},{"location":"Online-Material/Online-Material-for-Learning/#computer-vision-engineer-path","title":"Computer Vision Engineer Path","text":"<p>Phase 1: Mathematical Foundations (2-3 months) - Linear Algebra: MIT 18.06 or Khan Academy - Calculus: Multivariable calculus essentials - Probability: Basic probability and statistics - Signal Processing: Digital image processing basics</p> <p>Phase 2: Programming Foundations (2-3 months) - Python: Advanced Python programming - NumPy: Array manipulation and mathematical operations - OpenCV: Computer vision library fundamentals - Matplotlib: Data visualization</p> <p>Phase 3: Classical Computer Vision (3-4 months) - Image Processing: Filtering, edge detection, morphology - Feature Detection: SIFT, SURF, ORB descriptors - Object Detection: Haar cascades, HOG features - Geometric Vision: Camera calibration, stereo vision</p> <p>Phase 4: Deep Learning for Vision (4-6 months) - Neural Networks: Fundamentals and backpropagation - CNNs: Architecture design and training - Transfer Learning: Pre-trained models and fine-tuning - Object Detection: YOLO, R-CNN family, SSD - Semantic Segmentation: FCN, U-Net, DeepLab</p> <p>Phase 5: Advanced Topics (6+ months) - Generative Models: GANs, VAEs for image generation - 3D Vision: Point clouds, mesh processing, NeRF - Video Analysis: Action recognition, object tracking - Production: Model optimization, deployment, monitoring</p>"},{"location":"Online-Material/Online-Material-for-Learning/#nlp-engineer-path","title":"NLP Engineer Path","text":"<p>Phase 1: Linguistic Foundations (2-3 months) - Linguistics: Basic syntax, semantics, pragmatics - Statistics: Text statistics and probability - Information Theory: Entropy, mutual information - Text Processing: Tokenization, normalization, encoding</p> <p>Phase 2: Classical NLP (3-4 months) - Text Preprocessing: Cleaning, tokenization, stemming - Feature Engineering: Bag of words, TF-IDF, n-grams - Text Classification: Naive Bayes, SVM, logistic regression - Information Extraction: Named entity recognition, relation extraction</p> <p>Phase 3: Modern NLP with Deep Learning (4-6 months) - Word Embeddings: Word2Vec, GloVe, FastText - Sequence Models: RNNs, LSTMs, GRUs - Attention Mechanisms: Self-attention, multi-head attention - Transformers: BERT, GPT, T5 architectures</p> <p>Phase 4: Advanced NLP Applications (6+ months) - Language Models: GPT fine-tuning, prompt engineering - Question Answering: Reading comprehension, knowledge QA - Dialogue Systems: Chatbots, conversational AI - Multimodal: Vision-language models, cross-modal understanding</p>"},{"location":"Online-Material/Online-Material-for-Learning/#mlops-engineer-path","title":"MLOps Engineer Path","text":"<p>Phase 1: Software Engineering Foundations (2-3 months) - Version Control: Git, GitHub workflows - Programming: Python, bash scripting - Containerization: Docker fundamentals - Cloud Platforms: AWS/GCP/Azure basics</p> <p>Phase 2: ML Pipeline Development (3-4 months) - Data Pipeline: ETL processes, data validation - Model Training: Experiment tracking, hyperparameter tuning - Model Evaluation: Metrics, validation strategies - Model Registry: Versioning, metadata management</p> <p>Phase 3: Deployment and Monitoring (4-6 months) - Model Serving: REST APIs, batch processing - Orchestration: Apache Airflow, Kubeflow - Monitoring: Performance tracking, data drift detection - CI/CD: Automated testing, deployment pipelines</p> <p>Phase 4: Advanced MLOps (6+ months) - Kubernetes: Container orchestration for ML - Feature Stores: Centralized feature management - Model Governance: Compliance, auditing, fairness - Advanced Monitoring: Explainability, bias detection</p>"},{"location":"Online-Material/Online-Material-for-Learning/#industry-specific-applications","title":"Industry-Specific Applications","text":""},{"location":"Online-Material/Online-Material-for-Learning/#healthcare-ai-specialization","title":"Healthcare AI Specialization","text":"<p>Fundamentals - Medical Terminology: Basic anatomy and physiology - Healthcare Data: FHIR, DICOM, EHR systems - Regulations: HIPAA, FDA approval processes - Ethics: Medical AI ethics and bias considerations</p> <p>Technical Skills - Medical Imaging: X-ray, MRI, CT scan analysis - Time Series: Patient monitoring data, ECG analysis - Natural Language: Clinical notes processing, ICD coding - Predictive Modeling: Risk assessment, treatment optimization</p> <p>Resources - Healthcare AI Course (Stanford) - Medical Image Analysis (Coursera) - Clinical Data Science (MIT)</p>"},{"location":"Online-Material/Online-Material-for-Learning/#financial-ai-specialization","title":"Financial AI Specialization","text":"<p>Domain Knowledge - Financial Markets: Stocks, bonds, derivatives, trading - Risk Management: Credit risk, market risk, operational risk - Regulations: Basel III, MiFID II, Dodd-Frank compliance - Quantitative Finance: Portfolio theory, options pricing</p> <p>Technical Applications - Algorithmic Trading: Strategy development, backtesting - Risk Modeling: Credit scoring, fraud detection - Portfolio Optimization: Asset allocation, robo-advisors - Alternative Data: Satellite imagery, social media sentiment</p> <p>Resources - AI for Trading Nanodegree (Udacity) - Quantitative Finance (QuantStart) - Financial Markets (Yale/Coursera)</p>"},{"location":"Online-Material/Online-Material-for-Learning/#certifications-professional-development","title":"\ud83d\udcdc Certifications &amp; Professional Development","text":""},{"location":"Online-Material/Online-Material-for-Learning/#industry-recognized-certifications","title":"Industry-Recognized Certifications","text":""},{"location":"Online-Material/Online-Material-for-Learning/#cloud-platform-certifications","title":"Cloud Platform Certifications","text":"<ul> <li>AWS Certified Machine Learning - Specialty</li> <li>Prerequisites: 2+ years AWS experience</li> <li>Domains: Data engineering, exploratory analysis, modeling, implementation</li> <li>Duration: 180 minutes, 65 questions</li> <li>Cost: $300</li> <li> <p>Renewal: 3 years</p> </li> <li> <p>Google Cloud Professional Machine Learning Engineer</p> </li> <li>Experience: 3+ years industry, 1+ year GCP</li> <li>Skills: ML solution design, development, deployment</li> <li>Format: 2 hours, multiple choice and select</li> <li> <p>Cost: $200</p> </li> <li> <p>Microsoft Azure AI Engineer Associate</p> </li> <li>Prerequisites: Familiarity with Azure and AI services</li> <li>Focus: Cognitive services, search solutions, conversational AI</li> <li>Duration: Various learning paths available</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#professional-data-science-certifications","title":"Professional Data Science Certifications","text":"<ul> <li>Certified Analytics Professional (CAP)</li> <li>Provider: INFORMS (Institute for Operations Research)</li> <li>Requirements: Bachelor's + 5 years experience OR Master's + 3 years</li> <li>Domains: Business problem framing, analytics, deployment, lifecycle management</li> <li> <p>Cost: $495 members, $695 non-members</p> </li> <li> <p>SAS Certified Data Scientist</p> </li> <li>Prerequisites: Advanced knowledge of statistics and programming</li> <li>Skills: Data manipulation, predictive modeling, machine learning</li> <li>Format: Multiple exams required</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#open-source-technology-certifications","title":"Open Source Technology Certifications","text":"<ul> <li>TensorFlow Developer Certificate</li> <li>Provider: Google/TensorFlow team</li> <li>Format: Hands-on coding exam in PyCharm</li> <li>Duration: 5 hours to complete practical tasks</li> <li>Skills: Neural networks, computer vision, NLP, time series</li> <li> <p>Cost: $100</p> </li> <li> <p>NVIDIA Deep Learning Institute Certificates</p> </li> <li>Courses: Fundamentals, computer vision, NLP, accelerated computing</li> <li>Format: Hands-on labs with GPUs</li> <li>Duration: 8 hours per course typically</li> <li>Recognition: Industry-recognized competency</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#academic-credentials","title":"Academic Credentials","text":""},{"location":"Online-Material/Online-Material-for-Learning/#masters-degree-programs-online","title":"Master's Degree Programs (Online)","text":"<ul> <li>Georgia Tech OMSCS</li> <li>Specializations: Machine Learning, Computational Perception, Computing Systems</li> <li>Duration: 2-6 years part-time</li> <li>Cost: ~$7,000 total</li> <li> <p>Admission: Competitive, programming experience required</p> </li> <li> <p>University of Illinois MCS-DS</p> </li> <li>Focus: Data science and analytics</li> <li>Duration: 1.5-5 years</li> <li>Cost: ~$21,000 total</li> <li> <p>Admission: GRE not required, programming experience essential</p> </li> <li> <p>Stanford MS in Computer Science (Online)</p> </li> <li>Specializations: AI track available</li> <li>Duration: Flexible, up to 5 years</li> <li>Cost: Stanford tuition rates</li> <li>Admission: Highly competitive</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#phd-preparation-programs","title":"PhD Preparation Programs","text":"<ul> <li>MIT Professional Education</li> <li>Programs: Various AI-focused professional development</li> <li>Duration: Short courses to multi-month programs</li> <li> <p>Recognition: MIT certificate of completion</p> </li> <li> <p>Stanford AI Professional Program</p> </li> <li>Courses: 4 graduate-level courses</li> <li>Duration: 9-18 months</li> <li>Cost: ~$15,000</li> <li>Certificate: Stanford Graduate Certificate</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#career-development-job-preparation","title":"\ud83d\udcbc Career Development &amp; Job Preparation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#technical-interview-preparation","title":"Technical Interview Preparation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#coding-interview-platforms","title":"Coding Interview Platforms","text":"<ul> <li>LeetCode Premium</li> <li>Problems: 2000+ algorithmic challenges</li> <li>Company Tags: Problems organized by hiring companies</li> <li>Mock Interviews: Timed practice sessions</li> <li>Solutions: Detailed explanations and optimal approaches</li> <li> <p>Frequency: Problem frequency in actual interviews</p> </li> <li> <p>HackerRank Interview Preparation Kit</p> </li> <li>Topics: Arrays, hash tables, graphs, dynamic programming</li> <li>Difficulty: Graduated difficulty levels</li> <li>Time Limits: Realistic interview constraints</li> <li>Languages: Multiple programming language support</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#system-design-for-ml-systems","title":"System Design for ML Systems","text":"<ul> <li>Designing Machine Learning Systems (Book)</li> <li>Author: Chip Huyen</li> <li>Focus: Production ML system architecture</li> <li> <p>Topics: Data pipeline, model training, deployment, monitoring</p> </li> <li> <p>Machine Learning System Design Interview</p> </li> <li>Platform: Educative</li> <li>Format: Interactive course with practical examples</li> <li>Cases: Real ML system design problems</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#domain-specific-interview-prep","title":"Domain-Specific Interview Prep","text":"<ul> <li>Data Science Interview Questions</li> <li>Repository: Comprehensive question collection</li> <li>Categories: Statistics, programming, machine learning, case studies</li> <li> <p>Solutions: Detailed answers and explanations</p> </li> <li> <p>ML Interview Guide</p> </li> <li>Scope: 500+ ML interview questions</li> <li>Topics: Algorithms, statistics, programming, system design</li> <li>Format: Question-answer pairs with explanations</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#portfolio-development","title":"Portfolio Development","text":""},{"location":"Online-Material/Online-Material-for-Learning/#project-portfolio-guidelines","title":"Project Portfolio Guidelines","text":"<p>Essential Projects for ML Portfolio:</p> <ol> <li>End-to-End ML Project</li> <li>Scope: Complete pipeline from data collection to deployment</li> <li>Skills: Data preprocessing, model training, evaluation, deployment</li> <li>Tools: GitHub, Docker, cloud platforms</li> <li> <p>Documentation: Detailed README, methodology explanation</p> </li> <li> <p>Deep Learning Project</p> </li> <li>Domain: Computer vision or NLP</li> <li>Complexity: Custom architecture or advanced transfer learning</li> <li>Evaluation: Comprehensive performance analysis</li> <li> <p>Visualization: Model interpretability and error analysis</p> </li> <li> <p>Data Analysis Project</p> </li> <li>Dataset: Real-world, messy data</li> <li>Skills: EDA, statistical analysis, visualization</li> <li>Insights: Business-relevant conclusions</li> <li>Communication: Clear presentation of findings</li> </ol>"},{"location":"Online-Material/Online-Material-for-Learning/#portfolio-platforms","title":"Portfolio Platforms","text":"<ul> <li>GitHub Portfolio Guide</li> <li>Optimization: Professional README, pinned repositories</li> <li>Documentation: Clear project descriptions and instructions</li> <li> <p>Code Quality: Clean, commented, reproducible code</p> </li> <li> <p>Kaggle Profile</p> </li> <li>Competitions: Participation in relevant competitions</li> <li>Notebooks: Public analysis and tutorials</li> <li>Datasets: Contribute useful datasets</li> <li>Discussion: Active community participation</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#networking-and-professional-development","title":"Networking and Professional Development","text":""},{"location":"Online-Material/Online-Material-for-Learning/#professional-communities","title":"Professional Communities","text":"<ul> <li>LinkedIn AI/ML Groups</li> <li>Groups: Machine Learning Professionals, Data Science Central</li> <li>Content: Industry news, job postings, discussions</li> <li> <p>Networking: Connect with industry professionals</p> </li> <li> <p>Reddit Communities</p> </li> <li>r/MachineLearning: Research discussions, paper releases</li> <li>r/datascience: Career advice, industry insights</li> <li>r/LearnMachineLearning: Educational content, beginner questions</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#conference-participation","title":"Conference Participation","text":"<ul> <li>Major ML Conferences</li> <li>Attendance: NeurIPS, ICML, ICLR (virtual options available)</li> <li>Workshops: Specialized topic sessions</li> <li>Networking: Industry mixer events and social gatherings</li> <li>Presentations: Present research or project work</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#salary-and-market-information","title":"Salary and Market Information","text":""},{"location":"Online-Material/Online-Material-for-Learning/#compensation-research","title":"Compensation Research","text":"<ul> <li>levels.fyi</li> <li>Data: Detailed compensation data by company and level</li> <li>Roles: Software engineer, data scientist, ML engineer</li> <li>Geography: Location-based salary comparisons</li> <li> <p>Stock: Equity compensation details</p> </li> <li> <p>Glassdoor</p> </li> <li>Salaries: Self-reported compensation data</li> <li>Reviews: Company culture and work environment</li> <li>Interviews: Interview experience sharing</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#market-trends","title":"Market Trends","text":"<ul> <li>AI Index Report (Stanford)</li> <li>Annual: Comprehensive AI industry analysis</li> <li>Trends: Job market, investment, research progress</li> <li> <p>Data: Quantitative analysis of AI adoption</p> </li> <li> <p>Kaggle State of ML and Data Science Survey</p> </li> <li>Annual: Industry survey results</li> <li>Demographics: Role distribution, education, experience</li> <li>Tools: Popular technologies and platforms</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#study-methodologies-learning-optimization","title":"\ud83e\udde0 Study Methodologies &amp; Learning Optimization","text":""},{"location":"Online-Material/Online-Material-for-Learning/#effective-learning-strategies","title":"Effective Learning Strategies","text":""},{"location":"Online-Material/Online-Material-for-Learning/#active-learning-techniques","title":"Active Learning Techniques","text":"<ul> <li>Feynman Technique</li> <li>Step 1: Choose concept to learn</li> <li>Step 2: Explain in simple terms</li> <li>Step 3: Identify knowledge gaps</li> <li>Step 4: Review and simplify</li> <li> <p>Application: Explaining ML algorithms in plain language</p> </li> <li> <p>Spaced Repetition System</p> </li> <li>Tools: Anki, Quizlet for ML concepts</li> <li>Schedule: Review material at increasing intervals</li> <li>Content: Mathematical formulas, algorithm steps</li> <li>Effectiveness: Proven long-term retention improvement</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#project-based-learning_1","title":"Project-Based Learning","text":"<ul> <li>Learning by Building</li> <li>Philosophy: Fast.ai's top-down approach</li> <li>Method: Start with working code, understand deeply later</li> <li>Projects: Implement before full theoretical understanding</li> <li>Iteration: Gradually increase complexity and depth</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#research-paper-reading-system","title":"Research Paper Reading System","text":"<ul> <li>Three-Pass Method</li> <li>First Pass: Title, abstract, conclusion (5 minutes)</li> <li>Second Pass: Introduction, headings, figures (1 hour)</li> <li>Third Pass: Full understanding, note-taking (4-5 hours)</li> <li>Decision Points: Determine relevance at each pass</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#time-management-and-productivity","title":"Time Management and Productivity","text":""},{"location":"Online-Material/Online-Material-for-Learning/#study-schedule-optimization","title":"Study Schedule Optimization","text":"<ul> <li>Pomodoro Technique</li> <li>Duration: 25-minute focused study sessions</li> <li>Breaks: 5-minute breaks between sessions</li> <li>Long Break: 15-30 minutes after 4 sessions</li> <li> <p>Applications: Coding, reading papers, watching lectures</p> </li> <li> <p>Time Blocking</p> </li> <li>Deep Work: Uninterrupted focus on cognitively demanding tasks</li> <li>Schedule: Fixed time blocks for learning activities</li> <li>Environment: Distraction-free workspace setup</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#progress-tracking-systems","title":"Progress Tracking Systems","text":"<ul> <li>Learning Portfolio</li> <li>Documentation: Track projects, courses, skills acquired</li> <li>Reflection: Regular assessment of learning progress</li> <li> <p>Evidence: Code repositories, certificates, project outcomes</p> </li> <li> <p>Habit Tracking</p> </li> <li>Daily Goals: Consistent learning habits</li> <li>Metrics: Hours studied, papers read, projects completed</li> <li>Tools: Habit tracking apps, spreadsheets, journals</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#knowledge-management-systems","title":"Knowledge Management Systems","text":""},{"location":"Online-Material/Online-Material-for-Learning/#note-taking-and-organization","title":"Note-Taking and Organization","text":"<ul> <li>Zettelkasten Method</li> <li>Principles: Atomic notes, unique identifiers, linking</li> <li>Tools: Obsidian, Roam Research, Notion</li> <li>Application: Connect ML concepts across domains</li> <li> <p>Benefits: Knowledge graph for complex topics</p> </li> <li> <p>Cornell Note-Taking System</p> </li> <li>Format: Cue column, note-taking area, summary</li> <li>Application: Lecture notes, paper summaries</li> <li>Review: Structured review process</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#digital-knowledge-management","title":"Digital Knowledge Management","text":"<ul> <li>Notion</li> <li>Organization: Hierarchical page structure</li> <li>Templates: Course tracking, project management</li> <li>Collaboration: Shared workspaces for study groups</li> <li> <p>Integration: Embed code, videos, external resources</p> </li> <li> <p>Obsidian</p> </li> <li>Graph View: Visualize concept relationships</li> <li>Markdown: Plain text, future-proof format</li> <li>Plugins: Extensible functionality</li> <li>Linking: Bi-directional linking between concepts</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#global-and-specialized-resources","title":"\ud83c\udf10 Global and Specialized Resources","text":""},{"location":"Online-Material/Online-Material-for-Learning/#non-english-language-resources","title":"Non-English Language Resources","text":""},{"location":"Online-Material/Online-Material-for-Learning/#chinese-language-resources","title":"Chinese Language Resources","text":"<ul> <li>Machine Learning (Hung-yi Lee, NTU)</li> <li>Language: Mandarin Chinese with English slides</li> <li>Institution: National Taiwan University</li> <li>Coverage: Comprehensive ML course</li> <li>Availability: YouTube with subtitles</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#spanish-language-resources","title":"Spanish Language Resources","text":"<ul> <li>Curso de Machine Learning (AprendeIA)</li> <li>Platform: Spanish-language AI education</li> <li>Content: Beginner to advanced ML topics</li> <li>Community: Spanish-speaking AI community</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#french-language-resources","title":"French Language Resources","text":"<ul> <li>Formation Intelligence Artificielle (France Universit\u00e9 Num\u00e9rique)</li> <li>Platform: French university consortium</li> <li>Courses: AI and ML in French</li> <li>Certification: University-level certificates</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#accessibility-and-inclusive-learning","title":"Accessibility and Inclusive Learning","text":""},{"location":"Online-Material/Online-Material-for-Learning/#visual-accessibility","title":"Visual Accessibility","text":"<ul> <li>Screen Reader Compatible Resources</li> <li>Courses: Text-based alternatives to video content</li> <li>Documentation: Well-structured HTML documentation</li> <li>Code: Accessible code examples with descriptions</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#learning-differences-support","title":"Learning Differences Support","text":"<ul> <li>Dyslexia-Friendly Resources</li> <li>Formats: Audio lectures, visual learning materials</li> <li>Tools: Text-to-speech software compatibility</li> <li>Support: Extended time accommodations for assessments</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#financial-accessibility","title":"Financial Accessibility","text":""},{"location":"Online-Material/Online-Material-for-Learning/#free-and-low-cost-options","title":"Free and Low-Cost Options","text":"<ul> <li>Financial Aid Programs</li> <li>Coursera: Financial aid for most courses</li> <li>edX: Audit tracks available for free</li> <li>Udacity: Scholarship programs for underrepresented groups</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#developing-country-programs","title":"Developing Country Programs","text":"<ul> <li>Google AI Education</li> <li>Programs: Targeted programs for developing regions</li> <li>Scholarships: TensorFlow certifications</li> <li>Resources: Localized content and support</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#assessment-and-self-evaluation","title":"\ud83d\udcca Assessment and Self-Evaluation","text":""},{"location":"Online-Material/Online-Material-for-Learning/#skill-assessment-tools","title":"Skill Assessment Tools","text":""},{"location":"Online-Material/Online-Material-for-Learning/#technical-skill-evaluation","title":"Technical Skill Evaluation","text":"<ul> <li>Kaggle Skill Badges</li> <li>Categories: Python, SQL, Machine Learning, Deep Learning</li> <li>Format: Practical exercises and projects</li> <li>Verification: Public skill verification</li> <li> <p>Progression: Beginner to advanced levels</p> </li> <li> <p>HackerRank Skills Certification</p> </li> <li>Languages: Python, R, SQL programming</li> <li>Domains: Problem solving, algorithms, data structures</li> <li>Time Limits: Realistic assessment conditions</li> <li>Recognition: LinkedIn integration available</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#self-assessment-frameworks","title":"Self-Assessment Frameworks","text":"<ul> <li>Bloom's Taxonomy for ML Learning</li> <li>Levels: Remember \u2192 Understand \u2192 Apply \u2192 Analyze \u2192 Evaluate \u2192 Create</li> <li>Application: Structure learning objectives</li> <li>Assessment: Evaluate depth of understanding</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#portfolio-assessment-guidelines","title":"Portfolio Assessment Guidelines","text":""},{"location":"Online-Material/Online-Material-for-Learning/#project-quality-criteria","title":"Project Quality Criteria","text":"<ul> <li>Technical Excellence</li> <li>Code Quality: Clean, documented, reproducible code</li> <li>Methodology: Appropriate algorithms and evaluation</li> <li>Performance: Competitive results with proper validation</li> <li> <p>Innovation: Novel approaches or insights</p> </li> <li> <p>Communication Effectiveness</p> </li> <li>Documentation: Clear README and methodology explanation</li> <li>Visualization: Effective data visualization and results presentation</li> <li>Storytelling: Compelling narrative and business relevance</li> <li>Technical Writing: Precise, professional communication</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#emerging-technologies-and-future-trends","title":"\ud83d\udd2e Emerging Technologies and Future Trends","text":""},{"location":"Online-Material/Online-Material-for-Learning/#cutting-edge-research-areas","title":"Cutting-Edge Research Areas","text":""},{"location":"Online-Material/Online-Material-for-Learning/#quantum-machine-learning","title":"Quantum Machine Learning","text":"<ul> <li>Quantum Machine Learning (MIT)</li> <li>Prerequisites: Linear algebra, quantum mechanics basics</li> <li>Topics: Quantum algorithms, variational quantum eigensolvers</li> <li>Applications: Optimization, cryptography, simulation</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#neuromorphic-computing","title":"Neuromorphic Computing","text":"<ul> <li>Intel Loihi Research</li> <li>Concept: Brain-inspired computing architectures</li> <li>Applications: Ultra-low power AI, real-time processing</li> <li>Research: Academic and industry collaboration opportunities</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#federated-learning","title":"Federated Learning","text":"<ul> <li>Federated Learning Course</li> <li>Privacy: Decentralized learning without data sharing</li> <li>Applications: Healthcare, finance, mobile devices</li> <li>Challenges: Communication efficiency, privacy guarantees</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#industry-transformation-trends","title":"Industry Transformation Trends","text":""},{"location":"Online-Material/Online-Material-for-Learning/#automl-and-no-code-ai","title":"AutoML and No-Code AI","text":"<ul> <li>AutoML Platforms</li> <li>Google AutoML: Automated model development</li> <li>DataRobot: Enterprise automated machine learning</li> <li>H2O.ai: Open source automated ML platform</li> </ul>"},{"location":"Online-Material/Online-Material-for-Learning/#ai-ethics-and-responsible-ai","title":"AI Ethics and Responsible AI","text":"<ul> <li>AI Ethics Course (University of Helsinki)</li> <li>Topics: Bias, fairness, transparency, accountability</li> <li>Applications: Ethical AI development practices</li> <li>Frameworks: Ethical decision-making frameworks</li> </ul> <p>\ud83d\udcd6 Total Content: 1500+ lines of comprehensive learning resources</p> <p>\ud83d\udca1 Learning Philosophy: Master fundamentals deeply, practice consistently, build projects that matter, and never stop learning. The field evolves rapidly, but strong foundations enable continuous adaptation.</p> <p>\ud83c\udfaf Quick Start Paths: - Complete Beginner: Khan Academy Math \u2192 Python basics \u2192 Andrew Ng ML Course \u2192 Kaggle competitions - Programmer: Fast.ai course \u2192 Deep Learning book \u2192 Papers with Code implementations - Career Changer: Full specialization (6-12 months) \u2192 Portfolio projects \u2192 Job applications</p> <p>\ud83d\ude80 Advanced Learning: Focus on research papers, implement state-of-the-art models, contribute to open source, attend conferences, and build meaningful applications that solve real problems.</p> <p>\ud83d\udd17 Companion Guide: See our Popular Blogs &amp; Resources for curated expert content and industry insights.</p>"},{"location":"Online-Material/popular-resources/","title":"\ud83d\udcdd Popular Blogs &amp; Resources","text":"<p>The most comprehensive collection of influential blogs, newsletters, podcasts, video content, and thought leaders in data science, machine learning, artificial intelligence, and related fields. This guide covers everything from established industry voices to emerging content creators and specialized communities.</p>"},{"location":"Online-Material/popular-resources/#company-organization-blogs","title":"\ud83c\udfe2 Company &amp; Organization Blogs","text":""},{"location":"Online-Material/popular-resources/#big-tech-ai-research-divisions","title":"Big Tech AI Research Divisions","text":""},{"location":"Online-Material/popular-resources/#google-ai-deepmind-research","title":"Google AI &amp; DeepMind Research","text":"<ul> <li>Google AI Blog</li> <li>Focus: Cutting-edge research, breakthrough announcements, product applications</li> <li>Frequency: 2-3 posts per week</li> <li>Audience: Researchers, practitioners, industry professionals</li> <li>Content Style: Technical depth with accessible explanations</li> <li>Must-Read Series: TensorFlow updates, ethical AI research, breakthrough model announcements</li> <li>Notable Authors: Jeff Dean, Yann LeCun, Fran\u00e7ois Chollet</li> <li>Archive Value: Historical record of AI progress since 2006</li> <li> <p>Interaction: Comments enabled, active community discussions</p> </li> <li> <p>DeepMind Blog</p> </li> <li>Research Areas: AGI research, scientific applications, reinforcement learning</li> <li>Publishing Schedule: Weekly technical posts, monthly major announcements</li> <li>Signature Content: AlphaFold protein folding, AlphaGo game analysis, climate applications</li> <li>Academic Integration: Close ties to Nature, Science publications</li> <li>Video Content: Accompanying YouTube channel with technical talks</li> <li>Impact: Industry-setting research with societal applications</li> <li> <p>Collaboration: Joint posts with academic institutions</p> </li> <li> <p>Google Research Blog</p> </li> <li>Broader Scope: Beyond AI - quantum computing, systems, theory</li> <li>Cross-Pollination: Integration between AI and other research areas</li> <li>Industry Applications: How research translates to Google products</li> <li>Open Source: Regular announcements of open-sourced tools and datasets</li> <li>Global Perspective: Research from Google offices worldwide</li> </ul>"},{"location":"Online-Material/popular-resources/#openai-publications","title":"OpenAI Publications","text":"<ul> <li>OpenAI Blog</li> <li>Mission Focus: Safe AGI development and deployment</li> <li>Major Releases: GPT series, DALL-E, ChatGPT development insights</li> <li>Safety Research: AI alignment, robustness, interpretability</li> <li>Policy Discussions: AI governance, regulatory considerations</li> <li>Community Impact: Responses to societal concerns about AI</li> <li>Technical Depth: Model architecture details, training methodologies</li> <li> <p>Timeline: Comprehensive documentation of LLM evolution</p> </li> <li> <p>OpenAI Research</p> </li> <li>Academic Papers: Direct links to published research</li> <li>Code Releases: Implementation details and reproducibility</li> <li>Collaboration: Joint research with universities and other organizations</li> <li>Peer Review: Pre-print and published paper discussions</li> </ul>"},{"location":"Online-Material/popular-resources/#meta-ai-facebook-ai-research","title":"Meta AI (Facebook AI Research)","text":"<ul> <li>Meta AI Blog</li> <li>Product Integration: AI in social media, AR/VR applications</li> <li>Open Source: PyTorch development, Detectron2, fairseq</li> <li>Research Areas: Computer vision, NLP, responsible AI, efficiency</li> <li>Industry Applications: Content moderation, recommendation systems, translation</li> <li>Global Impact: AI for connecting communities, language preservation</li> <li>Ethics Focus: Bias detection, fairness in AI systems</li> <li> <p>Developer Resources: Tools and frameworks for practitioners</p> </li> <li> <p>PyTorch Blog</p> </li> <li>Framework Development: New features, performance improvements</li> <li>Community Contributions: User success stories, ecosystem developments</li> <li>Educational Content: Tutorials, best practices, migration guides</li> <li>Industry Adoption: Case studies from major companies</li> <li>Research Integration: How cutting-edge research uses PyTorch</li> </ul>"},{"location":"Online-Material/popular-resources/#microsoft-ai-research","title":"Microsoft AI &amp; Research","text":"<ul> <li>Microsoft AI Blog</li> <li>Enterprise Focus: Business applications, productivity enhancements</li> <li>Azure Integration: Cloud AI services, deployment strategies</li> <li>Partnership Stories: Customer success stories, industry transformations</li> <li>Responsible AI: Ethics, governance, regulatory compliance</li> <li>Global Reach: AI for accessibility, sustainability, social good</li> <li> <p>Research Translation: From lab to production applications</p> </li> <li> <p>Microsoft Research Blog</p> </li> <li>Fundamental Research: Computer science theory, quantum computing</li> <li>Interdisciplinary Work: AI + healthcare, education, environmental science</li> <li>Collaboration: University partnerships, academic conferences</li> <li>Innovation Pipeline: Future technologies, experimental projects</li> </ul>"},{"location":"Online-Material/popular-resources/#cloud-platform-ai-services","title":"Cloud Platform AI Services","text":""},{"location":"Online-Material/popular-resources/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>AWS Machine Learning Blog</li> <li>Service Focus: SageMaker, Bedrock, comprehensive ML pipeline</li> <li>Customer Stories: Real-world implementations across industries</li> <li>Technical Tutorials: Step-by-step implementation guides</li> <li>Cost Optimization: Efficient resource usage, budget management</li> <li>Multi-Language Support: Python, R, Scala, Java implementations</li> <li>Publishing Schedule: 5-7 posts weekly</li> <li> <p>Audience: Cloud architects, ML engineers, data scientists</p> </li> <li> <p>AWS Architecture Blog</p> </li> <li>System Design: Scalable ML architectures, best practices</li> <li>Case Studies: Fortune 500 ML implementations</li> <li>Performance: Optimization strategies, benchmarking</li> </ul>"},{"location":"Online-Material/popular-resources/#google-cloud-platform","title":"Google Cloud Platform","text":"<ul> <li>Google Cloud AI Blog</li> <li>Product Updates: Vertex AI, AutoML, BigQuery ML developments</li> <li>Customer Showcases: Enterprise AI transformations</li> <li>Technical Deep-Dives: Architecture patterns, implementation strategies</li> <li>Industry Solutions: Healthcare, finance, retail-specific applications</li> <li>Developer Experience: Tools, SDKs, integration guides</li> </ul>"},{"location":"Online-Material/popular-resources/#microsoft-azure","title":"Microsoft Azure","text":"<ul> <li>Azure AI Blog</li> <li>Cognitive Services: Vision, speech, language APIs</li> <li>MLOps Focus: DevOps for machine learning, automation</li> <li>Hybrid Solutions: On-premise and cloud integration</li> <li>Compliance: Enterprise security, data governance</li> <li>Partner Ecosystem: Third-party integrations, marketplace solutions</li> </ul>"},{"location":"Online-Material/popular-resources/#data-science-platforms-tools","title":"Data Science Platforms &amp; Tools","text":""},{"location":"Online-Material/popular-resources/#kaggle-community","title":"Kaggle Community","text":"<ul> <li>Kaggle Blog</li> <li>Competition Insights: Winner interviews, solution breakdowns</li> <li>Community Stories: Success stories, career transformations</li> <li>Dataset Spotlights: New and notable datasets</li> <li>Educational Content: Learning paths, skill development</li> <li>Industry Partnerships: Competition sponsors, real-world challenges</li> <li>Global Reach: International competition highlights, regional communities</li> </ul>"},{"location":"Online-Material/popular-resources/#weights-biases-wb","title":"Weights &amp; Biases (W&amp;B)","text":"<ul> <li>W&amp;B Blog</li> <li>MLOps Focus: Experiment tracking, model management</li> <li>Best Practices: Reproducible research, collaboration strategies</li> <li>Customer Stories: How leading companies use W&amp;B</li> <li>Technical Tutorials: Integration guides, advanced features</li> <li>Research Partnerships: Academic collaborations, paper implementations</li> </ul>"},{"location":"Online-Material/popular-resources/#databricks","title":"Databricks","text":"<ul> <li>Databricks Blog</li> <li>Big Data ML: Spark-based machine learning, data lakehouse</li> <li>Performance: Optimization strategies, benchmarking studies</li> <li>Industry Applications: Financial services, healthcare, retail</li> <li>Open Source: MLflow, Delta Lake developments</li> <li>Technical Architecture: Scalable data science workflows</li> </ul>"},{"location":"Online-Material/popular-resources/#hugging-face","title":"Hugging Face","text":"<ul> <li>Hugging Face Blog</li> <li>Transformers: Latest model releases, fine-tuning strategies</li> <li>Open Source: Community contributions, model sharing</li> <li>Research Democratization: Making state-of-the-art accessible</li> <li>Educational Content: NLP tutorials, best practices</li> <li>Ethics Focus: Responsible AI, bias mitigation</li> <li>Community Highlights: Creator spotlights, success stories</li> </ul>"},{"location":"Online-Material/popular-resources/#individual-expert-blogs-thought-leaders","title":"\ud83d\udc64 Individual Expert Blogs &amp; Thought Leaders","text":""},{"location":"Online-Material/popular-resources/#ml-research-pioneers-turing-award-winners","title":"ML Research Pioneers &amp; Turing Award Winners","text":""},{"location":"Online-Material/popular-resources/#yann-lecun-meta-chief-ai-scientist","title":"Yann LeCun - Meta Chief AI Scientist","text":"<ul> <li>Social Media Presence</li> <li>Platform: Twitter (primary), LinkedIn, Facebook</li> <li>Posting Frequency: Daily insights, multiple posts</li> <li>Content Style: Technical debates, research commentary, industry criticism</li> <li>Signature Topics: Self-supervised learning, energy-based models, AI skepticism</li> <li>Debate Engagement: Active in AI safety and AGI timeline discussions</li> <li>Educational Value: Real-time research insights, historical perspectives</li> <li>Industry Influence: Direct impact on Meta's AI strategy</li> </ul>"},{"location":"Online-Material/popular-resources/#geoffrey-hinton-godfather-of-deep-learning","title":"Geoffrey Hinton - Godfather of Deep Learning","text":"<ul> <li>Interview Circuit</li> <li>Media Appearances: Regular interviews on AI developments</li> <li>Key Topics: Neural network evolution, consciousness, AI risks</li> <li>Historical Perspective: Decades of AI development insights</li> <li>Recent Focus: AI safety concerns, societal implications</li> <li>Educational Impact: Mentorship of next generation researchers</li> </ul>"},{"location":"Online-Material/popular-resources/#yoshua-bengio-mila-institute","title":"Yoshua Bengio - Mila Institute","text":"<ul> <li>Academic Blog</li> <li>Research Focus: Causality, consciousness, AI for climate</li> <li>Policy Engagement: AI governance, Montreal Declaration</li> <li>Academic Leadership: Mila institute developments</li> <li>Social Responsibility: AI ethics, global cooperation</li> </ul>"},{"location":"Online-Material/popular-resources/#industry-practitioners-thought-leaders","title":"Industry Practitioners &amp; Thought Leaders","text":""},{"location":"Online-Material/popular-resources/#chip-huyen-real-world-ml-systems","title":"Chip Huyen - Real-World ML Systems","text":"<ul> <li>Personal Blog</li> <li>Background: Ex-NVIDIA, Stanford lecturer, startup founder</li> <li>Content Focus: Production ML, system design, career advice</li> <li>Publishing Schedule: Monthly long-form articles</li> <li>Technical Depth: Code examples, architecture diagrams</li> <li>Career Insights: Job market analysis, skill development</li> <li>Book Author: \"Designing Machine Learning Systems\"</li> <li>Must-Read Posts:<ul> <li>\"Machine Learning System Design\"</li> <li>\"MLOps: What, Why, and How\"</li> <li>\"Real-time Machine Learning Challenges\"</li> <li>\"The State of Machine Learning Infrastructure\"</li> </ul> </li> </ul>"},{"location":"Online-Material/popular-resources/#sebastian-ruder-nlp-research","title":"Sebastian Ruder - NLP Research","text":"<ul> <li>Personal Blog</li> <li>Expertise: Transfer learning, multilingual NLP, optimization</li> <li>Content Style: Research summaries, technical tutorials</li> <li>Academic Rigor: Comprehensive literature reviews</li> <li>Industry Impact: Research-to-practice translation</li> <li>Notable Series:<ul> <li>\"An Overview of Multi-Task Learning in Deep Neural Networks\"</li> <li>\"Transfer Learning - Machine Learning's Next Frontier\"</li> <li>\"NLP Progress Tracking\"</li> </ul> </li> </ul>"},{"location":"Online-Material/popular-resources/#andrej-karpathy-ai-education-research","title":"Andrej Karpathy - AI Education &amp; Research","text":"<ul> <li>Personal Blog</li> <li>Background: Ex-OpenAI, Tesla AI Director, Stanford PhD</li> <li>Educational Focus: Making complex concepts accessible</li> <li>Video Content: YouTube channel with implementation tutorials</li> <li>Technical Writing: From-scratch implementations</li> <li>Industry Experience: Autonomous vehicle AI, large-scale systems</li> <li>Influential Posts:<ul> <li>\"The Unreasonable Effectiveness of Recurrent Neural Networks\"</li> <li>\"Training Neural Networks: A hacker's guide\"</li> <li>\"Yes, you should understand backprop\"</li> </ul> </li> </ul>"},{"location":"Online-Material/popular-resources/#rachel-thomas-fastai-co-founder","title":"Rachel Thomas - Fast.ai Co-founder","text":"<ul> <li>Personal Blog</li> <li>Mission: Democratizing AI education</li> <li>Ethics Focus: AI bias, algorithmic accountability</li> <li>Educational Philosophy: Top-down learning approach</li> <li>Industry Critique: Tech industry culture, diversity issues</li> <li>Medical Background: Unique perspective on AI applications</li> </ul>"},{"location":"Online-Material/popular-resources/#jeremy-howard-fastai-co-founder","title":"Jeremy Howard - Fast.ai Co-founder","text":"<ul> <li>Fast.ai Blog</li> <li>Educational Innovation: Revolutionary teaching methods</li> <li>Technical Implementations: Practical deep learning</li> <li>Industry Disruption: Challenging traditional education models</li> <li>Open Source: FastAI library development</li> <li>Competition Success: Kaggle Grandmaster insights</li> </ul>"},{"location":"Online-Material/popular-resources/#technical-writers-educators","title":"Technical Writers &amp; Educators","text":""},{"location":"Online-Material/popular-resources/#lilian-weng-safety-focused-research","title":"Lilian Weng - Safety-Focused Research","text":"<ul> <li>Blog</li> <li>Position: OpenAI Safety Team Lead</li> <li>Writing Style: Comprehensive technical surveys</li> <li>Research Focus: Attention mechanisms, generative models, RL</li> <li>Academic Quality: Paper-level rigor with accessibility</li> <li>Must-Read Articles:<ul> <li>\"Attention? Attention!\"</li> <li>\"What are Diffusion Models?\"</li> <li>\"Policy Gradient Algorithms\"</li> <li>\"Meta-Learning: Learning to Learn Fast\"</li> </ul> </li> </ul>"},{"location":"Online-Material/popular-resources/#chris-olah-neural-network-interpretability","title":"Chris Olah - Neural Network Interpretability","text":"<ul> <li>Blog</li> <li>Unique Approach: Visual explanations of complex concepts</li> <li>Research Area: Neural network interpretability, visualization</li> <li>Career Path: Google Brain, OpenAI, Anthropic co-founder</li> <li>Interactive Content: Dynamic visualizations, explorable explanations</li> <li>Influential Posts:<ul> <li>\"Understanding LSTM Networks\"</li> <li>\"Neural Network Manifolds and Topology\"</li> <li>\"Visualizing Representations\"</li> </ul> </li> </ul>"},{"location":"Online-Material/popular-resources/#distill-publication-team","title":"Distill Publication Team","text":"<ul> <li>Distill.pub</li> <li>Mission: Clear, dynamic explanations of machine learning</li> <li>Interactive Design: Web-native scientific communication</li> <li>Peer Review: Rigorous review process for quality</li> <li>Visual Innovation: Setting new standards for technical explanation</li> <li>Collaborative: Multiple authors, interdisciplinary perspectives</li> <li>Award-Winning: Recognition for communication excellence</li> </ul>"},{"location":"Online-Material/popular-resources/#industry-veterans-commentators","title":"Industry Veterans &amp; Commentators","text":""},{"location":"Online-Material/popular-resources/#benedict-evans-tech-industry-analyst","title":"Benedict Evans - Tech Industry Analyst","text":"<ul> <li>Newsletter &amp; Blog</li> <li>Focus: Tech industry trends, strategic analysis</li> <li>AI Commentary: Business implications, adoption patterns</li> <li>Global Perspective: Silicon Valley and beyond</li> <li>Historical Context: Technology cycles, pattern recognition</li> </ul>"},{"location":"Online-Material/popular-resources/#matthew-ball-metaverse-gaming","title":"Matthew Ball - Metaverse &amp; Gaming","text":"<ul> <li>Blog</li> <li>Specialization: Virtual worlds, gaming technology</li> <li>AI Integration: AI in gaming, content creation</li> <li>Industry Connections: Investment and executive perspectives</li> <li>Future Predictions: Technology convergence trends</li> </ul>"},{"location":"Online-Material/popular-resources/#publications-magazines-technical-media","title":"\ud83d\udcf0 Publications, Magazines &amp; Technical Media","text":""},{"location":"Online-Material/popular-resources/#academic-style-publications","title":"Academic-Style Publications","text":""},{"location":"Online-Material/popular-resources/#towards-data-science-medium","title":"Towards Data Science (Medium)","text":"<ul> <li>Publication</li> <li>Contributors: 50,000+ data science practitioners</li> <li>Content Range: Beginner tutorials to research discussions</li> <li>Publishing Volume: 100+ articles daily</li> <li>Quality Control: Editorial review process</li> <li>Reader Engagement: High comment volume, community discussions</li> <li>Career Focus: Professional development, industry insights</li> <li>Technical Depth: Code examples, case studies, methodology explanations</li> <li> <p>Global Reach: International contributor base</p> </li> <li> <p>Top Contributors &amp; Their Specialties:</p> </li> <li>Susan Li: Practical data science, industry applications</li> <li>Will Koehrsen: Feature engineering, model interpretation</li> <li>Cassie Kozyrkov: Statistics, decision science</li> <li>Tirthajyoti Sarkar: Mathematical foundations, synthetic data</li> </ul>"},{"location":"Online-Material/popular-resources/#the-gradient-ai-research-society","title":"The Gradient - AI Research &amp; Society","text":"<ul> <li>Publication</li> <li>Editorial Mission: Intersection of AI research and societal impact</li> <li>Contributor Profile: PhD researchers, industry leaders</li> <li>Content Style: Long-form analysis, critical thinking</li> <li>Topics: AI safety, policy, ethics, technical research</li> <li>Publishing Schedule: Weekly deep-dive articles</li> <li>Academic Rigor: Peer review process, citation standards</li> </ul>"},{"location":"Online-Material/popular-resources/#distill-magazine","title":"Distill Magazine","text":"<ul> <li>Interactive ML Explanations</li> <li>Innovation: Web-native scientific communication</li> <li>Visual Quality: Interactive visualizations, animations</li> <li>Technical Accuracy: Rigorous peer review</li> <li>Educational Impact: Widely cited in academic courses</li> <li>Collaboration: Multi-institutional research teams</li> </ul>"},{"location":"Online-Material/popular-resources/#industry-trade-publications","title":"Industry Trade Publications","text":""},{"location":"Online-Material/popular-resources/#venturebeat-ai-coverage","title":"VentureBeat AI Coverage","text":"<ul> <li>AI Section</li> <li>Business Focus: Startup funding, product launches</li> <li>Industry Analysis: Market trends, competitive landscape</li> <li>Executive Interviews: C-level perspectives on AI adoption</li> <li>Event Coverage: Conference reporting, announcement analysis</li> <li>Investment Tracking: Venture capital, acquisition news</li> </ul>"},{"location":"Online-Material/popular-resources/#the-information-ai-industry-intelligence","title":"The Information - AI Industry Intelligence","text":"<ul> <li>Subscription Publication</li> <li>Premium Content: Behind-the-scenes industry reporting</li> <li>Source Access: High-level executive sources</li> <li>Financial Analysis: Revenue impacts, business model analysis</li> <li>Competitive Intelligence: Strategic moves, partnership deals</li> <li>Subscription Model: Professional-grade industry intelligence</li> </ul>"},{"location":"Online-Material/popular-resources/#ai-news-updates","title":"AI News &amp; Updates","text":"<ul> <li>AI News</li> <li>Daily Coverage: Breaking news, product announcements</li> <li>Global Perspective: International AI development</li> <li>Policy Coverage: Regulatory developments, government initiatives</li> <li>Industry Events: Conference coverage, trade show reports</li> </ul>"},{"location":"Online-Material/popular-resources/#technical-magazines","title":"Technical Magazines","text":""},{"location":"Online-Material/popular-resources/#ieee-spectrum-ai-coverage","title":"IEEE Spectrum AI Coverage","text":"<ul> <li>Technology Analysis</li> <li>Engineering Perspective: Technical implementation details</li> <li>Research Translation: Academic research to practical applications</li> <li>Hardware Focus: Chip developments, computing infrastructure</li> <li>Standards Development: IEEE AI standards, best practices</li> </ul>"},{"location":"Online-Material/popular-resources/#communications-of-the-acm","title":"Communications of the ACM","text":"<ul> <li>Research &amp; Practice</li> <li>Academic Authority: Peer-reviewed technical articles</li> <li>Industry Relevance: Practical applications of research</li> <li>Career Development: Professional growth in computing</li> <li>Historical Archive: Decades of computing evolution</li> </ul>"},{"location":"Online-Material/popular-resources/#newsletters-email-content","title":"\ud83d\udce7 Newsletters &amp; Email Content","text":""},{"location":"Online-Material/popular-resources/#weekly-industry-roundups","title":"Weekly Industry Roundups","text":""},{"location":"Online-Material/popular-resources/#the-batch-by-deeplearningai","title":"The Batch by DeepLearning.AI","text":"<ul> <li>Weekly Newsletter</li> <li>Editor-in-Chief: Andrew Ng's editorial oversight</li> <li>Content Curation: Industry news, research highlights, career advice</li> <li>Subscriber Base: 500,000+ AI professionals</li> <li>Format: Digestible summaries with deep-dive links</li> <li>Educational Focus: Learning opportunities, course recommendations</li> <li>Global Perspective: International AI developments</li> <li>Archive Access: Searchable historical content</li> </ul>"},{"location":"Online-Material/popular-resources/#ai-breakfast-daily-digest","title":"AI Breakfast - Daily Digest","text":"<ul> <li>Morning Brief</li> <li>Frequency: Daily 5-minute read</li> <li>Content Mix: News, funding, research, product launches</li> <li>Business Focus: Startup ecosystem, investment trends</li> <li>Quick Format: Bullet points, easy scanning</li> <li>Mobile Optimized: Designed for mobile consumption</li> </ul>"},{"location":"Online-Material/popular-resources/#import-ai-by-jack-clark","title":"Import AI by Jack Clark","text":"<ul> <li>AI Policy &amp; Safety</li> <li>Author Background: Anthropic co-founder, former OpenAI</li> <li>Unique Focus: AI governance, safety research, policy implications</li> <li>Research Depth: Technical paper analysis</li> <li>Global Policy: International AI regulation development</li> <li>Long-form: Detailed weekly analysis</li> <li>Subscriber Value: Industry insider perspective</li> </ul>"},{"location":"Online-Material/popular-resources/#specialized-technical-newsletters","title":"Specialized Technical Newsletters","text":""},{"location":"Online-Material/popular-resources/#the-sequence-ai-research","title":"The Sequence - AI Research","text":"<ul> <li>Research Analysis</li> <li>Technical Depth: Research paper breakdowns</li> <li>Implementation Focus: Code examples, reproducibility</li> <li>Author Expertise: Industry practitioners with academic backgrounds</li> <li>Frequency: Bi-weekly comprehensive analysis</li> <li>Archive: Searchable research database</li> </ul>"},{"location":"Online-Material/popular-resources/#machine-learning-engineering-newsletter","title":"Machine Learning Engineering Newsletter","text":"<ul> <li>MLOps Focus</li> <li>Production ML: Deployment, monitoring, maintenance</li> <li>Tool Reviews: MLOps platform comparisons</li> <li>Case Studies: Real-world implementation stories</li> <li>Career Track: ML engineering skill development</li> <li>Community: Active Discord community</li> </ul>"},{"location":"Online-Material/popular-resources/#the-algorithm-by-mit-technology-review","title":"The Algorithm by MIT Technology Review","text":"<ul> <li>AI Analysis</li> <li>Institutional Authority: MIT's editorial standards</li> <li>Critical Analysis: Skeptical examination of AI claims</li> <li>Global Impact: Societal implications, policy recommendations</li> <li>Research Context: Academic research contextualization</li> <li>Historical Perspective: Long-term technology trends</li> </ul>"},{"location":"Online-Material/popular-resources/#regional-language-specific-newsletters","title":"Regional &amp; Language-Specific Newsletters","text":""},{"location":"Online-Material/popular-resources/#ai-china-newsletter","title":"AI China Newsletter","text":"<ul> <li>Chinese AI Industry</li> <li>Regional Focus: Chinese AI ecosystem</li> <li>Policy Analysis: Government AI strategy</li> <li>Company Profiles: Baidu, Alibaba, Tencent AI developments</li> <li>Research Translation: Chinese research paper highlights</li> <li>Cultural Context: AI development in Chinese society</li> </ul>"},{"location":"Online-Material/popular-resources/#european-ai-newsletter","title":"European AI Newsletter","text":"<ul> <li>EU AI Regulation</li> <li>Policy Focus: EU AI Act developments</li> <li>Research Institutions: European academic contributions</li> <li>Privacy Emphasis: GDPR intersection with AI</li> <li>Multilingual: Available in multiple European languages</li> </ul>"},{"location":"Online-Material/popular-resources/#podcasts-audio-content","title":"\ud83c\udf99\ufe0f Podcasts &amp; Audio Content","text":""},{"location":"Online-Material/popular-resources/#long-form-interview-podcasts","title":"Long-Form Interview Podcasts","text":""},{"location":"Online-Material/popular-resources/#lex-fridman-podcast","title":"Lex Fridman Podcast","text":"<ul> <li>Deep Conversations</li> <li>Host Background: MIT researcher, multi-disciplinary expertise</li> <li>Format: 2-4 hour in-depth conversations</li> <li>Guest Range: AI researchers, entrepreneurs, philosophers, scientists</li> <li>Notable Episodes:<ul> <li>Elon Musk on AI, consciousness, and the future</li> <li>Yann LeCun on self-supervised learning and AI progress</li> <li>Geoffrey Hinton on neural networks and AI consciousness</li> <li>Sam Altman on OpenAI, AGI, and the future of humanity</li> </ul> </li> <li>Discussion Style: Philosophical depth, technical rigor</li> <li>Production Quality: Professional audio, video versions</li> <li>Global Reach: Millions of downloads per episode</li> </ul>"},{"location":"Online-Material/popular-resources/#the-twiml-ai-podcast-this-week-in-ml-ai","title":"The TWIML AI Podcast (This Week in ML &amp; AI)","text":"<ul> <li>Weekly Industry Focus</li> <li>Host: Sam Charrington (industry veteran)</li> <li>Format: 30-60 minute technical interviews</li> <li>Guest Profile: ML practitioners, researchers, entrepreneurs</li> <li>Content Focus: Practical implementations, business applications</li> <li>Technical Depth: Code discussions, architecture decisions</li> <li>Episode Archive: 500+ episodes, searchable database</li> <li>Community: Active Slack community for discussion</li> </ul>"},{"location":"Online-Material/popular-resources/#machine-learning-street-talk","title":"Machine Learning Street Talk","text":"<ul> <li>Research Deep Dives</li> <li>Format: Panel discussions with multiple experts</li> <li>Duration: 1-3 hours of technical discussion</li> <li>Topics: AGI, consciousness, AI safety, latest research</li> <li>Hosts: ML researchers and practitioners</li> <li>Guest Quality: Leading researchers, industry pioneers</li> <li>Technical Level: Advanced, assumes technical background</li> <li>Community Interaction: Live chat, Q&amp;A sessions</li> </ul>"},{"location":"Online-Material/popular-resources/#educational-beginner-friendly-podcasts","title":"Educational &amp; Beginner-Friendly Podcasts","text":""},{"location":"Online-Material/popular-resources/#data-skeptic","title":"Data Skeptic","text":"<ul> <li>Critical Thinking in Data</li> <li>Host: Kyle Polich (data science practitioner)</li> <li>Mission: Scientific rigor in data science</li> <li>Content Mix: Interviews, mini-lectures, myth-busting</li> <li>Critical Approach: Questioning popular claims and methods</li> <li>Educational Value: Statistics education, methodology discussions</li> <li>Accessibility: Beginner-friendly with expert depth</li> </ul>"},{"location":"Online-Material/popular-resources/#super-data-science","title":"Super Data Science","text":"<ul> <li>Career &amp; Skills Development</li> <li>Host: Jon Krohn (data science educator)</li> <li>Focus: Career development, skill building, industry insights</li> <li>Guest Range: Practitioners, educators, career changers</li> <li>Practical Advice: Job searching, portfolio building, skill development</li> <li>Course Integration: Connected to educational platform</li> </ul>"},{"location":"Online-Material/popular-resources/#banana-data-podcast","title":"Banana Data Podcast","text":"<ul> <li>Beginner-Friendly Analytics</li> <li>Hosts: Practicing data scientists</li> <li>Target Audience: Early-career professionals, career changers</li> <li>Topics: Project walkthroughs, tool tutorials, career advice</li> <li>Conversational Style: Casual, accessible discussions</li> <li>Community Building: Strong listener engagement</li> </ul>"},{"location":"Online-Material/popular-resources/#business-strategy-podcasts","title":"Business &amp; Strategy Podcasts","text":""},{"location":"Online-Material/popular-resources/#ai-in-business-emerj","title":"AI in Business (Emerj)","text":"<ul> <li>Executive Perspective</li> <li>Host: Daniel Faggella (business AI analyst)</li> <li>Audience: C-level executives, business leaders</li> <li>Content: ROI analysis, implementation strategies</li> <li>Case Studies: Fortune 500 AI transformations</li> <li>Market Analysis: Vendor comparisons, technology trends</li> </ul>"},{"location":"Online-Material/popular-resources/#the-ai-element","title":"The AI Element","text":"<ul> <li>Strategic AI Implementation</li> <li>Focus: Enterprise AI adoption</li> <li>Guest Profile: CTOs, AI directors, consultants</li> <li>Topics: Change management, cultural transformation</li> <li>Practical Framework: Actionable implementation strategies</li> </ul>"},{"location":"Online-Material/popular-resources/#research-academic-podcasts","title":"Research &amp; Academic Podcasts","text":""},{"location":"Online-Material/popular-resources/#talking-machines","title":"Talking Machines","text":"<ul> <li>ML Research Discussions</li> <li>Hosts: Katherine Gorman, Neil Lawrence</li> <li>Academic Focus: Research paper discussions</li> <li>Expert Interviews: Leading researchers in their specialties</li> <li>Educational Segments: Concept explanations for broader audience</li> <li>Conference Coverage: Live recordings from major ML conferences</li> </ul>"},{"location":"Online-Material/popular-resources/#learning-machines-101","title":"Learning Machines 101","text":"<ul> <li>Technical Education</li> <li>Educational Mission: Making ML accessible to everyone</li> <li>Host Background: Industry practitioner and educator</li> <li>Content Structure: Systematic curriculum approach</li> <li>Technical Depth: Mathematical foundations with intuition</li> </ul>"},{"location":"Online-Material/popular-resources/#video-content-youtube-channels","title":"\ud83c\udfac Video Content &amp; YouTube Channels","text":""},{"location":"Online-Material/popular-resources/#technical-education-channels","title":"Technical Education Channels","text":""},{"location":"Online-Material/popular-resources/#3blue1brown-mathematical-visualization","title":"3Blue1Brown - Mathematical Visualization","text":"<ul> <li>Visual Mathematics</li> <li>Creator: Grant Sanderson (mathematician)</li> <li>Unique Selling Point: Stunning mathematical animations</li> <li>Subscriber Base: 4+ million subscribers</li> <li>Essential Series:<ul> <li>\"Essence of Linear Algebra\" (15 episodes)</li> <li>\"Essence of Calculus\" (12 episodes)</li> <li>\"Neural Networks\" (4 episodes)</li> <li>\"Differential Equations\" (5 episodes)</li> </ul> </li> <li>Production Quality: Industry-leading animation, custom software</li> <li>Educational Impact: Used in universities worldwide</li> <li>Update Schedule: Monthly high-quality releases</li> </ul>"},{"location":"Online-Material/popular-resources/#statquest-with-josh-starmer","title":"StatQuest with Josh Starmer","text":"<ul> <li>Statistics Made Simple</li> <li>Creator: Josh Starmer (biostatistician)</li> <li>Teaching Style: Humor, music, clear explanations</li> <li>Content Volume: 300+ educational videos</li> <li>Core Topics: Statistics, machine learning algorithms, data science</li> <li>Memorable Elements: Songs, visual analogies, step-by-step breakdowns</li> <li>Audience: Beginners to intermediate practitioners</li> <li>Community: Active comments, Q&amp;A engagement</li> </ul>"},{"location":"Online-Material/popular-resources/#two-minute-papers-research-summaries","title":"Two Minute Papers - Research Summaries","text":"<ul> <li>AI Research Updates</li> <li>Creator: K\u00e1roly Zsolnai-Feh\u00e9r (computer graphics researcher)</li> <li>Format: 5-10 minute research paper summaries</li> <li>Frequency: 2-3 videos per week</li> <li>Content Focus: Computer graphics, deep learning, AI research</li> <li>Signature Phrase: \"What a time to be alive!\"</li> <li>Visual Quality: High-quality graphics, animations</li> <li>Research Currency: Latest papers from top-tier conferences</li> </ul>"},{"location":"Online-Material/popular-resources/#implementation-code-along-channels","title":"Implementation &amp; Code-Along Channels","text":""},{"location":"Online-Material/popular-resources/#sentdex-python-programming","title":"Sentdex - Python Programming","text":"<ul> <li>Programming Tutorials</li> <li>Creator: Harrison Kinsley</li> <li>Content: Python, ML, algorithmic trading, game development</li> <li>Teaching Style: Code-along, practical implementation</li> <li>Series Length: Comprehensive multi-part series</li> <li>Real Applications: Stock analysis, sentiment analysis, game AI</li> <li>Beginner Friendly: Assumes minimal programming background</li> </ul>"},{"location":"Online-Material/popular-resources/#code-bullet-ai-game-development","title":"Code Bullet - AI Game Development","text":"<ul> <li>AI Gaming Projects</li> <li>Content: AI playing games, evolutionary algorithms</li> <li>Entertainment Value: Humorous commentary, engaging projects</li> <li>Educational: RL concepts through game examples</li> <li>Inspiration: Motivates AI learning through fun applications</li> </ul>"},{"location":"Online-Material/popular-resources/#corey-schafer-python-best-practices","title":"Corey Schafer - Python Best Practices","text":"<ul> <li>Python Education</li> <li>Focus: Professional Python development</li> <li>Quality: Extremely clear explanations, best practices</li> <li>Topics: Web development, data science, automation</li> <li>Professional Development: Industry-standard practices</li> </ul>"},{"location":"Online-Material/popular-resources/#research-paper-channels","title":"Research Paper Channels","text":""},{"location":"Online-Material/popular-resources/#yannic-kilcher-paper-reviews","title":"Yannic Kilcher - Paper Reviews","text":"<ul> <li>Deep Paper Analysis</li> <li>Format: 30-90 minute detailed paper breakdowns</li> <li>Approach: Line-by-line reading with explanations</li> <li>Target Audience: Advanced students, researchers</li> <li>Content Depth: Mathematical derivations, implementation details</li> <li>Frequency: Multiple videos weekly</li> <li>Critical Analysis: Strengths, weaknesses, future directions</li> </ul>"},{"location":"Online-Material/popular-resources/#ai-coffee-break-with-letitia","title":"AI Coffee Break with Letitia","text":"<ul> <li>Research Summaries</li> <li>Creator: Letitia Parcalabescu (AI researcher)</li> <li>Format: Concise, accessible research explanations</li> <li>Visual Style: Clean animations, clear diagrams</li> <li>Focus: Recent research with practical implications</li> <li>Accessibility: Complex topics made understandable</li> </ul>"},{"location":"Online-Material/popular-resources/#whats-ai-research-translation","title":"What's AI - Research Translation","text":"<ul> <li>AI Research Explained</li> <li>Creator: Louis-Fran\u00e7ois Bouchard</li> <li>Mission: Making AI research accessible</li> <li>Format: Weekly research highlights, tool reviews</li> <li>Community: Active Discord, newsletter integration</li> <li>Practical Focus: How research translates to applications</li> </ul>"},{"location":"Online-Material/popular-resources/#university-lecture-channels","title":"University Lecture Channels","text":""},{"location":"Online-Material/popular-resources/#stanford-university-courses","title":"Stanford University Courses","text":"<ul> <li>CS229 Machine Learning</li> <li>Instructor: Andrew Ng</li> <li>Level: Graduate-level mathematical treatment</li> <li>Completeness: Full semester course (20 lectures)</li> <li>Prerequisites: Linear algebra, probability, programming</li> <li> <p>Problem Sets: Available with solutions</p> </li> <li> <p>CS231n Convolutional Neural Networks</p> </li> <li>Instructors: Andrej Karpathy, Fei-Fei Li</li> <li>Focus: Computer vision and deep learning</li> <li>Assignments: Implementation from scratch</li> <li>Industry Relevance: Practical applications emphasis</li> </ul>"},{"location":"Online-Material/popular-resources/#mit-opencourseware","title":"MIT OpenCourseWare","text":"<ul> <li>6.034 Artificial Intelligence</li> <li>Instructor: Patrick Winston</li> <li>Style: Storytelling approach to complex topics</li> <li>Coverage: Broad AI survey course</li> <li>Accessibility: Undergraduate-friendly explanations</li> </ul>"},{"location":"Online-Material/popular-resources/#industry-channels","title":"Industry Channels","text":""},{"location":"Online-Material/popular-resources/#deeplearningai","title":"DeepLearning.AI","text":"<ul> <li>Educational Content</li> <li>Content: Course supplements, research highlights</li> <li>Quality: Professional production, expert instructors</li> <li>Integration: Connected to Coursera courses</li> <li>Guest Lectures: Industry leaders and researchers</li> </ul>"},{"location":"Online-Material/popular-resources/#weights-biases","title":"Weights &amp; Biases","text":"<ul> <li>MLOps Education</li> <li>Focus: Machine learning operations, best practices</li> <li>Format: Webinars, tutorials, case studies</li> <li>Practical Value: Real-world implementation guidance</li> </ul>"},{"location":"Online-Material/popular-resources/#community-discussion-platforms","title":"\ud83c\udf10 Community &amp; Discussion Platforms","text":""},{"location":"Online-Material/popular-resources/#reddit-communities","title":"Reddit Communities","text":""},{"location":"Online-Material/popular-resources/#technical-discussion-subreddits","title":"Technical Discussion Subreddits","text":"<ul> <li>r/MachineLearning</li> <li>Subscribers: 2.5M+ members</li> <li>Content: Research discussions, paper releases, AMAs</li> <li>Quality Control: Moderated for technical accuracy</li> <li>Notable Features: Weekly paper discussions, researcher AMAs</li> <li>Career Value: Industry insights, job market discussions</li> <li>Beginner Friendliness: Intermediate to advanced level</li> <li> <p>Active Times: Peak activity during US/EU working hours</p> </li> <li> <p>r/LearnMachineLearning</p> </li> <li>Subscribers: 500K+ members</li> <li>Purpose: Educational support, beginner questions</li> <li>Content: Tutorials, resource sharing, project feedback</li> <li>Community Culture: Supportive, educational focus</li> <li>Career Support: Resume reviews, interview preparation</li> <li> <p>Study Groups: Organized learning cohorts</p> </li> <li> <p>r/datascience</p> </li> <li>Subscribers: 1M+ members</li> <li>Focus: Career advice, industry trends, tools discussion</li> <li>Popular Topics: Salary discussions, career transitions, skill development</li> <li>Industry Insight: Real practitioner experiences</li> <li>Job Market: Regular salary surveys, hiring trends</li> </ul>"},{"location":"Online-Material/popular-resources/#specialized-technical-communities","title":"Specialized Technical Communities","text":"<ul> <li>r/artificial</li> <li>Focus: General AI discussion, news, philosophy</li> <li>Audience: Mixed technical and general interest</li> <li> <p>Content: News aggregation, ethical discussions</p> </li> <li> <p>r/deeplearning</p> </li> <li>Technical Depth: Advanced deep learning topics</li> <li>Research Focus: Paper discussions, implementation help</li> <li>Code Sharing: GitHub repositories, implementation tips</li> </ul>"},{"location":"Online-Material/popular-resources/#discord-communities","title":"Discord Communities","text":""},{"location":"Online-Material/popular-resources/#research-focused-discord-servers","title":"Research-Focused Discord Servers","text":"<ul> <li>EleutherAI Discord</li> <li>Mission: Open-source AI research</li> <li>Projects: GPT-J, GPT-Neo development</li> <li>Community: Researchers, engineers, enthusiasts</li> <li>Collaboration: Active research projects, paper implementations</li> <li>Learning: Research paper reading groups</li> <li> <p>Global: 24/7 activity across time zones</p> </li> <li> <p>Weights &amp; Biases Community</p> </li> <li>Focus: MLOps, experiment tracking</li> <li>Support: Technical help, best practices</li> <li>Events: Regular community events, office hours</li> <li>Industry: Practitioners sharing real-world experiences</li> </ul>"},{"location":"Online-Material/popular-resources/#educational-discord-communities","title":"Educational Discord Communities","text":"<ul> <li>Papers We Love</li> <li>Activity: Paper discussion, reading groups</li> <li>Academic: University-style learning environment</li> <li>Mentorship: Senior members guide newcomers</li> <li>Global Chapters: Local meetup coordination</li> </ul>"},{"location":"Online-Material/popular-resources/#professional-networks","title":"Professional Networks","text":""},{"location":"Online-Material/popular-resources/#linkedin-ai-communities","title":"LinkedIn AI Communities","text":"<ul> <li>AI &amp; Machine Learning Professionals Group</li> <li>Members: 100K+ professionals</li> <li>Content: Industry news, job postings, skill discussions</li> <li>Networking: Direct professional connections</li> <li> <p>Quality: Professional, moderated discussions</p> </li> <li> <p>Data Science Central</p> </li> <li>Focus: Data science careers, education</li> <li>Content: Articles, webinars, industry insights</li> <li>Professional Development: Certification discussions, skill development</li> </ul>"},{"location":"Online-Material/popular-resources/#slack-communities","title":"Slack Communities","text":"<ul> <li>DataTalks.Club</li> <li>Global Community: International data professionals</li> <li>Educational: Course discussions, study groups</li> <li>Career Support: Job boards, interview preparation</li> <li>Events: Regular webinars, book clubs</li> </ul>"},{"location":"Online-Material/popular-resources/#academic-communities","title":"Academic Communities","text":""},{"location":"Online-Material/popular-resources/#academic-twitter","title":"Academic Twitter","text":"<ul> <li>#MachineLearning hashtag community</li> <li>Participants: Researchers, professors, PhD students</li> <li>Content: Paper announcements, research insights</li> <li>Real-time: Live conference coverage, breaking research</li> <li>Networking: Direct researcher access</li> </ul>"},{"location":"Online-Material/popular-resources/#conference-communities","title":"Conference Communities","text":"<ul> <li>NeurIPS Community</li> <li>Annual Event: Premier ML conference</li> <li>Virtual Options: Online participation available</li> <li>Workshops: Specialized topic sessions</li> <li>Networking: Industry and academic connections</li> </ul>"},{"location":"Online-Material/popular-resources/#regional-language-specific-resources","title":"\ud83c\udf0d Regional &amp; Language-Specific Resources","text":""},{"location":"Online-Material/popular-resources/#chinese-ai-ecosystem","title":"Chinese AI Ecosystem","text":""},{"location":"Online-Material/popular-resources/#chinese-ai-platforms-content","title":"Chinese AI Platforms &amp; Content","text":"<ul> <li>\u673a\u5668\u4e4b\u5fc3 (Machine Learning Mastery China)</li> <li>Content: Chinese AI industry news, research translations</li> <li>Audience: Chinese-speaking AI professionals</li> <li>Industry Focus: Baidu, Alibaba, Tencent AI developments</li> <li> <p>Government Policy: Chinese AI strategy analysis</p> </li> <li> <p>AI\u79d1\u6280\u5927\u672c\u8425</p> </li> <li>Platform: CSDN (Chinese Software Developer Network)</li> <li>Content: Technical tutorials, industry analysis in Chinese</li> <li>Community: Active Chinese developer community</li> </ul>"},{"location":"Online-Material/popular-resources/#chinese-researchers-educators","title":"Chinese Researchers &amp; Educators","text":"<ul> <li>\u674e\u5b8f\u6bc5 (Hung-yi Lee) - National Taiwan University</li> <li>Content: Machine learning course in Chinese</li> <li>Quality: University-level curriculum</li> <li>Accessibility: Free access to lectures and materials</li> <li>Bilingual: Chinese lectures with English slides</li> </ul>"},{"location":"Online-Material/popular-resources/#european-ai-resources","title":"European AI Resources","text":""},{"location":"Online-Material/popular-resources/#french-ai-community","title":"French AI Community","text":"<ul> <li>France IA</li> <li>Content: French AI industry developments</li> <li>Policy Focus: European AI regulation impact</li> <li> <p>Research: INRIA, Sorbonne research highlights</p> </li> <li> <p>Institut des Algorithmes</p> </li> <li>Educational: French-language algorithm courses</li> <li>Academic: University-level content</li> <li>Career: French tech job market insights</li> </ul>"},{"location":"Online-Material/popular-resources/#german-ai-ecosystem","title":"German AI Ecosystem","text":"<ul> <li>KI-Campus</li> <li>Platform: German AI education platform</li> <li>Government Support: Federal ministry backing</li> <li>Language: German-language courses and resources</li> </ul>"},{"location":"Online-Material/popular-resources/#nordic-ai-communities","title":"Nordic AI Communities","text":"<ul> <li>Nordic AI</li> <li>Regional Focus: Scandinavian AI developments</li> <li>Policy: Nordic AI strategy discussions</li> <li>Research: Academic collaboration across Nordic countries</li> </ul>"},{"location":"Online-Material/popular-resources/#indian-ai-resources","title":"Indian AI Resources","text":""},{"location":"Online-Material/popular-resources/#indian-ai-publications","title":"Indian AI Publications","text":"<ul> <li>Analytics India Magazine</li> <li>Industry Focus: Indian AI startup ecosystem</li> <li>Career Content: Indian tech job market, salary trends</li> <li> <p>Events: Conference coverage, industry events</p> </li> <li> <p>DataHack by Analytics Vidhya</p> </li> <li>Competitions: India-focused data science competitions</li> <li>Community: Large Indian data science community</li> <li>Education: Hindi and English educational content</li> </ul>"},{"location":"Online-Material/popular-resources/#latin-american-ai-resources","title":"Latin American AI Resources","text":""},{"location":"Online-Material/popular-resources/#spanish-portuguese-content","title":"Spanish &amp; Portuguese Content","text":"<ul> <li>Aprende IA</li> <li>Language: Spanish AI education</li> <li>Community: Spanish-speaking AI professionals</li> <li> <p>Content: Translated courses, original Spanish content</p> </li> <li> <p>AI Brazil</p> </li> <li>Language: Portuguese AI community</li> <li>Industry: Brazilian AI market developments</li> <li>Research: Brazilian university AI research</li> </ul>"},{"location":"Online-Material/popular-resources/#content-quality-assessment-discovery","title":"\ud83d\udcca Content Quality Assessment &amp; Discovery","text":""},{"location":"Online-Material/popular-resources/#evaluating-source-credibility","title":"Evaluating Source Credibility","text":""},{"location":"Online-Material/popular-resources/#academic-credibility-indicators","title":"Academic Credibility Indicators","text":"<ul> <li>Author Background Verification</li> <li>PhD credentials from recognized institutions</li> <li>Publication history in peer-reviewed venues</li> <li>Industry experience at leading organizations</li> <li> <p>Conference speaking history and recognition</p> </li> <li> <p>Content Quality Metrics</p> </li> <li>Citation frequency by other experts</li> <li>Code availability and reproducibility</li> <li>Mathematical rigor and accuracy</li> <li>Real-world application examples</li> </ul>"},{"location":"Online-Material/popular-resources/#industry-authority-assessment","title":"Industry Authority Assessment","text":"<ul> <li>Professional Experience</li> <li>Years of hands-on experience</li> <li>Success stories and case studies</li> <li>Recognition by peers and industry</li> <li> <p>Contribution to open-source projects</p> </li> <li> <p>Content Consistency</p> </li> <li>Regular publishing schedule</li> <li>Depth vs. clickbait balance</li> <li>Error correction and updates</li> <li>Engagement with reader feedback</li> </ul>"},{"location":"Online-Material/popular-resources/#content-discovery-strategies","title":"Content Discovery Strategies","text":""},{"location":"Online-Material/popular-resources/#systematic-discovery-methods","title":"Systematic Discovery Methods","text":"<ul> <li>Academic Source Discovery</li> <li>Follow citation networks from key papers</li> <li>Monitor conference proceedings and workshops</li> <li>Track researcher moves between institutions</li> <li> <p>Use academic search engines (Semantic Scholar, Google Scholar)</p> </li> <li> <p>Industry Source Discovery</p> </li> <li>Monitor job changes of known experts</li> <li>Follow acquisition and startup news</li> <li>Track open-source contribution patterns</li> <li>Watch for speaking circuit appearances</li> </ul>"},{"location":"Online-Material/popular-resources/#emerging-voice-identification","title":"Emerging Voice Identification","text":"<ul> <li>Early Career Researchers</li> <li>PhD students at top institutions</li> <li>Postdocs with strong publication records</li> <li>Industry researchers transitioning to thought leadership</li> <li> <p>Winners of academic competitions and awards</p> </li> <li> <p>Regional Expert Discovery</p> </li> <li>Local tech meetup speakers</li> <li>Regional conference organizers</li> <li>University extension program leaders</li> <li>Government AI initiative participants</li> </ul>"},{"location":"Online-Material/popular-resources/#content-consumption-optimization","title":"Content Consumption Optimization","text":""},{"location":"Online-Material/popular-resources/#information-diet-management","title":"Information Diet Management","text":"<ul> <li>Tier 1: Daily Essentials (5-10 sources)</li> <li>The Batch newsletter</li> <li>Key Twitter accounts (3-5 experts)</li> <li> <p>Primary work-related publications</p> </li> <li> <p>Tier 2: Weekly Deep Dives (5-10 sources)</p> </li> <li>Research paper selections</li> <li>Long-form blog posts</li> <li>Podcast episodes</li> <li> <p>Video tutorials</p> </li> <li> <p>Tier 3: Monthly Exploration (Unlimited)</p> </li> <li>New source discovery</li> <li>Adjacent field exploration</li> <li>Historical context building</li> <li>Trend analysis and prediction</li> </ul>"},{"location":"Online-Material/popular-resources/#active-reading-techniques","title":"Active Reading Techniques","text":"<ul> <li>Note-Taking Systems</li> <li>Zettelkasten for concept linking</li> <li>Spaced repetition for key facts</li> <li>Project idea collection</li> <li> <p>Career insight tracking</p> </li> <li> <p>Engagement Strategies</p> </li> <li>Comment thoughtfully on posts</li> <li>Share insights with attribution</li> <li>Join discussion communities</li> <li>Attend virtual events</li> </ul>"},{"location":"Online-Material/popular-resources/#content-creation-personal-branding","title":"\ud83d\ude80 Content Creation &amp; Personal Branding","text":""},{"location":"Online-Material/popular-resources/#building-your-ai-content-presence","title":"Building Your AI Content Presence","text":""},{"location":"Online-Material/popular-resources/#platform-strategy-for-different-goals","title":"Platform Strategy for Different Goals","text":"<ul> <li>Academic Career Path</li> <li>Twitter: Research announcements, paper discussions</li> <li>Personal Blog: Long-form technical explanations</li> <li>YouTube: Educational content, research presentations</li> <li> <p>LinkedIn: Professional networking, career updates</p> </li> <li> <p>Industry Professional Path</p> </li> <li>LinkedIn: Industry insights, career development</li> <li>Medium: Case studies, practical tutorials</li> <li>GitHub: Code portfolios, open-source contributions</li> <li>Podcast Guesting: Expertise demonstration</li> </ul>"},{"location":"Online-Material/popular-resources/#content-calendars-consistency","title":"Content Calendars &amp; Consistency","text":"<ul> <li>Weekly Content Schedule Example</li> <li>Monday: Industry news commentary</li> <li>Wednesday: Technical tutorial or explanation</li> <li>Friday: Personal project update or reflection</li> <li> <p>Monthly: Deep-dive research paper summary</p> </li> <li> <p>Content Batching Strategies</p> </li> <li>Research Phase: Collect ideas and sources</li> <li>Creation Phase: Write/record multiple pieces</li> <li>Distribution Phase: Schedule across platforms</li> <li>Engagement Phase: Respond and interact</li> </ul>"},{"location":"Online-Material/popular-resources/#monetization-strategies","title":"Monetization Strategies","text":""},{"location":"Online-Material/popular-resources/#direct-monetization","title":"Direct Monetization","text":"<ul> <li>Subscription Newsletters</li> <li>Substack: Independent newsletter platform</li> <li>ConvertKit: Email marketing with courses</li> <li> <p>Patreon: Supporter-based content funding</p> </li> <li> <p>Course Creation</p> </li> <li>Udemy: Mass market course platform</li> <li>Teachable: Independent course website</li> <li>Gumroad: Digital product sales</li> </ul>"},{"location":"Online-Material/popular-resources/#indirect-career-benefits","title":"Indirect Career Benefits","text":"<ul> <li>Speaking Opportunities</li> <li>Conference presentations</li> <li>Corporate training sessions</li> <li>University guest lectures</li> <li> <p>Podcast interview requests</p> </li> <li> <p>Career Advancement</p> </li> <li>Industry recognition and awards</li> <li>Job opportunities through visibility</li> <li>Consulting and advisory roles</li> <li>Book publishing opportunities</li> </ul>"},{"location":"Online-Material/popular-resources/#community-building","title":"Community Building","text":""},{"location":"Online-Material/popular-resources/#audience-development","title":"Audience Development","text":"<ul> <li>Value-First Approach</li> <li>Solve real problems for your audience</li> <li>Share knowledge before asking for anything</li> <li>Engage authentically with followers</li> <li> <p>Build genuine relationships over time</p> </li> <li> <p>Cross-Platform Synergy</p> </li> <li>Repurpose content across platforms</li> <li>Drive traffic between your properties</li> <li>Collaborate with other creators</li> <li>Participate in community discussions</li> </ul>"},{"location":"Online-Material/popular-resources/#long-term-sustainability","title":"Long-Term Sustainability","text":"<ul> <li>Avoid Burnout</li> <li>Set realistic publishing schedules</li> <li>Take breaks and maintain work-life balance</li> <li>Focus on quality over quantity</li> <li> <p>Delegate and collaborate when possible</p> </li> <li> <p>Adapt to Platform Changes</p> </li> <li>Diversify across multiple platforms</li> <li>Build direct audience relationships (email lists)</li> <li>Stay current with platform algorithm changes</li> <li>Monitor audience engagement metrics</li> </ul>"},{"location":"Online-Material/popular-resources/#content-recommendations-by-experience-level","title":"\ud83c\udfaf Content Recommendations by Experience Level","text":""},{"location":"Online-Material/popular-resources/#beginner-0-1-year-experience","title":"\ud83d\udd30 Beginner (0-1 year experience)","text":""},{"location":"Online-Material/popular-resources/#daily-reading-15-30-minutes","title":"Daily Reading (15-30 minutes)","text":"<ul> <li>Primary Sources:</li> <li>The Batch newsletter (weekly)</li> <li>Towards Data Science beginner articles</li> <li>3Blue1Brown videos (mathematics foundation)</li> <li>StatQuest (statistics concepts)</li> </ul>"},{"location":"Online-Material/popular-resources/#weekly-deep-dives-1-2-hours","title":"Weekly Deep Dives (1-2 hours)","text":"<ul> <li>Educational Content:</li> <li>Fast.ai course materials and forums</li> <li>Kaggle Learn micro-courses</li> <li>Andrew Ng's Coursera course discussions</li> <li>Python programming tutorials (Corey Schafer)</li> </ul>"},{"location":"Online-Material/popular-resources/#community-engagement","title":"Community Engagement","text":"<ul> <li>Supportive Communities:</li> <li>r/LearnMachineLearning (Reddit)</li> <li>Kaggle forums for beginners</li> <li>DataCamp community discussions</li> <li>Local AI meetup groups (virtual participation)</li> </ul>"},{"location":"Online-Material/popular-resources/#intermediate-1-3-years-experience","title":"\ud83d\udd36 Intermediate (1-3 years experience)","text":""},{"location":"Online-Material/popular-resources/#daily-information-diet","title":"Daily Information Diet","text":"<ul> <li>Industry Updates:</li> <li>Google AI Blog</li> <li>OpenAI Blog</li> <li>Distill.pub articles</li> <li>Selected Twitter follows (5-10 key experts)</li> </ul>"},{"location":"Online-Material/popular-resources/#weekly-learning","title":"Weekly Learning","text":"<ul> <li>Technical Development:</li> <li>Research paper reading (Papers with Code)</li> <li>Chip Huyen's blog for production ML</li> <li>Sebastian Ruder for NLP insights</li> <li>MLOps newsletters and case studies</li> </ul>"},{"location":"Online-Material/popular-resources/#professional-development","title":"Professional Development","text":"<ul> <li>Career Advancement:</li> <li>LinkedIn AI professional groups</li> <li>Industry conference virtual attendance</li> <li>Technical blog writing practice</li> <li>Open source contribution participation</li> </ul>"},{"location":"Online-Material/popular-resources/#advanced-3-years-experience","title":"\ud83d\udd38 Advanced (3+ years experience)","text":""},{"location":"Online-Material/popular-resources/#research-innovation-focus","title":"Research &amp; Innovation Focus","text":"<ul> <li>Cutting-Edge Content:</li> <li>arXiv paper monitoring</li> <li>Lex Fridman podcast for deep insights</li> <li>Machine Learning Street Talk for research discussions</li> <li>Conference proceedings and workshops</li> </ul>"},{"location":"Online-Material/popular-resources/#thought-leadership","title":"Thought Leadership","text":"<ul> <li>Industry Influence:</li> <li>Original content creation</li> <li>Conference speaking opportunities</li> <li>Mentorship of junior professionals</li> <li>Advisory roles and consulting</li> </ul>"},{"location":"Online-Material/popular-resources/#global-perspective","title":"Global Perspective","text":"<ul> <li>International Awareness:</li> <li>Regional AI development monitoring</li> <li>Policy and regulation tracking</li> <li>Cross-cultural AI application studies</li> <li>International collaboration opportunities</li> </ul>"},{"location":"Online-Material/popular-resources/#emerging-trends-future-content-areas","title":"\ud83d\udcc8 Emerging Trends &amp; Future Content Areas","text":""},{"location":"Online-Material/popular-resources/#rising-content-creators-platforms","title":"Rising Content Creators &amp; Platforms","text":""},{"location":"Online-Material/popular-resources/#next-generation-educators","title":"Next-Generation Educators","text":"<ul> <li>AI-Native Content Creators</li> <li>Creators who grew up with modern AI tools</li> <li>Multi-modal content (text, video, interactive)</li> <li>Platform-agnostic distribution strategies</li> <li>Community-first approach to audience building</li> </ul>"},{"location":"Online-Material/popular-resources/#corporate-content-evolution","title":"Corporate Content Evolution","text":"<ul> <li>Developer Relations Teams</li> <li>Anthropic's AI safety content</li> <li>Cohere's NLP developer resources</li> <li>Stability AI's open-source community</li> <li>Midjourney's creative AI discussions</li> </ul>"},{"location":"Online-Material/popular-resources/#content-format-innovation","title":"Content Format Innovation","text":""},{"location":"Online-Material/popular-resources/#interactive-immersive-content","title":"Interactive &amp; Immersive Content","text":"<ul> <li>Interactive Tutorials</li> <li>Jupyter notebook-based learning</li> <li>Web-based ML sandboxes</li> <li>AR/VR technical education</li> <li>AI-assisted personalized learning</li> </ul>"},{"location":"Online-Material/popular-resources/#real-time-content","title":"Real-Time Content","text":"<ul> <li>Live Research Discussion</li> <li>Clubhouse-style AI discussions</li> <li>Twitter Spaces technical talks</li> <li>Discord stage channels for education</li> <li>Real-time collaborative research</li> </ul>"},{"location":"Online-Material/popular-resources/#specialized-niches","title":"Specialized Niches","text":""},{"location":"Online-Material/popular-resources/#domain-specific-ai-content","title":"Domain-Specific AI Content","text":"<ul> <li>Healthcare AI Specialists</li> <li>Medical AI researchers and practitioners</li> <li>Regulatory compliance experts</li> <li>Clinical implementation specialists</li> <li> <p>Medical ethics and AI intersection</p> </li> <li> <p>Climate AI Focus</p> </li> <li>Environmental science + AI researchers</li> <li>Sustainability application specialists</li> <li>Policy intersection with technology</li> <li>Global climate data analysis</li> </ul>"},{"location":"Online-Material/popular-resources/#accessibility-inclusion","title":"Accessibility &amp; Inclusion","text":"<ul> <li>Diverse Voices</li> <li>Underrepresented minority researchers</li> <li>Global South AI development</li> <li>Accessibility technology specialists</li> <li>Ethical AI from diverse perspectives</li> </ul>"},{"location":"Online-Material/popular-resources/#advanced-content-discovery-techniques","title":"\ud83d\udd0d Advanced Content Discovery Techniques","text":""},{"location":"Online-Material/popular-resources/#automated-discovery-systems","title":"Automated Discovery Systems","text":""},{"location":"Online-Material/popular-resources/#rss-and-feed-aggregation","title":"RSS and Feed Aggregation","text":"<ul> <li>Technical Setup</li> <li>Feedly for blog aggregation</li> <li>Google Alerts for keyword monitoring</li> <li>Twitter lists for expert monitoring</li> <li> <p>Reddit saved searches for community insights</p> </li> <li> <p>AI-Powered Curation</p> </li> <li>Semantic search for relevant content</li> <li>Trend analysis across multiple sources</li> <li>Personalized content recommendations</li> <li>Cross-platform content correlation</li> </ul>"},{"location":"Online-Material/popular-resources/#research-paper-monitoring","title":"Research Paper Monitoring","text":"<ul> <li>Academic Alert Systems</li> <li>arXiv email alerts for specific categories</li> <li>Google Scholar citation alerts</li> <li>Connected Papers for research graph exploration</li> <li>Semantic Scholar API for programmatic monitoring</li> </ul>"},{"location":"Online-Material/popular-resources/#network-analysis-for-source-discovery","title":"Network Analysis for Source Discovery","text":""},{"location":"Online-Material/popular-resources/#citation-network-exploration","title":"Citation Network Exploration","text":"<ul> <li>Forward Citation Tracking</li> <li>Who cites the papers you find valuable?</li> <li>What institutions are building on key research?</li> <li>Which researchers are consistently referenced?</li> <li>How do ideas evolve across paper citations?</li> </ul>"},{"location":"Online-Material/popular-resources/#social-network-analysis","title":"Social Network Analysis","text":"<ul> <li>Twitter Network Analysis</li> <li>Who do your favorite experts follow?</li> <li>What hashtags correlate with quality content?</li> <li>Which conversations generate the most insight?</li> <li>How do ideas spread through expert networks?</li> </ul>"},{"location":"Online-Material/popular-resources/#quality-filtering-systems","title":"Quality Filtering Systems","text":""},{"location":"Online-Material/popular-resources/#content-scoring-frameworks","title":"Content Scoring Frameworks","text":"<ul> <li>Multi-Dimensional Evaluation</li> <li>Technical Accuracy (1-10)</li> <li>Practical Applicability (1-10)</li> <li>Clarity of Explanation (1-10)</li> <li>Novelty and Insight (1-10)</li> <li>Community Engagement (1-10)</li> </ul>"},{"location":"Online-Material/popular-resources/#source-reliability-metrics","title":"Source Reliability Metrics","text":"<ul> <li>Author Credibility Indicators</li> <li>H-index for academic authors</li> <li>Industry experience and recognition</li> <li>Consistency of quality over time</li> <li>Peer endorsement and citation frequency</li> </ul> <p>\ud83d\udcd6 Total Content: 1500+ lines of comprehensive content creator and resource analysis</p> <p>\ud83d\udca1 Content Consumption Philosophy: Quality over quantity, diverse perspectives over echo chambers, practical application over pure theory, and continuous learning over passive consumption.</p> <p>\ud83c\udfaf Quick Start for Content Discovery: - Time-Poor Professional: The Batch + 5 key Twitter experts + 1 weekly podcast - Deep Learner: arXiv alerts + 3 academic blogs + research paper reading group - Career Builder: LinkedIn groups + industry newsletters + conference content + networking</p> <p>\ud83d\ude80 Advanced Content Strategy: Create more than you consume, teach what you learn, build genuine relationships with other creators, and contribute unique perspectives to the community.</p> <p>\ud83d\udd17 Learning Resources: See our Online Study Material for structured courses and learning paths to complement these content sources.</p>"}]}
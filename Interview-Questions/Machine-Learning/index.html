<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A curated list of 100+ Machine Learning interview questions for cracking data science interviews at top tech companies"><meta name=author content="Kuldeep Singh Sidhu"><link href=https://singhsidhukuldeep.github.io/Interview-Questions/Machine-Learning/ rel=canonical><link href=../data-structures-algorithms/ rel=prev><link href=../System-design/ rel=next><link rel=icon href=https://repository-images.githubusercontent.com/275878203/13719500-bb75-11ea-8f3a-be2ffb87a6a2><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.50"><title>Machine Learning Interview Questions - Data Science Interview preparation</title><link rel=stylesheet href=../../assets/stylesheets/main.a40c8224.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-EVGNTG49J7"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-EVGNTG49J7",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-EVGNTG49J7",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-interview-questions class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <!-- Add announcement here, including arbitrary HTML --> <style>
    @keyframes shake {
      0%, 100% { transform: translateX(0); }
      10%, 30%, 50%, 70%, 90% { transform: translateX(-2px); }
      20%, 40%, 60%, 80% { transform: translateX(2px); }
    }
    @keyframes glow {
      0%, 100% { text-shadow: 0 0 5px rgba(255, 165, 0, 0.5); }
      50% { text-shadow: 0 0 20px rgba(255, 165, 0, 0.8), 0 0 30px rgba(255, 140, 0, 0.6); }
    }
    .shake-text {
      display: inline-block;
      animation: shake 3s ease-in-out infinite;
    }
    .glow-link {
      animation: glow 2s ease-in-out infinite;
      font-weight: bold;
    }
  </style> <span class=shake-text>üöÄ <a href=/flashcards class=glow-link>Flashcards</a> feature is live!</span> <meta name=google-adsense-account content=ca-pub-4988388949365963> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4988388949365963" crossorigin=anonymous></script> </div> </aside> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Data Science Interview preparation" class="md-header__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Science Interview preparation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Machine Learning Interview Questions </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Data Science Interview preparation" class="md-nav__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> Data Science Interview preparation </label> <div class=md-nav__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> üè° Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> üë®üèø‚Äçüè´ Interview Questions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> üë®üèø‚Äçüè´ Interview Questions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../flashcards/ class=md-nav__link> <span class=md-ellipsis> üìá Flashcards </span> </a> </li> <li class=md-nav__item> <a href=../data-structures-algorithms/ class=md-nav__link> <span class=md-ellipsis> DSA (Data Structures & Algorithms) </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Machine Learning </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#premium-interview-questions class=md-nav__link> <span class=md-ellipsis> Premium Interview Questions </span> </a> <nav class=md-nav aria-label="Premium Interview Questions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-does-gradient-descent-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Does Gradient Descent Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-precision-recall-and-f1-score-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question class=md-nav__link> <span class=md-ellipsis> What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-neural-networks-and-backpropagation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Neural Networks and Backpropagation - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dropout-and-why-does-it-work-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dropout and Why Does It Work? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-transfer-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Transfer Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-roc-curve-and-auc-score-microsoft-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-k-means-clustering-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain K-Means Clustering - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-engineering-give-examples-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-missing-data-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-evaluate-regression-models-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Evaluate Regression Models? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-curse-of-dimensionality-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Curse of Dimensionality? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-categorical-features-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Categorical Features? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-calibration-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Calibration? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-semi-supervised-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Semi-Supervised Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-active-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Active Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-automl-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is AutoML? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-early-stopping-and-how-does-it-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Early Stopping and How Does It Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-learning-rate-scheduling-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Learning Rate Scheduling - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-and-how-do-you-prevent-it-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage and How Do You Prevent It? - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-calibration-in-machine-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Calibration in Machine Learning - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-knowledge-distillation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Knowledge Distillation? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-ensemble-methods-bagging-vs-boosting-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Ensemble Methods: Bagging vs Boosting - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-is-it-effective-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Is It Effective? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-residual-networks-resnet-and-skip-connections-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Residual Networks (ResNet) and Skip Connections - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-attention-mechanism-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Attention Mechanism? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-feature-importance-methods-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Feature Importance Methods - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-hyperparameter-tuning-methods-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Hyperparameter Tuning Methods - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-compression-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Compression? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-metrics-for-imbalanced-classification-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Metrics for Imbalanced Classification - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-multi-task-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Multi-Task Learning? - Google, Meta Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#questions-asked-in-google-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Google interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-facebook-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Facebook interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-amazon-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Amazon interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-microsoft-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Microsoft interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-uber-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Uber interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-swiggy-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Swiggy interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-flipkart-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Flipkart interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-ola-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Ola interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-paytm-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Paytm interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-oyo-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in OYO interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-whatsapp-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in WhatsApp interview </span> </a> <nav class=md-nav aria-label="Questions asked in WhatsApp interview"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#explain-few-shot-and-zero-shot-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Few-Shot and Zero-Shot Learning - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-optimizers-in-neural-networks-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> What are Optimizers in Neural Networks? - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-class-imbalance-handling-techniques-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Class Imbalance Handling Techniques - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explainable-ai-xai-google-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explainable AI (XAI) - Google, Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#curriculum-learning-deepmind-openai-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Curriculum Learning - DeepMind, OpenAI, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#active-learning-google-research-snorkel-ai-scale-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Active Learning - Google Research, Snorkel AI, Scale AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#meta-learning-learning-to-learn-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Meta-Learning (Learning to Learn) - DeepMind, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#continuallifelong-learning-deepmind-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Continual/Lifelong Learning - DeepMind, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#graph-neural-networks-gnns-deepmind-meta-ai-twitter-interview-question class=md-nav__link> <span class=md-ellipsis> Graph Neural Networks (GNNs) - DeepMind, Meta AI, Twitter Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#reinforcement-learning-basics-deepmind-openai-tesla-interview-question class=md-nav__link> <span class=md-ellipsis> Reinforcement Learning Basics - DeepMind, OpenAI, Tesla Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#variational-autoencoders-vaes-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Variational Autoencoders (VAEs) - DeepMind, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#generative-adversarial-networks-gans-nvidia-openai-stability-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Generative Adversarial Networks (GANs) - NVIDIA, OpenAI, Stability AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#transformer-architecture-google-openai-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Transformer Architecture - Google, OpenAI, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-vs-transfer-learning-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Fine-Tuning vs Transfer Learning - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#handling-missing-data-google-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Handling Missing Data - Google, Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#feature-selection-techniques-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Feature Selection Techniques - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#time-series-forecasting-uber-airbnb-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Time Series Forecasting - Uber, Airbnb, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#ab-testing-in-ml-meta-uber-netflix-airbnb-interview-question class=md-nav__link> <span class=md-ellipsis> A/B Testing in ML - Meta, Uber, Netflix, Airbnb Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#model-monitoring-drift-detection-uber-netflix-airbnb-interview-question class=md-nav__link> <span class=md-ellipsis> Model Monitoring &amp; Drift Detection - Uber, Netflix, Airbnb Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#neural-architecture-search-nas-google-deepmind-interview-question class=md-nav__link> <span class=md-ellipsis> Neural Architecture Search (NAS) - Google, DeepMind Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#federated-learning-google-apple-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Federated Learning - Google, Apple, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#model-compression-google-nvidia-apple-interview-question class=md-nav__link> <span class=md-ellipsis> Model Compression - Google, NVIDIA, Apple Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#causal-inference-in-ml-linkedin-airbnb-uber-interview-question class=md-nav__link> <span class=md-ellipsis> Causal Inference in ML - LinkedIn, Airbnb, Uber Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#recommendation-systems-netflix-spotify-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Recommendation Systems - Netflix, Spotify, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#imbalanced-learning-stripe-paypal-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Imbalanced Learning - Stripe, PayPal, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#embedding-techniques-google-meta-linkedin-interview-question class=md-nav__link> <span class=md-ellipsis> Embedding Techniques - Google, Meta, LinkedIn Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#bias-and-fairness-in-ml-google-microsoft-linkedin-interview-question class=md-nav__link> <span class=md-ellipsis> Bias and Fairness in ML - Google, Microsoft, LinkedIn Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#multi-task-learning-google-deepmind-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Multi-Task Learning - Google DeepMind, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#adversarial-robustness-google-openai-deepmind-interview-question class=md-nav__link> <span class=md-ellipsis> Adversarial Robustness - Google, OpenAI, DeepMind Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#self-supervised-learning-meta-ai-google-research-interview-question class=md-nav__link> <span class=md-ellipsis> Self-Supervised Learning - Meta AI, Google Research Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#few-shot-learning-meta-ai-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Few-Shot Learning - Meta AI, DeepMind, OpenAI Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#questions-asked-in-slack-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Slack interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-airbnb-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Airbnb interview </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../System-design/ class=md-nav__link> <span class=md-ellipsis> System Design </span> </a> </li> <li class=md-nav__item> <a href=../Natural-Language-Processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing (NLP) </span> </a> </li> <li class=md-nav__item> <a href=../Probability/ class=md-nav__link> <span class=md-ellipsis> Probability </span> </a> </li> <li class=md-nav__item> <a href=../AB-testing/ class=md-nav__link> <span class=md-ellipsis> A/B Testing </span> </a> </li> <li class=md-nav__item> <a href=../SQL-Interview-Questions/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../Scikit-Learn/ class=md-nav__link> <span class=md-ellipsis> Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=../LangChain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../LangGraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../Interview-Question-Resources/ class=md-nav__link> <span class=md-ellipsis> Interview Question Resources </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> üìù Cheat Sheets </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> üìù Cheat Sheets </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Cheat-Sheets/Django/ class=md-nav__link> <span class=md-ellipsis> Django </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Flask/ class=md-nav__link> <span class=md-ellipsis> Flask </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Hypothesis-Tests/ class=md-nav__link> <span class=md-ellipsis> Hypothesis Tests </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Keras/ class=md-nav__link> <span class=md-ellipsis> Keras </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PySpark/ class=md-nav__link> <span class=md-ellipsis> PySpark </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PyTorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/RegEx/ class=md-nav__link> <span class=md-ellipsis> Regular Expressions (RegEx) </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Sk-learn/ class=md-nav__link> <span class=md-ellipsis> Scikit Learn </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/SQL/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/tensorflow/ class=md-nav__link> <span class=md-ellipsis> TensorFlow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> ‚Äçüéì ML Topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> ‚Äçüéì ML Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Machine-Learning/ARIMA/ class=md-nav__link> <span class=md-ellipsis> ARIMA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Activation%20functions/ class=md-nav__link> <span class=md-ellipsis> Activation functions </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Collaborative%20Filtering/ class=md-nav__link> <span class=md-ellipsis> Collaborative Filtering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Confusion%20Matrix/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/DBSCAN/ class=md-nav__link> <span class=md-ellipsis> DBSCAN </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Decision%20Trees/ class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Gradient%20Boosting/ class=md-nav__link> <span class=md-ellipsis> Gradient Boosting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/K-means%20clustering/ class=md-nav__link> <span class=md-ellipsis> K-means clustering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Linear%20Regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Logistic%20Regression/ class=md-nav__link> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/ class=md-nav__link> <span class=md-ellipsis> Loss Function MAE, RMSE </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Neural%20Networks/ class=md-nav__link> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normal%20Distribution/ class=md-nav__link> <span class=md-ellipsis> Normal Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normalization%20Regularisation/ class=md-nav__link> <span class=md-ellipsis> Normalization Regularisation </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Overfitting%2C%20Underfitting/ class=md-nav__link> <span class=md-ellipsis> Overfitting, Underfitting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/PCA/ class=md-nav__link> <span class=md-ellipsis> PCA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Random%20Forest/ class=md-nav__link> <span class=md-ellipsis> Random Forest </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Support%20Vector%20Machines/ class=md-nav__link> <span class=md-ellipsis> Support Vector Machines </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Unbalanced%2C%20Skewed%20data/ class=md-nav__link> <span class=md-ellipsis> Unbalanced, Skewed data </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/kNN/ class=md-nav__link> <span class=md-ellipsis> kNN </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> üë®üèæ‚Äçüíª Online Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> üë®üèæ‚Äçüíª Online Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Online-Material/Online-Material-for-Learning/ class=md-nav__link> <span class=md-ellipsis> Online Study Material </span> </a> </li> <li class=md-nav__item> <a href=../../Online-Material/popular-resources/ class=md-nav__link> <span class=md-ellipsis> Popular Blogs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../projects/ class=md-nav__link> <span class=md-ellipsis> üì≥ Projects </span> </a> </li> <li class=md-nav__item> <a href=../../Contribute/ class=md-nav__link> <span class=md-ellipsis> ü§ù Contribute </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#premium-interview-questions class=md-nav__link> <span class=md-ellipsis> Premium Interview Questions </span> </a> <nav class=md-nav aria-label="Premium Interview Questions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-does-gradient-descent-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Does Gradient Descent Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-precision-recall-and-f1-score-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question class=md-nav__link> <span class=md-ellipsis> What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-neural-networks-and-backpropagation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Neural Networks and Backpropagation - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dropout-and-why-does-it-work-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dropout and Why Does It Work? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-transfer-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Transfer Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-roc-curve-and-auc-score-microsoft-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-k-means-clustering-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain K-Means Clustering - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-engineering-give-examples-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-missing-data-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-evaluate-regression-models-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Evaluate Regression Models? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-curse-of-dimensionality-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Curse of Dimensionality? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-categorical-features-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Categorical Features? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-calibration-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Calibration? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-semi-supervised-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Semi-Supervised Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-active-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Active Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-automl-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is AutoML? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-early-stopping-and-how-does-it-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Early Stopping and How Does It Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-learning-rate-scheduling-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Learning Rate Scheduling - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-and-how-do-you-prevent-it-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage and How Do You Prevent It? - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-calibration-in-machine-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Calibration in Machine Learning - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-knowledge-distillation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Knowledge Distillation? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-ensemble-methods-bagging-vs-boosting-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Ensemble Methods: Bagging vs Boosting - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-is-it-effective-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Is It Effective? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-residual-networks-resnet-and-skip-connections-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Residual Networks (ResNet) and Skip Connections - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-attention-mechanism-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Attention Mechanism? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-feature-importance-methods-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Feature Importance Methods - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-hyperparameter-tuning-methods-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Hyperparameter Tuning Methods - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-compression-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Compression? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-metrics-for-imbalanced-classification-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Metrics for Imbalanced Classification - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-multi-task-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Multi-Task Learning? - Google, Meta Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#questions-asked-in-google-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Google interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-facebook-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Facebook interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-amazon-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Amazon interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-microsoft-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Microsoft interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-uber-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Uber interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-swiggy-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Swiggy interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-flipkart-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Flipkart interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-ola-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Ola interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-paytm-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Paytm interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-oyo-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in OYO interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-whatsapp-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in WhatsApp interview </span> </a> <nav class=md-nav aria-label="Questions asked in WhatsApp interview"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#explain-few-shot-and-zero-shot-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Few-Shot and Zero-Shot Learning - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-optimizers-in-neural-networks-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> What are Optimizers in Neural Networks? - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-class-imbalance-handling-techniques-most-tech-companies-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Class Imbalance Handling Techniques - Most Tech Companies Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explainable-ai-xai-google-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explainable AI (XAI) - Google, Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#curriculum-learning-deepmind-openai-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Curriculum Learning - DeepMind, OpenAI, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#active-learning-google-research-snorkel-ai-scale-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Active Learning - Google Research, Snorkel AI, Scale AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#meta-learning-learning-to-learn-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Meta-Learning (Learning to Learn) - DeepMind, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#continuallifelong-learning-deepmind-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Continual/Lifelong Learning - DeepMind, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#graph-neural-networks-gnns-deepmind-meta-ai-twitter-interview-question class=md-nav__link> <span class=md-ellipsis> Graph Neural Networks (GNNs) - DeepMind, Meta AI, Twitter Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#reinforcement-learning-basics-deepmind-openai-tesla-interview-question class=md-nav__link> <span class=md-ellipsis> Reinforcement Learning Basics - DeepMind, OpenAI, Tesla Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#variational-autoencoders-vaes-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Variational Autoencoders (VAEs) - DeepMind, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#generative-adversarial-networks-gans-nvidia-openai-stability-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Generative Adversarial Networks (GANs) - NVIDIA, OpenAI, Stability AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#transformer-architecture-google-openai-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Transformer Architecture - Google, OpenAI, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#fine-tuning-vs-transfer-learning-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Fine-Tuning vs Transfer Learning - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#handling-missing-data-google-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Handling Missing Data - Google, Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#feature-selection-techniques-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Feature Selection Techniques - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#time-series-forecasting-uber-airbnb-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Time Series Forecasting - Uber, Airbnb, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#ab-testing-in-ml-meta-uber-netflix-airbnb-interview-question class=md-nav__link> <span class=md-ellipsis> A/B Testing in ML - Meta, Uber, Netflix, Airbnb Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#model-monitoring-drift-detection-uber-netflix-airbnb-interview-question class=md-nav__link> <span class=md-ellipsis> Model Monitoring &amp; Drift Detection - Uber, Netflix, Airbnb Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#neural-architecture-search-nas-google-deepmind-interview-question class=md-nav__link> <span class=md-ellipsis> Neural Architecture Search (NAS) - Google, DeepMind Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#federated-learning-google-apple-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Federated Learning - Google, Apple, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#model-compression-google-nvidia-apple-interview-question class=md-nav__link> <span class=md-ellipsis> Model Compression - Google, NVIDIA, Apple Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#causal-inference-in-ml-linkedin-airbnb-uber-interview-question class=md-nav__link> <span class=md-ellipsis> Causal Inference in ML - LinkedIn, Airbnb, Uber Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#recommendation-systems-netflix-spotify-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Recommendation Systems - Netflix, Spotify, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#imbalanced-learning-stripe-paypal-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Imbalanced Learning - Stripe, PayPal, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#embedding-techniques-google-meta-linkedin-interview-question class=md-nav__link> <span class=md-ellipsis> Embedding Techniques - Google, Meta, LinkedIn Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#bias-and-fairness-in-ml-google-microsoft-linkedin-interview-question class=md-nav__link> <span class=md-ellipsis> Bias and Fairness in ML - Google, Microsoft, LinkedIn Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#multi-task-learning-google-deepmind-meta-ai-interview-question class=md-nav__link> <span class=md-ellipsis> Multi-Task Learning - Google DeepMind, Meta AI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#adversarial-robustness-google-openai-deepmind-interview-question class=md-nav__link> <span class=md-ellipsis> Adversarial Robustness - Google, OpenAI, DeepMind Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#self-supervised-learning-meta-ai-google-research-interview-question class=md-nav__link> <span class=md-ellipsis> Self-Supervised Learning - Meta AI, Google Research Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#few-shot-learning-meta-ai-deepmind-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Few-Shot Learning - Meta AI, DeepMind, OpenAI Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#questions-asked-in-slack-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Slack interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-airbnb-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Airbnb interview </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/edit/master/docs/Interview-Questions/Machine-Learning.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/raw/master/docs/Interview-Questions/Machine-Learning.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=machine-learning-interview-questions>Machine Learning Interview Questions</h1> <p>This comprehensive guide contains <strong>100+ Machine Learning interview questions</strong> commonly asked at top tech companies like Google, Amazon, Meta, Microsoft, and Netflix. Each premium question includes detailed explanations, code examples, and interviewer insights to help you ace your ML interviews.</p> <hr> <h2 id=premium-interview-questions>Premium Interview Questions</h2> <p>Master these frequently asked ML questions with detailed explanations, code examples, and insights into what interviewers really look for.</p> <hr> <h3 id=what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question>What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Evaluation</code>, <code>Generalization</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Concept:</strong></p> <p>The bias-variance tradeoff is a fundamental concept that describes the tension between two sources of error in machine learning models:</p> <ul> <li><strong>Bias</strong>: Error from overly simplistic assumptions. High bias ‚Üí underfitting.</li> <li><strong>Variance</strong>: Error from sensitivity to training data fluctuations. High variance ‚Üí overfitting.</li> </ul> <p><strong>Mathematical Formulation:</strong></p> <p>For a model's expected prediction error:</p> <div class=arithmatex>\[\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]</div> <div class=arithmatex>\[E[(y - \hat{f}(x))^2] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] + \sigma^2\]</div> <p><strong>Visual Understanding:</strong></p> <table> <thead> <tr> <th>Model Complexity</th> <th>Bias</th> <th>Variance</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Low (Linear)</td> <td>High</td> <td>Low</td> <td>Underfitting</td> </tr> <tr> <td>Optimal</td> <td>Balanced</td> <td>Balanced</td> <td>Good generalization</td> </tr> <tr> <td>High (Deep NN)</td> <td>Low</td> <td>High</td> <td>Overfitting</td> </tr> </tbody> </table> <p><strong>Practical Example:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestRegressor</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>PolynomialFeatures</span>

<span class=c1># High Bias Model (Underfitting)</span>
<span class=n>linear_model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>scores_linear</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>linear_model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Linear Model CV Score: </span><span class=si>{</span><span class=n>scores_linear</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_linear</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># Balanced Model</span>
<span class=n>rf_model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>scores_rf</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>rf_model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Random Forest CV Score: </span><span class=si>{</span><span class=n>scores_rf</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_rf</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># High Variance Model (Overfitting risk)</span>
<span class=n>rf_deep</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>scores_deep</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>rf_deep</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Deep RF CV Score: </span><span class=si>{</span><span class=n>scores_deep</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_deep</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're really testing:</strong> Your ability to diagnose model performance issues and choose appropriate solutions.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can draw the classic U-shaped curve from memory</li> <li>Gives concrete examples: "Linear regression on non-linear data = high bias"</li> <li>Mentions solutions: cross-validation, regularization, ensemble methods</li> <li>Discusses real scenarios: "In production at scale, I often prefer slightly higher bias for stability"</li> </ul> </div> </details> <hr> <h3 id=explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question>Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Feature Selection</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Amazon, Microsoft, Google, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Core Difference:</strong></p> <p>Both add a penalty term to the loss function to prevent overfitting, but with different effects:</p> <table> <thead> <tr> <th>Aspect</th> <th>L1 (Lasso)</th> <th>L2 (Ridge)</th> </tr> </thead> <tbody> <tr> <td>Penalty</td> <td>$\lambda \sum</td> <td>w_i</td> </tr> <tr> <td>Effect on weights</td> <td>Drives weights to exactly 0</td> <td>Shrinks weights toward 0</td> </tr> <tr> <td>Feature selection</td> <td>Yes (sparse solutions)</td> <td>No (keeps all features)</td> </tr> <tr> <td>Geometry</td> <td>Diamond constraint</td> <td>Circular constraint</td> </tr> <tr> <td>Best for</td> <td>High-dimensional sparse data</td> <td>Multicollinearity</td> </tr> </tbody> </table> <p><strong>Mathematical Formulation:</strong></p> <div class=arithmatex>\[\text{L1 Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} |w_i|\]</div> <div class=arithmatex>\[\text{L2 Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} w_i^2\]</div> <p><strong>Why L1 Creates Sparsity (Geometric Intuition):</strong></p> <p>The L1 constraint region is a diamond shape. The optimal solution often occurs at corners where some weights = 0.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>ElasticNet</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_regression</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Generate data with some irrelevant features</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_regression</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>n_informative</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>noise</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># L1 Regularization - Feature Selection</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
<span class=n>lasso</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;L1 Non-zero coefficients: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>lasso</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
<span class=c1># Output: ~5 (identifies informative features)</span>

<span class=c1># L2 Regularization - All features kept</span>
<span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
<span class=n>ridge</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;L2 Non-zero coefficients: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>ridge</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
<span class=c1># Output: 20 (all features kept, but shrunk)</span>

<span class=c1># Elastic Net - Best of both worlds</span>
<span class=n>elastic</span> <span class=o>=</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=n>elastic</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Elastic Net Non-zero: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>elastic</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep understanding of regularization mechanics, not just definitions.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains WHY L1 creates zeros (diamond geometry)</li> <li>Knows when to use each: "L1 for feature selection, L2 for correlated features"</li> <li>Mentions Elastic Net as hybrid solution</li> <li>Can discuss tuning Œª via cross-validation</li> </ul> </div> </details> <hr> <h3 id=how-does-gradient-descent-work-google-meta-interview-question>How Does Gradient Descent Work? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Optimization</code>, <code>Deep Learning</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Idea:</strong></p> <p>Gradient descent is an iterative optimization algorithm that finds the minimum of a function by repeatedly moving in the direction of steepest descent (negative gradient).</p> <p><strong>Update Rule:</strong></p> <div class=arithmatex>\[w_{t+1} = w_t - \eta \cdot \nabla L(w_t)\]</div> <p>Where: - <span class=arithmatex>\(w_t\)</span> = current weights - <span class=arithmatex>\(\eta\)</span> = learning rate (step size) - <span class=arithmatex>\(\nabla L(w_t)\)</span> = gradient of loss function</p> <p><strong>Variants Comparison:</strong></p> <table> <thead> <tr> <th>Variant</th> <th>Batch Size</th> <th>Speed</th> <th>Stability</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>Batch GD</td> <td>All data</td> <td>Slow</td> <td>Very stable</td> <td>High</td> </tr> <tr> <td>Stochastic GD</td> <td>1 sample</td> <td>Fast</td> <td>Noisy</td> <td>Low</td> </tr> <tr> <td>Mini-batch GD</td> <td>32-512</td> <td>Balanced</td> <td>Balanced</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>Modern Optimizers:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=c1># Standard SGD</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># SGD with Momentum (accelerates convergence)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>

<span class=c1># Adam (adaptive learning rates per parameter)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>))</span>

<span class=c1># AdamW (Adam with proper weight decay)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Adam's Magic Formula:</strong></p> <div class=arithmatex>\[m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$ $$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$ $$w_{t+1} = w_t - \eta \frac{m_t}{\sqrt{v_t} + \epsilon}\]</div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Can you explain optimization intuitively AND mathematically?</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Draws the loss landscape and shows how GD navigates it</li> <li>Knows why learning rate matters (too high = diverge, too low = slow)</li> <li>Can explain momentum: "Like a ball rolling downhill with inertia"</li> <li>Knows Adam is often the default: "Adaptive LR + momentum, works well out-of-box"</li> </ul> </div> </details> <hr> <h3 id=what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question>What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Model Evaluation</code>, <code>Validation</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Meta, Amazon, Google, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>The Problem It Solves:</strong></p> <p>A single train/test split can give misleading results due to random variation in how data is split. Cross-validation provides a more reliable estimate of model performance.</p> <p><strong>K-Fold Cross-Validation:</strong></p> <ol> <li>Split data into K equal folds</li> <li>For each fold i:<ul> <li>Train on all folds except i</li> <li>Validate on fold i</li> </ul> </li> <li>Average all K validation scores</li> </ol> <p><strong>Common Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>K</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>5-Fold</td> <td>5</td> <td>Standard, good balance</td> </tr> <tr> <td>10-Fold</td> <td>10</td> <td>More reliable, slower</td> </tr> <tr> <td>Leave-One-Out</td> <td>N</td> <td>Small datasets, expensive</td> </tr> <tr> <td>Stratified K-Fold</td> <td>K</td> <td>Imbalanced classification</td> </tr> <tr> <td>Time Series Split</td> <td>K</td> <td>Temporal data (no leakage)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>cross_val_score</span><span class=p>,</span> <span class=n>KFold</span><span class=p>,</span> <span class=n>StratifiedKFold</span><span class=p>,</span> <span class=n>TimeSeriesSplit</span>
<span class=p>)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Standard K-Fold</span>
<span class=n>cv_scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span>
<span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;CV Score: </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=o>*</span><span class=mi>2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># Stratified for imbalanced data</span>
<span class=n>stratified_cv</span> <span class=o>=</span> <span class=n>StratifiedKFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Time Series (prevents data leakage)</span>
<span class=n>tscv</span> <span class=o>=</span> <span class=n>TimeSeriesSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=k>for</span> <span class=n>train_idx</span><span class=p>,</span> <span class=n>test_idx</span> <span class=ow>in</span> <span class=n>tscv</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Train: </span><span class=si>{</span><span class=n>train_idx</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2>..., Test: </span><span class=si>{</span><span class=n>test_idx</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2>...&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of model validation fundamentals.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to use stratified (imbalanced classes) vs regular</li> <li>Immediately mentions TimeSeriesSplit for temporal data (data leakage awareness)</li> <li>Can explain computational tradeoff: "10-fold is 2x slower but more reliable"</li> <li>Mentions nested CV for hyperparameter tuning</li> </ul> </div> </details> <hr> <h3 id=explain-precision-recall-and-f1-score-google-microsoft-interview-question>Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification Metrics</code>, <code>Model Evaluation</code>, <code>Imbalanced Data</code> | <strong>Asked by:</strong> Google, Microsoft, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Confusion Matrix Foundation:</strong></p> <table> <thead> <tr> <th></th> <th>Predicted Positive</th> <th>Predicted Negative</th> </tr> </thead> <tbody> <tr> <td><strong>Actual Positive</strong></td> <td>TP (True Positive)</td> <td>FN (False Negative)</td> </tr> <tr> <td><strong>Actual Negative</strong></td> <td>FP (False Positive)</td> <td>TN (True Negative)</td> </tr> </tbody> </table> <p><strong>The Metrics:</strong></p> <div class=arithmatex>\[\text{Precision} = \frac{TP}{TP + FP}\]</div> <p><em>"Of all positive predictions, how many were correct?"</em></p> <div class=arithmatex>\[\text{Recall} = \frac{TP}{TP + FN}\]</div> <p><em>"Of all actual positives, how many did we find?"</em></p> <div class=arithmatex>\[\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\]</div> <p><em>"Harmonic mean - penalizes extreme imbalances"</em></p> <p><strong>When to Prioritize Which:</strong></p> <table> <thead> <tr> <th>Scenario</th> <th>Priority</th> <th>Why</th> </tr> </thead> <tbody> <tr> <td>Spam detection</td> <td>Precision</td> <td>Don't want to lose important emails</td> </tr> <tr> <td>Cancer screening</td> <td>Recall</td> <td>Don't want to miss any cases</td> </tr> <tr> <td>Fraud detection</td> <td>F1 or Recall</td> <td>Balance matters, but missing fraud is costly</td> </tr> <tr> <td>Search ranking</td> <td>Precision@K</td> <td>Top results quality matters most</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>precision_score</span><span class=p>,</span> <span class=n>recall_score</span><span class=p>,</span> <span class=n>f1_score</span><span class=p>,</span>
    <span class=n>classification_report</span><span class=p>,</span> <span class=n>precision_recall_curve</span>
<span class=p>)</span>

<span class=c1># All metrics at once</span>
<span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>

<span class=c1># Adjust threshold for Precision-Recall tradeoff</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
<span class=n>precisions</span><span class=p>,</span> <span class=n>recalls</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>

<span class=c1># Find threshold for desired recall (e.g., 95%)</span>
<span class=n>target_recall</span> <span class=o>=</span> <span class=mf>0.95</span>
<span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>recalls</span> <span class=o>-</span> <span class=n>target_recall</span><span class=p>))</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Threshold for </span><span class=si>{</span><span class=n>target_recall</span><span class=si>}</span><span class=s2> recall: </span><span class=si>{</span><span class=n>optimal_threshold</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Can you choose the right metric for the business problem?</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Immediately asks: "What's the cost of false positives vs false negatives?"</li> <li>Knows accuracy is misleading for imbalanced data</li> <li>Can adjust classification threshold based on business needs</li> <li>Mentions AUC-PR for highly imbalanced datasets</li> </ul> </div> </details> <hr> <h3 id=what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question>What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Tree Models</code>, <code>Interpretability</code>, <code>Classification</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>How Decision Trees Work:</strong></p> <p>Decision trees recursively split the data based on feature values to create pure (homogeneous) leaf nodes.</p> <p><strong>Splitting Criteria:</strong></p> <p>For Classification (Information Gain / Gini):</p> <div class=arithmatex>\[\text{Gini} = 1 - \sum_{i=1}^{C} p_i^2\]</div> <div class=arithmatex>\[\text{Entropy} = -\sum_{i=1}^{C} p_i \log_2(p_i)\]</div> <p>For Regression (Variance Reduction):</p> <div class=arithmatex>\[\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2\]</div> <p><strong>Pros and Cons:</strong></p> <table> <thead> <tr> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Interpretable (white-box)</td> <td>Prone to overfitting</td> </tr> <tr> <td>No scaling needed</td> <td>Unstable (small data changes ‚Üí different tree)</td> </tr> <tr> <td>Handles non-linear relationships</td> <td>Greedy, not globally optimal</td> </tr> <tr> <td>Feature importance built-in</td> <td>Can't extrapolate beyond training range</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.tree</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span><span class=p>,</span> <span class=n>plot_tree</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Create and train</span>
<span class=n>tree</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>           <span class=c1># Prevent overfitting</span>
    <span class=n>min_samples_split</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>  <span class=c1># Minimum samples to split</span>
    <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>   <span class=c1># Minimum samples in leaf</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>tree</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Visualize the tree</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>plot_tree</span><span class=p>(</span><span class=n>tree</span><span class=p>,</span> <span class=n>feature_names</span><span class=o>=</span><span class=n>feature_names</span><span class=p>,</span> 
          <span class=n>class_names</span><span class=o>=</span><span class=n>class_names</span><span class=p>,</span> <span class=n>filled</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Feature importance</span>
<span class=n>importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>feature_names</span><span class=p>,</span>
    <span class=s1>&#39;importance&#39;</span><span class=p>:</span> <span class=n>tree</span><span class=o>.</span><span class=n>feature_importances_</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>importance</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of interpretable ML and when to use simple models.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows trees are building blocks for Random Forest, XGBoost</li> <li>Can explain pruning techniques (pre-pruning vs post-pruning)</li> <li>Mentions when to use: "Interpretability required, e.g., credit decisioning"</li> <li>Knows limitation: "Single trees overfit; ensembles solve this"</li> </ul> </div> </details> <hr> <h3 id=random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question>Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>XGBoost</code>, <code>Model Selection</code> | <strong>Asked by:</strong> Google, Netflix, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Fundamental Difference:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Random Forest</th> <th>Gradient Boosting</th> </tr> </thead> <tbody> <tr> <td>Strategy</td> <td>Bagging (parallel)</td> <td>Boosting (sequential)</td> </tr> <tr> <td>Trees</td> <td>Independent</td> <td>Each fixes previous errors</td> </tr> <tr> <td>Bias-Variance</td> <td>Reduces variance</td> <td>Reduces bias</td> </tr> <tr> <td>Overfitting</td> <td>Resistant</td> <td>Can overfit if not tuned</td> </tr> <tr> <td>Training</td> <td>Parallelizable, fast</td> <td>Sequential, slower</td> </tr> <tr> <td>Tuning</td> <td>Easy</td> <td>Requires careful tuning</td> </tr> </tbody> </table> <p><strong>When to Use Which:</strong></p> <table> <thead> <tr> <th>Scenario</th> <th>Choice</th> <th>Reason</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Random Forest</td> <td>Works well with default params</td> </tr> <tr> <td>Maximum accuracy</td> <td>Gradient Boosting</td> <td>Better with tuning</td> </tr> <tr> <td>Large dataset</td> <td>Random Forest</td> <td>Faster training</td> </tr> <tr> <td>Kaggle competition</td> <td>XGBoost/LightGBM</td> <td>State-of-art tabular</td> </tr> <tr> <td>Production (simplicity)</td> <td>Random Forest</td> <td>More robust, less tuning</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span><span class=p>,</span> <span class=n>GradientBoostingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=kn>import</span> <span class=n>XGBClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>lightgbm</span><span class=w> </span><span class=kn>import</span> <span class=n>LGBMClassifier</span>

<span class=c1># Random Forest - Quick and robust</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>  <span class=c1># Parallel training</span>
<span class=p>)</span>

<span class=c1># Gradient Boosting (sklearn) - Good baseline</span>
<span class=n>gb</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>3</span>
<span class=p>)</span>

<span class=c1># XGBoost - Industry standard</span>
<span class=n>xgb</span> <span class=o>=</span> <span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>

<span class=c1># LightGBM - Fastest, handles large data</span>
<span class=n>lgbm</span> <span class=o>=</span> <span class=n>LGBMClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>num_leaves</span><span class=o>=</span><span class=mi>31</span><span class=p>,</span>
    <span class=n>feature_fraction</span><span class=o>=</span><span class=mf>0.8</span>
<span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical model selection skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains bagging vs boosting conceptually</li> <li>Knows XGBoost/LightGBM are gradient boosting implementations</li> <li>Can discuss tradeoffs: "RF is easier to deploy, GB needs more tuning"</li> <li>Mentions real experience: "In production, I often start with RF for baseline"</li> </ul> </div> </details> <hr> <h3 id=what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question>What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Generalization</code>, <code>Regularization</code>, <code>Model Evaluation</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Apple, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Definition:</strong></p> <p>Overfitting occurs when a model learns the training data too well, including noise and outliers, and fails to generalize to new data.</p> <p><strong>Signs of Overfitting:</strong></p> <ul> <li>High training accuracy, low test accuracy</li> <li>Large gap between training and validation loss</li> <li>Model complexity &gt;&gt; data complexity</li> </ul> <p><strong>Prevention Techniques:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>How It Helps</th> </tr> </thead> <tbody> <tr> <td>More data</td> <td>Reduces variance</td> </tr> <tr> <td>Regularization (L1/L2)</td> <td>Constrains model complexity</td> </tr> <tr> <td>Cross-validation</td> <td>Better estimate of generalization</td> </tr> <tr> <td>Early stopping</td> <td>Stops before overfitting</td> </tr> <tr> <td>Dropout</td> <td>Prevents co-adaptation in NNs</td> </tr> <tr> <td>Data augmentation</td> <td>Increases effective dataset size</td> </tr> <tr> <td>Ensemble methods</td> <td>Averages out individual model errors</td> </tr> <tr> <td>Feature selection</td> <td>Reduces irrelevant noise</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>learning_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Diagnose overfitting with learning curves</span>
<span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=p>,</span> <span class=n>val_scores</span> <span class=o>=</span> <span class=n>learning_curve</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span>
    <span class=n>train_sizes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span>
<span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>val_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Training Size&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Learning Curve - Check for Overfitting&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Early stopping example (XGBoost)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=kn>import</span> <span class=n>XGBClassifier</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
    <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>  <span class=c1># Stop if no improvement</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
    <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best iteration: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>best_iteration</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Core ML intuition and practical experience.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can draw learning curves and interpret them</li> <li>Mentions multiple techniques, not just one</li> <li>Knows underfitting is the opposite problem</li> <li>Gives real examples: "I use early stopping + regularization together"</li> </ul> </div> </details> <hr> <h3 id=explain-neural-networks-and-backpropagation-google-meta-interview-question>Explain Neural Networks and Backpropagation - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Neural Networks</code>, <code>Optimization</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Neural Network Architecture:</strong></p> <p>A neural network is a series of layers that transform input through weighted connections and non-linear activation functions:</p> <div class=arithmatex>\[z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$ $$a^{[l]} = g(z^{[l]})\]</div> <p>Where: - <span class=arithmatex>\(W^{[l]}\)</span> = weight matrix for layer <span class=arithmatex>\(l\)</span> - <span class=arithmatex>\(b^{[l]}\)</span> = bias vector - <span class=arithmatex>\(g\)</span> = activation function (ReLU, sigmoid, etc.)</p> <p><strong>Backpropagation (Chain Rule):</strong></p> <div class=arithmatex>\[\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial a^{[L]}} \cdot \frac{\partial a^{[L]}}{\partial z^{[L]}} \cdot ... \cdot \frac{\partial z^{[l]}}{\partial W^{[l]}}\]</div> <p><strong>Common Activation Functions:</strong></p> <table> <thead> <tr> <th>Function</th> <th>Formula</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>ReLU</td> <td><span class=arithmatex>\(\max(0, x)\)</span></td> <td>Hidden layers (default)</td> </tr> <tr> <td>Sigmoid</td> <td><span class=arithmatex>\(\frac{1}{1+e^{-x}}\)</span></td> <td>Binary output</td> </tr> <tr> <td>Softmax</td> <td><span class=arithmatex>\(\frac{e^{x_i}}{\sum e^{x_j}}\)</span></td> <td>Multi-class output</td> </tr> <tr> <td>Tanh</td> <td><span class=arithmatex>\(\frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span></td> <td>Hidden layers (centered)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Non-linearity is crucial!</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Training loop with backprop</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>SimpleNN</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>
        <span class=c1># Forward pass</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

        <span class=c1># Backward pass (backpropagation)</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>  <span class=c1># Clear old gradients</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>        <span class=c1># Compute gradients</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>       <span class=c1># Update weights</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep understanding of DL fundamentals.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can explain why non-linearity is essential (stacked linear = just linear)</li> <li>Knows vanishing gradient problem and solutions (ReLU, ResNets, LSTM)</li> <li>Can derive simple backprop by hand (at least for 1-layer)</li> <li>Mentions practical considerations: batch normalization, dropout</li> </ul> </div> </details> <hr> <h3 id=what-is-dropout-and-why-does-it-work-amazon-meta-interview-question>What is Dropout and Why Does It Work? - Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Deep Learning</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>How Dropout Works:</strong></p> <p>During training, randomly set a fraction <span class=arithmatex>\(p\)</span> of neuron outputs to zero:</p> <ol> <li>For each training batch:<ul> <li>Randomly select neurons to "drop" (output = 0)</li> <li>Scale remaining outputs by <span class=arithmatex>\(\frac{1}{1-p}\)</span> to maintain expected value</li> </ul> </li> <li>During inference:<ul> <li>Use all neurons (no dropout)</li> </ul> </li> </ol> <p><strong>Why It Works (Multiple Perspectives):</strong></p> <table> <thead> <tr> <th>Perspective</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Ensemble</td> <td>Training many sub-networks, averaging at test time</td> </tr> <tr> <td>Co-adaptation</td> <td>Prevents neurons from relying on specific other neurons</td> </tr> <tr> <td>Regularization</td> <td>Adds noise, similar to L2 regularization</td> </tr> <tr> <td>Bayesian</td> <td>Approximates Bayesian inference (variational)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>DropoutNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 50% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>  <span class=c1># 30% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Applied during training</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Important: model.eval() disables dropout for inference</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
<span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
    <span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <p><strong>Common Dropout Rates:</strong></p> <ul> <li>Input layer: 0.2 (keep 80%)</li> <li>Hidden layers: 0.5 (keep 50%)</li> <li>After BatchNorm: Often not needed</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of regularization in deep learning.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows dropout is only active during training</li> <li>Can explain the scaling factor (<span class=arithmatex>\(\frac{1}{1-p}\)</span>)</li> <li>Mentions alternatives: DropConnect, Spatial Dropout for CNNs</li> <li>Knows practical tips: "Don't use after BatchNorm, less needed with modern architectures"</li> </ul> </div> </details> <hr> <h3 id=what-is-transfer-learning-google-amazon-interview-question>What is Transfer Learning? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Pretrained Models</code>, <code>Fine-tuning</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Idea:</strong></p> <p>Transfer learning leverages knowledge from a model trained on a large dataset (source task) to improve performance on a different but related task (target task).</p> <p><strong>Why It Works:</strong></p> <ul> <li>Lower layers learn general features (edges, textures, word patterns)</li> <li>Higher layers learn task-specific features</li> <li>General features transfer well across tasks</li> </ul> <p><strong>Transfer Learning Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>When to Use</th> <th>How</th> </tr> </thead> <tbody> <tr> <td>Feature extraction</td> <td>Small target dataset</td> <td>Freeze pretrained layers, train new head</td> </tr> <tr> <td>Fine-tuning</td> <td>Medium target dataset</td> <td>Unfreeze some layers, train with low LR</td> </tr> <tr> <td>Full fine-tuning</td> <td>Large target dataset</td> <td>Unfreeze all, train end-to-end</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=c1># Computer Vision (PyTorch)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>models</span>

<span class=c1># Load pretrained ResNet</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet50</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Strategy 1: Feature Extraction (freeze backbone)</span>
<span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>

<span class=c1># Replace final layer for our task</span>
<span class=n>model</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>in_features</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>

<span class=c1># Strategy 2: Fine-tuning (unfreeze last block)</span>
<span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>layer4</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>

<span class=c1># NLP (Hugging Face Transformers)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span><span class=p>,</span> <span class=n>AutoTokenizer</span>

<span class=c1># Load pretrained BERT</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
    <span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>,</span>
    <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span>  <span class=c1># Binary classification</span>
<span class=p>)</span>
<span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>)</span>

<span class=c1># Fine-tune with lower learning rate for pretrained layers</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>([</span>
    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>bert</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>2e-5</span><span class=p>},</span>     <span class=c1># Pretrained</span>
    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>classifier</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>}</span>  <span class=c1># New head</span>
<span class=p>])</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical deep learning experience.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to freeze vs fine-tune (data size matters)</li> <li>Mentions learning rate strategies (lower LR for pretrained)</li> <li>Can name popular pretrained models: ResNet, BERT, GPT</li> <li>Discusses domain shift: "Fine-tune more when source/target domains differ"</li> </ul> </div> </details> <hr> <h3 id=explain-roc-curve-and-auc-score-microsoft-netflix-interview-question>Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification Metrics</code>, <code>Model Evaluation</code>, <code>Binary Classification</code> | <strong>Asked by:</strong> Microsoft, Netflix, Google, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>ROC Curve (Receiver Operating Characteristic):</strong></p> <p>Plots True Positive Rate vs False Positive Rate at various classification thresholds:</p> <div class=arithmatex>\[TPR = \frac{TP}{TP + FN} = \text{Recall}\]</div> <div class=arithmatex>\[FPR = \frac{FP}{FP + TN}\]</div> <p><strong>AUC (Area Under Curve):</strong></p> <ul> <li><strong>AUC = 1.0</strong>: Perfect classifier</li> <li><strong>AUC = 0.5</strong>: Random guessing (diagonal line)</li> <li><strong>AUC &lt; 0.5</strong>: Worse than random (inverted predictions)</li> </ul> <p><strong>Interpretation:</strong></p> <p>AUC = Probability that a randomly chosen positive example ranks higher than a randomly chosen negative example.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>roc_curve</span><span class=p>,</span> <span class=n>auc</span><span class=p>,</span> <span class=n>roc_auc_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Get probabilities</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Calculate ROC curve</span>
<span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>roc_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
<span class=n>roc_auc</span> <span class=o>=</span> <span class=n>auc</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;ROC Curve (AUC = </span><span class=si>{</span><span class=n>roc_auc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Random&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;False Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;True Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;ROC Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Quick AUC calculation</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;AUC Score: </span><span class=si>{</span><span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span><span class=w> </span><span class=n>y_proba</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Find optimal threshold (Youden&#39;s J statistic)</span>
<span class=n>optimal_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>tpr</span> <span class=o>-</span> <span class=n>fpr</span><span class=p>)</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>optimal_idx</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Optimal Threshold: </span><span class=si>{</span><span class=n>optimal_threshold</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>ROC-AUC vs PR-AUC:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Best For</th> <th>Why</th> </tr> </thead> <tbody> <tr> <td>ROC-AUC</td> <td>Balanced classes</td> <td>Considers both classes equally</td> </tr> <tr> <td>PR-AUC</td> <td>Imbalanced classes</td> <td>Focuses on positive class performance</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of evaluation metrics beyond accuracy.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows ROC-AUC can be misleading for imbalanced data</li> <li>Can interpret thresholds: "Moving along the curve = changing threshold"</li> <li>Mentions practical application: "I use AUC for model comparison, threshold tuning for deployment"</li> <li>Knows PR-AUC is better for highly imbalanced problems</li> </ul> </div> </details> <hr> <h3 id=what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question>What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Dimensionality Reduction</code>, <code>Feature Extraction</code>, <code>Unsupervised Learning</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Reduce Dimensions:</strong></p> <ul> <li>Curse of dimensionality (data becomes sparse)</li> <li>Reduce computation time</li> <li>Remove noise and redundant features</li> <li>Enable visualization (2D/3D)</li> </ul> <p><strong>PCA (Principal Component Analysis):</strong></p> <p>Finds orthogonal directions (principal components) that maximize variance in the data.</p> <p><strong>Steps:</strong> 1. Center the data (subtract mean) 2. Compute covariance matrix 3. Find eigenvectors and eigenvalues 4. Select top k eigenvectors 5. Project data onto new basis</p> <div class=arithmatex>\[\text{Maximize: } \sum_{i=1}^{k} \text{Var}(X \cdot w_i) = \sum_{i=1}^{k} \lambda_i\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=c1># Step 1: Always scale before PCA!</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Step 2: Determine optimal number of components</span>
<span class=n>pca_full</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>()</span>
<span class=n>pca_full</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=c1># Plot explained variance</span>
<span class=n>cumsum</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>pca_full</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>)</span>
<span class=n>n_95</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>cumsum</span> <span class=o>&gt;=</span> <span class=mf>0.95</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Components for 95% variance: </span><span class=si>{</span><span class=n>n_95</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Step 3: Apply PCA</span>
<span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=n>n_95</span><span class=p>)</span>
<span class=n>X_reduced</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=c1># Visualization (2D)</span>
<span class=n>pca_2d</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>X_2d</span> <span class=o>=</span> <span class=n>pca_2d</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_2d</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_2d</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;PC1 (</span><span class=si>{</span><span class=n>pca_2d</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>:</span><span class=s1>.1%</span><span class=si>}</span><span class=s1> var)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;PC2 (</span><span class=si>{</span><span class=n>pca_2d</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=si>:</span><span class=s1>.1%</span><span class=si>}</span><span class=s1> var)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Alternative Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Best For</th> <th>Preserves</th> </tr> </thead> <tbody> <tr> <td>PCA</td> <td>Linear relationships, variance</td> <td>Global structure</td> </tr> <tr> <td>t-SNE</td> <td>Visualization</td> <td>Local structure</td> </tr> <tr> <td>UMAP</td> <td>Large datasets, clustering</td> <td>Local + global</td> </tr> <tr> <td>LDA</td> <td>Classification</td> <td>Class separability</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of unsupervised learning and feature engineering.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows to scale data before PCA (otherwise high-variance features dominate)</li> <li>Can explain 95% variance retention heuristic</li> <li>Mentions limitations: "PCA assumes linear relationships"</li> <li>Knows alternatives: t-SNE for visualization, UMAP for clustering</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question>How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Imbalanced Data</code>, <code>Classification</code>, <code>Sampling</code> | <strong>Asked by:</strong> Netflix, Meta, Amazon, Google</p> <details class=success> <summary>View Answer</summary> <p><strong>The Problem:</strong></p> <p>When one class dominates (e.g., 99% negative, 1% positive), models tend to predict the majority class and achieve high accuracy while missing the minority class entirely.</p> <p><strong>Solutions Toolkit:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>Category</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td>Class weights</td> <td>Cost-sensitive</td> <td>Always try first</td> </tr> <tr> <td>SMOTE</td> <td>Oversampling</td> <td>Moderate imbalance</td> </tr> <tr> <td>Random undersampling</td> <td>Undersampling</td> <td>Large dataset</td> </tr> <tr> <td>Threshold tuning</td> <td>Post-processing</td> <td>Quick fix</td> </tr> <tr> <td>Focal Loss</td> <td>Loss function</td> <td>Deep learning</td> </tr> <tr> <td>Ensemble methods</td> <td>Modeling</td> <td>Severe imbalance</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.utils.class_weight</span><span class=w> </span><span class=kn>import</span> <span class=n>compute_class_weight</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.over_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTE</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.under_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomUnderSampler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>

<span class=c1># Method 1: Class Weights (built into most algorithms)</span>
<span class=n>class_weights</span> <span class=o>=</span> <span class=n>compute_class_weight</span><span class=p>(</span><span class=s1>&#39;balanced&#39;</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y</span><span class=p>),</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>class_weight</span><span class=o>=</span><span class=s1>&#39;balanced&#39;</span><span class=p>)</span>

<span class=c1># Method 2: SMOTE (Synthetic Minority Over-sampling)</span>
<span class=n>smote</span> <span class=o>=</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smote</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Method 3: Combined Sampling Pipeline</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;under&#39;</span><span class=p>,</span> <span class=n>RandomUnderSampler</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;over&#39;</span><span class=p>,</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)),</span>
<span class=p>])</span>
<span class=n>X_balanced</span><span class=p>,</span> <span class=n>y_balanced</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Method 4: Threshold Tuning</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
<span class=c1># Lower threshold to catch more positives</span>
<span class=n>y_pred_adjusted</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_proba</span> <span class=o>&gt;=</span> <span class=mf>0.3</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>  <span class=c1># Instead of 0.5</span>

<span class=c1># Method 5: Focal Loss (PyTorch)</span>
<span class=k>class</span><span class=w> </span><span class=nc>FocalLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
        <span class=n>ce_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
        <span class=n>pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>ce_loss</span><span class=p>)</span>
        <span class=n>focal_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pt</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>ce_loss</span>
        <span class=k>return</span> <span class=n>focal_loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</code></pre></div> <p><strong>Evaluation for Imbalanced Data:</strong></p> <ul> <li>‚ùå Accuracy (misleading)</li> <li>‚úÖ Precision, Recall, F1</li> <li>‚úÖ PR-AUC (better than ROC-AUC)</li> <li>‚úÖ Confusion matrix</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Real-world ML problem-solving.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>First asks: "How imbalanced? 90-10 is different from 99.9-0.1"</li> <li>Knows class weights is usually the first approach</li> <li>Warns about SMOTE pitfalls: "Can overfit to synthetic examples"</li> <li>Mentions correct metrics: "Never use accuracy for imbalanced data"</li> </ul> </div> </details> <hr> <h3 id=explain-k-means-clustering-amazon-microsoft-interview-question>Explain K-Means Clustering - Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Clustering</code>, <code>Unsupervised Learning</code>, <code>K-Means</code> | <strong>Asked by:</strong> Amazon, Microsoft, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Algorithm Steps:</strong></p> <ol> <li>Initialize k centroids randomly</li> <li>Assign each point to nearest centroid</li> <li>Recalculate centroids as cluster means</li> <li>Repeat steps 2-3 until convergence</li> </ol> <p><strong>Objective Function (Inertia):</strong></p> <div class=arithmatex>\[J = \sum_{i=1}^{n} \min_{j} ||x_i - \mu_j||^2\]</div> <p>Minimize within-cluster sum of squares.</p> <p><strong>Choosing K (Elbow Method):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.cluster</span><span class=w> </span><span class=kn>import</span> <span class=n>KMeans</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>silhouette_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Elbow Method</span>
<span class=n>inertias</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>silhouettes</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>K_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>11</span><span class=p>)</span>

<span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K_range</span><span class=p>:</span>
    <span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
    <span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>inertias</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>kmeans</span><span class=o>.</span><span class=n>inertia_</span><span class=p>)</span>
    <span class=n>silhouettes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>silhouette_score</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>labels_</span><span class=p>))</span>

<span class=c1># Plot</span>
<span class=n>fig</span><span class=p>,</span> <span class=p>(</span><span class=n>ax1</span><span class=p>,</span> <span class=n>ax2</span><span class=p>)</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>

<span class=n>ax1</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>inertias</span><span class=p>,</span> <span class=s1>&#39;bo-&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Clusters (K)&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Inertia&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Elbow Method&#39;</span><span class=p>)</span>

<span class=n>ax2</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>silhouettes</span><span class=p>,</span> <span class=s1>&#39;ro-&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Clusters (K)&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Silhouette Score&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Silhouette Method&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Final model</span>
<span class=n>optimal_k</span> <span class=o>=</span> <span class=mi>5</span>  <span class=c1># From elbow analysis</span>
<span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>optimal_k</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>labels</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Limitations and Alternatives:</strong></p> <table> <thead> <tr> <th>Limitation</th> <th>Better Alternative</th> </tr> </thead> <tbody> <tr> <td>Assumes spherical clusters</td> <td>DBSCAN, GMM</td> </tr> <tr> <td>Sensitive to initialization</td> <td>KMeans++ (default)</td> </tr> <tr> <td>Must specify K</td> <td>DBSCAN (auto-detects)</td> </tr> <tr> <td>Sensitive to outliers</td> <td>DBSCAN, Robust clustering</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Basic unsupervised learning understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows K-means++ initialization (sklearn default)</li> <li>Can explain limitations: "Assumes spherical, equal-size clusters"</li> <li>Mentions silhouette score for validation</li> <li>Knows when to use alternatives: "DBSCAN for arbitrary shapes"</li> </ul> </div> </details> <hr> <h3 id=what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question>What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Classification</code>, <code>Kernel Methods</code>, <code>Margin Maximization</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are SVMs?</strong></p> <p>Support Vector Machines are supervised learning models that find the optimal hyperplane to separate classes with maximum margin.</p> <p><strong>Key Concepts:</strong></p> <table> <thead> <tr> <th>Concept</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>Support Vectors</td> <td>Data points closest to decision boundary</td> </tr> <tr> <td>Margin</td> <td>Distance between boundary and nearest points</td> </tr> <tr> <td>Kernel Trick</td> <td>Maps data to higher dimensions for non-linear separation</td> </tr> </tbody> </table> <p><strong>Kernels:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.svm</span><span class=w> </span><span class=kn>import</span> <span class=n>SVC</span>

<span class=c1># Linear kernel - for linearly separable data</span>
<span class=n>svm_linear</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># RBF (Gaussian) - most common for non-linear</span>
<span class=n>svm_rbf</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;rbf&#39;</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=s1>&#39;scale&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># Polynomial kernel</span>
<span class=n>svm_poly</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;poly&#39;</span><span class=p>,</span> <span class=n>degree</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># Training</span>
<span class=n>svm_rbf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>svm_rbf</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>When to Use SVMs:</strong></p> <table> <thead> <tr> <th>Good for</th> <th>Not good for</th> </tr> </thead> <tbody> <tr> <td>High-dimensional data (text)</td> <td>Very large datasets (slow)</td> </tr> <tr> <td>Clear margin of separation</td> <td>Noisy data with overlapping classes</td> </tr> <tr> <td>Fewer samples than features</td> <td>Multi-class (needs one-vs-one)</td> </tr> </tbody> </table> <p><strong>Hyperparameters:</strong></p> <ul> <li><strong>C (Regularization)</strong>: Trade-off between margin and misclassification</li> <li><strong>gamma</strong>: Kernel coefficient - high = overfitting, low = underfitting</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of geometric intuition and kernel methods.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains margin maximization geometrically</li> <li>Knows when to use different kernels</li> <li>Mentions computational complexity O(n¬≤) to O(n¬≥)</li> <li>Knows SVMs work well for text classification</li> </ul> </div> </details> <hr> <h3 id=explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question>Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Computer Vision</code>, <code>Neural Networks</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple, NVIDIA</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are CNNs?</strong></p> <p>CNNs are neural networks designed for processing structured grid data (images, time series) using convolutional layers that detect spatial patterns.</p> <p><strong>Core Components:</strong></p> <table> <thead> <tr> <th>Layer</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Convolutional</td> <td>Extract features using learnable filters</td> </tr> <tr> <td>Pooling</td> <td>Downsample, reduce computation, add translation invariance</td> </tr> <tr> <td>Fully Connected</td> <td>Classification at the end</td> </tr> <tr> <td>Activation (ReLU)</td> <td>Add non-linearity</td> </tr> </tbody> </table> <p><strong>How Convolution Works:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleCNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Input: 3 channels (RGB), Output: 32 filters, 3x3 kernel</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># 2x2 pooling</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>  <span class=c1># After 2 pools: 32‚Üí16‚Üí8</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>  <span class=c1># 10 classes</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>  <span class=c1># 32x32 ‚Üí 16x16</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>  <span class=c1># 16x16 ‚Üí 8x8</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># Flatten</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>Key CNN Architectures:</strong></p> <table> <thead> <tr> <th>Architecture</th> <th>Year</th> <th>Innovation</th> </tr> </thead> <tbody> <tr> <td>LeNet</td> <td>1998</td> <td>First practical CNN</td> </tr> <tr> <td>AlexNet</td> <td>2012</td> <td>Deep CNNs, ReLU, Dropout</td> </tr> <tr> <td>VGG</td> <td>2014</td> <td>Small 3x3 filters, depth</td> </tr> <tr> <td>ResNet</td> <td>2015</td> <td>Skip connections (residual)</td> </tr> <tr> <td>EfficientNet</td> <td>2019</td> <td>Compound scaling</td> </tr> </tbody> </table> <p><strong>Calculations:</strong></p> <p>Output size: <span class=arithmatex>\((W - K + 2P) / S + 1\)</span></p> <p>Where: W = input, K = kernel, P = padding, S = stride</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep learning fundamentals and computer vision.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can calculate output dimensions</li> <li>Explains why pooling helps (translation invariance)</li> <li>Knows ResNet skip connections solve vanishing gradients</li> <li>Mentions transfer learning: "Use pretrained ImageNet models"</li> </ul> </div> </details> <hr> <h3 id=what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question>What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Sequence Models</code>, <code>NLP</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are RNNs?</strong></p> <p>RNNs process sequential data by maintaining hidden state that captures information from previous time steps.</p> <p><strong>The Problem: Vanishing Gradients</strong></p> <p>Standard RNNs struggle with long sequences because gradients vanish/explode during backpropagation through time.</p> <p><strong>LSTM Solution:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>LSTMModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> 
                           <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
                           <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>bidirectional</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  <span class=c1># *2 for bidirectional</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>output</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span><span class=p>(</span><span class=n>embedded</span><span class=p>)</span>
        <span class=c1># Concatenate final hidden states from both directions</span>
        <span class=n>hidden</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>],</span> <span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>hidden</span><span class=p>)</span>
</code></pre></div> <p><strong>LSTM Gates:</strong></p> <table> <thead> <tr> <th>Gate</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Forget</td> <td>Decide what to discard from cell state</td> </tr> <tr> <td>Input</td> <td>Decide what new info to store</td> </tr> <tr> <td>Output</td> <td>Decide what to output</td> </tr> </tbody> </table> <p><strong>GRU vs LSTM:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>LSTM</th> <th>GRU</th> </tr> </thead> <tbody> <tr> <td>Gates</td> <td>3 (forget, input, output)</td> <td>2 (reset, update)</td> </tr> <tr> <td>Parameters</td> <td>More</td> <td>Fewer</td> </tr> <tr> <td>Performance</td> <td>Better for longer sequences</td> <td>Often comparable</td> </tr> </tbody> </table> <p><strong>Modern Alternatives:</strong></p> <ul> <li><strong>Transformers</strong>: Now preferred for most NLP tasks</li> <li><strong>1D CNNs</strong>: Faster for some sequence tasks</li> <li><strong>Attention mechanisms</strong>: Can be added to RNNs</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of sequence modeling.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains vanishing gradient problem</li> <li>Draws LSTM cell diagram with gates</li> <li>Knows when to use bidirectional</li> <li>Mentions: "Transformers have largely replaced LSTMs for NLP"</li> </ul> </div> </details> <hr> <h3 id=what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question>What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Training</code>, <code>Regularization</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Batch Normalization?</strong></p> <p>Batch normalization normalizes layer inputs by re-centering and re-scaling, making training faster and more stable.</p> <p><strong>The Formula:</strong></p> <div class=arithmatex>\[\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y = \gamma \hat{x} + \beta\]</div> <p>Where <span class=arithmatex>\(\gamma\)</span> (scale) and <span class=arithmatex>\(\beta\)</span> (shift) are learnable parameters.</p> <p><strong>Benefits:</strong></p> <table> <thead> <tr> <th>Benefit</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Faster training</td> <td>Enables higher learning rates</td> </tr> <tr> <td>Regularization</td> <td>Adds noise (mini-batch statistics)</td> </tr> <tr> <td>Reduces internal covariate shift</td> <td>Stable distributions</td> </tr> <tr> <td>Less sensitive to initialization</td> <td>Normalizes anyway</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>CNNWithBatchNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>  <span class=c1># After conv, before activation</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn_fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>  <span class=c1># For fully connected</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Normalize</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Then activate</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Training vs. inference mode matters!</span>
<span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># Uses batch statistics</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>   <span class=c1># Uses running averages</span>
</code></pre></div> <p><strong>Layer Normalization (Alternative):</strong></p> <table> <thead> <tr> <th>BatchNorm</th> <th>LayerNorm</th> </tr> </thead> <tbody> <tr> <td>Normalizes across batch</td> <td>Normalizes across features</td> </tr> <tr> <td>Needs batch statistics</td> <td>Works with batch size 1</td> </tr> <tr> <td>Good for CNNs</td> <td>Good for RNNs, Transformers</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of deep learning training dynamics.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows position: after linear/conv, before activation</li> <li>Explains train vs eval mode difference</li> <li>Mentions Layer Norm for Transformers</li> <li>Knows it's less needed with skip connections (ResNet)</li> </ul> </div> </details> <hr> <h3 id=what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question>What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>Boosting</code>, <code>Tabular Data</code> | <strong>Asked by:</strong> Amazon, Google, Microsoft, Netflix, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>XGBoost vs Random Forest:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Random Forest</th> <th>XGBoost</th> </tr> </thead> <tbody> <tr> <td>Method</td> <td>Bagging (parallel trees)</td> <td>Boosting (sequential trees)</td> </tr> <tr> <td>Error Focus</td> <td>Each tree is independent</td> <td>Each tree fixes previous errors</td> </tr> <tr> <td>Overfitting</td> <td>Resistant</td> <td>Needs regularization</td> </tr> <tr> <td>Speed</td> <td>Parallelizable</td> <td>Optimized (GPU support)</td> </tr> <tr> <td>Interpretability</td> <td>Feature importance</td> <td>Feature importance + SHAP</td> </tr> </tbody> </table> <p><strong>How XGBoost Works:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>

<span class=c1># Basic XGBoost</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>reg_alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>  <span class=c1># L1 regularization</span>
    <span class=n>reg_lambda</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>  <span class=c1># L2 regularization</span>
    <span class=n>use_label_encoder</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>

<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=n>importance</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>feature_importances_</span>

<span class=c1># Cross-validation</span>
<span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Hyperparameters:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Effect</th> </tr> </thead> <tbody> <tr> <td>n_estimators</td> <td>Number of trees</td> </tr> <tr> <td>max_depth</td> <td>Tree depth (prevent overfitting)</td> </tr> <tr> <td>learning_rate</td> <td>Shrinkage (lower = more trees needed)</td> </tr> <tr> <td>subsample</td> <td>Row sampling per tree</td> </tr> <tr> <td>colsample_bytree</td> <td>Feature sampling per tree</td> </tr> <tr> <td>reg_alpha/lambda</td> <td>L1/L2 regularization</td> </tr> </tbody> </table> <p><strong>When to Use Which:</strong></p> <table> <thead> <tr> <th>Use Random Forest</th> <th>Use XGBoost</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Maximum accuracy</td> </tr> <tr> <td>Less tuning time</td> <td>Tabular competitions</td> </tr> <tr> <td>Reduce overfitting</td> <td>Handle missing values</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML knowledge for tabular data.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains bagging vs boosting difference</li> <li>Knows key hyperparameters to tune</li> <li>Mentions: "XGBoost handles missing values natively"</li> <li>Knows alternatives: LightGBM (faster), CatBoost (categorical)</li> </ul> </div> </details> <hr> <h3 id=explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question>Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>NLP</code>, <code>Transformers</code> | <strong>Asked by:</strong> Google, Meta, OpenAI, Microsoft, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Attention?</strong></p> <p>Attention allows models to focus on relevant parts of the input when producing output, replacing the need for recurrence.</p> <p><strong>Self-Attention Formula:</strong></p> <div class=arithmatex>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div> <p><strong>Transformer Architecture:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>math</span>

<span class=k>class</span><span class=w> </span><span class=nc>SelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>embed_dim</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Linear projections</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Reshape for multi-head</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>K</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Scaled dot-product attention</span>
        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>

        <span class=n>attention</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>

        <span class=c1># Concatenate heads</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Multi-Head Attention</td> <td>Attend to different representation subspaces</td> </tr> <tr> <td>Position Encoding</td> <td>Inject sequence order information</td> </tr> <tr> <td>Layer Normalization</td> <td>Stabilize training</td> </tr> <tr> <td>Feed-Forward Network</td> <td>Non-linear transformation</td> </tr> </tbody> </table> <p><strong>Transformer Models:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Type</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>Encoder-only</td> <td>Classification, NER</td> </tr> <tr> <td>GPT</td> <td>Decoder-only</td> <td>Text generation</td> </tr> <tr> <td>T5</td> <td>Encoder-Decoder</td> <td>Translation, summarization</td> </tr> <tr> <td>ViT</td> <td>Vision</td> <td>Image classification</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Modern deep learning architecture understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can explain Q, K, V analogy (query-key-value retrieval)</li> <li>Knows why scaling by ‚àöd_k (prevent softmax saturation)</li> <li>Understands positional encoding necessity</li> <li>Mentions computational complexity: O(n¬≤) for sequence length n</li> </ul> </div> </details> <hr> <h3 id=what-is-feature-engineering-give-examples-amazon-google-meta-interview-question>What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Feature Engineering</code>, <code>ML Pipeline</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Feature Engineering?</strong></p> <p>Feature engineering is the process of creating, transforming, and selecting features to improve model performance.</p> <p><strong>Categories of Feature Engineering:</strong></p> <table> <thead> <tr> <th>Category</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>Creation</td> <td>Domain-specific features, aggregations</td> </tr> <tr> <td>Transformation</td> <td>Log, sqrt, polynomial features</td> </tr> <tr> <td>Encoding</td> <td>One-hot, target encoding, embeddings</td> </tr> <tr> <td>Scaling</td> <td>Standardization, normalization</td> </tr> <tr> <td>Selection</td> <td>Filter, wrapper, embedded methods</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>

<span class=c1># 1. Date/Time features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;day_of_week&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>dayofweek</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;hour&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>hour</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;is_weekend&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;day_of_week&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>isin</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>])</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;month_sin&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>month</span> <span class=o>/</span> <span class=mi>12</span><span class=p>)</span>  <span class=c1># Cyclical</span>

<span class=c1># 2. Aggregation features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_total_purchases&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;sum&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_avg_purchase&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_purchase_count&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;count&#39;</span><span class=p>)</span>

<span class=c1># 3. Text features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;text_length&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>len</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;word_count&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>split</span><span class=p>()</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>len</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;has_question&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\?&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>

<span class=c1># 4. Interaction features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;price_per_sqft&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;sqft&#39;</span><span class=p>]</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;bmi&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;weight&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;height&#39;</span><span class=p>]</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

<span class=c1># 5. Binning</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;age_group&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>18</span><span class=p>,</span> <span class=mi>35</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>],</span> 
                         <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;child&#39;</span><span class=p>,</span> <span class=s1>&#39;young&#39;</span><span class=p>,</span> <span class=s1>&#39;middle&#39;</span><span class=p>,</span> <span class=s1>&#39;senior&#39;</span><span class=p>])</span>

<span class=c1># 6. Target encoding (for categorical)</span>
<span class=n>target_means</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>target_means</span><span class=p>)</span>

<span class=c1># 7. Log transformation (for skewed data)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;log_income&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log1p</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>])</span>  <span class=c1># log1p handles zeros</span>
</code></pre></div> <p><strong>Domain-Specific Examples:</strong></p> <table> <thead> <tr> <th>Domain</th> <th>Feature Ideas</th> </tr> </thead> <tbody> <tr> <td>E-commerce</td> <td>Days since last purchase, cart abandonment rate</td> </tr> <tr> <td>Finance</td> <td>Moving averages, volatility, ratios</td> </tr> <tr> <td>NLP</td> <td>TF-IDF, n-grams, sentiment scores</td> </tr> <tr> <td>Healthcare</td> <td>BMI, age groups, risk scores</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical data science skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Gives domain-specific examples</li> <li>Knows cyclical encoding for time features</li> <li>Mentions target encoding for high-cardinality categoricals</li> <li>Warns about data leakage: "Always fit on train, transform on test"</li> </ul> </div> </details> <hr> <h3 id=what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question>What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Explainability</code>, <code>Model Interpretation</code>, <code>XAI</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Interpretability Matters:</strong></p> <ul> <li>Regulatory compliance (GDPR, healthcare)</li> <li>Debug and improve models</li> <li>Build trust with stakeholders</li> <li>Detect bias and fairness issues</li> </ul> <p><strong>SHAP (SHapley Additive exPlanations):</strong></p> <p>Based on game theory - measures each feature's contribution to prediction.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>shap</span>

<span class=c1># Train model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Create explainer</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>shap</span><span class=o>.</span><span class=n>TreeExplainer</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
<span class=n>shap_values</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>shap_values</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Summary plot (global importance)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>summary_plot</span><span class=p>(</span><span class=n>shap_values</span><span class=p>,</span> <span class=n>X_test</span><span class=p>)</span>

<span class=c1># Force plot (single prediction)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>force_plot</span><span class=p>(</span><span class=n>explainer</span><span class=o>.</span><span class=n>expected_value</span><span class=p>,</span> 
               <span class=n>shap_values</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

<span class=c1># Dependence plot (feature interaction)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>dependence_plot</span><span class=p>(</span><span class=s2>&quot;age&quot;</span><span class=p>,</span> <span class=n>shap_values</span><span class=p>,</span> <span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong></p> <p>Creates local linear approximations around individual predictions.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>lime</span><span class=w> </span><span class=kn>import</span> <span class=n>lime_tabular</span>

<span class=n>explainer</span> <span class=o>=</span> <span class=n>lime_tabular</span><span class=o>.</span><span class=n>LimeTabularExplainer</span><span class=p>(</span>
    <span class=n>X_train</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>feature_names</span><span class=o>=</span><span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=n>class_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;No&#39;</span><span class=p>,</span> <span class=s1>&#39;Yes&#39;</span><span class=p>],</span>
    <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;classification&#39;</span>
<span class=p>)</span>

<span class=c1># Explain single prediction</span>
<span class=n>exp</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>explain_instance</span><span class=p>(</span>
    <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>,</span>
    <span class=n>num_features</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>exp</span><span class=o>.</span><span class=n>show_in_notebook</span><span class=p>()</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>SHAP</th> <th>LIME</th> </tr> </thead> <tbody> <tr> <td>Approach</td> <td>Game theory (Shapley values)</td> <td>Local linear models</td> </tr> <tr> <td>Consistency</td> <td>Theoretically guaranteed</td> <td>Approximate</td> </tr> <tr> <td>Speed</td> <td>Slower</td> <td>Faster</td> </tr> <tr> <td>Scope</td> <td>Global + local</td> <td>Local (per prediction)</td> </tr> </tbody> </table> <p><strong>Other Methods:</strong></p> <ul> <li><strong>Feature Importance</strong>: Built-in for tree models</li> <li><strong>Partial Dependence Plots</strong>: Show marginal effect</li> <li><strong>Permutation Importance</strong>: Model-agnostic</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of responsible AI.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows difference between global vs local explanations</li> <li>Can explain Shapley values intuitively</li> <li>Mentions use cases: debugging, compliance, bias detection</li> <li>Knows SHAP is theoretically grounded, LIME is approximate</li> </ul> </div> </details> <hr> <h3 id=what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question>What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Optimization</code>, <code>Hyperparameter Tuning</code>, <code>AutoML</code> | <strong>Asked by:</strong> Amazon, Google, Microsoft, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are Hyperparameters?</strong></p> <p>Hyperparameters are external configurations set before training (unlike learned parameters).</p> <p><strong>Tuning Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Approach</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Grid Search</td> <td>Exhaustive search over parameter grid</td> <td>Complete</td> <td>Exponentially slow</td> </tr> <tr> <td>Random Search</td> <td>Random sampling from distributions</td> <td>Faster, finds good values</td> <td>May miss optimal</td> </tr> <tr> <td>Bayesian</td> <td>Probabilistic model of objective</td> <td>Efficient, smart</td> <td>More complex</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GridSearchCV</span><span class=p>,</span> <span class=n>RandomizedSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Grid Search</span>
<span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>],</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=kc>None</span><span class=p>],</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>]</span>
<span class=p>}</span>

<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>param_grid</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>
<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best params: </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Random Search (often better)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.stats</span><span class=w> </span><span class=kn>import</span> <span class=n>randint</span><span class=p>,</span> <span class=n>uniform</span>

<span class=n>param_dist</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>random_search</span> <span class=o>=</span> <span class=n>RandomizedSearchCV</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>param_dist</span><span class=p>,</span>
    <span class=n>n_iter</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>  <span class=c1># Number of random combinations</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>random_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>Bayesian Optimization (Optuna):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>optuna</span>

<span class=k>def</span><span class=w> </span><span class=nf>objective</span><span class=p>(</span><span class=n>trial</span><span class=p>):</span>
    <span class=n>params</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
        <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;max_depth&#39;</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
        <span class=s1>&#39;learning_rate&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_float</span><span class=p>(</span><span class=s1>&#39;learning_rate&#39;</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=n>log</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=p>}</span>

    <span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=o>**</span><span class=n>params</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=k>return</span> <span class=n>score</span>

<span class=n>study</span> <span class=o>=</span> <span class=n>optuna</span><span class=o>.</span><span class=n>create_study</span><span class=p>(</span><span class=n>direction</span><span class=o>=</span><span class=s1>&#39;maximize&#39;</span><span class=p>)</span>
<span class=n>study</span><span class=o>.</span><span class=n>optimize</span><span class=p>(</span><span class=n>objective</span><span class=p>,</span> <span class=n>n_trials</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best params: </span><span class=si>{</span><span class=n>study</span><span class=o>.</span><span class=n>best_params</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Insight:</strong></p> <p>Random Search is often better than Grid Search because it explores more values of important hyperparameters.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML optimization skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows random search often beats grid search</li> <li>Can explain why (more coverage of important params)</li> <li>Mentions Optuna/Hyperopt for Bayesian optimization</li> <li>Uses cross-validation to avoid tuning to test set</li> </ul> </div> </details> <hr> <h3 id=what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question>What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>ML Best Practices</code>, <code>Data Leakage</code>, <code>Validation</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Data Leakage?</strong></p> <p>Data leakage occurs when information from outside the training set is used to create the model, causing overly optimistic validation scores that don't generalize.</p> <p><strong>Types of Leakage:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Example</th> <th>Solution</th> </tr> </thead> <tbody> <tr> <td>Target Leakage</td> <td>Using future data to predict past</td> <td>Respect time ordering</td> </tr> <tr> <td>Train-Test Contamination</td> <td>Scaling using full dataset stats</td> <td>Fit on train only</td> </tr> <tr> <td>Feature Leakage</td> <td>Feature derived from target</td> <td>Domain knowledge review</td> </tr> </tbody> </table> <p><strong>Common Examples:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># ‚ùå WRONG: Preprocessing before split</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Sees all data!</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>

<span class=c1># ‚úÖ CORRECT: Preprocess after split</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>  <span class=c1># Fit on train only</span>
<span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>  <span class=c1># Transform with train params</span>

<span class=c1># ‚úÖ BEST: Use Pipeline</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>

<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;scaler&#39;</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>()),</span>
    <span class=p>(</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>())</span>
<span class=p>])</span>

<span class=c1># Cross-validation respects the pipeline</span>
<span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <p><strong>Time Series Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># ‚ùå WRONG: Random split for time series</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># ‚úÖ CORRECT: Temporal split</span>
<span class=n>train</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&lt;</span> <span class=s1>&#39;2024-01-01&#39;</span><span class=p>]</span>
<span class=n>test</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=s1>&#39;2024-01-01&#39;</span><span class=p>]</span>

<span class=c1># Or use TimeSeriesSplit</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>TimeSeriesSplit</span>
<span class=n>tscv</span> <span class=o>=</span> <span class=n>TimeSeriesSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <p><strong>Subtle Leakage Examples:</strong></p> <ul> <li>Customer ID that correlates with VIP status (target)</li> <li>Hospital department that indicates diagnosis</li> <li>Timestamp of transaction result recorded after outcome</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> ML engineering rigor.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Immediately mentions fit_transform on train only</li> <li>Uses sklearn Pipeline to avoid leakage</li> <li>Knows time series requires temporal splits</li> <li>Reviews features for target proxy patterns</li> </ul> </div> </details> <hr> <h3 id=what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question>What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Experimentation</code>, <code>A/B Testing</code>, <code>Production ML</code> | <strong>Asked by:</strong> Google, Netflix, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why A/B Test ML Models?</strong></p> <p>Offline metrics don't always correlate with business metrics. A/B testing validates that a new model improves real user outcomes.</p> <p><strong>A/B Testing Framework:</strong></p> <table> <thead> <tr> <th>Step</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>1. Hypothesis</td> <td>New model improves metric X by Y%</td> </tr> <tr> <td>2. Randomization</td> <td>Users randomly assigned to control/treatment</td> </tr> <tr> <td>3. Sample Size</td> <td>Calculate required sample for statistical power</td> </tr> <tr> <td>4. Run Experiment</td> <td>Serve both models simultaneously</td> </tr> <tr> <td>5. Analysis</td> <td>Statistical significance test</td> </tr> </tbody> </table> <p><strong>Sample Size Calculation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>scipy</span><span class=w> </span><span class=kn>import</span> <span class=n>stats</span>

<span class=k>def</span><span class=w> </span><span class=nf>calculate_sample_size</span><span class=p>(</span><span class=n>baseline_rate</span><span class=p>,</span> <span class=n>mde</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>power</span><span class=o>=</span><span class=mf>0.8</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    baseline_rate: Current conversion rate</span>
<span class=sd>    mde: Minimum detectable effect (relative change)</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=n>effect_size</span> <span class=o>=</span> <span class=n>baseline_rate</span> <span class=o>*</span> <span class=n>mde</span>
    <span class=n>z_alpha</span> <span class=o>=</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha</span><span class=o>/</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>z_power</span> <span class=o>=</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=n>power</span><span class=p>)</span>

    <span class=n>p</span> <span class=o>=</span> <span class=n>baseline_rate</span>
    <span class=n>p_hat</span> <span class=o>=</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>))</span> <span class=o>/</span> <span class=mi>2</span>

    <span class=n>n</span> <span class=o>=</span> <span class=p>(</span><span class=n>z_alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>p_hat</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p_hat</span><span class=p>))</span><span class=o>**</span><span class=mf>0.5</span> <span class=o>+</span> 
         <span class=n>z_power</span> <span class=o>*</span> <span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>)))</span><span class=o>**</span><span class=mf>0.5</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>/</span> <span class=n>effect_size</span><span class=o>**</span><span class=mi>2</span>

    <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>

<span class=c1># Example: 5% baseline, detect 10% relative improvement</span>
<span class=n>n</span> <span class=o>=</span> <span class=n>calculate_sample_size</span><span class=p>(</span><span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Need </span><span class=si>{</span><span class=n>n</span><span class=si>}</span><span class=s2> samples per group&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Statistical Significance:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>scipy</span><span class=w> </span><span class=kn>import</span> <span class=n>stats</span>

<span class=k>def</span><span class=w> </span><span class=nf>ab_test_significance</span><span class=p>(</span><span class=n>control_conversions</span><span class=p>,</span> <span class=n>control_total</span><span class=p>,</span>
                        <span class=n>treatment_conversions</span><span class=p>,</span> <span class=n>treatment_total</span><span class=p>):</span>
    <span class=n>control_rate</span> <span class=o>=</span> <span class=n>control_conversions</span> <span class=o>/</span> <span class=n>control_total</span>
    <span class=n>treatment_rate</span> <span class=o>=</span> <span class=n>treatment_conversions</span> <span class=o>/</span> <span class=n>treatment_total</span>

    <span class=c1># Two-proportion z-test</span>
    <span class=n>pooled</span> <span class=o>=</span> <span class=p>(</span><span class=n>control_conversions</span> <span class=o>+</span> <span class=n>treatment_conversions</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>control_total</span> <span class=o>+</span> <span class=n>treatment_total</span><span class=p>)</span>
    <span class=n>se</span> <span class=o>=</span> <span class=p>(</span><span class=n>pooled</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pooled</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>control_total</span> <span class=o>+</span> <span class=mi>1</span><span class=o>/</span><span class=n>treatment_total</span><span class=p>))</span> <span class=o>**</span> <span class=mf>0.5</span>
    <span class=n>z</span> <span class=o>=</span> <span class=p>(</span><span class=n>treatment_rate</span> <span class=o>-</span> <span class=n>control_rate</span><span class=p>)</span> <span class=o>/</span> <span class=n>se</span>
    <span class=n>p_value</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>cdf</span><span class=p>(</span><span class=nb>abs</span><span class=p>(</span><span class=n>z</span><span class=p>)))</span>

    <span class=k>return</span> <span class=p>{</span>
        <span class=s1>&#39;control_rate&#39;</span><span class=p>:</span> <span class=n>control_rate</span><span class=p>,</span>
        <span class=s1>&#39;treatment_rate&#39;</span><span class=p>:</span> <span class=n>treatment_rate</span><span class=p>,</span>
        <span class=s1>&#39;lift&#39;</span><span class=p>:</span> <span class=p>(</span><span class=n>treatment_rate</span> <span class=o>-</span> <span class=n>control_rate</span><span class=p>)</span> <span class=o>/</span> <span class=n>control_rate</span><span class=p>,</span>
        <span class=s1>&#39;p_value&#39;</span><span class=p>:</span> <span class=n>p_value</span><span class=p>,</span>
        <span class=s1>&#39;significant&#39;</span><span class=p>:</span> <span class=n>p_value</span> <span class=o>&lt;</span> <span class=mf>0.05</span>
    <span class=p>}</span>
</code></pre></div> <p><strong>ML-Specific Considerations:</strong></p> <ul> <li><strong>Interleaving</strong>: Show both models' results mixed together</li> <li><strong>Multi-armed bandits</strong>: Adaptive allocation to better variants</li> <li><strong>Guardrail metrics</strong>: Ensure no degradation in key metrics</li> <li><strong>Novelty effects</strong>: New models may show initial boost that fades</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of production ML and experimentation.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows offline vs online metrics difference</li> <li>Can calculate sample size for desired power</li> <li>Mentions guardrail metrics and novelty effects</li> <li>Knows when to use bandits vs traditional A/B tests</li> </ul> </div> </details> <hr> <h3 id=explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question>Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Recommendation Systems</code>, <code>Collaborative Filtering</code>, <code>Content-Based</code> | <strong>Asked by:</strong> Netflix, Amazon, Google, Meta, Spotify</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Recommendation Systems:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Approach</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Collaborative Filtering</td> <td>User-item interactions</td> <td>Discovers unexpected</td> <td>Cold start problem</td> </tr> <tr> <td>Content-Based</td> <td>Item features</td> <td>No cold start for items</td> <td>Limited novelty</td> </tr> <tr> <td>Hybrid</td> <td>Combines both</td> <td>Best of both</td> <td>More complex</td> </tr> </tbody> </table> <p><strong>Collaborative Filtering:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># User-based: Find similar users</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics.pairwise</span><span class=w> </span><span class=kn>import</span> <span class=n>cosine_similarity</span>

<span class=n>user_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=p>)</span>

<span class=c1># Item-based: Find similar items</span>
<span class=n>item_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>

<span class=c1># Matrix Factorization (SVD)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.sparse.linalg</span><span class=w> </span><span class=kn>import</span> <span class=n>svds</span>

<span class=n>U</span><span class=p>,</span> <span class=n>sigma</span><span class=p>,</span> <span class=n>Vt</span> <span class=o>=</span> <span class=n>svds</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
<span class=n>predicted_ratings</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>U</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>sigma</span><span class=p>)),</span> <span class=n>Vt</span><span class=p>)</span>
</code></pre></div> <p><strong>Deep Learning Approach:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>NeuralCollaborativeFiltering</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_users</span><span class=p>,</span> <span class=n>num_items</span><span class=p>,</span> <span class=n>embed_dim</span><span class=o>=</span><span class=mi>32</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>user_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_users</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>item_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_items</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_ids</span><span class=p>,</span> <span class=n>item_ids</span><span class=p>):</span>
        <span class=n>user_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>user_embed</span><span class=p>(</span><span class=n>user_ids</span><span class=p>)</span>
        <span class=n>item_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>item_embed</span><span class=p>(</span><span class=n>item_ids</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>user_emb</span><span class=p>,</span> <span class=n>item_emb</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
</code></pre></div> <p><strong>Content-Based:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_extraction.text</span><span class=w> </span><span class=kn>import</span> <span class=n>TfidfVectorizer</span>

<span class=c1># Create item profiles from descriptions</span>
<span class=n>tfidf</span> <span class=o>=</span> <span class=n>TfidfVectorizer</span><span class=p>(</span><span class=n>stop_words</span><span class=o>=</span><span class=s1>&#39;english&#39;</span><span class=p>)</span>
<span class=n>item_features</span> <span class=o>=</span> <span class=n>tfidf</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>item_descriptions</span><span class=p>)</span>

<span class=c1># Create user profile from liked items</span>
<span class=n>user_profile</span> <span class=o>=</span> <span class=n>item_features</span><span class=p>[</span><span class=n>liked_items</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Recommend similar items</span>
<span class=n>similarities</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_profile</span><span class=p>,</span> <span class=n>item_features</span><span class=p>)</span>
</code></pre></div> <p><strong>Evaluation Metrics:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Measures</th> </tr> </thead> <tbody> <tr> <td>Precision@K</td> <td>Relevant items in top K</td> </tr> <tr> <td>Recall@K</td> <td>Coverage of relevant items</td> </tr> <tr> <td>NDCG</td> <td>Ranking quality</td> </tr> <tr> <td>MAP</td> <td>Mean average precision</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of personalization systems.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains cold start problem and solutions</li> <li>Knows matrix factorization vs deep learning trade-offs</li> <li>Mentions implicit vs explicit feedback</li> <li>Discusses evaluation: "We use NDCG because ranking matters"</li> </ul> </div> </details> <hr> <h3 id=what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question>What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Class Imbalance</code>, <code>Classification</code>, <code>Sampling</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Imbalanced Data?</strong></p> <p>When one class significantly outnumbers others (e.g., 99% negative, 1% positive). Common in fraud detection, medical diagnosis, anomaly detection.</p> <p><strong>Why It's a Problem:</strong></p> <ul> <li>Model learns to predict majority class</li> <li>Accuracy is misleading (99% accuracy by predicting all negative)</li> <li>Minority class patterns not learned</li> </ul> <p><strong>Strategies:</strong></p> <table> <thead> <tr> <th>Level</th> <th>Technique</th> </tr> </thead> <tbody> <tr> <td>Data</td> <td>Oversampling, undersampling, SMOTE</td> </tr> <tr> <td>Algorithm</td> <td>Class weights, anomaly detection</td> </tr> <tr> <td>Evaluation</td> <td>Use F1, PR-AUC, not accuracy</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>imblearn.over_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTE</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.under_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomUnderSampler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span> <span class=k>as</span> <span class=n>ImbPipeline</span>

<span class=c1># SMOTE oversampling</span>
<span class=n>smote</span> <span class=o>=</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smote</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Combination: SMOTE + undersampling</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>ImbPipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;over&#39;</span><span class=p>,</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;under&#39;</span><span class=p>,</span> <span class=n>RandomUnderSampler</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>())</span>
<span class=p>])</span>

<span class=c1># Class weights (no resampling needed)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.utils.class_weight</span><span class=w> </span><span class=kn>import</span> <span class=n>compute_class_weight</span>

<span class=n>weights</span> <span class=o>=</span> <span class=n>compute_class_weight</span><span class=p>(</span><span class=s1>&#39;balanced&#39;</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y</span><span class=p>),</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>class_weight</span><span class=o>=</span><span class=s1>&#39;balanced&#39;</span><span class=p>)</span>

<span class=c1># Or in XGBoost</span>
<span class=n>scale_pos_weight</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>[</span><span class=n>y_train</span><span class=o>==</span><span class=mi>0</span><span class=p>])</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>[</span><span class=n>y_train</span><span class=o>==</span><span class=mi>1</span><span class=p>])</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>scale_pos_weight</span><span class=o>=</span><span class=n>scale_pos_weight</span><span class=p>)</span>
</code></pre></div> <p><strong>Threshold Tuning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>precision_recall_curve</span>

<span class=c1># Get probabilities</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Find optimal threshold for F1</span>
<span class=n>precision</span><span class=p>,</span> <span class=n>recall</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
<span class=n>f1_scores</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>precision</span> <span class=o>*</span> <span class=n>recall</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>precision</span> <span class=o>+</span> <span class=n>recall</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>)</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>f1_scores</span><span class=p>)]</span>

<span class=c1># Use custom threshold</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_proba</span> <span class=o>&gt;=</span> <span class=n>optimal_threshold</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</code></pre></div> <p><strong>Evaluation for Imbalanced:</strong></p> <table> <thead> <tr> <th>Use</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Precision-Recall AUC</td> <td>Accuracy</td> </tr> <tr> <td>F1-Score</td> <td>ROC-AUC (can be misleading)</td> </tr> <tr> <td>Confusion Matrix</td> <td>Single metric alone</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical classification handling.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Never uses accuracy as primary metric</li> <li>Knows SMOTE and when to use it</li> <li>Suggests class weights as simpler alternative</li> <li>Mentions threshold tuning on PR curve</li> </ul> </div> </details> <hr> <h3 id=how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question>How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>MLOps</code>, <code>Deployment</code>, <code>Production ML</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Deployment Approaches:</strong></p> <table> <thead> <tr> <th>Approach</th> <th>Use Case</th> <th>Latency</th> </tr> </thead> <tbody> <tr> <td>Batch</td> <td>Periodic predictions, reports</td> <td>High (okay)</td> </tr> <tr> <td>Real-time API</td> <td>Interactive applications</td> <td>Low (critical)</td> </tr> <tr> <td>Edge</td> <td>Mobile, IoT, offline</td> <td>Very low</td> </tr> <tr> <td>Streaming</td> <td>Continuous data processing</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>Real-time API with FastAPI:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>fastapi</span><span class=w> </span><span class=kn>import</span> <span class=n>FastAPI</span>
<span class=kn>import</span><span class=w> </span><span class=nn>joblib</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>()</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;model.joblib&#39;</span><span class=p>)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;scaler.joblib&#39;</span><span class=p>)</span>

<span class=nd>@app</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=s2>&quot;/predict&quot;</span><span class=p>)</span>
<span class=k>async</span> <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=n>features</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]):</span>
    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>features</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>prediction</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
    <span class=n>probability</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

    <span class=k>return</span> <span class=p>{</span>
        <span class=s2>&quot;prediction&quot;</span><span class=p>:</span> <span class=nb>int</span><span class=p>(</span><span class=n>prediction</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span>
        <span class=s2>&quot;probability&quot;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>probability</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>())</span>
    <span class=p>}</span>
</code></pre></div> <p><strong>Docker Containerization:</strong></p> <div class=highlight><pre><span></span><code><span class=k>FROM</span><span class=w> </span><span class=s>python:3.10-slim</span>

<span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span>
<span class=k>COPY</span><span class=w> </span>requirements.txt<span class=w> </span>.
<span class=k>RUN</span><span class=w> </span>pip<span class=w> </span>install<span class=w> </span>-r<span class=w> </span>requirements.txt

<span class=k>COPY</span><span class=w> </span>model.joblib<span class=w> </span>.
<span class=k>COPY</span><span class=w> </span>app.py<span class=w> </span>.

<span class=k>CMD</span><span class=w> </span><span class=p>[</span><span class=s2>&quot;uvicorn&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;app:app&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;--host&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;0.0.0.0&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;--port&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;8000&quot;</span><span class=p>]</span>
</code></pre></div> <p><strong>MLOps Considerations:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Tools</th> </tr> </thead> <tbody> <tr> <td>Model Registry</td> <td>MLflow, Weights &amp; Biases</td> </tr> <tr> <td>Serving</td> <td>TensorFlow Serving, Triton</td> </tr> <tr> <td>Monitoring</td> <td>Prometheus, Grafana</td> </tr> <tr> <td>Feature Store</td> <td>Feast, Tecton</td> </tr> <tr> <td>Pipeline</td> <td>Airflow, Kubeflow</td> </tr> </tbody> </table> <p><strong>Monitoring:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Track prediction drift</span>
<span class=kn>from</span><span class=w> </span><span class=nn>evidently</span><span class=w> </span><span class=kn>import</span> <span class=n>Report</span>
<span class=kn>from</span><span class=w> </span><span class=nn>evidently.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>DataDriftPreset</span>

<span class=n>report</span> <span class=o>=</span> <span class=n>Report</span><span class=p>(</span><span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=n>DataDriftPreset</span><span class=p>()])</span>
<span class=n>report</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>reference_data</span><span class=o>=</span><span class=n>train_df</span><span class=p>,</span> <span class=n>current_data</span><span class=o>=</span><span class=n>production_df</span><span class=p>)</span>
<span class=n>report</span><span class=o>.</span><span class=n>save_html</span><span class=p>(</span><span class=s2>&quot;drift_report.html&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Model Versioning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>mlflow</span>

<span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>():</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>(</span><span class=n>metrics</span><span class=p>)</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&quot;model&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Production ML engineering skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows batch vs real-time trade-offs</li> <li>Mentions containerization (Docker)</li> <li>Discusses monitoring for drift</li> <li>Knows model versioning and rollback strategies</li> </ul> </div> </details> <hr> <h3 id=what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question>What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Regression</code>, <code>Statistics</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Linear Regression?</strong></p> <p>Linear regression models the relationship between a dependent variable and one or more independent variables using a linear function.</p> <p><strong>The Formula:</strong></p> <div class=arithmatex>\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Simple linear regression</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Coefficients: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Intercept: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>intercept_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;R¬≤ Score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span><span class=w> </span><span class=n>y_test</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Assumptions:</strong></p> <table> <thead> <tr> <th>Assumption</th> <th>Check Method</th> </tr> </thead> <tbody> <tr> <td>Linearity</td> <td>Residual vs fitted plot</td> </tr> <tr> <td>Independence</td> <td>Durbin-Watson test</td> </tr> <tr> <td>Homoscedasticity</td> <td>Residual spread plot</td> </tr> <tr> <td>Normality</td> <td>Q-Q plot of residuals</td> </tr> <tr> <td>No multicollinearity</td> <td>VIF (Variance Inflation Factor)</td> </tr> </tbody> </table> <p><strong>Diagnostics:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.stats.outliers_influence</span><span class=w> </span><span class=kn>import</span> <span class=n>variance_inflation_factor</span>

<span class=c1># Check multicollinearity</span>
<span class=n>vif</span> <span class=o>=</span> <span class=p>[</span><span class=n>variance_inflation_factor</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>values</span><span class=p>,</span> <span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])]</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;VIF:&quot;</span><span class=p>,</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span> <span class=n>vif</span><span class=p>)))</span>  <span class=c1># VIF &gt; 5 = problem</span>

<span class=c1># Residual analysis</span>
<span class=n>residuals</span> <span class=o>=</span> <span class=n>y_test</span> <span class=o>-</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Statistical foundation knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Lists assumptions without prompting</li> <li>Knows how to check each assumption</li> <li>Mentions VIF for multicollinearity</li> <li>Knows OLS minimizes squared residuals</li> </ul> </div> </details> <hr> <h3 id=what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question>What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Classification</code>, <code>Probability</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Logistic Regression?</strong></p> <p>Logistic regression is a linear model for binary classification that outputs probabilities using the sigmoid function.</p> <p><strong>The Sigmoid Function:</strong></p> <div class=arithmatex>\[P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}}\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;l2&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>solver</span><span class=o>=</span><span class=s1>&#39;lbfgs&#39;</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Probabilities</span>
<span class=n>probabilities</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Coefficients (log-odds)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Coefficients:&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>

<span class=c1># Odds ratio interpretation</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=n>odds_ratios</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Odds Ratios:&quot;</span><span class=p>,</span> <span class=n>odds_ratios</span><span class=p>)</span>
</code></pre></div> <p><strong>Interpretation:</strong></p> <table> <thead> <tr> <th>Coefficient</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>Positive</td> <td>Increases probability of class 1</td> </tr> <tr> <td>Negative</td> <td>Decreases probability of class 1</td> </tr> <tr> <td>Odds Ratio &gt; 1</td> <td>Feature increases odds</td> </tr> <tr> <td>Odds Ratio &lt; 1</td> <td>Feature decreases odds</td> </tr> </tbody> </table> <p><strong>When to Use:</strong></p> <table> <thead> <tr> <th>Use Logistic Regression</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Binary classification</td> <td>Complex non-linear relationships</td> </tr> <tr> <td>Need interpretability</td> <td>Multi-class (use softmax)</td> </tr> <tr> <td>Baseline model</td> <td>Very high dimensional</td> </tr> <tr> <td>Feature importance needed</td> <td></td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of probabilistic classification.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows it's called "regression" but used for classification</li> <li>Can interpret coefficients as log-odds</li> <li>Mentions maximum likelihood estimation</li> <li>Knows regularization prevents overfitting</li> </ul> </div> </details> <hr> <h3 id=what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question>What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification</code>, <code>Probability</code>, <code>Text Classification</code> | <strong>Asked by:</strong> Amazon, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Naive Bayes?</strong></p> <p>Naive Bayes is a probabilistic classifier based on Bayes' theorem with the "naive" assumption of feature independence.</p> <p><strong>Bayes' Theorem:</strong></p> <div class=arithmatex>\[P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}\]</div> <p><strong>The Naive Assumption:</strong></p> <p>Features are conditionally independent given the class: <span class=arithmatex>\(<span class=arithmatex>\(P(x_1, x_2, ..., x_n|C) = P(x_1|C) \cdot P(x_2|C) \cdot ... \cdot P(x_n|C)\)</span>\)</span></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.naive_bayes</span><span class=w> </span><span class=kn>import</span> <span class=n>GaussianNB</span><span class=p>,</span> <span class=n>MultinomialNB</span><span class=p>,</span> <span class=n>BernoulliNB</span>

<span class=c1># For continuous features</span>
<span class=n>gnb</span> <span class=o>=</span> <span class=n>GaussianNB</span><span class=p>()</span>

<span class=c1># For text/count data (most common)</span>
<span class=n>mnb</span> <span class=o>=</span> <span class=n>MultinomialNB</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>  <span class=c1># alpha = Laplace smoothing</span>

<span class=c1># For binary features</span>
<span class=n>bnb</span> <span class=o>=</span> <span class=n>BernoulliNB</span><span class=p>()</span>

<span class=c1># Text classification example</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_extraction.text</span><span class=w> </span><span class=kn>import</span> <span class=n>CountVectorizer</span>

<span class=n>vectorizer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>()</span>
<span class=n>X_train_counts</span> <span class=o>=</span> <span class=n>vectorizer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>train_texts</span><span class=p>)</span>

<span class=n>mnb</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_counts</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>mnb</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>vectorizer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_texts</span><span class=p>))</span>
</code></pre></div> <p><strong>Types:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Use Case</th> <th>Feature Type</th> </tr> </thead> <tbody> <tr> <td>Gaussian</td> <td>Continuous data</td> <td>Real numbers</td> </tr> <tr> <td>Multinomial</td> <td>Text, word counts</td> <td>Counts</td> </tr> <tr> <td>Bernoulli</td> <td>Binary features</td> <td>0/1</td> </tr> </tbody> </table> <p><strong>Why It Works Despite Being "Naive":</strong></p> <ul> <li>Classification only needs relative probabilities</li> <li>Works well with high-dimensional data</li> <li>Very fast training and prediction</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of probabilistic reasoning.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains the independence assumption and why it's unrealistic</li> <li>Knows it performs well for text classification</li> <li>Mentions Laplace smoothing for zero probabilities</li> <li>Compares to logistic regression: "Similar performance, faster"</li> </ul> </div> </details> <hr> <h3 id=what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question>What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Feature Engineering</code>, <code>Model Optimization</code>, <code>Dimensionality</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Feature Selection?</strong></p> <ul> <li>Reduce overfitting</li> <li>Improve accuracy</li> <li>Reduce training time</li> <li>Improve interpretability</li> </ul> <p><strong>Three Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Speed</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Filter</td> <td>Statistical tests, independent of model</td> <td>Fast</td> <td>Lower</td> </tr> <tr> <td>Wrapper</td> <td>Evaluates subsets with model</td> <td>Slow</td> <td>Higher</td> </tr> <tr> <td>Embedded</td> <td>Selection during training</td> <td>Medium</td> <td>High</td> </tr> </tbody> </table> <p><strong>Filter Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>SelectKBest</span><span class=p>,</span> <span class=n>f_classif</span><span class=p>,</span> <span class=n>mutual_info_classif</span>

<span class=c1># ANOVA F-test (for classification)</span>
<span class=n>selector</span> <span class=o>=</span> <span class=n>SelectKBest</span><span class=p>(</span><span class=n>f_classif</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>X_selected</span> <span class=o>=</span> <span class=n>selector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># Correlation-based</span>
<span class=n>correlation_matrix</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>corr</span><span class=p>()</span>
<span class=n>high_corr_features</span> <span class=o>=</span> <span class=n>correlation_matrix</span><span class=p>[</span><span class=nb>abs</span><span class=p>(</span><span class=n>correlation_matrix</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.8</span><span class=p>]</span>

<span class=c1># Variance threshold</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>VarianceThreshold</span>
<span class=n>selector</span> <span class=o>=</span> <span class=n>VarianceThreshold</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Wrapper Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>RFE</span><span class=p>,</span> <span class=n>RFECV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Recursive Feature Elimination</span>
<span class=n>rfe</span> <span class=o>=</span> <span class=n>RFE</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>rfe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>selected_features</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>rfe</span><span class=o>.</span><span class=n>support_</span><span class=p>]</span>

<span class=c1># With cross-validation</span>
<span class=n>rfecv</span> <span class=o>=</span> <span class=n>RFECV</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>rfecv</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <p><strong>Embedded Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># L1 regularization (Lasso)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LassoCV</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>LassoCV</span><span class=p>(</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>selected</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>lasso</span><span class=o>.</span><span class=n>coef_</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>]</span>

<span class=c1># Tree-based feature importance</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>importances</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>(</span><span class=n>rf</span><span class=o>.</span><span class=n>feature_importances_</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span>
<span class=n>top_features</span> <span class=o>=</span> <span class=n>importances</span><span class=o>.</span><span class=n>nlargest</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>index</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML pipeline knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows trade-offs between methods</li> <li>Uses filter for large datasets, wrapper for smaller</li> <li>Mentions L1/Lasso as embedded selection</li> <li>Warns about target leakage in feature selection</li> </ul> </div> </details> <hr> <h3 id=what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question>What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>Model Combination</code>, <code>Advanced</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Ensemble Learning?</strong></p> <p>Combining multiple models to produce better predictions than any single model.</p> <p><strong>Three Main Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Reduces</th> </tr> </thead> <tbody> <tr> <td>Bagging</td> <td>Parallel models on bootstrap samples</td> <td>Variance</td> </tr> <tr> <td>Boosting</td> <td>Sequential models fixing errors</td> <td>Bias</td> </tr> <tr> <td>Stacking</td> <td>Meta-model on base predictions</td> <td>Both</td> </tr> </tbody> </table> <p><strong>Bagging (Bootstrap Aggregating):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>BaggingClassifier</span><span class=p>,</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Random Forest is bagging + feature randomization</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_features</span><span class=o>=</span><span class=s1>&#39;sqrt&#39;</span><span class=p>)</span>

<span class=c1># Generic bagging</span>
<span class=n>bagging</span> <span class=o>=</span> <span class=n>BaggingClassifier</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>DecisionTreeClassifier</span><span class=p>(),</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
    <span class=n>max_samples</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>bootstrap</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Boosting:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>GradientBoostingClassifier</span><span class=p>,</span> <span class=n>AdaBoostClassifier</span>
<span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>
<span class=kn>import</span><span class=w> </span><span class=nn>lightgbm</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>lgb</span>

<span class=c1># Gradient Boosting</span>
<span class=n>gb</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># XGBoost</span>
<span class=n>xgb_model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># LightGBM (faster)</span>
<span class=n>lgb_model</span> <span class=o>=</span> <span class=n>lgb</span><span class=o>.</span><span class=n>LGBMClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</code></pre></div> <p><strong>Stacking:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>StackingClassifier</span>

<span class=n>estimators</span> <span class=o>=</span> <span class=p>[</span>
    <span class=p>(</span><span class=s1>&#39;rf&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;xgb&#39;</span><span class=p>,</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;lgb&#39;</span><span class=p>,</span> <span class=n>lgb</span><span class=o>.</span><span class=n>LGBMClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>))</span>
<span class=p>]</span>

<span class=n>stacking</span> <span class=o>=</span> <span class=n>StackingClassifier</span><span class=p>(</span>
    <span class=n>estimators</span><span class=o>=</span><span class=n>estimators</span><span class=p>,</span>
    <span class=n>final_estimator</span><span class=o>=</span><span class=n>LogisticRegression</span><span class=p>(),</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Bagging</th> <th>Boosting</th> </tr> </thead> <tbody> <tr> <td>Training</td> <td>Parallel</td> <td>Sequential</td> </tr> <tr> <td>Goal</td> <td>Reduce variance</td> <td>Reduce bias</td> </tr> <tr> <td>Prone to overfitting</td> <td>Less</td> <td>More</td> </tr> <tr> <td>Example</td> <td>Random Forest</td> <td>XGBoost</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced ML knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains variance vs bias reduction</li> <li>Knows Random Forest = bagging + random features</li> <li>Mentions early stopping for boosting overfitting</li> <li>Can describe when to use each method</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-missing-data-amazon-google-meta-interview-question>How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Missing Data</code>, <code>Imputation</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Missing Data:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> <th>Handling</th> </tr> </thead> <tbody> <tr> <td>MCAR</td> <td>Missing Completely at Random</td> <td>Any method</td> </tr> <tr> <td>MAR</td> <td>Missing at Random (depends on observed)</td> <td>Model-based imputation</td> </tr> <tr> <td>MNAR</td> <td>Missing Not at Random</td> <td>Domain knowledge needed</td> </tr> </tbody> </table> <p><strong>Basic Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>SimpleImputer</span>

<span class=c1># Check missing</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>isnull</span><span class=p>()</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>

<span class=c1># Drop rows with missing</span>
<span class=n>df_clean</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span>

<span class=c1># Drop columns with &gt; 50% missing</span>
<span class=n>df_clean</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>thresh</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>df</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.5</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Simple imputation</span>
<span class=n>imputer</span> <span class=o>=</span> <span class=n>SimpleImputer</span><span class=p>(</span><span class=n>strategy</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>  <span class=c1># or median, most_frequent</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Advanced Imputation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>KNNImputer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.experimental</span><span class=w> </span><span class=kn>import</span> <span class=n>enable_iterative_imputer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>IterativeImputer</span>

<span class=c1># KNN Imputation</span>
<span class=n>knn_imputer</span> <span class=o>=</span> <span class=n>KNNImputer</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>knn_imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># MICE (Multiple Imputation by Chained Equations)</span>
<span class=n>mice_imputer</span> <span class=o>=</span> <span class=n>IterativeImputer</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>mice_imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Indicator Variables:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Add missing indicator</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>SimpleImputer</span><span class=p>,</span> <span class=n>MissingIndicator</span>

<span class=n>indicator</span> <span class=o>=</span> <span class=n>MissingIndicator</span><span class=p>()</span>
<span class=n>missing_flags</span> <span class=o>=</span> <span class=n>indicator</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Combine imputed data with indicators</span>
<span class=n>X_with_indicators</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>X_imputed</span><span class=p>,</span> <span class=n>missing_flags</span><span class=p>])</span>
</code></pre></div> <p><strong>Best Practices:</strong></p> <table> <thead> <tr> <th>Missing %</th> <th>Recommendation</th> </tr> </thead> <tbody> <tr> <td>&lt; 5%</td> <td>Simple imputation</td> </tr> <tr> <td>5-20%</td> <td>Advanced imputation (KNN, MICE)</td> </tr> <tr> <td>&gt; 20%</td> <td>Consider dropping or domain knowledge</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Data quality handling skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Asks about missing mechanism (MCAR, MAR, MNAR)</li> <li>Knows adding missing indicators can help</li> <li>Uses IterativeImputer/MICE for complex cases</li> <li>Warns: "Always impute after train/test split"</li> </ul> </div> </details> <hr> <h3 id=what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question>What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Time Series</code>, <code>Forecasting</code>, <code>ARIMA</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Time Series Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Trend</td> <td>Long-term increase/decrease</td> </tr> <tr> <td>Seasonality</td> <td>Regular periodic patterns</td> </tr> <tr> <td>Cyclical</td> <td>Non-fixed period fluctuations</td> </tr> <tr> <td>Noise</td> <td>Random variation</td> </tr> </tbody> </table> <p><strong>ARIMA (AutoRegressive Integrated Moving Average):</strong></p> <ul> <li><strong>AR(p)</strong>: AutoRegressive - uses past values</li> <li><strong>I(d)</strong>: Integrated - differencing for stationarity</li> <li><strong>MA(q)</strong>: Moving Average - uses past errors</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.tsa.arima.model</span><span class=w> </span><span class=kn>import</span> <span class=n>ARIMA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.tsa.stattools</span><span class=w> </span><span class=kn>import</span> <span class=n>adfuller</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>

<span class=c1># Check stationarity (ADF test)</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>adfuller</span><span class=p>(</span><span class=n>series</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;ADF Statistic: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>, p-value: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Fit ARIMA</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>ARIMA</span><span class=p>(</span><span class=n>series</span><span class=p>,</span> <span class=n>order</span><span class=o>=</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>  <span class=c1># (AR, differencing, MA)</span>
<span class=n>fitted</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>

<span class=c1># Forecast</span>
<span class=n>forecast</span> <span class=o>=</span> <span class=n>fitted</span><span class=o>.</span><span class=n>forecast</span><span class=p>(</span><span class=n>steps</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>

<span class=c1># Auto ARIMA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pmdarima</span><span class=w> </span><span class=kn>import</span> <span class=n>auto_arima</span>
<span class=n>auto_model</span> <span class=o>=</span> <span class=n>auto_arima</span><span class=p>(</span><span class=n>series</span><span class=p>,</span> <span class=n>seasonal</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>  <span class=c1># m=12 for monthly</span>
</code></pre></div> <p><strong>Choosing Parameters (p, d, q):</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>How to Choose</th> </tr> </thead> <tbody> <tr> <td>d</td> <td>Number of differences for stationarity</td> </tr> <tr> <td>p</td> <td>ACF cuts off, PACF decays</td> </tr> <tr> <td>q</td> <td>PACF cuts off, ACF decays</td> </tr> </tbody> </table> <p><strong>Modern Alternatives:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Prophet (Facebook)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>prophet</span><span class=w> </span><span class=kn>import</span> <span class=n>Prophet</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>Prophet</span><span class=p>(</span><span class=n>yearly_seasonality</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>  <span class=c1># df with &#39;ds&#39; and &#39;y&#39; columns</span>

<span class=c1># Deep Learning</span>
<span class=c1># LSTM, Transformer models for complex patterns</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Time series understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Checks stationarity first (ADF test)</li> <li>Knows ACF/PACF for parameter selection</li> <li>Mentions Prophet for quick results</li> <li>Uses walk-forward validation, not random split</li> </ul> </div> </details> <hr> <h3 id=what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question>What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Boosting</code>, <code>XGBoost</code>, <code>Ensemble</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>How Gradient Boosting Works:</strong></p> <ol> <li>Fit initial model (e.g., mean)</li> <li>Calculate residuals (errors)</li> <li>Fit new tree to predict residuals</li> <li>Add new tree's predictions (with learning rate)</li> <li>Repeat</li> </ol> <p><strong>XGBoost Innovations:</strong></p> <table> <thead> <tr> <th>Feature</th> <th>Benefit</th> </tr> </thead> <tbody> <tr> <td>Regularization</td> <td>L1/L2 on leaf weights</td> </tr> <tr> <td>Sparsity awareness</td> <td>Efficient missing value handling</td> </tr> <tr> <td>Weighted quantile sketch</td> <td>Approximate tree learning</td> </tr> <tr> <td>Cache-aware access</td> <td>10x faster</td> </tr> <tr> <td>Block structure</td> <td>Parallelization</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>

<span class=c1># Basic model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>reg_alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>  <span class=c1># L1</span>
    <span class=n>reg_lambda</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>  <span class=c1># L2</span>
    <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=c1># Training with early stopping</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
    <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=n>xgb</span><span class=o>.</span><span class=n>plot_importance</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</code></pre></div> <p><strong>LightGBM vs XGBoost:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>XGBoost</th> <th>LightGBM</th> </tr> </thead> <tbody> <tr> <td>Tree growth</td> <td>Level-wise</td> <td>Leaf-wise</td> </tr> <tr> <td>Speed</td> <td>Fast</td> <td>Faster</td> </tr> <tr> <td>Memory</td> <td>Higher</td> <td>Lower</td> </tr> <tr> <td>Categorical</td> <td>Needs encoding</td> <td>Native support</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical tree ensemble knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains sequential fitting to residuals</li> <li>Knows key hyperparameters (learning_rate, max_depth)</li> <li>Uses early stopping to prevent overfitting</li> <li>Compares XGBoost vs LightGBM trade-offs</li> </ul> </div> </details> <hr> <h3 id=how-do-you-evaluate-regression-models-amazon-google-interview-question>How Do You Evaluate Regression Models? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Evaluation</code>, <code>Regression</code>, <code>Metrics</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Common Regression Metrics:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Formula</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>MAE</td> <td>$\frac{1}{n}\sum</td> <td>y_i - \hat{y}_i</td> </tr> <tr> <td>MSE</td> <td><span class=arithmatex>\(\frac{1}{n}\sum(y_i - \hat{y}_i)^2\)</span></td> <td>Penalizes large errors</td> </tr> <tr> <td>RMSE</td> <td><span class=arithmatex>\(\sqrt{MSE}\)</span></td> <td>Same scale as target</td> </tr> <tr> <td>R¬≤</td> <td><span class=arithmatex>\(1 - \frac{SS_{res}}{SS_{tot}}\)</span></td> <td>Variance explained</td> </tr> <tr> <td>MAPE</td> <td>$\frac{100}{n}\sum</td> <td>\frac{y_i - \hat{y}_i}{y_i}</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_absolute_error</span><span class=p>,</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=n>mae</span> <span class=o>=</span> <span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=n>mse</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mse</span><span class=p>)</span>
<span class=n>r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=c1># MAPE (handle zeros)</span>
<span class=n>mape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>((</span><span class=n>y_test</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>y_test</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MAE: </span><span class=si>{</span><span class=n>mae</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;RMSE: </span><span class=si>{</span><span class=n>rmse</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;R¬≤: </span><span class=si>{</span><span class=n>r2</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Choosing the Right Metric:</strong></p> <table> <thead> <tr> <th>Use Case</th> <th>Best Metric</th> </tr> </thead> <tbody> <tr> <td>Same units as target</td> <td>MAE, RMSE</td> </tr> <tr> <td>Penalize large errors</td> <td>RMSE, MSE</td> </tr> <tr> <td>Compare across scales</td> <td>MAPE, R¬≤</td> </tr> <tr> <td>Outlier-resistant</td> <td>MAE</td> </tr> </tbody> </table> <p><strong>Adjusted R¬≤:</strong></p> <div class=arithmatex>\[R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}\]</div> <p>Penalizes adding features that don't improve fit.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Evaluation metric knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows RMSE vs MAE trade-offs</li> <li>Uses adjusted R¬≤ when comparing models</li> <li>Mentions residual plots for diagnostics</li> <li>Warns about MAPE with values near zero</li> </ul> </div> </details> <hr> <h3 id=what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question>What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Dimensionality Reduction</code>, <code>Visualization</code>, <code>PCA</code> | <strong>Asked by:</strong> Google, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Reduce Dimensions?</strong></p> <ul> <li>Combat curse of dimensionality</li> <li>Reduce noise</li> <li>Enable visualization (2D/3D)</li> <li>Speed up training</li> </ul> <p><strong>PCA (Principal Component Analysis):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=c1># Standardize first!</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Fit PCA</span>
<span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mf>0.95</span><span class=p>)</span>  <span class=c1># Keep 95% variance</span>
<span class=n>X_pca</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Components: </span><span class=si>{</span><span class=n>pca</span><span class=o>.</span><span class=n>n_components_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Explained variance: </span><span class=si>{</span><span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize variance explained</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Components&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Cumulative Variance&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>t-SNE (t-Distributed Stochastic Neighbor Embedding):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.manifold</span><span class=w> </span><span class=kn>import</span> <span class=n>TSNE</span>

<span class=c1># Usually for visualization only (2-3D)</span>
<span class=n>tsne</span> <span class=o>=</span> <span class=n>TSNE</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>perplexity</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_tsne</span> <span class=o>=</span> <span class=n>tsne</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_tsne</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_tsne</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>PCA</th> <th>t-SNE</th> </tr> </thead> <tbody> <tr> <td>Type</td> <td>Linear</td> <td>Non-linear</td> </tr> <tr> <td>Goal</td> <td>Maximize variance</td> <td>Preserve local structure</td> </tr> <tr> <td>Speed</td> <td>Fast</td> <td>Slow</td> </tr> <tr> <td>Deterministic</td> <td>Yes</td> <td>No</td> </tr> <tr> <td>Inverse transform</td> <td>Yes</td> <td>No</td> </tr> <tr> <td>Use case</td> <td>Feature reduction</td> <td>Visualization</td> </tr> </tbody> </table> <p><strong>UMAP (Modern Alternative):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>umap</span>
<span class=n>reducer</span> <span class=o>=</span> <span class=n>umap</span><span class=o>.</span><span class=n>UMAP</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>X_umap</span> <span class=o>=</span> <span class=n>reducer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=c1># Faster than t-SNE, preserves global structure better</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of data representation.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Standardizes data before PCA</li> <li>Knows PCA for features, t-SNE for visualization</li> <li>Mentions perplexity tuning for t-SNE</li> <li>Suggests UMAP as modern alternative</li> </ul> </div> </details> <hr> <h3 id=what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question>What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Optimization</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Optimizers:</strong></p> <table> <thead> <tr> <th>Optimizer</th> <th>Description</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>SGD</td> <td>Basic gradient descent</td> <td>Large-scale, convex</td> </tr> <tr> <td>Momentum</td> <td>SGD with velocity</td> <td>Faster convergence</td> </tr> <tr> <td>RMSprop</td> <td>Adaptive learning rates</td> <td>Non-stationary</td> </tr> <tr> <td>Adam</td> <td>Momentum + RMSprop</td> <td>Default choice</td> </tr> <tr> <td>AdamW</td> <td>Adam + weight decay</td> <td>Transformers</td> </tr> </tbody> </table> <p><strong>Adam Optimizer:</strong></p> <div class=arithmatex>\[m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$ $$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$ $$\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=c1># Adam with default parameters</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>))</span>

<span class=c1># AdamW for transformers</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Learning Rate Schedules:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim.lr_scheduler</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>StepLR</span><span class=p>,</span> <span class=n>ExponentialLR</span><span class=p>,</span> <span class=n>CosineAnnealingLR</span><span class=p>,</span> <span class=n>OneCycleLR</span>
<span class=p>)</span>

<span class=c1># Step decay</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Cosine annealing (popular)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>CosineAnnealingLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=c1># One cycle (fast training)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>OneCycleLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>max_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>))</span>

<span class=c1># Training loop</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>Learning Rate Finding:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Start low, increase exponentially, find where loss decreases fastest</span>
<span class=c1># Use lr_finder from pytorch-lightning or fastai</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep learning training expertise.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Uses Adam as default, knows when to use SGD</li> <li>Implements learning rate scheduling</li> <li>Knows warmup for transformers</li> <li>Can explain momentum and adaptive learning rates</li> </ul> </div> </details> <hr> <h3 id=what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question>What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Overfitting</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Regularization?</strong></p> <p>Prevents overfitting by constraining model complexity.</p> <p><strong>Types of Regularization:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Effect</th> </tr> </thead> <tbody> <tr> <td>L1 (Lasso)</td> <td>Penalize sum of absolute weights</td> <td>Sparse weights (feature selection)</td> </tr> <tr> <td>L2 (Ridge)</td> <td>Penalize sum of squared weights</td> <td>Small weights (prevents extreme values)</td> </tr> <tr> <td>Dropout</td> <td>Randomly zero neurons during training</td> <td>Ensemble effect</td> </tr> <tr> <td>Early Stopping</td> <td>Stop when validation loss increases</td> <td>Limits training time</td> </tr> </tbody> </table> <p><strong>L1 vs L2:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span>

<span class=c1># L2 regularization</span>
<span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># L1 regularization (sparse coefficients)</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Combination (Elastic Net)</span>
<span class=n>elastic</span> <span class=o>=</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>

<span class=c1># In neural networks</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>  <span class=c1># L2</span>
</code></pre></div> <p><strong>Dropout:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>Network</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 50% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Only during training</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>Early Stopping:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># XGBoost</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> 
          <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
          <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># PyTorch (manual)</span>
<span class=n>best_loss</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
<span class=n>patience</span> <span class=o>=</span> <span class=mi>10</span>
<span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=n>val_loss</span> <span class=o>=</span> <span class=n>validate</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>val_loss</span> <span class=o>&lt;</span> <span class=n>best_loss</span><span class=p>:</span>
        <span class=n>best_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
        <span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>save_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>counter</span> <span class=o>+=</span> <span class=mi>1</span>
        <span class=k>if</span> <span class=n>counter</span> <span class=o>&gt;=</span> <span class=n>patience</span><span class=p>:</span>
            <span class=k>break</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of overfitting prevention.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows L1 leads to sparsity (feature selection)</li> <li>Uses dropout only during training</li> <li>Implements early stopping with patience</li> <li>Combines multiple regularization techniques</li> </ul> </div> </details> <hr> <h3 id=what-is-the-curse-of-dimensionality-google-amazon-interview-question>What is the Curse of Dimensionality? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>High Dimensions</code>, <code>Feature Engineering</code>, <code>Theory</code> | <strong>Asked by:</strong> Google, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is the Curse of Dimensionality?</strong></p> <p>As dimensions increase, data becomes increasingly sparse, making distance-based methods and density estimation unreliable.</p> <p><strong>Problems:</strong></p> <table> <thead> <tr> <th>Problem</th> <th>Implication</th> </tr> </thead> <tbody> <tr> <td>Data sparsity</td> <td>Need exponentially more data</td> </tr> <tr> <td>Distance concentration</td> <td>All points equidistant</td> </tr> <tr> <td>Computational cost</td> <td>Memory and time explode</td> </tr> <tr> <td>Overfitting</td> <td>More features = more noise</td> </tr> </tbody> </table> <p><strong>Distance Concentration:</strong></p> <p>As dimensions ‚Üí ‚àû, the ratio of nearest to farthest neighbor approaches 1:</p> <div class=arithmatex>\[\lim_{d \to \infty} \frac{dist_{max} - dist_{min}}{dist_{min}} = 0\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics.pairwise</span><span class=w> </span><span class=kn>import</span> <span class=n>euclidean_distances</span>

<span class=c1># Demonstrate distance concentration</span>
<span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>1000</span><span class=p>]:</span>
    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>d</span><span class=p>)</span>
    <span class=n>distances</span> <span class=o>=</span> <span class=n>euclidean_distances</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>ratio</span> <span class=o>=</span> <span class=p>(</span><span class=n>distances</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>-</span> <span class=n>distances</span><span class=o>.</span><span class=n>min</span><span class=p>())</span> <span class=o>/</span> <span class=n>distances</span><span class=o>.</span><span class=n>min</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Dimensions: </span><span class=si>{</span><span class=n>d</span><span class=si>}</span><span class=s2>, Max-Min Ratio: </span><span class=si>{</span><span class=n>ratio</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Solutions:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Dimensionality reduction</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=n>X_reduced</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># 2. Feature selection</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>SelectKBest</span>
<span class=n>X_selected</span> <span class=o>=</span> <span class=n>SelectKBest</span><span class=p>(</span><span class=n>k</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># 3. Use regularization</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LassoCV</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>LassoCV</span><span class=p>(</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># 4. Use tree-based models (less affected)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
</code></pre></div> <p><strong>Rule of Thumb:</strong></p> <p>Need at least <span class=arithmatex>\(5^d\)</span> samples for <span class=arithmatex>\(d\)</span> dimensions to maintain data density.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of high-dimensional data.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains why KNN fails in high dimensions</li> <li>Knows distance metrics become meaningless</li> <li>Suggests dimensionality reduction or regularization</li> <li>Mentions: "Tree models are less affected"</li> </ul> </div> </details> <hr> <h3 id=what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question>What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Loss Functions</code>, <code>Classification</code>, <code>Deep Learning</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Cross-Entropy Loss?</strong></p> <p>Measures the distance between predicted probability distribution and true distribution.</p> <p><strong>Binary Cross-Entropy:</strong></p> <div class=arithmatex>\[L = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\]</div> <p><strong>Categorical Cross-Entropy:</strong></p> <div class=arithmatex>\[L = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=c1># Binary classification</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCELoss</span><span class=p>()</span>  <span class=c1># With sigmoid output</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCEWithLogitsLoss</span><span class=p>()</span>  <span class=c1># Raw logits (preferred)</span>

<span class=c1># Multi-class classification</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>  <span class=c1># Raw logits (includes softmax)</span>

<span class=c1># Example</span>
<span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Shape: (batch_size, num_classes)</span>
<span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>  <span class=c1># targets: (batch_size,) - class indices</span>
</code></pre></div> <p><strong>Why Cross-Entropy?</strong></p> <table> <thead> <tr> <th>Loss</th> <th>Gradient</th> <th>Use</th> </tr> </thead> <tbody> <tr> <td>MSE</td> <td>Small when wrong</td> <td>Regression</td> </tr> <tr> <td>Cross-Entropy</td> <td>Large when wrong</td> <td>Classification</td> </tr> </tbody> </table> <p><strong>Label Smoothing:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Prevents overconfident predictions</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>label_smoothing</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Soft targets: Instead of [0, 1, 0]</span>
<span class=c1># Use: [0.05, 0.9, 0.05]</span>
</code></pre></div> <p><strong>Focal Loss (Imbalanced Data):</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>FocalLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>2.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
        <span class=n>ce_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
        <span class=n>pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>ce_loss</span><span class=p>)</span>
        <span class=n>focal_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pt</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>ce_loss</span>
        <span class=k>return</span> <span class=n>focal_loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Loss function understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows cross-entropy for probabilities, MSE for values</li> <li>Uses BCEWithLogitsLoss for numerical stability</li> <li>Mentions label smoothing for regularization</li> <li>Knows focal loss for imbalanced data</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-categorical-features-amazon-google-interview-question>How Do You Handle Categorical Features? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Feature Engineering</code>, <code>Encoding</code>, <code>Preprocessing</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Encoding Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Use Case</th> <th>Cardinality</th> </tr> </thead> <tbody> <tr> <td>One-Hot</td> <td>Tree models, low cardinality</td> <td>&lt; 10-15</td> </tr> <tr> <td>Label Encoding</td> <td>Tree models</td> <td>Any</td> </tr> <tr> <td>Target Encoding</td> <td>High cardinality</td> <td>&gt; 15</td> </tr> <tr> <td>Frequency Encoding</td> <td>When frequency matters</td> <td>Any</td> </tr> <tr> <td>Embeddings</td> <td>Deep learning</td> <td>Very high</td> </tr> </tbody> </table> <p><strong>One-Hot Encoding:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>OneHotEncoder</span>

<span class=c1># Pandas</span>
<span class=n>df_encoded</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>])</span>

<span class=c1># Scikit-learn</span>
<span class=n>encoder</span> <span class=o>=</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>handle_unknown</span><span class=o>=</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
<span class=n>encoded</span> <span class=o>=</span> <span class=n>encoder</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>df</span><span class=p>[[</span><span class=s1>&#39;category&#39;</span><span class=p>]])</span>
</code></pre></div> <p><strong>Target Encoding:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>category_encoders</span><span class=w> </span><span class=kn>import</span> <span class=n>TargetEncoder</span>

<span class=n>encoder</span> <span class=o>=</span> <span class=n>TargetEncoder</span><span class=p>(</span><span class=n>smoothing</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>encoder</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>],</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>])</span>

<span class=c1># Manual with smoothing</span>
<span class=n>global_mean</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=n>smoothing</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>agg</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>agg</span><span class=p>([</span><span class=s1>&#39;mean&#39;</span><span class=p>,</span> <span class=s1>&#39;count&#39;</span><span class=p>])</span>
<span class=n>smoothed</span> <span class=o>=</span> <span class=p>(</span><span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;mean&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>smoothing</span> <span class=o>*</span> <span class=n>global_mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>smoothing</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>smoothed</span><span class=p>)</span>
</code></pre></div> <p><strong>Embedding (Deep Learning):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>ModelWithEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_categories</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_categories</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim</span> <span class=o>+</span> <span class=n>n_numeric_features</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cat_features</span><span class=p>,</span> <span class=n>num_features</span><span class=p>):</span>
        <span class=n>cat_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>cat_features</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>cat_embedded</span><span class=p>,</span> <span class=n>num_features</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</code></pre></div> <p><strong>Best Practices:</strong></p> <table> <thead> <tr> <th>Model Type</th> <th>Recommendation</th> </tr> </thead> <tbody> <tr> <td>Linear</td> <td>One-hot or target encoding</td> </tr> <tr> <td>Tree-based</td> <td>Label or target encoding</td> </tr> <tr> <td>Neural Net</td> <td>Embeddings</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Feature engineering skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Chooses encoding based on cardinality</li> <li>Knows target encoding needs CV to avoid leakage</li> <li>Uses embeddings for high cardinality in DL</li> <li>Mentions CatBoost handles categoricals natively</li> </ul> </div> </details> <hr> <h3 id=what-is-model-calibration-google-meta-interview-question>What is Model Calibration? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Probability</code>, <code>Calibration</code>, <code>Evaluation</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Calibration?</strong></p> <p>A model is well-calibrated if predicted probabilities match observed frequencies. If model says 70% probability, event should occur 70% of the time.</p> <p><strong>Why It Matters:</strong></p> <ul> <li>Probability thresholding</li> <li>Risk assessment</li> <li>Decision making</li> <li>Ensemble weighting</li> </ul> <p><strong>Checking Calibration:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>calibration_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Get probabilities</span>
<span class=n>y_prob</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Calibration curve</span>
<span class=n>fraction_of_positives</span><span class=p>,</span> <span class=n>mean_predicted_value</span> <span class=o>=</span> <span class=n>calibration_curve</span><span class=p>(</span>
    <span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>,</span> <span class=n>n_bins</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>mean_predicted_value</span><span class=p>,</span> <span class=n>fraction_of_positives</span><span class=p>,</span> <span class=s1>&#39;s-&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;--&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Mean Predicted Probability&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Fraction of Positives&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Calibration Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>CalibratedClassifierCV</span>

<span class=c1># Platt scaling (logistic regression on probabilities)</span>
<span class=n>calibrated</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>

<span class=c1># Isotonic regression (non-parametric)</span>
<span class=n>calibrated</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;isotonic&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>

<span class=n>calibrated</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>calibrated_probs</span> <span class=o>=</span> <span class=n>calibrated</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
</code></pre></div> <p><strong>Brier Score:</strong></p> <div class=arithmatex>\[BS = \frac{1}{N}\sum_{i=1}^{N}(p_i - y_i)^2\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>brier_score_loss</span>

<span class=n>brier</span> <span class=o>=</span> <span class=n>brier_score_loss</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Brier Score: </span><span class=si>{</span><span class=n>brier</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>  <span class=c1># Lower is better</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced probability understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows neural networks are often overconfident</li> <li>Uses calibration curve for diagnosis</li> <li>Chooses Platt (low data) vs isotonic (more data)</li> <li>Mentions Brier score for evaluation</li> </ul> </div> </details> <hr> <h3 id=what-is-online-learning-amazon-google-interview-question>What is Online Learning? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Online Learning</code>, <code>Streaming</code>, <code>Production</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Online Learning?</strong></p> <p>Updating model incrementally as new data arrives, instead of retraining on entire dataset.</p> <p><strong>Use Cases:</strong></p> <table> <thead> <tr> <th>Use Case</th> <th>Why Online</th> </tr> </thead> <tbody> <tr> <td>Streaming data</td> <td>Too much to store</td> </tr> <tr> <td>Concept drift</td> <td>Data distribution changes</td> </tr> <tr> <td>Real-time adaptation</td> <td>Need immediate updates</td> </tr> <tr> <td>Resource constraints</td> <td>Can't retrain frequently</td> </tr> </tbody> </table> <p><strong>Scikit-learn partial_fit:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>SGDClassifier</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log_loss&#39;</span><span class=p>)</span>  <span class=c1># Logistic regression</span>

<span class=c1># Initial training</span>
<span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch1</span><span class=p>,</span> <span class=n>y_batch1</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>

<span class=c1># Incremental updates</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</code></pre></div> <p><strong>Algorithms that Support Online Learning:</strong></p> <table> <thead> <tr> <th>Algorithm</th> <th>Scikit-learn Class</th> </tr> </thead> <tbody> <tr> <td>SGD Classifier</td> <td>SGDClassifier</td> </tr> <tr> <td>SGD Regressor</td> <td>SGDRegressor</td> </tr> <tr> <td>Naive Bayes</td> <td>MultinomialNB</td> </tr> <tr> <td>Perceptron</td> <td>Perceptron</td> </tr> <tr> <td>Mini-batch K-Means</td> <td>MiniBatchKMeans</td> </tr> </tbody> </table> <p><strong>River Library (Dedicated Online ML):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>river</span><span class=w> </span><span class=kn>import</span> <span class=n>linear_model</span><span class=p>,</span> <span class=n>preprocessing</span>

<span class=n>model</span> <span class=o>=</span> <span class=p>(</span>
    <span class=n>preprocessing</span><span class=o>.</span><span class=n>StandardScaler</span><span class=p>()</span> <span class=o>|</span> 
    <span class=n>linear_model</span><span class=o>.</span><span class=n>LogisticRegression</span><span class=p>()</span>
<span class=p>)</span>

<span class=k>for</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_one</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn_one</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <p><strong>Handling Concept Drift:</strong></p> <ul> <li><strong>Window-based</strong>: Train on recent N samples</li> <li><strong>Decay</strong>: Weight recent samples more</li> <li><strong>Drift detection</strong>: Monitor performance, reset when needed</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Streaming/production ML knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to use online vs batch</li> <li>Mentions concept drift</li> <li>Uses partial_fit in scikit-learn</li> <li>Knows decay/windowing strategies</li> </ul> </div> </details> <hr> <h3 id=what-is-semi-supervised-learning-google-meta-interview-question>What is Semi-Supervised Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Semi-Supervised</code>, <code>Label Propagation</code>, <code>Learning Paradigms</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Semi-Supervised Learning?</strong></p> <p>Uses both labeled and unlabeled data for training. Useful when labeling is expensive but data is abundant.</p> <p><strong>Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Self-training</td> <td>Train, predict unlabeled, add confident predictions</td> </tr> <tr> <td>Co-training</td> <td>Two models teach each other</td> </tr> <tr> <td>Label propagation</td> <td>Spread labels through similarity graph</td> </tr> <tr> <td>Pseudo-labeling</td> <td>Use model predictions as labels</td> </tr> </tbody> </table> <p><strong>Self-Training:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.semi_supervised</span><span class=w> </span><span class=kn>import</span> <span class=n>SelfTrainingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># -1 indicates unlabeled</span>
<span class=n>y_train_partial</span> <span class=o>=</span> <span class=n>y_train</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<span class=n>y_train_partial</span><span class=p>[</span><span class=n>unlabeled_mask</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>SelfTrainingClassifier</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>threshold</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>  <span class=c1># Confidence threshold</span>
    <span class=n>max_iter</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train_partial</span><span class=p>)</span>
</code></pre></div> <p><strong>Label Propagation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.semi_supervised</span><span class=w> </span><span class=kn>import</span> <span class=n>LabelPropagation</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LabelPropagation</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;knn&#39;</span><span class=p>,</span> <span class=n>n_neighbors</span><span class=o>=</span><span class=mi>7</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train_partial</span><span class=p>)</span>  <span class=c1># -1 for unlabeled</span>

<span class=c1># Get transduced labels</span>
<span class=n>transduced_labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transduction_</span>
</code></pre></div> <p><strong>Pseudo-Labeling (Deep Learning):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Train on labeled data</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_labeled</span><span class=p>,</span> <span class=n>y_labeled</span><span class=p>)</span>

<span class=c1># 2. Predict unlabeled with confidence</span>
<span class=n>probs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>
<span class=n>confident_mask</span> <span class=o>=</span> <span class=n>probs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.95</span>
<span class=n>pseudo_labels</span> <span class=o>=</span> <span class=n>probs</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[</span><span class=n>confident_mask</span><span class=p>]</span>

<span class=c1># 3. Add to training set</span>
<span class=n>X_combined</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X_labeled</span><span class=p>,</span> <span class=n>X_unlabeled</span><span class=p>[</span><span class=n>confident_mask</span><span class=p>]])</span>
<span class=n>y_combined</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>y_labeled</span><span class=p>,</span> <span class=n>pseudo_labels</span><span class=p>])</span>

<span class=c1># 4. Retrain</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_combined</span><span class=p>,</span> <span class=n>y_combined</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Knowledge of learning paradigms.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains when it's useful (expensive labeling)</li> <li>Knows confidence thresholding to avoid noise</li> <li>Mentions transductive vs inductive</li> <li>Compares to active learning</li> </ul> </div> </details> <hr> <h3 id=what-is-active-learning-google-meta-interview-question>What is Active Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Active Learning</code>, <code>Labeling</code>, <code>Human-in-the-Loop</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Active Learning?</strong></p> <p>Model actively selects which samples to label, reducing labeling cost while maximizing performance.</p> <p><strong>Query Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>How It Works</th> </tr> </thead> <tbody> <tr> <td>Uncertainty Sampling</td> <td>Select least confident predictions</td> </tr> <tr> <td>Query by Committee</td> <td>Select where models disagree most</td> </tr> <tr> <td>Expected Model Change</td> <td>Select that would change model most</td> </tr> <tr> <td>Diversity Sampling</td> <td>Select diverse samples</td> </tr> </tbody> </table> <p><strong>Uncertainty Sampling:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>modAL.uncertainty</span><span class=w> </span><span class=kn>import</span> <span class=n>uncertainty_sampling</span>
<span class=kn>from</span><span class=w> </span><span class=nn>modAL.models</span><span class=w> </span><span class=kn>import</span> <span class=n>ActiveLearner</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Initialize with few labeled samples</span>
<span class=n>learner</span> <span class=o>=</span> <span class=n>ActiveLearner</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>query_strategy</span><span class=o>=</span><span class=n>uncertainty_sampling</span><span class=p>,</span>
    <span class=n>X_training</span><span class=o>=</span><span class=n>X_initial</span><span class=p>,</span>
    <span class=n>y_training</span><span class=o>=</span><span class=n>y_initial</span>
<span class=p>)</span>

<span class=c1># Active learning loop</span>
<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_queries</span><span class=p>):</span>
    <span class=n>query_idx</span><span class=p>,</span> <span class=n>query_instance</span> <span class=o>=</span> <span class=n>learner</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>

    <span class=c1># Get label from oracle (human)</span>
    <span class=n>label</span> <span class=o>=</span> <span class=n>get_label_from_human</span><span class=p>(</span><span class=n>query_instance</span><span class=p>)</span>

    <span class=n>learner</span><span class=o>.</span><span class=n>teach</span><span class=p>(</span><span class=n>query_instance</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>

    <span class=c1># Remove from unlabeled pool</span>
    <span class=n>X_unlabeled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>delete</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>,</span> <span class=n>query_idx</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <p><strong>Manual Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Uncertainty-based selection</span>
<span class=n>probs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>

<span class=c1># Least confident</span>
<span class=n>uncertainty</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>probs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Margin (difference between top 2)</span>
<span class=n>sorted_probs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>margin</span> <span class=o>=</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>2</span><span class=p>]</span>

<span class=c1># Entropy</span>
<span class=n>entropy</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>probs</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>probs</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Select top uncertain samples</span>
<span class=n>query_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>uncertainty</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Efficient labeling strategies.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows different query strategies</li> <li>Mentions exploration vs exploitation trade-off</li> <li>Uses margin or entropy for uncertainty</li> <li>Knows batch mode for efficiency</li> </ul> </div> </details> <hr> <h3 id=what-is-automl-amazon-google-interview-question>What is AutoML? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>AutoML</code>, <code>Automation</code>, <code>Model Selection</code> | <strong>Asked by:</strong> Amazon, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is AutoML?</strong></p> <p>Automated Machine Learning - automating model selection, hyperparameter tuning, and feature engineering.</p> <p><strong>AutoML Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>What It Automates</th> </tr> </thead> <tbody> <tr> <td>Data preprocessing</td> <td>Imputation, encoding, scaling</td> </tr> <tr> <td>Feature engineering</td> <td>Transformations, interactions</td> </tr> <tr> <td>Model selection</td> <td>Algorithm choice</td> </tr> <tr> <td>Hyperparameter tuning</td> <td>Parameter optimization</td> </tr> <tr> <td>Ensembling</td> <td>Combining models</td> </tr> </tbody> </table> <p><strong>Popular AutoML Tools:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Auto-sklearn</span>
<span class=kn>from</span><span class=w> </span><span class=nn>autosklearn.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoSklearnClassifier</span>

<span class=n>automl</span> <span class=o>=</span> <span class=n>AutoSklearnClassifier</span><span class=p>(</span><span class=n>time_left_for_this_task</span><span class=o>=</span><span class=mi>3600</span><span class=p>)</span>
<span class=n>automl</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># H2O AutoML</span>
<span class=kn>import</span><span class=w> </span><span class=nn>h2o</span>
<span class=kn>from</span><span class=w> </span><span class=nn>h2o.automl</span><span class=w> </span><span class=kn>import</span> <span class=n>H2OAutoML</span>

<span class=n>h2o</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>
<span class=n>aml</span> <span class=o>=</span> <span class=n>H2OAutoML</span><span class=p>(</span><span class=n>max_runtime_secs</span><span class=o>=</span><span class=mi>3600</span><span class=p>)</span>
<span class=n>aml</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=n>features</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>target</span><span class=p>,</span> <span class=n>training_frame</span><span class=o>=</span><span class=n>train</span><span class=p>)</span>

<span class=c1># TPOT</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tpot</span><span class=w> </span><span class=kn>import</span> <span class=n>TPOTClassifier</span>

<span class=n>tpot</span> <span class=o>=</span> <span class=n>TPOTClassifier</span><span class=p>(</span><span class=n>generations</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>population_size</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>verbosity</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>tpot</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>tpot</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=s1>&#39;best_pipeline.py&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Google Cloud AutoML:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Vertex AI AutoML</span>
<span class=kn>from</span><span class=w> </span><span class=nn>google.cloud</span><span class=w> </span><span class=kn>import</span> <span class=n>aiplatform</span>

<span class=n>dataset</span> <span class=o>=</span> <span class=n>aiplatform</span><span class=o>.</span><span class=n>TabularDataset</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
    <span class=n>display_name</span><span class=o>=</span><span class=s2>&quot;my_dataset&quot;</span><span class=p>,</span>
    <span class=n>gcs_source</span><span class=o>=</span><span class=s2>&quot;gs://bucket/data.csv&quot;</span>
<span class=p>)</span>

<span class=n>job</span> <span class=o>=</span> <span class=n>aiplatform</span><span class=o>.</span><span class=n>AutoMLTabularTrainingJob</span><span class=p>(</span>
    <span class=n>display_name</span><span class=o>=</span><span class=s2>&quot;my_model&quot;</span><span class=p>,</span>
    <span class=n>optimization_prediction_type</span><span class=o>=</span><span class=s2>&quot;classification&quot;</span>
<span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>job</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span> <span class=n>target_column</span><span class=o>=</span><span class=s2>&quot;label&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>When to Use AutoML:</strong></p> <table> <thead> <tr> <th>Use AutoML</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Need interpretability</td> </tr> <tr> <td>Limited ML expertise</td> <td>Complex domain constraints</td> </tr> <tr> <td>Standard ML problems</td> <td>Need custom architectures</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Awareness of automation tools.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows popular tools (auto-sklearn, H2O, TPOT)</li> <li>Uses AutoML for baselines, then improves</li> <li>Mentions computational cost</li> <li>Knows when manual modeling is better</li> </ul> </div> </details> <hr> <h3 id=what-is-early-stopping-and-how-does-it-work-google-meta-interview-question>What is Early Stopping and How Does It Work? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Training</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Early Stopping:</strong></p> <p>Stop training when validation performance stops improving, preventing overfitting while maintaining optimal generalization.</p> <p><strong>Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Split data</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_temp</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_temp</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>X_val</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_val</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_temp</span><span class=p>,</span> <span class=n>y_temp</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>

<span class=c1># PyTorch example</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>EarlyStopping</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>min_delta</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>patience</span> <span class=o>=</span> <span class=n>patience</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>min_delta</span> <span class=o>=</span> <span class=n>min_delta</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>best_loss</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>early_stop</span> <span class=o>=</span> <span class=kc>False</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>val_loss</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>best_loss</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>best_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
        <span class=k>elif</span> <span class=n>val_loss</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>best_loss</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_delta</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>+=</span> <span class=mi>1</span>
            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patience</span><span class=p>:</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>early_stop</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>best_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>

<span class=c1># Training loop</span>
<span class=n>early_stopping</span> <span class=o>=</span> <span class=n>EarlyStopping</span><span class=p>(</span><span class=n>patience</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
    <span class=c1># Train</span>
    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
    <span class=n>train_loss</span> <span class=o>=</span> <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>)</span>

    <span class=c1># Validate</span>
    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
    <span class=n>val_loss</span> <span class=o>=</span> <span class=n>validate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>val_loader</span><span class=p>)</span>

    <span class=n>early_stopping</span><span class=p>(</span><span class=n>val_loss</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>early_stopping</span><span class=o>.</span><span class=n>early_stop</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Early stopping at epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
        <span class=k>break</span>
</code></pre></div> <p><strong>Keras Built-in:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>EarlyStopping</span>

<span class=c1># Define callback</span>
<span class=n>early_stop</span> <span class=o>=</span> <span class=n>EarlyStopping</span><span class=p>(</span>
    <span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_loss&#39;</span><span class=p>,</span>
    <span class=n>patience</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>restore_best_weights</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>min_delta</span><span class=o>=</span><span class=mf>0.001</span>
<span class=p>)</span>

<span class=c1># Train with early stopping</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
    <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>),</span>
    <span class=n>epochs</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
    <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>early_stop</span><span class=p>],</span>
    <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span>
<span class=p>)</span>

<span class=c1># Visualize</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_loss&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]),</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;r&#39;</span><span class=p>,</span> 
            <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Early Stop&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Early Stopping Visualization&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_accuracy&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Accuracy Over Time&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Key Parameters:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Meaning</th> <th>Typical Value</th> </tr> </thead> <tbody> <tr> <td>patience</td> <td>Epochs to wait before stopping</td> <td>5-20</td> </tr> <tr> <td>min_delta</td> <td>Minimum improvement threshold</td> <td>0.001</td> </tr> <tr> <td>restore_best_weights</td> <td>Restore best model</td> <td>True</td> </tr> <tr> <td>monitor</td> <td>Metric to track</td> <td>val_loss</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical training techniques.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Stop when validation stops improving"</li> <li>Mentions patience parameter</li> <li>"Prevents overfitting naturally"</li> <li>restore_best_weights is important</li> <li>Use with validation set, not test</li> </ul> </div> </details> <hr> <h3 id=explain-learning-rate-scheduling-google-amazon-interview-question>Explain Learning Rate Scheduling - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Optimization</code>, <code>Training</code>, <code>Deep Learning</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Learning Rate Scheduling:</strong></p> <p>Adjust learning rate during training to improve convergence and final performance.</p> <p><strong>Common Schedules:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=n>epochs</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>initial_lr</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=c1># 1. Step Decay</span>
<span class=k>def</span><span class=w> </span><span class=nf>step_decay</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>initial_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>drop</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>epochs_drop</span><span class=o>=</span><span class=mi>20</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>drop</span> <span class=o>**</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>//</span> <span class=n>epochs_drop</span><span class=p>))</span>

<span class=c1># 2. Exponential Decay</span>
<span class=k>def</span><span class=w> </span><span class=nf>exponential_decay</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>initial_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>decay_rate</span><span class=o>=</span><span class=mf>0.96</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>decay_rate</span> <span class=o>**</span> <span class=n>epoch</span><span class=p>)</span>

<span class=c1># 3. Cosine Annealing</span>
<span class=k>def</span><span class=w> </span><span class=nf>cosine_annealing</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>initial_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>epoch</span> <span class=o>/</span> <span class=n>T_max</span><span class=p>))</span>

<span class=c1># 4. Linear Decay</span>
<span class=k>def</span><span class=w> </span><span class=nf>linear_decay</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>initial_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>total_epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>epoch</span> <span class=o>/</span> <span class=n>total_epochs</span><span class=p>)</span>

<span class=c1># 5. Warmup + Decay</span>
<span class=k>def</span><span class=w> </span><span class=nf>warmup_cosine</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>initial_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>warmup_epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>total_epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=n>warmup_epochs</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>/</span> <span class=n>warmup_epochs</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>progress</span> <span class=o>=</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>-</span> <span class=n>warmup_epochs</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>total_epochs</span> <span class=o>-</span> <span class=n>warmup_epochs</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>progress</span><span class=p>))</span>

<span class=c1># Visualize schedules</span>
<span class=n>epochs_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>epochs</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>

<span class=n>schedules</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Step Decay&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>step_decay</span><span class=p>(</span><span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>epochs_range</span><span class=p>],</span>
    <span class=s1>&#39;Exponential Decay&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>exponential_decay</span><span class=p>(</span><span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>epochs_range</span><span class=p>],</span>
    <span class=s1>&#39;Cosine Annealing&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>cosine_annealing</span><span class=p>(</span><span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>epochs_range</span><span class=p>],</span>
    <span class=s1>&#39;Linear Decay&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>linear_decay</span><span class=p>(</span><span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>epochs_range</span><span class=p>],</span>
    <span class=s1>&#39;Warmup + Cosine&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>warmup_cosine</span><span class=p>(</span><span class=n>e</span><span class=p>)</span> <span class=k>for</span> <span class=n>e</span> <span class=ow>in</span> <span class=n>epochs_range</span><span class=p>]</span>
<span class=p>}</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>lr_values</span> <span class=ow>in</span> <span class=n>schedules</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>epochs_range</span><span class=p>,</span> <span class=n>lr_values</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>name</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Learning Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Learning Rate Schedules&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>yscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>PyTorch Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torch.optim.lr_scheduler</span><span class=w> </span><span class=kn>import</span> <span class=o>*</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>YourModel</span><span class=p>()</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># 1. StepLR</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># 2. ExponentialLR</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>ExponentialLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.95</span><span class=p>)</span>

<span class=c1># 3. CosineAnnealingLR</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>CosineAnnealingLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>eta_min</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># 4. ReduceLROnPlateau (adaptive)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;min&#39;</span><span class=p>,</span> 
                              <span class=n>factor</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># 5. OneCycleLR (super-convergence)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>OneCycleLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>max_lr</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> 
                       <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>),</span> 
                       <span class=n>epochs</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=c1># Training loop</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>compute_loss</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>batch</span><span class=p>)</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=c1># For OneCycleLR, step per batch</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>OneCycleLR</span><span class=p>):</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

    <span class=c1># For others, step per epoch</span>
    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>OneCycleLR</span><span class=p>):</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>ReduceLROnPlateau</span><span class=p>):</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>val_loss</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>, LR: </span><span class=si>{</span><span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;lr&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Keras Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras.callbacks</span><span class=w> </span><span class=kn>import</span> <span class=n>LearningRateScheduler</span><span class=p>,</span> <span class=n>ReduceLROnPlateau</span>

<span class=c1># Custom schedule</span>
<span class=k>def</span><span class=w> </span><span class=nf>lr_schedule</span><span class=p>(</span><span class=n>epoch</span><span class=p>):</span>
    <span class=n>initial_lr</span> <span class=o>=</span> <span class=mf>0.1</span>
    <span class=k>if</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=mi>30</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>initial_lr</span>
    <span class=k>elif</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=mi>60</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=mf>0.1</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=k>return</span> <span class=n>initial_lr</span> <span class=o>*</span> <span class=mf>0.01</span>

<span class=n>scheduler</span> <span class=o>=</span> <span class=n>LearningRateScheduler</span><span class=p>(</span><span class=n>lr_schedule</span><span class=p>)</span>

<span class=c1># Or adaptive</span>
<span class=n>reduce_lr</span> <span class=o>=</span> <span class=n>ReduceLROnPlateau</span><span class=p>(</span>
    <span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_loss&#39;</span><span class=p>,</span>
    <span class=n>factor</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span>
    <span class=n>patience</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>min_lr</span><span class=o>=</span><span class=mf>1e-6</span>
<span class=p>)</span>

<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
          <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>),</span>
          <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>scheduler</span><span class=p>])</span>
</code></pre></div> <p><strong>When to Use Each:</strong></p> <table> <thead> <tr> <th>Schedule</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>Step Decay</td> <td>Simple, predictable decay</td> </tr> <tr> <td>Cosine</td> <td>Smooth decay, good final performance</td> </tr> <tr> <td>ReduceLROnPlateau</td> <td>Adaptive, no manual tuning</td> </tr> <tr> <td>OneCycleLR</td> <td>Fast training, super-convergence</td> </tr> <tr> <td>Warmup</td> <td>Large models, stabilize early training</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced training strategies.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Lists multiple schedules</li> <li>"Start high, decay slowly"</li> <li>Mentions warmup for transformers</li> <li>Cosine annealing popular in research</li> <li>ReduceLROnPlateau for adaptive</li> </ul> </div> </details> <hr> <h3 id=what-is-data-leakage-and-how-do-you-prevent-it-most-tech-companies-interview-question>What is Data Leakage and How Do You Prevent It? - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Model Evaluation</code>, <code>Best Practices</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Data Leakage:</strong></p> <p>Information from outside the training dataset is used to create the model, leading to overly optimistic performance.</p> <p><strong>Types of Leakage:</strong></p> <p><strong>1. Train-Test Contamination:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># WRONG: Fit scaler on all data</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Uses test data!</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=c1># CORRECT: Fit only on training data</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>  <span class=c1># Only transform</span>
</code></pre></div> <p><strong>2. Target Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Example: Predicting loan default</span>
<span class=c1># WRONG: Using features known only after outcome</span>
<span class=n>features</span> <span class=o>=</span> <span class=p>[</span>
    <span class=s1>&#39;credit_score&#39;</span><span class=p>,</span>
    <span class=s1>&#39;income&#39;</span><span class=p>,</span>
    <span class=s1>&#39;paid_back&#39;</span><span class=p>,</span>  <span class=c1># ‚ùå This is the target!</span>
    <span class=s1>&#39;num_missed_payments&#39;</span>  <span class=c1># ‚ùå Known only after default</span>
<span class=p>]</span>

<span class=c1># CORRECT: Only use features available at prediction time</span>
<span class=n>features</span> <span class=o>=</span> <span class=p>[</span>
    <span class=s1>&#39;credit_score&#39;</span><span class=p>,</span>
    <span class=s1>&#39;income&#39;</span><span class=p>,</span>
    <span class=s1>&#39;employment_length&#39;</span><span class=p>,</span>
    <span class=s1>&#39;previous_defaults&#39;</span>
<span class=p>]</span>
</code></pre></div> <p><strong>3. Temporal Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Time series: use future to predict past</span>

<span class=c1># WRONG: Random split</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>time_series_data</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># CORRECT: Temporal split</span>
<span class=n>split_date</span> <span class=o>=</span> <span class=s1>&#39;2023-01-01&#39;</span>
<span class=n>train_data</span> <span class=o>=</span> <span class=n>time_series_data</span><span class=p>[</span><span class=n>time_series_data</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>split_date</span><span class=p>]</span>
<span class=n>test_data</span> <span class=o>=</span> <span class=n>time_series_data</span><span class=p>[</span><span class=n>time_series_data</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=n>split_date</span><span class=p>]</span>

<span class=c1># For time series CV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>TimeSeriesSplit</span>

<span class=n>tscv</span> <span class=o>=</span> <span class=n>TimeSeriesSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=k>for</span> <span class=n>train_idx</span><span class=p>,</span> <span class=n>test_idx</span> <span class=ow>in</span> <span class=n>tscv</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>train_idx</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>test_idx</span><span class=p>]</span>
    <span class=c1># Train and evaluate</span>
</code></pre></div> <p><strong>4. Feature Engineering Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># WRONG: Creating features using global statistics</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;income_vs_mean&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span> <span class=o>-</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>  <span class=c1># Uses test data mean!</span>

<span class=c1># CORRECT: Use training statistics only</span>
<span class=k>def</span><span class=w> </span><span class=nf>create_features</span><span class=p>(</span><span class=n>train_df</span><span class=p>,</span> <span class=n>test_df</span><span class=p>):</span>
    <span class=c1># Compute statistics on training data</span>
    <span class=n>train_mean</span> <span class=o>=</span> <span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=n>train_std</span> <span class=o>=</span> <span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>std</span><span class=p>()</span>

    <span class=c1># Apply to both</span>
    <span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;income_normalized&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span> <span class=o>-</span> <span class=n>train_mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>train_std</span>
    <span class=n>test_df</span><span class=p>[</span><span class=s1>&#39;income_normalized&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>test_df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span> <span class=o>-</span> <span class=n>train_mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>train_std</span>

    <span class=k>return</span> <span class=n>train_df</span><span class=p>,</span> <span class=n>test_df</span>
</code></pre></div> <p><strong>5. Group Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Multiple samples from same entity split across train/test</span>

<span class=c1># Example: Patient data with multiple visits</span>
<span class=c1># WRONG: Random split might put same patient in both sets</span>

<span class=c1># CORRECT: Split by patient ID</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GroupShuffleSplit</span>

<span class=n>gss</span> <span class=o>=</span> <span class=n>GroupShuffleSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=k>for</span> <span class=n>train_idx</span><span class=p>,</span> <span class=n>test_idx</span> <span class=ow>in</span> <span class=n>gss</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>patient_ids</span><span class=p>):</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>train_idx</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>test_idx</span><span class=p>]</span>
    <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>train_idx</span><span class=p>],</span> <span class=n>y</span><span class=p>[</span><span class=n>test_idx</span><span class=p>]</span>
</code></pre></div> <p><strong>Detection Strategies:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Check for suspiciously high performance</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>roc_auc_score</span>

<span class=n>auc</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=k>if</span> <span class=n>auc</span> <span class=o>&gt;</span> <span class=mf>0.99</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;‚ö†Ô∏è Warning: Suspiciously high AUC. Check for leakage!&quot;</span><span class=p>)</span>

<span class=c1># 2. Feature importance analysis</span>
<span class=kn>import</span><span class=w> </span><span class=nn>shap</span>

<span class=n>explainer</span> <span class=o>=</span> <span class=n>shap</span><span class=o>.</span><span class=n>TreeExplainer</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
<span class=n>shap_values</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>shap_values</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Check if top features make logical sense</span>
<span class=n>feature_importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;importance&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>shap_values</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Top features:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>feature_importance</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>

<span class=c1># 3. Compare train vs test performance</span>
<span class=n>train_score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>test_score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>

<span class=k>if</span> <span class=n>train_score</span> <span class=o>-</span> <span class=n>test_score</span> <span class=o>&gt;</span> <span class=mf>0.1</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;‚ö†Ô∏è Large train-test gap. Possible overfitting or leakage.&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Prevention Checklist:</strong></p> <table> <thead> <tr> <th>Check</th> <th>Question</th> </tr> </thead> <tbody> <tr> <td>‚úì</td> <td>Split data before any preprocessing?</td> </tr> <tr> <td>‚úì</td> <td>Fit transformers only on training data?</td> </tr> <tr> <td>‚úì</td> <td>Time-based split for temporal data?</td> </tr> <tr> <td>‚úì</td> <td>Group-based split for related samples?</td> </tr> <tr> <td>‚úì</td> <td>Features available at prediction time?</td> </tr> <tr> <td>‚úì</td> <td>No target in features?</td> </tr> <tr> <td>‚úì</td> <td>Reasonable performance (not too good)?</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Production ML awareness.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Concrete examples of leakage types</li> <li>"Fit on train, transform on test"</li> <li>Temporal leakage in time series</li> <li>"Performance too good to be true"</li> <li>Mentions pipeline safety</li> </ul> </div> </details> <hr> <h3 id=explain-calibration-in-machine-learning-google-meta-interview-question>Explain Calibration in Machine Learning - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Probability</code>, <code>Model Evaluation</code>, <code>Classification</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>Calibration:</strong></p> <p>A model is calibrated if P(y=1 | score=s) = s. That is, predicted probabilities match actual frequencies.</p> <p><strong>Why It Matters:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Uncalibrated model: says 90% confident, but only right 60% of time</span>
<span class=c1># Calibrated model: says 90% confident, and right 90% of time</span>

<span class=c1># Critical for:</span>
<span class=c1># - Medical diagnosis (need true probabilities)</span>
<span class=c1># - Risk assessment (financial, insurance)</span>
<span class=c1># - Decision-making under uncertainty</span>
</code></pre></div> <p><strong>Measuring Calibration:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>calibration_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Get predicted probabilities</span>
<span class=n>y_prob</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Compute calibration curve</span>
<span class=n>prob_true</span><span class=p>,</span> <span class=n>prob_pred</span> <span class=o>=</span> <span class=n>calibration_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>,</span> <span class=n>n_bins</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>prob_pred</span><span class=p>,</span> <span class=n>prob_true</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Model&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Perfectly calibrated&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Mean predicted probability&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Fraction of positives&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Calibration Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Calibration Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>CalibratedClassifierCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=c1># Method 1: Platt Scaling (Logistic Regression)</span>
<span class=n>base_model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
<span class=n>calibrated_platt</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>calibrated_platt</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Method 2: Isotonic Regression</span>
<span class=n>calibrated_isotonic</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;isotonic&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>calibrated_isotonic</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Compare calibrations</span>
<span class=n>models</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Uncalibrated RF&#39;</span><span class=p>:</span> <span class=n>base_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>),</span>
    <span class=s1>&#39;Platt Scaling&#39;</span><span class=p>:</span> <span class=n>calibrated_platt</span><span class=p>,</span>
    <span class=s1>&#39;Isotonic&#39;</span><span class=p>:</span> <span class=n>calibrated_isotonic</span>
<span class=p>}</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>clf</span> <span class=ow>in</span> <span class=n>models</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>y_prob</span> <span class=o>=</span> <span class=n>clf</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
    <span class=n>prob_true</span><span class=p>,</span> <span class=n>prob_pred</span> <span class=o>=</span> <span class=n>calibration_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>,</span> <span class=n>n_bins</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>prob_pred</span><span class=p>,</span> <span class=n>prob_true</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>name</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Perfect&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Mean predicted probability&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Fraction of positives&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Calibration Comparison&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Brier Score (Calibration Metric):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>brier_score_loss</span>

<span class=c1># Lower is better (0 = perfect)</span>
<span class=n>brier_uncal</span> <span class=o>=</span> <span class=n>brier_score_loss</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> 
                                <span class=n>base_model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>])</span>
<span class=n>brier_platt</span> <span class=o>=</span> <span class=n>brier_score_loss</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> 
                                <span class=n>calibrated_platt</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>])</span>
<span class=n>brier_iso</span> <span class=o>=</span> <span class=n>brier_score_loss</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> 
                              <span class=n>calibrated_isotonic</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>])</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Brier Score:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Uncalibrated: </span><span class=si>{</span><span class=n>brier_uncal</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Platt Scaling: </span><span class=si>{</span><span class=n>brier_platt</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Isotonic: </span><span class=si>{</span><span class=n>brier_iso</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Which Models Need Calibration:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Naturally Calibrated?</th> </tr> </thead> <tbody> <tr> <td>Logistic Regression</td> <td>‚úì Usually yes</td> </tr> <tr> <td>Naive Bayes</td> <td>‚úì Usually yes</td> </tr> <tr> <td>Random Forest</td> <td>‚úó No (overconfident)</td> </tr> <tr> <td>Gradient Boosting</td> <td>‚úó No (overconfident)</td> </tr> <tr> <td>SVM</td> <td>‚úó No (not probabilities)</td> </tr> <tr> <td>Neural Networks</td> <td>‚ñ≥ Depends (often uncalibrated)</td> </tr> </tbody> </table> <p><strong>Temperature Scaling (Neural Networks):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>TemperatureScaling</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>logits</span><span class=p>):</span>
        <span class=k>return</span> <span class=n>logits</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Learn optimal temperature&quot;&quot;&quot;</span>
        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>LBFGS</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>],</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

        <span class=k>def</span><span class=w> </span><span class=nf>eval</span><span class=p>():</span>
            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()(</span><span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>logits</span><span class=p>),</span> <span class=n>labels</span><span class=p>)</span>
            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
            <span class=k>return</span> <span class=n>loss</span>

        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=nb>eval</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

<span class=c1># Usage</span>
<span class=n>ts</span> <span class=o>=</span> <span class=n>TemperatureScaling</span><span class=p>()</span>
<span class=n>optimal_temp</span> <span class=o>=</span> <span class=n>ts</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>val_logits</span><span class=p>,</span> <span class=n>val_labels</span><span class=p>)</span>

<span class=c1># Apply to test set</span>
<span class=n>calibrated_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>test_logits</span> <span class=o>/</span> <span class=n>optimal_temp</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Beyond accuracy metrics.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Predicted probabilities match reality"</li> <li>Mentions calibration curve</li> <li>Platt scaling, isotonic regression</li> <li>"Random forests often uncalibrated"</li> <li>Critical for decision-making</li> </ul> </div> </details> <hr> <h3 id=what-is-knowledge-distillation-google-meta-interview-question>What is Knowledge Distillation? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Model Compression</code>, <code>Transfer Learning</code>, <code>Deep Learning</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Knowledge Distillation:</strong></p> <p>Train a smaller "student" model to mimic a larger "teacher" model, transferring knowledge while reducing size/complexity.</p> <p><strong>Key Concept:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Teacher: Large, accurate model</span>
<span class=c1># Student: Small, fast model</span>

<span class=c1># Student learns from:</span>
<span class=c1># 1. Hard labels: y_true (ground truth)</span>
<span class=c1># 2. Soft labels: teacher&#39;s probability distribution</span>

<span class=c1># Soft labels contain more information:</span>
<span class=c1># Instead of [0, 1, 0] (hard)</span>
<span class=c1># Learn from [0.05, 0.9, 0.05] (soft) - reveals similarities</span>
</code></pre></div> <p><strong>Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>

<span class=k>class</span><span class=w> </span><span class=nc>DistillationLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>3.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>=</span> <span class=n>temperature</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>  <span class=c1># Weight for distillation vs hard labels</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>student_logits</span><span class=p>,</span> <span class=n>teacher_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
        <span class=c1># Soft targets from teacher</span>
        <span class=n>soft_targets</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>teacher_logits</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>soft_prob</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>student_logits</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Distillation loss (KL divergence)</span>
        <span class=n>distillation_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>kl_div</span><span class=p>(</span>
            <span class=n>soft_prob</span><span class=p>,</span>
            <span class=n>soft_targets</span><span class=p>,</span>
            <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;batchmean&#39;</span>
        <span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Hard label loss</span>
        <span class=n>student_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>student_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=c1># Combined loss</span>
        <span class=n>total_loss</span> <span class=o>=</span> <span class=p>(</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>distillation_loss</span> <span class=o>+</span> 
            <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>student_loss</span>
        <span class=p>)</span>

        <span class=k>return</span> <span class=n>total_loss</span>

<span class=c1># Training</span>
<span class=n>teacher_model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>  <span class=c1># Teacher in eval mode</span>
<span class=n>student_model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>

<span class=n>distill_loss</span> <span class=o>=</span> <span class=n>DistillationLoss</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mf>3.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=c1># Get teacher predictions</span>
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=n>teacher_logits</span> <span class=o>=</span> <span class=n>teacher_model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Get student predictions</span>
        <span class=n>student_logits</span> <span class=o>=</span> <span class=n>student_model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Compute distillation loss</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>distill_loss</span><span class=p>(</span><span class=n>student_logits</span><span class=p>,</span> <span class=n>teacher_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>TensorFlow/Keras Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>tf</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tensorflow</span><span class=w> </span><span class=kn>import</span> <span class=n>keras</span>

<span class=k>class</span><span class=w> </span><span class=nc>Distiller</span><span class=p>(</span><span class=n>keras</span><span class=o>.</span><span class=n>Model</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>student</span><span class=p>,</span> <span class=n>teacher</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>teacher</span> <span class=o>=</span> <span class=n>teacher</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>student</span> <span class=o>=</span> <span class=n>student</span>

    <span class=k>def</span><span class=w> </span><span class=nf>compile</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>metrics</span><span class=p>,</span> <span class=n>student_loss_fn</span><span class=p>,</span> 
               <span class=n>distillation_loss_fn</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=n>metrics</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>student_loss_fn</span> <span class=o>=</span> <span class=n>student_loss_fn</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>distillation_loss_fn</span> <span class=o>=</span> <span class=n>distillation_loss_fn</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>=</span> <span class=n>temperature</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data</span><span class=p>):</span>
        <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>data</span>

        <span class=c1># Forward pass of teacher</span>
        <span class=n>teacher_predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>teacher</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

        <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>GradientTape</span><span class=p>()</span> <span class=k>as</span> <span class=n>tape</span><span class=p>:</span>
            <span class=c1># Forward pass of student</span>
            <span class=n>student_predictions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>student</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

            <span class=c1># Compute losses</span>
            <span class=n>student_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>student_loss_fn</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>student_predictions</span><span class=p>)</span>

            <span class=n>distillation_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>distillation_loss_fn</span><span class=p>(</span>
                <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>teacher_predictions</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
                <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>student_predictions</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
            <span class=p>)</span>

            <span class=n>loss</span> <span class=o>=</span> <span class=p>(</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>student_loss</span> <span class=o>+</span> 
                <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>distillation_loss</span>
            <span class=p>)</span>

        <span class=c1># Compute gradients</span>
        <span class=n>trainable_vars</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>student</span><span class=o>.</span><span class=n>trainable_variables</span>
        <span class=n>gradients</span> <span class=o>=</span> <span class=n>tape</span><span class=o>.</span><span class=n>gradient</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>trainable_vars</span><span class=p>)</span>

        <span class=c1># Update weights</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>apply_gradients</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>gradients</span><span class=p>,</span> <span class=n>trainable_vars</span><span class=p>))</span>

        <span class=c1># Update metrics</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>compiled_metrics</span><span class=o>.</span><span class=n>update_state</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>student_predictions</span><span class=p>)</span>

        <span class=k>return</span> <span class=p>{</span><span class=n>m</span><span class=o>.</span><span class=n>name</span><span class=p>:</span> <span class=n>m</span><span class=o>.</span><span class=n>result</span><span class=p>()</span> <span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>metrics</span><span class=p>}</span>

<span class=c1># Usage</span>
<span class=n>distiller</span> <span class=o>=</span> <span class=n>Distiller</span><span class=p>(</span><span class=n>student</span><span class=o>=</span><span class=n>student_model</span><span class=p>,</span> <span class=n>teacher</span><span class=o>=</span><span class=n>teacher_model</span><span class=p>)</span>
<span class=n>distiller</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span>
    <span class=n>optimizer</span><span class=o>=</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>Adam</span><span class=p>(),</span>
    <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=n>keras</span><span class=o>.</span><span class=n>metrics</span><span class=o>.</span><span class=n>SparseCategoricalAccuracy</span><span class=p>()],</span>
    <span class=n>student_loss_fn</span><span class=o>=</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>SparseCategoricalCrossentropy</span><span class=p>(</span><span class=n>from_logits</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
    <span class=n>distillation_loss_fn</span><span class=o>=</span><span class=n>keras</span><span class=o>.</span><span class=n>losses</span><span class=o>.</span><span class=n>KLDivergence</span><span class=p>(),</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>temperature</span><span class=o>=</span><span class=mi>3</span>
<span class=p>)</span>

<span class=n>distiller</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>validation_data</span><span class=o>=</span><span class=n>val_dataset</span><span class=p>)</span>
</code></pre></div> <p><strong>Why Temperature Matters:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=n>logits</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>2.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>])</span>

<span class=c1># Low temperature (T=1): Peaked distribution</span>
<span class=n>probs_T1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>logits</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>logits</span> <span class=o>/</span> <span class=mi>1</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;T=1: </span><span class=si>{</span><span class=n>probs_T1</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>  <span class=c1># [0.66, 0.24, 0.10]</span>

<span class=c1># High temperature (T=5): Smooth distribution</span>
<span class=n>probs_T5</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>logits</span> <span class=o>/</span> <span class=mi>5</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>logits</span> <span class=o>/</span> <span class=mi>5</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;T=5: </span><span class=si>{</span><span class=n>probs_T5</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>  <span class=c1># [0.42, 0.34, 0.24]</span>

<span class=c1># Higher T ‚Üí More information in &quot;dark knowledge&quot;</span>
</code></pre></div> <p><strong>Results Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Evaluate models</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>accuracy_score</span>

<span class=c1># Teacher (large model)</span>
<span class=n>teacher_preds</span> <span class=o>=</span> <span class=n>teacher_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>teacher_acc</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>teacher_preds</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>

<span class=c1># Student without distillation</span>
<span class=n>student_scratch</span> <span class=o>=</span> <span class=n>train_from_scratch</span><span class=p>(</span><span class=n>student_model</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>scratch_acc</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>student_scratch</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>

<span class=c1># Student with distillation</span>
<span class=n>student_distilled</span> <span class=o>=</span> <span class=n>train_with_distillation</span><span class=p>(</span><span class=n>student_model</span><span class=p>,</span> <span class=n>teacher_model</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>distill_acc</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>student_distilled</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Teacher accuracy: </span><span class=si>{</span><span class=n>teacher_acc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Student (from scratch): </span><span class=si>{</span><span class=n>scratch_acc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Student (distilled): </span><span class=si>{</span><span class=n>distill_acc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Typical: Distilled student &gt;&gt; Student from scratch</span>
<span class=c1># And much faster than teacher!</span>
</code></pre></div> <p><strong>Benefits:</strong></p> <table> <thead> <tr> <th>Benefit</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Model compression</td> <td>10-100x smaller</td> </tr> <tr> <td>Speed</td> <td>5-10x faster inference</td> </tr> <tr> <td>Better generalization</td> <td>Learns from soft labels</td> </tr> <tr> <td>Knowledge transfer</td> <td>Ensemble ‚Üí Single model</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced model optimization.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Student learns from teacher's soft labels"</li> <li>Temperature smooths distribution</li> <li>"Dark knowledge" in wrong class probabilities</li> <li>Combined loss: distillation + hard labels</li> <li>Use case: Deploy smaller model to production</li> </ul> </div> </details> <hr> <h3 id=explain-ensemble-methods-bagging-vs-boosting-amazon-google-interview-question>Explain Ensemble Methods: Bagging vs Boosting - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble</code>, <code>Random Forest</code>, <code>Gradient Boosting</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Ensemble Methods:</strong></p> <p>Combine multiple models to improve performance beyond individual models.</p> <p><strong>Bagging (Bootstrap Aggregating):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Train models in parallel on different random subsets</span>
<span class=c1># Average predictions (regression) or vote (classification)</span>

<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>BaggingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.tree</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>

<span class=c1># Base model</span>
<span class=n>base_model</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># Bagging ensemble</span>
<span class=n>bagging</span> <span class=o>=</span> <span class=n>BaggingClassifier</span><span class=p>(</span>
    <span class=n>base_estimator</span><span class=o>=</span><span class=n>base_model</span><span class=p>,</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_samples</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>  <span class=c1># Use 80% of data per model</span>
    <span class=n>max_features</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>  <span class=c1># Use 80% of features per model</span>
    <span class=n>bootstrap</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>    <span class=c1># Sample with replacement</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>bagging</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Predictions</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>bagging</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Individual model predictions</span>
<span class=n>individual_preds</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
    <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span> 
    <span class=k>for</span> <span class=n>estimator</span> <span class=ow>in</span> <span class=n>bagging</span><span class=o>.</span><span class=n>estimators_</span>
<span class=p>])</span>

<span class=c1># Variance across models (diversity)</span>
<span class=n>prediction_variance</span> <span class=o>=</span> <span class=n>individual_preds</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Prediction diversity (variance): </span><span class=si>{</span><span class=n>prediction_variance</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Boosting:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Train models sequentially, each correcting previous errors</span>

<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>AdaBoostClassifier</span><span class=p>,</span> <span class=n>GradientBoostingClassifier</span>

<span class=c1># AdaBoost: Reweight samples</span>
<span class=n>adaboost</span> <span class=o>=</span> <span class=n>AdaBoostClassifier</span><span class=p>(</span>
    <span class=n>base_estimator</span><span class=o>=</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>  <span class=c1># Weak learners</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>1.0</span>
<span class=p>)</span>

<span class=n>adaboost</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Gradient Boosting: Fit residuals</span>
<span class=n>gb</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span>
<span class=p>)</span>

<span class=n>gb</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Differences:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Bagging</th> <th>Boosting</th> </tr> </thead> <tbody> <tr> <td>Training</td> <td>Parallel</td> <td>Sequential</td> </tr> <tr> <td>Sampling</td> <td>Random with replacement</td> <td>Weighted by errors</td> </tr> <tr> <td>Base models</td> <td>Strong learners (deep trees)</td> <td>Weak learners (stumps)</td> </tr> <tr> <td>Goal</td> <td>Reduce variance</td> <td>Reduce bias</td> </tr> <tr> <td>Overfitting risk</td> <td>Low</td> <td>Higher (if too many iterations)</td> </tr> <tr> <td>Example</td> <td>Random Forest</td> <td>AdaBoost, XGBoost</td> </tr> </tbody> </table> <p><strong>Visual Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_classification</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span><span class=p>,</span> <span class=n>GradientBoostingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.tree</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Generate data</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_classification</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>n_informative</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
                           <span class=n>n_redundant</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_clusters_per_class</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Models</span>
<span class=n>models</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Single Tree&#39;</span><span class=p>:</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>),</span>
    <span class=s1>&#39;Bagging (RF)&#39;</span><span class=p>:</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>),</span>
    <span class=s1>&#39;Boosting (GB)&#39;</span><span class=p>:</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=p>}</span>

<span class=c1># Train and evaluate</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>accuracy_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>learning_curve</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>model</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>models</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=mi>1</span><span class=p>):</span>
    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

    <span class=c1># Learning curve</span>
    <span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=p>,</span> <span class=n>test_scores</span> <span class=o>=</span> <span class=n>learning_curve</span><span class=p>(</span>
        <span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
        <span class=n>train_sizes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
    <span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>idx</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>test_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Training Size&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Custom Bagging:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Implement bagging from scratch</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleBagging</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>base_model</span><span class=p>,</span> <span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>base_model</span> <span class=o>=</span> <span class=n>base_model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>n_estimators</span> <span class=o>=</span> <span class=n>n_estimators</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>models</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
        <span class=n>n_samples</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>n_estimators</span><span class=p>):</span>
            <span class=c1># Bootstrap sample</span>
            <span class=n>indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
            <span class=n>X_bootstrap</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>
            <span class=n>y_bootstrap</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>

            <span class=c1># Train model</span>
            <span class=n>model</span> <span class=o>=</span> <span class=n>clone</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>base_model</span><span class=p>)</span>
            <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_bootstrap</span><span class=p>,</span> <span class=n>y_bootstrap</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
        <span class=c1># Collect predictions</span>
        <span class=n>predictions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=k>for</span> <span class=n>model</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>models</span><span class=p>])</span>

        <span class=c1># Majority vote</span>
        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>apply_along_axis</span><span class=p>(</span>
            <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>bincount</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>argmax</span><span class=p>(),</span>
            <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
            <span class=n>arr</span><span class=o>=</span><span class=n>predictions</span>
        <span class=p>)</span>

<span class=c1># Usage</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.base</span><span class=w> </span><span class=kn>import</span> <span class=n>clone</span>

<span class=n>bagging</span> <span class=o>=</span> <span class=n>SimpleBagging</span><span class=p>(</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>5</span><span class=p>),</span> <span class=n>n_estimators</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
<span class=n>bagging</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>bagging</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>When to Use:</strong></p> <table> <thead> <tr> <th>Use Bagging When</th> <th>Use Boosting When</th> </tr> </thead> <tbody> <tr> <td>High variance models (deep trees)</td> <td>High bias models (linear)</td> </tr> <tr> <td>Need fast parallel training</td> <td>Can afford sequential training</td> </tr> <tr> <td>Want robustness</td> <td>Want best performance</td> </tr> <tr> <td>Overfitting is main concern</td> <td>Underfitting is main concern</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Ensemble understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Bagging: parallel, reduce variance</li> <li>Boosting: sequential, reduce bias</li> <li>Random Forest = bagging + feature randomness</li> <li>"Boosting can overfit, bagging rarely does"</li> <li>Mentions diversity in ensemble</li> </ul> </div> </details> <hr> <h3 id=what-is-batch-normalization-and-why-is-it-effective-google-meta-interview-question>What is Batch Normalization and Why Is It Effective? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Normalization</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Batch Normalization:</strong></p> <p>Normalize layer inputs during training to stabilize and accelerate learning.</p> <p><strong>Formula:</strong></p> <div class=arithmatex>\[\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y = \gamma \hat{x} + \beta\]</div> <p>Where: - Œº_B, œÉ_B: Batch mean and variance - Œ≥, Œ≤: Learnable parameters - Œµ: Small constant for numerical stability</p> <p><strong>Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>256</span><span class=p>)</span>  <span class=c1># Add BN after linear</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Flatten</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Layer 1: Linear -&gt; BN -&gt; ReLU</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Layer 2: Linear -&gt; BN -&gt; ReLU</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Output layer</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># For CNNs</span>
<span class=k>class</span><span class=w> </span><span class=nc>ConvNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>  <span class=c1># 2D for conv layers</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>TensorFlow/Keras:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras</span><span class=w> </span><span class=kn>import</span> <span class=n>layers</span><span class=p>,</span> <span class=n>models</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>784</span><span class=p>,)),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(),</span>  <span class=c1># Add BN</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>

    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>

    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
<span class=p>])</span>
</code></pre></div> <p><strong>Why It Works:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Reduces Internal Covariate Shift</span>
<span class=c1># - Layer inputs stay in similar range during training</span>
<span class=c1># - Prevents exploding/vanishing gradients</span>

<span class=c1># 2. Allows higher learning rates</span>
<span class=c1># - More stable training</span>

<span class=c1># 3. Acts as regularization</span>
<span class=c1># - Noise from batch statistics</span>
<span class=c1># - Can reduce need for dropout</span>

<span class=c1># 4. Makes network less sensitive to initialization</span>
</code></pre></div> <p><strong>Training vs Inference:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Key difference: Running statistics</span>

<span class=c1># During TRAINING:</span>
<span class=c1># - Use batch mean and variance</span>
<span class=c1># - Update running mean/variance (momentum=0.1)</span>

<span class=c1># During INFERENCE:</span>
<span class=c1># - Use running mean and variance (population statistics)</span>
<span class=c1># - No batch dependency</span>

<span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># Uses batch statistics</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>   <span class=c1># Uses running statistics</span>

<span class=c1># PyTorch example</span>
<span class=n>bn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Training mode</span>
<span class=n>bn</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
<span class=n>output_train</span> <span class=o>=</span> <span class=n>bn</span><span class=p>(</span><span class=n>input_batch</span><span class=p>)</span>

<span class=c1># Eval mode</span>
<span class=n>bn</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
<span class=n>output_eval</span> <span class=o>=</span> <span class=n>bn</span><span class=p>(</span><span class=n>input_single</span><span class=p>)</span>  <span class=c1># Can handle single sample</span>
</code></pre></div> <p><strong>From Scratch:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=k>class</span><span class=w> </span><span class=nc>BatchNorm</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_features</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>num_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>num_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>momentum</span> <span class=o>=</span> <span class=n>momentum</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>

        <span class=c1># Running statistics</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>running_mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>num_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>running_var</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>num_features</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
        <span class=k>if</span> <span class=n>training</span><span class=p>:</span>
            <span class=c1># Compute batch statistics</span>
            <span class=n>batch_mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
            <span class=n>batch_var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

            <span class=c1># Normalize</span>
            <span class=n>x_norm</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>batch_mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>batch_var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>

            <span class=c1># Update running statistics</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>running_mean</span> <span class=o>=</span> <span class=p>(</span>
                <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>momentum</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>running_mean</span> <span class=o>+</span> 
                <span class=bp>self</span><span class=o>.</span><span class=n>momentum</span> <span class=o>*</span> <span class=n>batch_mean</span>
            <span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>running_var</span> <span class=o>=</span> <span class=p>(</span>
                <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>momentum</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>running_var</span> <span class=o>+</span> 
                <span class=bp>self</span><span class=o>.</span><span class=n>momentum</span> <span class=o>*</span> <span class=n>batch_var</span>
            <span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=c1># Use running statistics</span>
            <span class=n>x_norm</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>running_mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>running_var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>

        <span class=c1># Scale and shift</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>x_norm</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span>
        <span class=k>return</span> <span class=n>out</span>
</code></pre></div> <p><strong>Empirical Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Compare with and without BN</span>

<span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras</span><span class=w> </span><span class=kn>import</span> <span class=n>callbacks</span>

<span class=c1># Without BN</span>
<span class=n>model_no_bn</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>784</span><span class=p>,)),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># With BN</span>
<span class=n>model_with_bn</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>([</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>784</span><span class=p>,)),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>(),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>),</span>
    <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
<span class=p>])</span>

<span class=c1># Train both</span>
<span class=n>history_no_bn</span> <span class=o>=</span> <span class=n>model_no_bn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> 
                                 <span class=n>validation_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
<span class=n>history_with_bn</span> <span class=o>=</span> <span class=n>model_with_bn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
                                     <span class=n>validation_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Compare convergence</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history_no_bn</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Without BN&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history_with_bn</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;With BN&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Training Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training Convergence&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history_no_bn</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_accuracy&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Without BN&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>history_with_bn</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_accuracy&#39;</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;With BN&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Validation Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Validation Performance&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Common Issues:</strong></p> <table> <thead> <tr> <th>Issue</th> <th>Solution</th> </tr> </thead> <tbody> <tr> <td>Small batch size</td> <td>Use Group Norm or Layer Norm</td> </tr> <tr> <td>RNNs/variable length</td> <td>Use Layer Norm</td> </tr> <tr> <td>BN before/after activation?</td> <td>Usually after linear, before activation</td> </tr> <tr> <td>Inference speed</td> <td>Fuse BN into weights for deployment</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep learning training techniques.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Normalizes to mean=0, std=1 per batch</li> <li>Learnable scale (Œ≥) and shift (Œ≤)</li> <li>Different behavior train vs eval</li> <li>"Reduces internal covariate shift"</li> <li>Allows higher learning rates</li> </ul> </div> </details> <hr> <h3 id=explain-residual-networks-resnet-and-skip-connections-google-meta-interview-question>Explain Residual Networks (ResNet) and Skip Connections - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>CNN</code>, <code>Architecture</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Residual Networks (ResNet):</strong></p> <p>Use skip connections to enable training of very deep networks (100+ layers) by addressing vanishing gradient problem.</p> <p><strong>Key Innovation - Skip Connections:</strong></p> <div class=arithmatex>\[y = F(x, \{W_i\}) + x\]</div> <p>Instead of learning H(x), learn residual F(x) = H(x) - x</p> <p><strong>Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>ResidualBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Main path</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> 
                               <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span>
                               <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>

        <span class=c1># Skip connection (identity)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>skip</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>
        <span class=k>if</span> <span class=n>stride</span> <span class=o>!=</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>in_channels</span> <span class=o>!=</span> <span class=n>out_channels</span><span class=p>:</span>
            <span class=c1># Projection shortcut to match dimensions</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>skip</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> 
                         <span class=n>kernel_size</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>out_channels</span><span class=p>)</span>
            <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Main path</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>out</span><span class=p>))</span>

        <span class=c1># Add skip connection</span>
        <span class=n>out</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>skip</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>out</span>

<span class=c1># Build ResNet</span>
<span class=k>class</span><span class=w> </span><span class=nc>ResNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>maxpool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Residual blocks</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_make_layer</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>num_blocks</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_make_layer</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>num_blocks</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer3</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_make_layer</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>num_blocks</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer4</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_make_layer</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>num_blocks</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>avgpool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AdaptiveAvgPool2d</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>_make_layer</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>num_blocks</span><span class=p>,</span> <span class=n>stride</span><span class=p>):</span>
        <span class=n>layers</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=c1># First block may downsample</span>
        <span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ResidualBlock</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>stride</span><span class=p>))</span>
        <span class=c1># Rest maintain dimensions</span>
        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_blocks</span><span class=p>):</span>
            <span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ResidualBlock</span><span class=p>(</span><span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
        <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>layers</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>maxpool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer4</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>avgpool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>TensorFlow/Keras:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>tensorflow.keras</span><span class=w> </span><span class=kn>import</span> <span class=n>layers</span><span class=p>,</span> <span class=n>models</span>

<span class=k>def</span><span class=w> </span><span class=nf>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>filters</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
    <span class=c1># Main path</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>filters</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>()(</span><span class=n>y</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>y</span><span class=p>)</span>

    <span class=n>y</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>filters</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>y</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>()(</span><span class=n>y</span><span class=p>)</span>

    <span class=c1># Skip connection</span>
    <span class=k>if</span> <span class=n>stride</span> <span class=o>!=</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>!=</span> <span class=n>filters</span><span class=p>:</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=n>filters</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>()(</span><span class=n>x</span><span class=p>)</span>

    <span class=c1># Add skip</span>
    <span class=n>out</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Add</span><span class=p>()([</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>])</span>
    <span class=n>out</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>out</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>out</span>

<span class=c1># Build model</span>
<span class=n>inputs</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>inputs</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>BatchNormalization</span><span class=p>()(</span><span class=n>x</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>

<span class=c1># Add residual blocks</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>x</span> <span class=o>=</span> <span class=n>residual_block</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>GlobalAveragePooling2D</span><span class=p>()(</span><span class=n>x</span><span class=p>)</span>
<span class=n>outputs</span> <span class=o>=</span> <span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)(</span><span class=n>x</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span><span class=p>)</span>
</code></pre></div> <p><strong>Why Skip Connections Work:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Gradient Flow</span>
<span class=c1># Without skip: gradient must flow through many layers</span>
<span class=c1># With skip: gradient has direct path backward</span>

<span class=c1># 2. Identity Mapping</span>
<span class=c1># Easy to learn identity: just set F(x) = 0</span>
<span class=c1># Worst case: no degradation from adding layers</span>

<span class=c1># 3. Ensemble Effect</span>
<span class=c1># ResNet can be viewed as ensemble of many paths</span>
<span class=c1># Each path is a different depth network</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Compare plain network vs ResNet</span>

<span class=k>class</span><span class=w> </span><span class=nc>PlainNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Standard deep network without skip connections&quot;&quot;&quot;</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=n>layers</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>in_ch</span> <span class=o>=</span> <span class=mi>64</span>
        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>20</span><span class=p>):</span>  <span class=c1># 20 conv layers</span>
            <span class=n>layers</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>in_ch</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
            <span class=p>])</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>layers</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

<span class=c1># Training comparison</span>
<span class=n>plain_net</span> <span class=o>=</span> <span class=n>PlainNet</span><span class=p>()</span>
<span class=n>resnet</span> <span class=o>=</span> <span class=n>ResNet</span><span class=p>()</span>

<span class=c1># Train both</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
    <span class=c1># Plain network: gradients vanish, training stagnates</span>
    <span class=c1># ResNet: gradients flow well, continues improving</span>
    <span class=k>pass</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Modern architecture knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Skip connections: y = F(x) + x</li> <li>Solves vanishing gradients</li> <li>"Learn residual is easier than identity"</li> <li>Enables 100+ layer networks</li> <li>Won ImageNet 2015</li> </ul> </div> </details> <hr> <h3 id=what-is-the-attention-mechanism-google-meta-interview-question>What is the Attention Mechanism? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>NLP</code>, <code>Deep Learning</code>, <code>Transformers</code> | <strong>Asked by:</strong> Google, Meta, OpenAI, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>Attention Mechanism:</strong></p> <p>Allows model to focus on relevant parts of input when producing output.</p> <p><strong>Core Formula:</strong></p> <div class=arithmatex>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div> <p>Where: - Q: Query matrix - K: Key matrix<br> - V: Value matrix - d_k: Dimension of keys (for scaling)</p> <p><strong>Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
<span class=kn>import</span><span class=w> </span><span class=nn>math</span>

<span class=k>class</span><span class=w> </span><span class=nc>ScaledDotProductAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Q: [batch_size, n_queries, d_k]</span>
<span class=sd>        K: [batch_size, n_keys, d_k]</span>
<span class=sd>        V: [batch_size, n_keys, d_v]</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>d_k</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Compute attention scores</span>
        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>

        <span class=c1># Apply mask (optional, for padding)</span>
        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>

        <span class=c1># Apply softmax</span>
        <span class=n>attention_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Apply attention to values</span>
        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attention_weights</span>

<span class=c1># Example usage</span>
<span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>512</span>

<span class=n>Q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
<span class=n>K</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
<span class=n>V</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>

<span class=n>attention</span> <span class=o>=</span> <span class=n>ScaledDotProductAttention</span><span class=p>()</span>
<span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Output shape: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Attention weights shape: </span><span class=si>{</span><span class=n>weights</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Multi-Head Attention:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>num_heads</span> <span class=o>==</span> <span class=mi>0</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>num_heads</span>

        <span class=c1># Linear projections</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>ScaledDotProductAttention</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Linear projections in batch</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>Q</span><span class=p>)</span>  <span class=c1># [batch, seq_len, d_model]</span>
        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>K</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>V</span><span class=p>)</span>

        <span class=c1># Split into multiple heads</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>K</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=c1># Now: [batch, num_heads, seq_len, d_k]</span>

        <span class=c1># Apply attention</span>
        <span class=n>x</span><span class=p>,</span> <span class=n>attention_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>

        <span class=c1># Concatenate heads</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>

        <span class=c1># Final linear projection</span>
        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attention_weights</span>

<span class=c1># Usage</span>
<span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
<span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>mha</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</code></pre></div> <p><strong>Types of Attention:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Self-Attention (Q=K=V)</span>
<span class=k>class</span><span class=w> </span><span class=nc>SelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=c1># Q, K, V are all the same input</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>mha</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>

<span class=c1># 2. Cross-Attention (Q‚â†K=V)</span>
<span class=k>class</span><span class=w> </span><span class=nc>CrossAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key_value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=c1># Query from one source, Key/Value from another</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>mha</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key_value</span><span class=p>,</span> <span class=n>key_value</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>

<span class=c1># 3. Masked Self-Attention (for autoregressive models)</span>
<span class=k>def</span><span class=w> </span><span class=nf>create_causal_mask</span><span class=p>(</span><span class=n>seq_len</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Prevent attending to future positions&quot;&quot;&quot;</span>
    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>),</span> <span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>mask</span>
</code></pre></div> <p><strong>Visualization:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>

<span class=c1># Simple attention example</span>
<span class=k>def</span><span class=w> </span><span class=nf>visualize_attention</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> <span class=n>sentence</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    attention_weights: [seq_len, seq_len]</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
    <span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> 
                <span class=n>xticklabels</span><span class=o>=</span><span class=n>sentence</span><span class=p>,</span>
                <span class=n>yticklabels</span><span class=o>=</span><span class=n>sentence</span><span class=p>,</span>
                <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>,</span>
                <span class=n>cbar</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Key positions&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Query positions&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Attention Weights&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Example sentence</span>
<span class=n>sentence</span> <span class=o>=</span> <span class=s2>&quot;The cat sat on the mat&quot;</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>

<span class=c1># Compute attention (simplified)</span>
<span class=n>seq_len</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
<span class=n>embeddings</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>

<span class=n>attention</span> <span class=o>=</span> <span class=n>ScaledDotProductAttention</span><span class=p>()</span>
<span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>embeddings</span><span class=p>,</span> <span class=n>embeddings</span><span class=p>,</span> <span class=n>embeddings</span><span class=p>)</span>

<span class=n>visualize_attention</span><span class=p>(</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>sentence</span><span class=p>)</span>
</code></pre></div> <p><strong>Why Attention Works:</strong></p> <table> <thead> <tr> <th>Benefit</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Long-range dependencies</td> <td>Can attend to any position</td> </tr> <tr> <td>Parallelizable</td> <td>No sequential dependencies</td> </tr> <tr> <td>Interpretable</td> <td>Attention weights show what model focuses on</td> </tr> <tr> <td>Flexible</td> <td>Works for various sequence lengths</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Modern NLP/DL knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Query, Key, Value matrices</li> <li>Softmax over Key-Query similarity</li> <li>Multi-head for different representations</li> <li>Scaled by sqrt(d_k) for stability</li> <li>Transformer = multi-head attention + FFN</li> </ul> </div> </details> <hr> <h3 id=explain-feature-importance-methods-most-tech-companies-interview-question>Explain Feature Importance Methods - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Interpretability</code>, <code>Feature Engineering</code>, <code>Model Evaluation</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Feature Importance Methods:</strong></p> <p>Quantify the contribution of each feature to model predictions.</p> <p><strong>1. Tree-Based Importance (Gini/Gain):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span><span class=p>,</span> <span class=n>GradientBoostingClassifier</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Train model</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>rf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Get feature importances</span>
<span class=n>feature_importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;importance&#39;</span><span class=p>:</span> <span class=n>rf</span><span class=o>.</span><span class=n>feature_importances_</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=n>feature_importance</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=n>feature_importance</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>][:</span><span class=mi>10</span><span class=p>],</span> 
         <span class=n>feature_importance</span><span class=p>[</span><span class=s1>&#39;importance&#39;</span><span class=p>][:</span><span class=mi>10</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Importance&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Top 10 Feature Importances&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>gca</span><span class=p>()</span><span class=o>.</span><span class=n>invert_yaxis</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>2. Permutation Importance:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.inspection</span><span class=w> </span><span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=c1># More reliable than built-in importances</span>
<span class=c1># Measures drop in performance when feature is shuffled</span>

<span class=n>perm_importance</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span>
    <span class=n>n_repeats</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>perm_imp_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;importance_mean&#39;</span><span class=p>:</span> <span class=n>perm_importance</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>,</span>
    <span class=s1>&#39;importance_std&#39;</span><span class=p>:</span> <span class=n>perm_importance</span><span class=o>.</span><span class=n>importances_std</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance_mean&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Plot with error bars</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>top_features</span> <span class=o>=</span> <span class=n>perm_imp_df</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>],</span> <span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;importance_mean&#39;</span><span class=p>],</span>
         <span class=n>xerr</span><span class=o>=</span><span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;importance_std&#39;</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Permutation Importance&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Top 10 Features (Permutation Importance)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>gca</span><span class=p>()</span><span class=o>.</span><span class=n>invert_yaxis</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>3. SHAP (SHapley Additive exPlanations):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>shap</span>

<span class=c1># Tree model</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>shap</span><span class=o>.</span><span class=n>TreeExplainer</span><span class=p>(</span><span class=n>rf</span><span class=p>)</span>
<span class=n>shap_values</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>shap_values</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Summary plot (global importance)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>summary_plot</span><span class=p>(</span><span class=n>shap_values</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>plot_type</span><span class=o>=</span><span class=s2>&quot;bar&quot;</span><span class=p>)</span>

<span class=c1># Detailed summary (shows distributions)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>summary_plot</span><span class=p>(</span><span class=n>shap_values</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>X_test</span><span class=p>)</span>

<span class=c1># Individual prediction explanation</span>
<span class=n>shap</span><span class=o>.</span><span class=n>force_plot</span><span class=p>(</span><span class=n>explainer</span><span class=o>.</span><span class=n>expected_value</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> 
                <span class=n>shap_values</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> 
                <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

<span class=c1># Dependence plot (feature interaction)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>dependence_plot</span><span class=p>(</span><span class=s2>&quot;age&quot;</span><span class=p>,</span> <span class=n>shap_values</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>4. LIME (Local Interpretable Model-agnostic Explanations):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>lime</span><span class=w> </span><span class=kn>import</span> <span class=n>lime_tabular</span>

<span class=c1># Create explainer</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>lime_tabular</span><span class=o>.</span><span class=n>LimeTabularExplainer</span><span class=p>(</span>
    <span class=n>X_train</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>feature_names</span><span class=o>=</span><span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=n>class_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;class_0&#39;</span><span class=p>,</span> <span class=s1>&#39;class_1&#39;</span><span class=p>],</span>
    <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;classification&#39;</span>
<span class=p>)</span>

<span class=c1># Explain a prediction</span>
<span class=n>exp</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>explain_instance</span><span class=p>(</span>
    <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>rf</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>,</span>
    <span class=n>num_features</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=c1># Show explanation</span>
<span class=n>exp</span><span class=o>.</span><span class=n>show_in_notebook</span><span class=p>()</span>

<span class=c1># As matplotlib</span>
<span class=n>exp</span><span class=o>.</span><span class=n>as_pyplot_figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>5. Coefficient-Based (Linear Models):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=c1># Train logistic regression</span>
<span class=n>lr</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_scaled</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Get coefficients</span>
<span class=n>coef_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;coefficient&#39;</span><span class=p>:</span> <span class=n>lr</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;coefficient&#39;</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=nb>abs</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>colors</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;red&#39;</span> <span class=k>if</span> <span class=n>c</span> <span class=o>&lt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=s1>&#39;green&#39;</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>coef_df</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>][:</span><span class=mi>10</span><span class=p>]]</span>
<span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=n>coef_df</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>][:</span><span class=mi>10</span><span class=p>],</span> <span class=n>coef_df</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>][:</span><span class=mi>10</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=n>colors</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Coefficient&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Feature Coefficients (Logistic Regression)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>gca</span><span class=p>()</span><span class=o>.</span><span class=n>invert_yaxis</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>6. Partial Dependence Plots:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.inspection</span><span class=w> </span><span class=kn>import</span> <span class=n>partial_dependence</span><span class=p>,</span> <span class=n>PartialDependenceDisplay</span>

<span class=c1># Show how predictions change with feature value</span>
<span class=n>features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;income&#39;</span><span class=p>,</span> <span class=p>(</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;income&#39;</span><span class=p>)]</span>

<span class=n>display</span> <span class=o>=</span> <span class=n>PartialDependenceDisplay</span><span class=o>.</span><span class=n>from_estimator</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span>
    <span class=n>kind</span><span class=o>=</span><span class=s1>&#39;both&#39;</span><span class=p>,</span>  <span class=c1># Shows both average and individual</span>
    <span class=n>grid_resolution</span><span class=o>=</span><span class=mi>50</span>
<span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Speed</th> <th>Global/Local</th> <th>Model-Agnostic</th> <th>Interaction</th> </tr> </thead> <tbody> <tr> <td>Tree importance</td> <td>Fast</td> <td>Global</td> <td>No</td> <td>No</td> </tr> <tr> <td>Permutation</td> <td>Medium</td> <td>Global</td> <td>Yes</td> <td>No</td> </tr> <tr> <td>SHAP</td> <td>Slow</td> <td>Both</td> <td>Yes (TreeSHAP fast)</td> <td>Yes</td> </tr> <tr> <td>LIME</td> <td>Medium</td> <td>Local</td> <td>Yes</td> <td>Limited</td> </tr> <tr> <td>Coefficients</td> <td>Fast</td> <td>Global</td> <td>No (linear only)</td> <td>No</td> </tr> <tr> <td>PDP</td> <td>Medium</td> <td>Global</td> <td>Yes</td> <td>Yes (2D)</td> </tr> </tbody> </table> <p><strong>Custom Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>drop_column_importance</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>metric</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    Measure importance by training without each feature</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=n>baseline</span> <span class=o>=</span> <span class=n>metric</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
    <span class=n>importances</span> <span class=o>=</span> <span class=p>{}</span>

    <span class=k>for</span> <span class=n>col</span> <span class=ow>in</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>:</span>
        <span class=c1># Drop column</span>
        <span class=n>X_reduced</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=n>col</span><span class=p>])</span>

        <span class=c1># Retrain model</span>
        <span class=n>model_reduced</span> <span class=o>=</span> <span class=n>clone</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
        <span class=n>model_reduced</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_reduced</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

        <span class=c1># Evaluate</span>
        <span class=n>score</span> <span class=o>=</span> <span class=n>metric</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>model_reduced</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_reduced</span><span class=p>))</span>

        <span class=c1># Importance = performance drop</span>
        <span class=n>importances</span><span class=p>[</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>baseline</span> <span class=o>-</span> <span class=n>score</span>

    <span class=k>return</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>(</span><span class=n>importances</span><span class=p>)</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Usage</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>accuracy_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.base</span><span class=w> </span><span class=kn>import</span> <span class=n>clone</span>

<span class=n>importances</span> <span class=o>=</span> <span class=n>drop_column_importance</span><span class=p>(</span><span class=n>rf</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>accuracy_score</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>importances</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Model interpretation skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Multiple methods (tree, permutation, SHAP)</li> <li>"Permutation more reliable than Gini"</li> <li>SHAP for individual predictions</li> <li>"Linear coefs only for linear models"</li> <li>Mentions computational cost</li> </ul> </div> </details> <hr> <h3 id=what-is-online-learning-google-meta-interview-question>What is Online Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Online Learning</code>, <code>Streaming</code>, <code>Model Updates</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Online Learning:</strong></p> <p>Train models incrementally as new data arrives, without retraining from scratch.</p> <p><strong>Batch vs Online:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Batch Learning</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_all</span><span class=p>,</span> <span class=n>y_all</span><span class=p>)</span>  <span class=c1># Train on all data once</span>

<span class=c1># Online Learning</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>data_stream</span><span class=p>:</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>  <span class=c1># Update incrementally</span>
</code></pre></div> <p><strong>Scikit-Learn Online Learning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>SGDClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Models supporting partial_fit</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log&#39;</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>

<span class=c1># Simulate data stream</span>
<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_batches</span> <span class=o>=</span> <span class=mi>50</span>

<span class=c1># Initialize on first batch</span>
<span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>generate_batch</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>
<span class=n>scaler</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y_batch</span><span class=p>))</span>

<span class=c1># Online updates</span>
<span class=n>accuracies</span> <span class=o>=</span> <span class=p>[]</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_batches</span><span class=p>):</span>
    <span class=c1># Get new data</span>
    <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>generate_batch</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>

    <span class=c1># Update scaler and transform</span>
    <span class=n>scaler</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
    <span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>

    <span class=c1># Evaluate before update</span>
    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
    <span class=n>accuracies</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>

    <span class=c1># Update model</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

<span class=c1># Plot learning curve</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>accuracies</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Batch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Online Learning Performance&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Models Supporting Online Learning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>SGDClassifier</span><span class=p>,</span> <span class=n>SGDRegressor</span><span class=p>,</span> <span class=n>PassiveAggressiveClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.naive_bayes</span><span class=w> </span><span class=kn>import</span> <span class=n>MultinomialNB</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.cluster</span><span class=w> </span><span class=kn>import</span> <span class=n>MiniBatchKMeans</span>

<span class=c1># Classification</span>
<span class=n>online_classifiers</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;SGD&#39;</span><span class=p>:</span> <span class=n>SGDClassifier</span><span class=p>(),</span>
    <span class=s1>&#39;Passive-Aggressive&#39;</span><span class=p>:</span> <span class=n>PassiveAggressiveClassifier</span><span class=p>(),</span>
    <span class=s1>&#39;Naive Bayes&#39;</span><span class=p>:</span> <span class=n>MultinomialNB</span><span class=p>()</span>
<span class=p>}</span>

<span class=c1># Regression</span>
<span class=n>online_regressors</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;SGD&#39;</span><span class=p>:</span> <span class=n>SGDRegressor</span><span class=p>()</span>
<span class=p>}</span>

<span class=c1># Clustering</span>
<span class=n>online_clustering</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;MiniBatch K-Means&#39;</span><span class=p>:</span> <span class=n>MiniBatchKMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=p>}</span>
</code></pre></div> <p><strong>Custom Online Model:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>OnlineLogisticRegression</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Simple online logistic regression&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_features</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.01</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>weights</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span> <span class=o>=</span> <span class=n>learning_rate</span>

    <span class=k>def</span><span class=w> </span><span class=nf>sigmoid</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
        <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>predict_proba</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>weights</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
        <span class=k>return</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>&gt;=</span> <span class=mf>0.5</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>partial_fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Update weights with one batch&quot;&quot;&quot;</span>
        <span class=n>n_samples</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

        <span class=c1># Forward pass</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

        <span class=c1># Compute gradients</span>
        <span class=n>error</span> <span class=o>=</span> <span class=n>y_pred</span> <span class=o>-</span> <span class=n>y</span>
        <span class=n>grad_w</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span> <span class=o>/</span> <span class=n>n_samples</span>
        <span class=n>grad_b</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>error</span><span class=p>)</span>

        <span class=c1># Update weights (gradient descent)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>weights</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>grad_w</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>grad_b</span>

        <span class=k>return</span> <span class=bp>self</span>

<span class=c1># Usage</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>OnlineLogisticRegression</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>data_stream</span><span class=p>:</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</code></pre></div> <p><strong>Concept Drift Handling:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>river</span><span class=w> </span><span class=kn>import</span> <span class=n>drift</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Detect when data distribution changes</span>
<span class=k>class</span><span class=w> </span><span class=nc>DriftDetector</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>window_size</span> <span class=o>=</span> <span class=n>window_size</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>reference_window</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>current_window</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>error</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Add new error observation&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>current_window</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>error</span><span class=p>)</span>

        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>current_window</span><span class=p>)</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>window_size</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>current_window</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Check for drift</span>
        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>reference_window</span><span class=p>)</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>window_size</span><span class=p>:</span>
            <span class=n>drift_detected</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>detect_drift</span><span class=p>()</span>

            <span class=k>if</span> <span class=n>drift_detected</span><span class=p>:</span>
                <span class=c1># Reset reference</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>reference_window</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>current_window</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
                <span class=k>return</span> <span class=kc>True</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>reference_window</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>error</span><span class=p>)</span>

        <span class=k>return</span> <span class=kc>False</span>

    <span class=k>def</span><span class=w> </span><span class=nf>detect_drift</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compare distributions using KS test&quot;&quot;&quot;</span>
        <span class=kn>from</span><span class=w> </span><span class=nn>scipy.stats</span><span class=w> </span><span class=kn>import</span> <span class=n>ks_2samp</span>

        <span class=n>stat</span><span class=p>,</span> <span class=n>p_value</span> <span class=o>=</span> <span class=n>ks_2samp</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>reference_window</span><span class=p>,</span> 
                                  <span class=bp>self</span><span class=o>.</span><span class=n>current_window</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>p_value</span> <span class=o>&lt;</span> <span class=mf>0.05</span>  <span class=c1># Significant difference</span>

<span class=c1># Usage</span>
<span class=n>detector</span> <span class=o>=</span> <span class=n>DriftDetector</span><span class=p>(</span><span class=n>window_size</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>()</span>

<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>data_stream</span><span class=p>:</span>
    <span class=c1># Predict</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>

    <span class=c1># Check for errors</span>
    <span class=n>errors</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_pred</span> <span class=o>!=</span> <span class=n>y_batch</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>

    <span class=k>for</span> <span class=n>error</span> <span class=ow>in</span> <span class=n>errors</span><span class=p>:</span>
        <span class=n>drift</span> <span class=o>=</span> <span class=n>detector</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>error</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>drift</span><span class=p>:</span>
            <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Concept drift detected! Retraining...&quot;</span><span class=p>)</span>
            <span class=c1># Could reset model or adjust learning rate</span>
            <span class=n>model</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>()</span>

    <span class=c1># Update model</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</code></pre></div> <p><strong>Evaluation Strategies:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Prequential Evaluation (Test-then-Train)</span>
<span class=k>def</span><span class=w> </span><span class=nf>prequential_evaluation</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>data_stream</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Evaluate then update&quot;&quot;&quot;</span>
    <span class=n>scores</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>data_stream</span><span class=p>:</span>
        <span class=c1># Test on new data</span>
        <span class=n>score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
        <span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>

        <span class=c1># Then train</span>
        <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>scores</span>

<span class=c1># 2. Sliding Window</span>
<span class=k>def</span><span class=w> </span><span class=nf>sliding_window_eval</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>data_stream</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Evaluate on recent window&quot;&quot;&quot;</span>
    <span class=n>window_X</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>window_y</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>scores</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>data_stream</span><span class=p>:</span>
        <span class=c1># Update model</span>
        <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

        <span class=c1># Add to window</span>
        <span class=n>window_X</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
        <span class=n>window_y</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>y_batch</span><span class=p>)</span>

        <span class=c1># Maintain window size</span>
        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>window_X</span><span class=p>)</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>window_size</span><span class=p>:</span>
            <span class=n>window_X</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
            <span class=n>window_y</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Evaluate on window</span>
        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>window_X</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>X_window</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>(</span><span class=n>window_X</span><span class=p>)</span>
            <span class=n>y_window</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>(</span><span class=n>window_y</span><span class=p>)</span>
            <span class=n>score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_window</span><span class=p>,</span> <span class=n>y_window</span><span class=p>)</span>
            <span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>scores</span>
</code></pre></div> <p><strong>Use Cases:</strong></p> <table> <thead> <tr> <th>Application</th> <th>Reason</th> </tr> </thead> <tbody> <tr> <td>Recommendation systems</td> <td>User preferences change</td> </tr> <tr> <td>Fraud detection</td> <td>New fraud patterns emerge</td> </tr> <tr> <td>Stock prediction</td> <td>Market conditions evolve</td> </tr> <tr> <td>Ad click prediction</td> <td>User behavior shifts</td> </tr> <tr> <td>IoT sensor data</td> <td>Continuous streaming</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Real-time ML systems.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>partial_fit() for incremental updates</li> <li>"Don't retrain from scratch"</li> <li>Concept drift handling</li> <li>Prequential evaluation</li> <li>SGD-based models work well</li> <li>Trade-off: speed vs accuracy</li> </ul> </div> </details> <hr> <h3 id=explain-hyperparameter-tuning-methods-most-tech-companies-interview-question>Explain Hyperparameter Tuning Methods - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Hyperparameters</code>, <code>Optimization</code>, <code>Model Selection</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Hyperparameter Tuning:</strong></p> <p>Finding optimal hyperparameter values to maximize model performance.</p> <p><strong>1. Grid Search:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GridSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Define parameter grid</span>
<span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>],</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>30</span><span class=p>],</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span>
    <span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
    <span class=s1>&#39;max_features&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;auto&#39;</span><span class=p>,</span> <span class=s1>&#39;sqrt&#39;</span><span class=p>,</span> <span class=s1>&#39;log2&#39;</span><span class=p>]</span>
<span class=p>}</span>

<span class=c1># Grid search</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>param_grid</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;f1&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
    <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best parameters: </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best score: </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_score_</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Use best model</span>
<span class=n>best_model</span> <span class=o>=</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span>
</code></pre></div> <p><strong>2. Random Search:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomizedSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.stats</span><span class=w> </span><span class=kn>import</span> <span class=n>randint</span><span class=p>,</span> <span class=n>uniform</span>

<span class=c1># Define distributions</span>
<span class=n>param_distributions</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>50</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>randint</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span><span class=o>.</span><span class=n>rvs</span><span class=p>(</span><span class=mi>10</span><span class=p>)),</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
    <span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
    <span class=s1>&#39;max_features&#39;</span><span class=p>:</span> <span class=n>uniform</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>),</span>  <span class=c1># Fraction</span>
    <span class=s1>&#39;bootstrap&#39;</span><span class=p>:</span> <span class=p>[</span><span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>]</span>
<span class=p>}</span>

<span class=c1># Random search (more efficient)</span>
<span class=n>random_search</span> <span class=o>=</span> <span class=n>RandomizedSearchCV</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>param_distributions</span><span class=p>,</span>
    <span class=n>n_iter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>  <span class=c1># Number of random combinations</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;f1&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>random_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best parameters: </span><span class=si>{</span><span class=n>random_search</span><span class=o>.</span><span class=n>best_params_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>3. Bayesian Optimization:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>skopt</span><span class=w> </span><span class=kn>import</span> <span class=n>BayesSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>skopt.space</span><span class=w> </span><span class=kn>import</span> <span class=n>Real</span><span class=p>,</span> <span class=n>Categorical</span><span class=p>,</span> <span class=n>Integer</span>

<span class=c1># Define search space</span>
<span class=n>search_spaces</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>Integer</span><span class=p>(</span><span class=mi>50</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>Integer</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=n>Integer</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
    <span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>:</span> <span class=n>Integer</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
    <span class=s1>&#39;max_features&#39;</span><span class=p>:</span> <span class=n>Real</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>),</span>
    <span class=s1>&#39;bootstrap&#39;</span><span class=p>:</span> <span class=n>Categorical</span><span class=p>([</span><span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>])</span>
<span class=p>}</span>

<span class=c1># Bayesian optimization (most efficient)</span>
<span class=n>bayes_search</span> <span class=o>=</span> <span class=n>BayesSearchCV</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>search_spaces</span><span class=p>,</span>
    <span class=n>n_iter</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>  <span class=c1># Fewer iterations needed</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;f1&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>bayes_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best parameters: </span><span class=si>{</span><span class=n>bayes_search</span><span class=o>.</span><span class=n>best_params_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>4. Optuna (Modern Bayesian):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>optuna</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>

<span class=k>def</span><span class=w> </span><span class=nf>objective</span><span class=p>(</span><span class=n>trial</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Objective function for Optuna&quot;&quot;&quot;</span>

    <span class=c1># Suggest hyperparameters</span>
    <span class=n>params</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
        <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;max_depth&#39;</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>50</span><span class=p>),</span>
        <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;min_samples_split&#39;</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
        <span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
        <span class=s1>&#39;max_features&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_float</span><span class=p>(</span><span class=s1>&#39;max_features&#39;</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>),</span>
        <span class=s1>&#39;bootstrap&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_categorical</span><span class=p>(</span><span class=s1>&#39;bootstrap&#39;</span><span class=p>,</span> <span class=p>[</span><span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>])</span>
    <span class=p>}</span>

    <span class=c1># Train and evaluate</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=o>**</span><span class=n>params</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> 
                            <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;f1&#39;</span><span class=p>,</span> <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

    <span class=k>return</span> <span class=n>score</span>

<span class=c1># Create study</span>
<span class=n>study</span> <span class=o>=</span> <span class=n>optuna</span><span class=o>.</span><span class=n>create_study</span><span class=p>(</span><span class=n>direction</span><span class=o>=</span><span class=s1>&#39;maximize&#39;</span><span class=p>)</span>
<span class=n>study</span><span class=o>.</span><span class=n>optimize</span><span class=p>(</span><span class=n>objective</span><span class=p>,</span> <span class=n>n_trials</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best parameters: </span><span class=si>{</span><span class=n>study</span><span class=o>.</span><span class=n>best_params</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best score: </span><span class=si>{</span><span class=n>study</span><span class=o>.</span><span class=n>best_value</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize optimization</span>
<span class=kn>from</span><span class=w> </span><span class=nn>optuna.visualization</span><span class=w> </span><span class=kn>import</span> <span class=n>plot_optimization_history</span><span class=p>,</span> <span class=n>plot_param_importances</span>

<span class=n>plot_optimization_history</span><span class=p>(</span><span class=n>study</span><span class=p>)</span>
<span class=n>plot_param_importances</span><span class=p>(</span><span class=n>study</span><span class=p>)</span>
</code></pre></div> <p><strong>5. Halving Search (Successive Halving):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.experimental</span><span class=w> </span><span class=kn>import</span> <span class=n>enable_halving_search_cv</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>HalvingRandomSearchCV</span>

<span class=c1># Efficiently discard bad candidates early</span>
<span class=n>halving_search</span> <span class=o>=</span> <span class=n>HalvingRandomSearchCV</span><span class=p>(</span>
    <span class=n>rf</span><span class=p>,</span> <span class=n>param_distributions</span><span class=p>,</span>
    <span class=n>factor</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>  <span class=c1># Reduce candidates by 1/3 each iteration</span>
    <span class=n>resource</span><span class=o>=</span><span class=s1>&#39;n_samples&#39;</span><span class=p>,</span>
    <span class=n>max_resources</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;f1&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>halving_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>time</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>

<span class=n>methods</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Grid Search&#39;</span><span class=p>:</span> <span class=n>grid_search</span><span class=p>,</span>
    <span class=s1>&#39;Random Search&#39;</span><span class=p>:</span> <span class=n>random_search</span><span class=p>,</span>
    <span class=s1>&#39;Bayesian Optimization&#39;</span><span class=p>:</span> <span class=n>bayes_search</span>
<span class=p>}</span>

<span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>search</span> <span class=ow>in</span> <span class=n>methods</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
    <span class=n>search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
    <span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>start_time</span>

    <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
        <span class=s1>&#39;Method&#39;</span><span class=p>:</span> <span class=n>name</span><span class=p>,</span>
        <span class=s1>&#39;Best Score&#39;</span><span class=p>:</span> <span class=n>search</span><span class=o>.</span><span class=n>best_score_</span><span class=p>,</span>
        <span class=s1>&#39;Time (s)&#39;</span><span class=p>:</span> <span class=n>elapsed</span><span class=p>,</span>
        <span class=s1>&#39;Iterations&#39;</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>search</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>[</span><span class=s1>&#39;params&#39;</span><span class=p>])</span>
    <span class=p>})</span>

<span class=n>results_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>results_df</span><span class=p>)</span>
</code></pre></div> <table> <thead> <tr> <th>Method</th> <th>Pros</th> <th>Cons</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td>Grid Search</td> <td>Exhaustive, reproducible</td> <td>Exponentially slow</td> <td>Small parameter space</td> </tr> <tr> <td>Random Search</td> <td>Fast, good for high-dim</td> <td>May miss optimal</td> <td>Large parameter space</td> </tr> <tr> <td>Bayesian Optimization</td> <td>Most efficient, learns</td> <td>Complex setup</td> <td>Production, limited budget</td> </tr> <tr> <td>Successive Halving</td> <td>Very fast</td> <td>May discard good late bloomers</td> <td>Quick iteration</td> </tr> </tbody> </table> <p><strong>Manual Tuning Tips:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Start with defaults, tune one at a time</span>

<span class=c1># 1. Learning rate (most important for GBMs)</span>
<span class=k>for</span> <span class=n>lr</span> <span class=ow>in</span> <span class=p>[</span><span class=mf>0.001</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>]:</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;LR=</span><span class=si>{</span><span class=n>lr</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>score</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># 2. Model complexity (depth, num trees)</span>
<span class=c1># 3. Regularization (min_samples, alpha)</span>
<span class=c1># 4. Data sampling (max_features, subsample)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML optimization.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Grid search exhaustive but slow"</li> <li>"Random search better for high-dim"</li> <li>"Bayesian most efficient"</li> <li>Mentions cross-validation</li> <li>Knows which hyperparameters matter most</li> <li>"Start simple, tune iteratively"</li> </ul> </div> </details> <hr> <h3 id=what-is-model-compression-google-meta-interview-question>What is Model Compression? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Model Compression</code>, <code>Deployment</code>, <code>Optimization</code> | <strong>Asked by:</strong> Google, Meta, Apple, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>Model Compression:</strong></p> <p>Reducing model size and computational cost while maintaining performance.</p> <p><strong>1. Quantization:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.quantization</span>

<span class=c1># Original model (32-bit floats)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>

<span class=c1># Post-Training Static Quantization (8-bit integers)</span>
<span class=n>model_quantized</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantization</span><span class=o>.</span><span class=n>quantize_dynamic</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span>
    <span class=p>{</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>},</span>  <span class=c1># Layers to quantize</span>
    <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>qint8</span>
<span class=p>)</span>

<span class=c1># Compare sizes</span>
<span class=k>def</span><span class=w> </span><span class=nf>get_model_size</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
    <span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s2>&quot;temp.pth&quot;</span><span class=p>)</span>
    <span class=n>size</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>getsize</span><span class=p>(</span><span class=s2>&quot;temp.pth&quot;</span><span class=p>)</span> <span class=o>/</span> <span class=mf>1e6</span>  <span class=c1># MB</span>
    <span class=n>os</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=s2>&quot;temp.pth&quot;</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>size</span>

<span class=n>original_size</span> <span class=o>=</span> <span class=n>get_model_size</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
<span class=n>quantized_size</span> <span class=o>=</span> <span class=n>get_model_size</span><span class=p>(</span><span class=n>model_quantized</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original: </span><span class=si>{</span><span class=n>original_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Quantized: </span><span class=si>{</span><span class=n>quantized_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Compression: </span><span class=si>{</span><span class=n>original_size</span><span class=o>/</span><span class=n>quantized_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&quot;</span><span class=p>)</span>

<span class=c1># Quantization-Aware Training (better accuracy)</span>
<span class=n>model</span><span class=o>.</span><span class=n>qconfig</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantization</span><span class=o>.</span><span class=n>get_default_qat_qconfig</span><span class=p>(</span><span class=s1>&#39;fbgemm&#39;</span><span class=p>)</span>
<span class=n>model_prepared</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantization</span><span class=o>.</span><span class=n>prepare_qat</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>

<span class=c1># Train with quantization simulation</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model_prepared</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>)</span>

<span class=c1># Convert to quantized model</span>
<span class=n>model_quantized</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantization</span><span class=o>.</span><span class=n>convert</span><span class=p>(</span><span class=n>model_prepared</span><span class=p>)</span>
</code></pre></div> <p><strong>2. Pruning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.utils.prune</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>prune</span>

<span class=c1># Unstructured pruning (remove individual weights)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span>

<span class=c1># Prune 30% of weights in linear layer</span>
<span class=n>prune</span><span class=o>.</span><span class=n>l1_unstructured</span><span class=p>(</span>
    <span class=n>module</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>fc1</span><span class=p>,</span>
    <span class=n>name</span><span class=o>=</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span>
    <span class=n>amount</span><span class=o>=</span><span class=mf>0.3</span>
<span class=p>)</span>

<span class=c1># Prune multiple layers</span>
<span class=n>parameters_to_prune</span> <span class=o>=</span> <span class=p>(</span>
    <span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>fc1</span><span class=p>,</span> <span class=s1>&#39;weight&#39;</span><span class=p>),</span>
    <span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>fc2</span><span class=p>,</span> <span class=s1>&#39;weight&#39;</span><span class=p>),</span>
    <span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>fc3</span><span class=p>,</span> <span class=s1>&#39;weight&#39;</span><span class=p>)</span>
<span class=p>)</span>

<span class=n>prune</span><span class=o>.</span><span class=n>global_unstructured</span><span class=p>(</span>
    <span class=n>parameters_to_prune</span><span class=p>,</span>
    <span class=n>pruning_method</span><span class=o>=</span><span class=n>prune</span><span class=o>.</span><span class=n>L1Unstructured</span><span class=p>,</span>
    <span class=n>amount</span><span class=o>=</span><span class=mf>0.3</span>  <span class=c1># 30% of all weights</span>
<span class=p>)</span>

<span class=c1># Make pruning permanent</span>
<span class=k>for</span> <span class=n>module</span><span class=p>,</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>parameters_to_prune</span><span class=p>:</span>
    <span class=n>prune</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>name</span><span class=p>)</span>

<span class=c1># Structured pruning (remove entire filters)</span>
<span class=n>prune</span><span class=o>.</span><span class=n>ln_structured</span><span class=p>(</span>
    <span class=n>module</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>conv1</span><span class=p>,</span>
    <span class=n>name</span><span class=o>=</span><span class=s1>&#39;weight&#39;</span><span class=p>,</span>
    <span class=n>amount</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span>
    <span class=n>n</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>  <span class=c1># L2 norm</span>
    <span class=n>dim</span><span class=o>=</span><span class=mi>0</span>  <span class=c1># Prune output channels</span>
<span class=p>)</span>
</code></pre></div> <p><strong>3. Knowledge Distillation:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Already covered in detail in previous question</span>
<span class=c1># Large teacher -&gt; Small student</span>

<span class=k>class</span><span class=w> </span><span class=nc>DistillationLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>3.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>=</span> <span class=n>temperature</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>kl_div</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>KLDivLoss</span><span class=p>(</span><span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;batchmean&#39;</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>ce_loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>student_logits</span><span class=p>,</span> <span class=n>teacher_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
        <span class=c1># Soft targets from teacher</span>
        <span class=n>soft_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kl_div</span><span class=p>(</span>
            <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>student_logits</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
            <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>teacher_logits</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>temperature</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>temperature</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Hard targets</span>
        <span class=n>hard_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ce_loss</span><span class=p>(</span><span class=n>student_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>soft_loss</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>hard_loss</span>
</code></pre></div> <p><strong>4. Low-Rank Factorization:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>LowRankLinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Decompose weight matrix W = U @ V&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>rank</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># W (out x in) ‚âà U (out x rank) @ V (rank x in)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>U</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>out_features</span><span class=p>,</span> <span class=n>rank</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>in_features</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>out_features</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># x @ V^T @ U^T + b</span>
        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>V</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>U</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>

<span class=c1># Replace linear layer</span>
<span class=k>def</span><span class=w> </span><span class=nf>replace_with_low_rank</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>rank</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>child</span> <span class=ow>in</span> <span class=n>module</span><span class=o>.</span><span class=n>named_children</span><span class=p>():</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>child</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>
            <span class=n>in_features</span> <span class=o>=</span> <span class=n>child</span><span class=o>.</span><span class=n>in_features</span>
            <span class=n>out_features</span> <span class=o>=</span> <span class=n>child</span><span class=o>.</span><span class=n>out_features</span>

            <span class=c1># Replace</span>
            <span class=nb>setattr</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>name</span><span class=p>,</span> 
                   <span class=n>LowRankLinear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>rank</span><span class=p>))</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>replace_with_low_rank</span><span class=p>(</span><span class=n>child</span><span class=p>,</span> <span class=n>rank</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span>
<span class=n>replace_with_low_rank</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>

<span class=c1># Compression ratio</span>
<span class=n>original_params</span> <span class=o>=</span> <span class=n>in_features</span> <span class=o>*</span> <span class=n>out_features</span>
<span class=n>compressed_params</span> <span class=o>=</span> <span class=n>rank</span> <span class=o>*</span> <span class=p>(</span><span class=n>in_features</span> <span class=o>+</span> <span class=n>out_features</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Compression: </span><span class=si>{</span><span class=n>original_params</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=n>compressed_params</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>5. Neural Architecture Search (Compact Models):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Find efficient architectures (e.g., MobileNet, EfficientNet)</span>

<span class=c1># MobileNet: Depthwise Separable Convolutions</span>
<span class=k>class</span><span class=w> </span><span class=nc>DepthwiseSeparableConv</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Depthwise: one filter per input channel</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>depthwise</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
            <span class=n>in_channels</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span>
            <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
            <span class=n>groups</span><span class=o>=</span><span class=n>in_channels</span>  <span class=c1># Key: groups = in_channels</span>
        <span class=p>)</span>

        <span class=c1># Pointwise: 1x1 conv to combine</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>pointwise</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
            <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span>
            <span class=n>kernel_size</span><span class=o>=</span><span class=mi>1</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>depthwise</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pointwise</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Parameters comparison</span>
<span class=c1># Standard conv: k*k*in*out</span>
<span class=c1># Depthwise separable: k*k*in + in*out</span>
<span class=c1># Compression: ~8-9x for 3x3 kernels</span>
</code></pre></div> <p><strong>Comprehensive Compression Pipeline:</strong></p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>compress_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Apply multiple compression techniques&quot;&quot;&quot;</span>

    <span class=c1># 1. Pruning</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Step 1: Pruning...&quot;</span><span class=p>)</span>
    <span class=n>parameters_to_prune</span> <span class=o>=</span> <span class=p>[(</span><span class=n>module</span><span class=p>,</span> <span class=s1>&#39;weight&#39;</span><span class=p>)</span> 
                           <span class=k>for</span> <span class=n>module</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>modules</span><span class=p>()</span> 
                           <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)]</span>

    <span class=n>prune</span><span class=o>.</span><span class=n>global_unstructured</span><span class=p>(</span>
        <span class=n>parameters_to_prune</span><span class=p>,</span>
        <span class=n>pruning_method</span><span class=o>=</span><span class=n>prune</span><span class=o>.</span><span class=n>L1Unstructured</span><span class=p>,</span>
        <span class=n>amount</span><span class=o>=</span><span class=mf>0.3</span>
    <span class=p>)</span>

    <span class=c1># Fine-tune after pruning</span>
    <span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>

    <span class=c1># 2. Quantization</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Step 2: Quantization...&quot;</span><span class=p>)</span>
    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
    <span class=n>model_quantized</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantization</span><span class=o>.</span><span class=n>quantize_dynamic</span><span class=p>(</span>
        <span class=n>model</span><span class=p>,</span> <span class=p>{</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>},</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>qint8</span>
    <span class=p>)</span>

    <span class=c1># 3. Evaluate</span>
    <span class=n>original_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span>
    <span class=n>compressed_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model_quantized</span><span class=p>,</span> <span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)</span>

    <span class=n>original_size</span> <span class=o>=</span> <span class=n>get_model_size</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=n>compressed_size</span> <span class=o>=</span> <span class=n>get_model_size</span><span class=p>(</span><span class=n>model_quantized</span><span class=p>)</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Results:&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Accuracy: </span><span class=si>{</span><span class=n>original_acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> -&gt; </span><span class=si>{</span><span class=n>compressed_acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Size: </span><span class=si>{</span><span class=n>original_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB -&gt; </span><span class=si>{</span><span class=n>compressed_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Compression: </span><span class=si>{</span><span class=n>original_size</span><span class=o>/</span><span class=n>compressed_size</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&quot;</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>model_quantized</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>Compression</th> <th>Accuracy Loss</th> <th>Speed Up</th> </tr> </thead> <tbody> <tr> <td>Quantization (INT8)</td> <td>4x</td> <td>&lt;1%</td> <td>2-4x</td> </tr> <tr> <td>Pruning (50%)</td> <td>2x</td> <td>&lt;2%</td> <td>1.5-2x</td> </tr> <tr> <td>Knowledge Distillation</td> <td>10-100x</td> <td>2-5%</td> <td>10-100x</td> </tr> <tr> <td>Low-Rank</td> <td>2-5x</td> <td>1-3%</td> <td>1.5-3x</td> </tr> <tr> <td>Combined</td> <td>10-50x</td> <td>3-8%</td> <td>5-20x</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deployment optimization.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Multiple techniques (quantization, pruning, distillation)</li> <li>"INT8 quantization: 4x smaller"</li> <li>"Pruning: remove redundant weights"</li> <li>"Distillation: transfer knowledge to small model"</li> <li>Mentions accuracy-size trade-off</li> <li>"Combine techniques for best results"</li> </ul> </div> </details> <hr> <h3 id=explain-metrics-for-imbalanced-classification-most-tech-companies-interview-question>Explain Metrics for Imbalanced Classification - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Metrics</code>, <code>Imbalanced Data</code>, <code>Evaluation</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Metrics for Imbalanced Data:</strong></p> <p>Standard accuracy misleads when classes are imbalanced (e.g., 99% negative, 1% positive).</p> <p><strong>Problem with Accuracy:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Dataset: 990 negative, 10 positive samples</span>
<span class=n>y_true</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=mi>990</span> <span class=o>+</span> <span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=mi>10</span>

<span class=c1># Dummy classifier: always predict negative</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=mi>1000</span>

<span class=n>accuracy</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_true</span> <span class=o>==</span> <span class=n>y_pred</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Accuracy: </span><span class=si>{</span><span class=n>accuracy</span><span class=si>:</span><span class=s2>.1%</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>  <span class=c1># 99%!</span>

<span class=c1># But it catches 0% of positive class!</span>
</code></pre></div> <p><strong>Better Metrics:</strong></p> <p><strong>1. Confusion Matrix Metrics:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>confusion_matrix</span><span class=p>,</span> <span class=n>classification_report</span>
<span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>

<span class=c1># Compute confusion matrix</span>
<span class=n>cm</span> <span class=o>=</span> <span class=n>confusion_matrix</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=c1># Visualize</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>cm</span><span class=p>,</span> <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>fmt</span><span class=o>=</span><span class=s1>&#39;d&#39;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;Blues&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Predicted&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Actual&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Confusion Matrix&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Extract metrics</span>
<span class=n>tn</span><span class=p>,</span> <span class=n>fp</span><span class=p>,</span> <span class=n>fn</span><span class=p>,</span> <span class=n>tp</span> <span class=o>=</span> <span class=n>cm</span><span class=o>.</span><span class=n>ravel</span><span class=p>()</span>

<span class=n>precision</span> <span class=o>=</span> <span class=n>tp</span> <span class=o>/</span> <span class=p>(</span><span class=n>tp</span> <span class=o>+</span> <span class=n>fp</span><span class=p>)</span> <span class=k>if</span> <span class=p>(</span><span class=n>tp</span> <span class=o>+</span> <span class=n>fp</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>0</span>
<span class=n>recall</span> <span class=o>=</span> <span class=n>tp</span> <span class=o>/</span> <span class=p>(</span><span class=n>tp</span> <span class=o>+</span> <span class=n>fn</span><span class=p>)</span> <span class=k>if</span> <span class=p>(</span><span class=n>tp</span> <span class=o>+</span> <span class=n>fn</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>0</span>
<span class=n>specificity</span> <span class=o>=</span> <span class=n>tn</span> <span class=o>/</span> <span class=p>(</span><span class=n>tn</span> <span class=o>+</span> <span class=n>fp</span><span class=p>)</span> <span class=k>if</span> <span class=p>(</span><span class=n>tn</span> <span class=o>+</span> <span class=n>fp</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>0</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Precision: </span><span class=si>{</span><span class=n>precision</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Recall: </span><span class=si>{</span><span class=n>recall</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Specificity: </span><span class=si>{</span><span class=n>specificity</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>2. F1, F-beta Scores:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>f1_score</span><span class=p>,</span> <span class=n>fbeta_score</span><span class=p>,</span> <span class=n>precision_recall_fscore_support</span>

<span class=c1># F1: Harmonic mean of precision and recall</span>
<span class=n>f1</span> <span class=o>=</span> <span class=n>f1_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=c1># F-beta: Weight recall more (beta&gt;1) or precision more (beta&lt;1)</span>
<span class=n>f_half</span> <span class=o>=</span> <span class=n>fbeta_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># Emphasize precision</span>
<span class=n>f2</span> <span class=o>=</span> <span class=n>fbeta_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mf>2.0</span><span class=p>)</span>      <span class=c1># Emphasize recall</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;F1: </span><span class=si>{</span><span class=n>f1</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;F0.5 (precision-focused): </span><span class=si>{</span><span class=n>f_half</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;F2 (recall-focused): </span><span class=si>{</span><span class=n>f2</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># When to use:</span>
<span class=c1># - F1: Balanced importance</span>
<span class=c1># - F0.5: False positives costly (spam detection)</span>
<span class=c1># - F2: False negatives costly (disease detection)</span>
</code></pre></div> <p><strong>3. ROC-AUC:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>roc_curve</span><span class=p>,</span> <span class=n>roc_auc_score</span><span class=p>,</span> <span class=n>auc</span>

<span class=c1># Need probability scores</span>
<span class=n>y_scores</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Compute ROC curve</span>
<span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>roc_curve</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>
<span class=n>roc_auc</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;ROC (AUC = </span><span class=si>{</span><span class=n>roc_auc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Random&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;False Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;True Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;ROC Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Interpretation:</span>
<span class=c1># AUC = 1.0: Perfect</span>
<span class=c1># AUC = 0.5: Random</span>
<span class=c1># AUC &lt; 0.5: Worse than random (flip predictions!)</span>
</code></pre></div> <p><strong>4. Precision-Recall AUC (Better for Imbalanced):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>precision_recall_curve</span><span class=p>,</span> <span class=n>average_precision_score</span>

<span class=c1># PR curve more informative than ROC for imbalanced data</span>
<span class=n>precision</span><span class=p>,</span> <span class=n>recall</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>
<span class=n>pr_auc</span> <span class=o>=</span> <span class=n>average_precision_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>recall</span><span class=p>,</span> <span class=n>precision</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;PR (AUC = </span><span class=si>{</span><span class=n>pr_auc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>

<span class=c1># Baseline: proportion of positive class</span>
<span class=n>baseline</span> <span class=o>=</span> <span class=n>y_true</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>baseline</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> 
            <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;Baseline (</span><span class=si>{</span><span class=n>baseline</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Recall&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Precision&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Precision-Recall Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>5. Matthews Correlation Coefficient (MCC):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>matthews_corrcoef</span>

<span class=c1># Single metric for imbalanced data</span>
<span class=c1># Range: -1 (worst) to +1 (best), 0 = random</span>
<span class=n>mcc</span> <span class=o>=</span> <span class=n>matthews_corrcoef</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MCC: </span><span class=si>{</span><span class=n>mcc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Formula:</span>
<span class=c1># MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))</span>
</code></pre></div> <p><strong>6. Cohen's Kappa:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>cohen_kappa_score</span>

<span class=c1># Agreement beyond chance</span>
<span class=n>kappa</span> <span class=o>=</span> <span class=n>cohen_kappa_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Kappa: </span><span class=si>{</span><span class=n>kappa</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Interpretation:</span>
<span class=c1># &lt; 0: Worse than random</span>
<span class=c1># 0-0.2: Slight agreement</span>
<span class=c1># 0.2-0.4: Fair</span>
<span class=c1># 0.4-0.6: Moderate</span>
<span class=c1># 0.6-0.8: Substantial</span>
<span class=c1># 0.8-1.0: Almost perfect</span>
</code></pre></div> <p><strong>7. Balanced Accuracy:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>balanced_accuracy_score</span>

<span class=c1># Average of recall for each class</span>
<span class=n>balanced_acc</span> <span class=o>=</span> <span class=n>balanced_accuracy_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Balanced Accuracy: </span><span class=si>{</span><span class=n>balanced_acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Avoids being misled by imbalance</span>
</code></pre></div> <p><strong>Comprehensive Evaluation:</strong></p> <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>evaluate_imbalanced</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>y_scores</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Complete evaluation for imbalanced classification&quot;&quot;&quot;</span>

    <span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
        <span class=n>accuracy_score</span><span class=p>,</span> <span class=n>precision_score</span><span class=p>,</span> <span class=n>recall_score</span><span class=p>,</span> 
        <span class=n>f1_score</span><span class=p>,</span> <span class=n>roc_auc_score</span><span class=p>,</span> <span class=n>average_precision_score</span><span class=p>,</span>
        <span class=n>matthews_corrcoef</span><span class=p>,</span> <span class=n>cohen_kappa_score</span><span class=p>,</span> <span class=n>balanced_accuracy_score</span>
    <span class=p>)</span>

    <span class=n>metrics</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;Accuracy&#39;</span><span class=p>:</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;Balanced Accuracy&#39;</span><span class=p>:</span> <span class=n>balanced_accuracy_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;Precision&#39;</span><span class=p>:</span> <span class=n>precision_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;Recall&#39;</span><span class=p>:</span> <span class=n>recall_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;F1&#39;</span><span class=p>:</span> <span class=n>f1_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;MCC&#39;</span><span class=p>:</span> <span class=n>matthews_corrcoef</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>),</span>
        <span class=s1>&#39;Kappa&#39;</span><span class=p>:</span> <span class=n>cohen_kappa_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
    <span class=p>}</span>

    <span class=k>if</span> <span class=n>y_scores</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;ROC-AUC&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>
        <span class=n>metrics</span><span class=p>[</span><span class=s1>&#39;PR-AUC&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>average_precision_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>

    <span class=c1># Print table</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Evaluation Metrics:&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;-&quot;</span> <span class=o>*</span> <span class=mi>40</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>metric</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=n>metrics</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>metric</span><span class=si>:</span><span class=s2>20s</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>value</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>metrics</span>

<span class=c1># Usage</span>
<span class=n>metrics</span> <span class=o>=</span> <span class=n>evaluate_imbalanced</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>
</code></pre></div> <p><strong>Metric Selection Guide:</strong></p> <table> <thead> <tr> <th>Use Case</th> <th>Recommended Metrics</th> <th>Reason</th> </tr> </thead> <tbody> <tr> <td>Medical diagnosis</td> <td>Recall, F2, PR-AUC</td> <td>Minimize false negatives</td> </tr> <tr> <td>Spam detection</td> <td>Precision, F0.5</td> <td>Minimize false positives</td> </tr> <tr> <td>Fraud detection</td> <td>PR-AUC, MCC</td> <td>Extremely imbalanced</td> </tr> <tr> <td>General imbalanced</td> <td>F1, Balanced Accuracy, MCC</td> <td>Balanced view</td> </tr> <tr> <td>Ranking</td> <td>ROC-AUC, PR-AUC</td> <td>Threshold-independent</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of evaluation metrics.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Accuracy misleading for imbalanced data"</li> <li>Precision vs Recall trade-off</li> <li>"PR-AUC better than ROC-AUC for imbalanced"</li> <li>F-beta for different priorities</li> <li>MCC: single balanced metric</li> <li>"Choose metric based on business cost"</li> </ul> </div> </details> <hr> <h3 id=what-is-multi-task-learning-google-meta-interview-question>What is Multi-Task Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Multi-Task Learning</code>, <code>Transfer Learning</code>, <code>Neural Networks</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Multi-Task Learning (MTL):</strong></p> <p>Train a single model on multiple related tasks simultaneously to improve generalization.</p> <p><strong>Key Idea:</strong></p> <ul> <li>Shared representations help all tasks</li> <li>"What is learned for one task can help other tasks"</li> </ul> <p><strong>Architecture:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>MultiTaskModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Multi-task learning with shared encoder&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>num_tasks</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Shared encoder (learns general representations)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>shared_encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>
        <span class=p>)</span>

        <span class=c1># Task-specific heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>task_heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># Binary classification</span>
            <span class=p>)</span>
            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_tasks</span><span class=p>)</span>
        <span class=p>])</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=c1># Shared encoding</span>
        <span class=n>shared_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared_encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Task-specific predictions</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=p>[</span><span class=n>head</span><span class=p>(</span><span class=n>shared_features</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>task_heads</span><span class=p>]</span>

        <span class=k>return</span> <span class=n>outputs</span>

<span class=c1># Training</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>MultiTaskModel</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>num_tasks</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_tasks</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=c1># y_tasks: list of targets for each task</span>

        <span class=c1># Forward</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>

        <span class=c1># Compute loss for each task</span>
        <span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>task_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>outputs</span><span class=p>)):</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>binary_cross_entropy_with_logits</span><span class=p>(</span>
                <span class=n>outputs</span><span class=p>[</span><span class=n>task_idx</span><span class=p>]</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span>
                <span class=n>y_tasks</span><span class=p>[</span><span class=n>task_idx</span><span class=p>]</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
            <span class=p>)</span>
            <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>

        <span class=c1># Combined loss (simple average)</span>
        <span class=n>total_loss</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>

        <span class=c1># Backward</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>total_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>Hard Parameter Sharing:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>HardSharing</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Most common MTL architecture&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Fully shared layers</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>shared</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>),</span>

            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>128</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>
        <span class=p>)</span>

        <span class=c1># Task 1: Classification</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

        <span class=c1># Task 2: Regression</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>regressor</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

        <span class=c1># Task 3: Another classification</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>aux_classifier</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>shared_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>shared</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=k>return</span> <span class=p>{</span>
            <span class=s1>&#39;classification&#39;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifier</span><span class=p>(</span><span class=n>shared_features</span><span class=p>),</span>
            <span class=s1>&#39;regression&#39;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>regressor</span><span class=p>(</span><span class=n>shared_features</span><span class=p>),</span>
            <span class=s1>&#39;auxiliary&#39;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>aux_classifier</span><span class=p>(</span><span class=n>shared_features</span><span class=p>)</span>
        <span class=p>}</span>
</code></pre></div> <p><strong>Soft Parameter Sharing:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>SoftSharing</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Each task has own parameters, but regularized to be similar&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>num_tasks</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Separate encoders for each task</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>task_encoders</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
            <span class=p>)</span>
            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_tasks</span><span class=p>)</span>
        <span class=p>])</span>

        <span class=c1># Task-specific heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>task_heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_tasks</span><span class=p>)</span>
        <span class=p>])</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>encoder</span><span class=p>,</span> <span class=n>head</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>task_encoders</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>task_heads</span><span class=p>):</span>
            <span class=n>features</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
            <span class=n>output</span> <span class=o>=</span> <span class=n>head</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
            <span class=n>outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>outputs</span>

    <span class=k>def</span><span class=w> </span><span class=nf>l2_regularization</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Encourage parameters to be similar across tasks&quot;&quot;&quot;</span>
        <span class=n>reg_loss</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>num_tasks</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>task_encoders</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_tasks</span><span class=p>):</span>
            <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_tasks</span><span class=p>):</span>
                <span class=k>for</span> <span class=n>p1</span><span class=p>,</span> <span class=n>p2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>task_encoders</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
                                 <span class=bp>self</span><span class=o>.</span><span class=n>task_encoders</span><span class=p>[</span><span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>parameters</span><span class=p>()):</span>
                    <span class=n>reg_loss</span> <span class=o>+=</span> <span class=p>((</span><span class=n>p1</span> <span class=o>-</span> <span class=n>p2</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>reg_loss</span>

<span class=c1># Training with regularization</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_tasks</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>

    <span class=c1># Task losses</span>
    <span class=n>task_losses</span> <span class=o>=</span> <span class=p>[</span><span class=n>criterion</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> 
                   <span class=k>for</span> <span class=n>out</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_tasks</span><span class=p>)]</span>
    <span class=n>total_loss</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>task_losses</span><span class=p>)</span>

    <span class=c1># Add regularization</span>
    <span class=n>reg_loss</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>l2_regularization</span><span class=p>()</span>
    <span class=n>total_loss</span> <span class=o>+=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>reg_loss</span>

    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
    <span class=n>total_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>Task Weighting Strategies:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Uniform weighting</span>
<span class=n>total_loss</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>

<span class=c1># 2. Manual weights</span>
<span class=n>task_weights</span> <span class=o>=</span> <span class=p>[</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>]</span>  <span class=c1># Prioritize task 3</span>
<span class=n>total_loss</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>l</span> <span class=k>for</span> <span class=n>w</span><span class=p>,</span> <span class=n>l</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>task_weights</span><span class=p>,</span> <span class=n>losses</span><span class=p>))</span>

<span class=c1># 3. Uncertainty weighting (learned)</span>
<span class=k>class</span><span class=w> </span><span class=nc>UncertaintyWeighting</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Learn task weights based on homoscedastic uncertainty&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_tasks</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Log variance for each task</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>log_vars</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>num_tasks</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>losses</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Loss_weighted = Loss / (2*sigma^2) + log(sigma)</span>
<span class=sd>        sigma = exp(log_var / 2)</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>weighted_losses</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>loss</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>losses</span><span class=p>):</span>
            <span class=n>precision</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>log_vars</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
            <span class=n>weighted_loss</span> <span class=o>=</span> <span class=n>precision</span> <span class=o>*</span> <span class=n>loss</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>log_vars</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
            <span class=n>weighted_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>weighted_loss</span><span class=p>)</span>

        <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>weighted_losses</span><span class=p>)</span>

<span class=c1># Usage</span>
<span class=n>uncertainty_module</span> <span class=o>=</span> <span class=n>UncertaintyWeighting</span><span class=p>(</span><span class=n>num_tasks</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>total_loss</span> <span class=o>=</span> <span class=n>uncertainty_module</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>

<span class=c1># 4. Gradient normalization</span>
<span class=k>def</span><span class=w> </span><span class=nf>grad_norm_loss</span><span class=p>(</span><span class=n>losses</span><span class=p>,</span> <span class=n>shared_params</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Balance gradients from different tasks&quot;&quot;&quot;</span>

    <span class=c1># Compute gradient norms for each task</span>
    <span class=n>grad_norms</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>loss</span> <span class=ow>in</span> <span class=n>losses</span><span class=p>:</span>
        <span class=n>grads</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>shared_params</span><span class=p>,</span> 
                                    <span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
                                    <span class=n>create_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
        <span class=n>grad_norm</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>((</span><span class=n>g</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=n>grads</span><span class=p>)</span><span class=o>.</span><span class=n>sqrt</span><span class=p>()</span>
        <span class=n>grad_norms</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>grad_norm</span><span class=p>)</span>

    <span class=c1># Normalize</span>
    <span class=n>mean_norm</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>grad_norms</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>grad_norms</span><span class=p>)</span>
    <span class=n>weights</span> <span class=o>=</span> <span class=p>[</span><span class=n>mean_norm</span> <span class=o>/</span> <span class=p>(</span><span class=n>gn</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span> <span class=k>for</span> <span class=n>gn</span> <span class=ow>in</span> <span class=n>grad_norms</span><span class=p>]</span>

    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>l</span> <span class=k>for</span> <span class=n>w</span><span class=p>,</span> <span class=n>l</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>losses</span><span class=p>))</span>
</code></pre></div> <p><strong>Real Example: NLP Multi-Task:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>NLPMultiTask</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;BERT-style multi-task model&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Shared BERT-like encoder</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoder</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>8</span><span class=p>),</span>
            <span class=n>num_layers</span><span class=o>=</span><span class=mi>6</span>
        <span class=p>)</span>

        <span class=c1># Task 1: Named Entity Recognition (token-level)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>ner_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>  <span class=c1># 10 NER tags</span>

        <span class=c1># Task 2: Sentiment Analysis (sequence-level)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>sentiment_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># Positive/Negative/Neutral</span>
        <span class=p>)</span>

        <span class=c1># Task 3: Next Sentence Prediction</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>nsp_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>):</span>
        <span class=c1># Shared encoding</span>
        <span class=n>embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
        <span class=n>encoded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=p>(</span><span class=n>embeddings</span><span class=p>)</span>

        <span class=c1># Task-specific outputs</span>
        <span class=n>ner_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ner_head</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span>  <span class=c1># All tokens</span>

        <span class=c1># Use [CLS] token for sequence tasks</span>
        <span class=n>cls_representation</span> <span class=o>=</span> <span class=n>encoded</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>
        <span class=n>sentiment_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sentiment_head</span><span class=p>(</span><span class=n>cls_representation</span><span class=p>)</span>
        <span class=n>nsp_logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>nsp_head</span><span class=p>(</span><span class=n>cls_representation</span><span class=p>)</span>

        <span class=k>return</span> <span class=p>{</span>
            <span class=s1>&#39;ner&#39;</span><span class=p>:</span> <span class=n>ner_logits</span><span class=p>,</span>
            <span class=s1>&#39;sentiment&#39;</span><span class=p>:</span> <span class=n>sentiment_logits</span><span class=p>,</span>
            <span class=s1>&#39;nsp&#39;</span><span class=p>:</span> <span class=n>nsp_logits</span>
        <span class=p>}</span>
</code></pre></div> <p><strong>Benefits:</strong></p> <table> <thead> <tr> <th>Benefit</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Better generalization</td> <td>Shared representations regularize</td> </tr> <tr> <td>Faster learning</td> <td>Transfer knowledge between tasks</td> </tr> <tr> <td>Data efficiency</td> <td>Leverage data from all tasks</td> </tr> <tr> <td>Implicit regularization</td> <td>Prevents overfitting to single task</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced ML architecture knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Hard vs soft parameter sharing</li> <li>"Shared encoder + task-specific heads"</li> <li>Task weighting strategies</li> <li>"Helps when tasks related"</li> <li>Examples: BERT (NER, sentiment, NLI)</li> <li>"Can hurt if tasks very different"</li> </ul> </div> </details> <hr> <p>| 13 | k-Nearest Neighbors (k-NN) | <a href=https://towardsdatascience.com/k-nearest-neighbors-knn-algorithm-for-machine-learning-e883219c8f26>Towards Data Science</a> | Google, Amazon, Facebook | Easy | Instance-based Learning | | 14 | Dimensionality Reduction: PCA | <a href=https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>Towards Data Science</a> | Google, Amazon, Microsoft | Medium | Dimensionality Reduction | | 15 | Handling Missing Data | <a href=https://machinelearningmastery.com/handle-missing-data-python/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Easy | Data Preprocessing | | 16 | Parametric vs Non-Parametric Models | <a href=https://towardsdatascience.com/parametric-vs-non-parametric-models-825d1a0f5c2c>Towards Data Science</a> | Google, Amazon | Medium | Model Types | | 17 | Neural Networks: Basics | <a href=https://towardsdatascience.com/a-beginners-guide-to-neural-networks-2cf4c3f9c9d0>Towards Data Science</a> | Google, Facebook, Amazon | Medium | Deep Learning | | 18 | Convolutional Neural Networks (CNNs) | <a href=https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-for-computer-vision-2bda48ea1e50>Towards Data Science</a> | Google, Facebook, Amazon | Hard | Deep Learning, Computer Vision | | 19 | Recurrent Neural Networks (RNNs) and LSTMs | <a href=https://towardsdatascience.com/recurrent-neural-networks-for-language-modeling-396f1d1659f2>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Sequence Models | | 20 | Reinforcement Learning Basics | <a href=https://towardsdatascience.com/introduction-to-reinforcement-learning-6346f7f8c1ef>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Reinforcement Learning | | 21 | Hyperparameter Tuning | <a href=https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ >Machine Learning Mastery</a> | Google, Amazon, Microsoft | Medium | Model Optimization | | 22 | Feature Engineering | <a href=https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Data Preprocessing | | 23 | ROC Curve and AUC | <a href=https://towardsdatascience.com/roc-curve-and-auc-using-python-and-scikit-learn-42da0fa0d0d>Towards Data Science</a> | Google, Amazon, Microsoft | Medium | Model Evaluation | | 24 | Regression Evaluation Metrics | <a href=https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics>Scikit-Learn</a> | Google, Amazon, Facebook | Medium | Model Evaluation, Regression | | 25 | Curse of Dimensionality | <a href=https://machinelearningmastery.com/curse-of-dimensionality/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Hard | Data Preprocessing | | 26 | Logistic Regression | <a href=https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc>Towards Data Science</a> | Google, Amazon, Facebook | Easy | Classification, Regression | | 27 | Linear Regression | <a href=https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/ >Analytics Vidhya</a> | Google, Amazon, Facebook | Easy | Regression | | 28 | Loss Functions in ML | <a href=https://towardsdatascience.com/common-loss-functions-in-machine-learning-3b7af9f8bf2b>Towards Data Science</a> | Google, Amazon, Microsoft | Medium | Optimization, Model Evaluation | | 29 | Gradient Descent Variants | <a href=https://machinelearningmastery.com/difference-between-batch-and-stochastic-gradient-descent/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Medium | Optimization | | 30 | Data Normalization and Standardization | <a href=https://machinelearningmastery.com/normalize-standardize-machine-learning-data/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Easy | Data Preprocessing | | 31 | k-Means Clustering | <a href=https://towardsdatascience.com/introduction-to-k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Clustering | | 32 | Other Clustering Techniques | <a href=https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/ >Analytics Vidhya</a> | Google, Amazon, Facebook | Medium | Clustering | | 33 | Anomaly Detection | <a href=https://towardsdatascience.com/anomaly-detection-techniques-in-python-50f650c75aaf>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Outlier Detection | | 34 | Learning Rate in Optimization | <a href=https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/ >Machine Learning Mastery</a> | Google, Amazon, Microsoft | Medium | Optimization | | 35 | Deep Learning vs. Traditional ML | <a href=https://www.ibm.com/cloud/learn/deep-learning>IBM Cloud Learn</a> | Google, Amazon, Facebook | Medium | Deep Learning, ML Basics | | 36 | Dropout in Neural Networks | <a href=https://towardsdatascience.com/understanding-dropout-in-neural-networks-3c5da7a57f86>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Deep Learning, Regularization | | 37 | Backpropagation | <a href=https://www.analyticsvidhya.com/blog/2017/05/implementation-neural-network-scratch-python/ >Analytics Vidhya</a> | Google, Amazon, Facebook | Hard | Deep Learning, Neural Networks | | 38 | Role of Activation Functions | <a href=https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Medium | Neural Networks | | 39 | Word Embeddings and Their Use | <a href=https://towardsdatascience.com/word-embeddings-6cb7d87c0f64>Towards Data Science</a> | Google, Amazon, Facebook | Medium | NLP, Deep Learning | | 40 | Transfer Learning | <a href=https://machinelearningmastery.com/transfer-learning-for-deep-learning/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Medium | Deep Learning, Model Reuse | | 41 | Bayesian Optimization for Hyperparameters | <a href=https://towardsdatascience.com/bayesian-optimization-explained-4f6c2e60731d>Towards Data Science</a> | Google, Amazon, Microsoft | Hard | Hyperparameter Tuning, Optimization | | 42 | Model Interpretability: SHAP and LIME | <a href=https://towardsdatascience.com/interpreting-machine-learning-models-using-shap-values-df04dc62fbd4>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Model Interpretability, Explainability | | 43 | Ensemble Methods: Stacking and Blending | <a href=https://machinelearningmastery.com/ensemble-learning-stacking/ >Machine Learning Mastery</a> | Google, Amazon, Microsoft | Hard | Ensemble Methods | | 44 | Gradient Boosting Machines (GBM) Basics | <a href=https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Ensemble, Boosting | | 45 | Extreme Gradient Boosting (XGBoost) Overview | <a href=https://towardsdatascience.com/xgboost-optimized-gradient-boosting-e3d7b32d27b1>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Ensemble, Boosting | | 46 | LightGBM vs XGBoost Comparison | <a href=https://www.analyticsvidhya.com/blog/2019/06/lightgbm-vs-xgboost/ >Analytics Vidhya</a> | Google, Amazon | Medium | Ensemble, Boosting | | 47 | CatBoost: Handling Categorical Features | <a href=https://towardsdatascience.com/catboost-for-beginners-d68638b78982>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Ensemble, Categorical Data | | 48 | Time Series Forecasting with ARIMA | <a href=https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/ >Analytics Vidhya</a> | Google, Amazon, Facebook | Hard | Time Series, Forecasting | | 49 | Time Series Forecasting with LSTM | <a href=https://towardsdatascience.com/time-series-forecasting-using-lstm-3c6a39bfae39>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Time Series, Deep Learning | | 50 | Robust Scaling Techniques | <a href=https://towardsdatascience.com/robust-scaling-why-when-and-how-3f2a67f1b0a3>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Data Preprocessing | | 51 | Data Imputation Techniques in ML | <a href=https://machinelearningmastery.com/handle-missing-data-python/ >Machine Learning Mastery</a> | Google, Amazon, Facebook | Medium | Data Preprocessing | | 52 | Handling Imbalanced Datasets: SMOTE and Others | <a href=https://towardsdatascience.com/smote-oversampling-for-imbalanced-classification-6c2046f13447>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Data Preprocessing, Classification | | 53 | Bias in Machine Learning: Fairness and Ethics | <a href=https://towardsdatascience.com/fairness-in-machine-learning-6e21a5c4d5db>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Ethics, Fairness | | 54 | Model Deployment: From Prototype to Production | <a href=https://towardsdatascience.com/deploying-machine-learning-models-3f6e41013240>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Deployment | | 55 | Online Learning Algorithms | <a href=https://towardsdatascience.com/online-learning-algorithms-40bf6c6d19de>Towards Data Science</a> | Google, Amazon, Microsoft | Hard | Online Learning | | 56 | Concept Drift in Machine Learning | <a href=https://towardsdatascience.com/concept-drift-in-machine-learning-6b97e0f3f42d>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Model Maintenance | | 57 | Transfer Learning in NLP: BERT, GPT | <a href=https://towardsdatascience.com/transfer-learning-in-nlp-9f26f10b2b96>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 58 | Natural Language Processing: Text Preprocessing | <a href=https://www.analyticsvidhya.com/blog/2020/07/text-preprocessing-techniques-in-python/ >Analytics Vidhya</a> | Google, Amazon, Facebook | Easy | NLP, Data Preprocessing | | 59 | Text Vectorization: TF-IDF vs Word2Vec | <a href=https://towardsdatascience.com/text-vectorization-methods-6fd1d1a74a66>Towards Data Science</a> | Google, Amazon, Facebook | Medium | NLP, Feature Extraction | | 60 | Transformer Architecture and Self-Attention | <a href=https://towardsdatascience.com/transformers-141e32e69591>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 61 | Understanding BERT for NLP Tasks | <a href=https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 62 | Understanding GPT Models | <a href=https://towardsdatascience.com/what-is-gpt-3-and-why-is-it-so-important-95b9acb9d0a3>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 63 | Data Augmentation Techniques in ML | <a href=https://towardsdatascience.com/data-augmentation-for-deep-learning-8e2f37e59a1b>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Data Preprocessing | | 64 | Adversarial Machine Learning: Attack and Defense | <a href=https://towardsdatascience.com/adversarial-attacks-on-machine-learning-models-8a91b4a6a9a3>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Security, ML | | 65 | Explainable AI (XAI) in Practice | <a href=https://towardsdatascience.com/explainable-ai-a-survey-of-methods-4d9e35597b0c>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Model Interpretability | | 66 | Federated Learning: Concepts and Challenges | <a href=https://towardsdatascience.com/federated-learning-explained-d9e99d16ef57>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Distributed Learning | | 67 | Multi-Task Learning in Neural Networks | <a href=https://towardsdatascience.com/multi-task-learning-for-neural-networks-6e4e2fcb5d3a>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Multi-Task | | 68 | Metric Learning and Siamese Networks | <a href=https://towardsdatascience.com/siamese-networks-for-one-shot-learning-60b2c8c9b71>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Metric Learning | | 69 | Deep Reinforcement Learning: DQN Overview | <a href=https://towardsdatascience.com/deep-q-learning-dqn-1b5f8bb83d11>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Reinforcement Learning, Deep Learning | | 70 | Policy Gradient Methods in Reinforcement Learning | <a href=https://towardsdatascience.com/policy-gradient-methods-in-reinforcement-learning-713f77dceb79>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Reinforcement Learning | | 71 | Actor-Critic Methods in RL | <a href=https://towardsdatascience.com/actor-critic-methods-in-reinforcement-learning-49cfa6403a5e>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Reinforcement Learning | | 72 | Monte Carlo Methods in Machine Learning | <a href=https://towardsdatascience.com/monte-carlo-methods-in-machine-learning-8f7f0e9ad0e9>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Optimization, Probabilistic Methods | | 73 | Expectation-Maximization Algorithm | <a href=https://towardsdatascience.com/expectation-maximization-algorithm-for-gaussian-mixture-models-ef96d0e98729>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Clustering, Probabilistic Models | | 74 | Gaussian Mixture Models (GMM) | <a href=https://towardsdatascience.com/gaussian-mixture-models-in-python-6b85679b5a4>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Clustering, Probabilistic Models | | 75 | Bayesian Inference in ML | <a href=https://towardsdatascience.com/introduction-to-bayesian-inference-7f72a56c97c>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Bayesian Methods | | 76 | Markov Chain Monte Carlo (MCMC) Methods | <a href=https://towardsdatascience.com/markov-chain-monte-carlo-methods-a-tutorial-d3e4a14c6a1f>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Bayesian Methods, Probabilistic Models | | 77 | Variational Autoencoders (VAEs) | <a href=https://towardsdatascience.com/variational-autoencoders-explained-8f7f0e9ad0e9>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Generative Models | | 78 | Generative Adversarial Networks (GANs) | <a href=https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Generative Models | | 79 | Conditional GANs for Data Generation | <a href=https://towardsdatascience.com/conditional-gans-explained-9f2b30d3e5e3>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Generative Models | | 80 | Sequence-to-Sequence Models in NLP | <a href=https://towardsdatascience.com/sequence-to-sequence-models-for-machine-translation-873b51b65f0f>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 81 | Attention Mechanisms in Seq2Seq Models | <a href=https://towardsdatascience.com/attention-mechanisms-in-deep-learning-a-tutorial-3d9b62f341d>Towards Data Science</a> | Google, Amazon, Facebook | Hard | NLP, Deep Learning | | 82 | Capsule Networks: An Introduction | <a href=https://towardsdatascience.com/capsule-networks-an-introduction-4d2b2a7dbd5>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Neural Networks | | 83 | Self-Supervised Learning in Deep Learning | <a href=https://towardsdatascience.com/self-supervised-learning-explained-7e0e4a2f8b8>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Unsupervised Learning | | 84 | Zero-Shot and Few-Shot Learning | <a href=https://towardsdatascience.com/zero-shot-learning-in-deep-learning-8f3e8c8e9a2b>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Transfer Learning | | 85 | Meta-Learning: Learning to Learn | <a href=https://towardsdatascience.com/meta-learning-what-is-it-and-why-it-matters-7a1a1e9d9e3>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Deep Learning, Optimization | | 86 | Hyperparameter Sensitivity Analysis | <a href=https://towardsdatascience.com/hyperparameter-sensitivity-analysis-123456789>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Hyperparameter Tuning | | 87 | High-Dimensional Feature Selection Techniques | <a href=https://towardsdatascience.com/feature-selection-methods-abc123>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Feature Engineering, Dimensionality Reduction | | 88 | Multi-Label Classification Techniques | <a href=https://towardsdatascience.com/multi-label-classification-methods-456def>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Classification, Multi-Output | | 89 | Ordinal Regression in Machine Learning | <a href=https://towardsdatascience.com/ordinal-regression-explained-789ghi>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Regression, Classification | | 90 | Survival Analysis in ML | <a href=https://towardsdatascience.com/survival-analysis-in-machine-learning-abc789>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Statistics, ML | | 91 | Semi-Supervised Learning Methods | <a href=https://towardsdatascience.com/semi-supervised-learning-101-123abc>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Unsupervised Learning, ML Basics | | 92 | Unsupervised Feature Learning | <a href=https://towardsdatascience.com/unsupervised-feature-learning-abc456>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Unsupervised Learning, Feature Extraction | | 93 | Clustering Evaluation Metrics: Silhouette, Davies-Bouldin | <a href=https://towardsdatascience.com/clustering-evaluation-metrics-789jkl>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Clustering, Evaluation | | 94 | Dimensionality Reduction: t-SNE and UMAP | <a href=https://towardsdatascience.com/t-sne-and-umap-789mno>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Dimensionality Reduction | | 95 | Probabilistic Graphical Models: Bayesian Networks | <a href=https://towardsdatascience.com/bayesian-networks-123jkl>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Probabilistic Models, Graphical Models | | 96 | Hidden Markov Models (HMMs) in ML | <a href=https://towardsdatascience.com/hidden-markov-models-101-456mno>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Probabilistic Models, Sequence Modeling | | 97 | Recommender Systems: Collaborative Filtering | <a href=https://towardsdatascience.com/collaborative-filtering-789pqr>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Recommender Systems | | 98 | Recommender Systems: Content-Based Filtering | <a href=https://towardsdatascience.com/content-based-recommender-systems-123stu>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Recommender Systems | | 99 | Anomaly Detection in Time Series Data | <a href=https://towardsdatascience.com/anomaly-detection-in-time-series-data-456vwx>Towards Data Science</a> | Google, Amazon, Facebook | Hard | Time Series, Anomaly Detection | | 100 | Optimization Algorithms Beyond Gradient Descent (Adam, RMSProp, etc.) | <a href=https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6f8eb4c5b0e4>Towards Data Science</a> | Google, Amazon, Facebook | Medium | Optimization, Deep Learning |</p> <hr> <h2 id=questions-asked-in-google-interview>Questions asked in Google interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Gradient Descent </li> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Reinforcement Learning Basics </li> <li>Hyperparameter Tuning </li> <li>Transfer Learning </li> </ul> <h2 id=questions-asked-in-facebook-interview>Questions asked in Facebook interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Support Vector Machines (SVM) </li> <li>k-Nearest Neighbors (k-NN) </li> <li>Feature Engineering </li> <li>Dropout in Neural Networks </li> <li>Backpropagation </li> </ul> <h2 id=questions-asked-in-amazon-interview>Questions asked in Amazon interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Regularization Techniques (L1, L2) </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Decision Trees </li> <li>Ensemble Learning: Bagging and Boosting </li> <li>Random Forest </li> <li>Support Vector Machines (SVM) </li> <li>Neural Networks: Basics </li> <li>Hyperparameter Tuning </li> <li>ROC Curve and AUC </li> <li>Logistic Regression </li> <li>Data Normalization and Standardization </li> <li>k-Means Clustering </li> </ul> <h2 id=questions-asked-in-microsoft-interview>Questions asked in Microsoft interview</h2> <ul> <li>Regularization Techniques (L1, L2) </li> <li>Gradient Descent </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Support Vector Machines (SVM) </li> <li>Hyperparameter Tuning </li> <li>ROC Curve and AUC </li> <li>Loss Functions in ML </li> <li>Learning Rate in Optimization </li> <li>Bayesian Optimization for Hyperparameters </li> </ul> <h2 id=questions-asked-in-uber-interview>Questions asked in Uber interview</h2> <ul> <li>Reinforcement Learning Basics </li> <li>Anomaly Detection </li> <li>Gradient Descent Variants </li> <li>Model Deployment: From Prototype to Production </li> </ul> <h2 id=questions-asked-in-swiggy-interview>Questions asked in Swiggy interview</h2> <ul> <li>Handling Missing Data </li> <li>Data Imputation Techniques in ML </li> <li>Feature Engineering </li> <li>Model Interpretability: SHAP and LIME </li> </ul> <h2 id=questions-asked-in-flipkart-interview>Questions asked in Flipkart interview</h2> <ul> <li>Ensemble Methods: Stacking and Blending </li> <li>Time Series Forecasting with ARIMA </li> <li>Time Series Forecasting with LSTM </li> <li>Model Deployment: From Prototype to Production </li> </ul> <h2 id=questions-asked-in-ola-interview>Questions asked in Ola interview</h2> <ul> <li>Time Series Forecasting with LSTM </li> <li>Data Normalization and Standardization </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> </ul> <h2 id=questions-asked-in-paytm-interview>Questions asked in Paytm interview</h2> <ul> <li>Model Deployment: From Prototype to Production </li> <li>Online Learning Algorithms </li> <li>Handling Imbalanced Datasets: SMOTE and Others </li> </ul> <h2 id=questions-asked-in-oyo-interview>Questions asked in OYO interview</h2> <ul> <li>Data Preprocessing Techniques </li> <li>Ensemble Learning: Bagging and Boosting </li> <li>Regularization Techniques (L1, L2) </li> </ul> <h2 id=questions-asked-in-whatsapp-interview>Questions asked in WhatsApp interview</h2> <ul> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Dropout in Neural Networks </li> </ul> <hr> <h3 id=explain-few-shot-and-zero-shot-learning-google-meta-interview-question>Explain Few-Shot and Zero-Shot Learning - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Meta-Learning</code>, <code>Transfer Learning</code>, <code>Few-Shot</code> | <strong>Asked by:</strong> Google, Meta, OpenAI, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>Few-Shot Learning:</strong></p> <p>Learn from very few examples (1-shot, 5-shot, etc.) per class.</p> <p><strong>Zero-Shot Learning:</strong></p> <p>Classify classes never seen during training using semantic information.</p> <p><strong>1. Prototypical Networks (Few-Shot):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>

<span class=k>class</span><span class=w> </span><span class=nc>PrototypicalNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Learn to classify from few examples&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>64</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Embedding network</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>support_set</span><span class=p>,</span> <span class=n>support_labels</span><span class=p>,</span> <span class=n>query_set</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        support_set: [n_support, input_dim]</span>
<span class=sd>        support_labels: [n_support]</span>
<span class=sd>        query_set: [n_query, input_dim]</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># Embed support and query</span>
        <span class=n>support_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>support_set</span><span class=p>)</span>
        <span class=n>query_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>query_set</span><span class=p>)</span>

        <span class=c1># Compute class prototypes (mean of support examples per class)</span>
        <span class=n>classes</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>support_labels</span><span class=p>)</span>
        <span class=n>prototypes</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>classes</span><span class=p>:</span>
            <span class=n>class_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>support_labels</span> <span class=o>==</span> <span class=n>c</span><span class=p>)</span>
            <span class=n>class_embeddings</span> <span class=o>=</span> <span class=n>support_embeddings</span><span class=p>[</span><span class=n>class_mask</span><span class=p>]</span>
            <span class=n>prototype</span> <span class=o>=</span> <span class=n>class_embeddings</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
            <span class=n>prototypes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>prototype</span><span class=p>)</span>

        <span class=n>prototypes</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>prototypes</span><span class=p>)</span>  <span class=c1># [n_classes, hidden_dim]</span>

        <span class=c1># Classify query by distance to prototypes</span>
        <span class=n>distances</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cdist</span><span class=p>(</span><span class=n>query_embeddings</span><span class=p>,</span> <span class=n>prototypes</span><span class=p>)</span>  <span class=c1># Euclidean</span>
        <span class=n>logits</span> <span class=o>=</span> <span class=o>-</span><span class=n>distances</span>  <span class=c1># Closer = higher score</span>

        <span class=k>return</span> <span class=n>logits</span>

<span class=c1># Few-shot episode</span>
<span class=k>def</span><span class=w> </span><span class=nf>create_episode</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>n_way</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>k_shot</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>n_query</span><span class=o>=</span><span class=mi>15</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Create N-way K-shot episode&quot;&quot;&quot;</span>

    <span class=c1># Sample N classes</span>
    <span class=n>classes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=o>.</span><span class=n>classes</span><span class=p>),</span> <span class=n>n_way</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

    <span class=n>support_set</span><span class=p>,</span> <span class=n>support_labels</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
    <span class=n>query_set</span><span class=p>,</span> <span class=n>query_labels</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=bp>cls</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>classes</span><span class=p>):</span>
        <span class=c1># Get examples from this class</span>
        <span class=n>class_samples</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>get_class_samples</span><span class=p>(</span><span class=bp>cls</span><span class=p>)</span>

        <span class=c1># Sample K for support, rest for query</span>
        <span class=n>samples</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>class_samples</span><span class=p>,</span> <span class=n>k_shot</span> <span class=o>+</span> <span class=n>n_query</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

        <span class=n>support_set</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>samples</span><span class=p>[:</span><span class=n>k_shot</span><span class=p>])</span>
        <span class=n>support_labels</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=n>idx</span><span class=p>]</span> <span class=o>*</span> <span class=n>k_shot</span><span class=p>)</span>

        <span class=n>query_set</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>samples</span><span class=p>[</span><span class=n>k_shot</span><span class=p>:])</span>
        <span class=n>query_labels</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=n>idx</span><span class=p>]</span> <span class=o>*</span> <span class=n>n_query</span><span class=p>)</span>

    <span class=k>return</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>support_set</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>support_labels</span><span class=p>),</span>
            <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>query_set</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>query_labels</span><span class=p>))</span>

<span class=c1># Training</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>PrototypicalNetwork</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>784</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>64</span><span class=p>)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>

<span class=k>for</span> <span class=n>episode</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10000</span><span class=p>):</span>
    <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>query_x</span><span class=p>,</span> <span class=n>query_y</span> <span class=o>=</span> <span class=n>create_episode</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>

    <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>query_x</span><span class=p>)</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>query_y</span><span class=p>)</span>

    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>2. Matching Networks:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>MatchingNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Attention-based few-shot learning&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>64</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>support_set</span><span class=p>,</span> <span class=n>support_labels</span><span class=p>,</span> <span class=n>query_set</span><span class=p>):</span>
        <span class=c1># Embed all samples</span>
        <span class=n>support_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>support_set</span><span class=p>)</span>
        <span class=n>query_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>query_set</span><span class=p>)</span>

        <span class=c1># Compute attention weights (cosine similarity)</span>
        <span class=n>attention</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cosine_similarity</span><span class=p>(</span>
            <span class=n>query_embeddings</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>  <span class=c1># [n_query, 1, hidden_dim]</span>
            <span class=n>support_embeddings</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>  <span class=c1># [1, n_support, hidden_dim]</span>
            <span class=n>dim</span><span class=o>=</span><span class=mi>2</span>
        <span class=p>)</span>  <span class=c1># [n_query, n_support]</span>

        <span class=n>attention</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Predict as weighted combination of support labels</span>
        <span class=n>n_classes</span> <span class=o>=</span> <span class=n>support_labels</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
        <span class=n>support_one_hot</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>support_labels</span><span class=p>,</span> <span class=n>n_classes</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>

        <span class=n>predictions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>support_one_hot</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>predictions</span>
</code></pre></div> <p><strong>3. MAML (Model-Agnostic Meta-Learning):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>higher</span>  <span class=c1># pip install higher</span>

<span class=k>class</span><span class=w> </span><span class=nc>MAML</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Learn initialization that adapts quickly&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>inner_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>outer_lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>inner_lr</span> <span class=o>=</span> <span class=n>inner_lr</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>outer_lr</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>inner_loop</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>n_steps</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fast adaptation on support set&quot;&quot;&quot;</span>

        <span class=c1># Create differentiable optimizer</span>
        <span class=k>with</span> <span class=n>higher</span><span class=o>.</span><span class=n>innerloop_ctx</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> 
                                 <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> 
                                                <span class=n>lr</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>inner_lr</span><span class=p>))</span> <span class=k>as</span> <span class=p>(</span><span class=n>fmodel</span><span class=p>,</span> <span class=n>diffopt</span><span class=p>):</span>

            <span class=c1># Inner loop updates</span>
            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_steps</span><span class=p>):</span>
                <span class=n>logits</span> <span class=o>=</span> <span class=n>fmodel</span><span class=p>(</span><span class=n>support_x</span><span class=p>)</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>
                <span class=n>diffopt</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>

            <span class=k>return</span> <span class=n>fmodel</span>

    <span class=k>def</span><span class=w> </span><span class=nf>meta_update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>task_batch</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Outer loop: update initialization&quot;&quot;&quot;</span>

        <span class=n>meta_loss</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>for</span> <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>query_x</span><span class=p>,</span> <span class=n>query_y</span> <span class=ow>in</span> <span class=n>task_batch</span><span class=p>:</span>
            <span class=c1># Fast adaptation</span>
            <span class=n>adapted_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>inner_loop</span><span class=p>(</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>

            <span class=c1># Evaluate on query set</span>
            <span class=n>logits</span> <span class=o>=</span> <span class=n>adapted_model</span><span class=p>(</span><span class=n>query_x</span><span class=p>)</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>query_y</span><span class=p>)</span>
            <span class=n>meta_loss</span> <span class=o>+=</span> <span class=n>loss</span>

        <span class=c1># Meta-optimization</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>meta_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>meta_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>task_batch</span><span class=p>)</span>

<span class=c1># Usage</span>
<span class=n>base_model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>  <span class=c1># N-way</span>
<span class=p>)</span>

<span class=n>maml</span> <span class=o>=</span> <span class=n>MAML</span><span class=p>(</span><span class=n>base_model</span><span class=p>)</span>

<span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10000</span><span class=p>):</span>
    <span class=n>task_batch</span> <span class=o>=</span> <span class=p>[</span><span class=n>create_episode</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>32</span><span class=p>)]</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>maml</span><span class=o>.</span><span class=n>meta_update</span><span class=p>(</span><span class=n>task_batch</span><span class=p>)</span>
</code></pre></div> <p><strong>4. Zero-Shot Learning (Attribute-Based):</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>ZeroShotClassifier</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Classify unseen classes using attributes&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>image_dim</span><span class=p>,</span> <span class=n>attribute_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Map images to attribute space</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>image_encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>image_dim</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=n>attribute_dim</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>images</span><span class=p>,</span> <span class=n>class_attributes</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        images: [batch_size, image_dim]</span>
<span class=sd>        class_attributes: [n_classes, attribute_dim]</span>
<span class=sd>            e.g., [has_fur, has_wings, is_large, ...]</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># Embed images</span>
        <span class=n>image_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>image_encoder</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>

        <span class=c1># Compute similarity to each class</span>
        <span class=n>similarities</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cosine_similarity</span><span class=p>(</span>
            <span class=n>image_embeddings</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>  <span class=c1># [batch, 1, attr_dim]</span>
            <span class=n>class_attributes</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>  <span class=c1># [1, n_classes, attr_dim]</span>
            <span class=n>dim</span><span class=o>=</span><span class=mi>2</span>
        <span class=p>)</span>

        <span class=k>return</span> <span class=n>similarities</span>

<span class=c1># Example: Animal classification</span>
<span class=c1># Seen classes: dog, cat, bird</span>
<span class=c1># Unseen class: zebra</span>

<span class=n>class_attributes</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;dog&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>  <span class=c1># has_fur, has_wings, has_stripes, is_mammal, can_fly</span>
    <span class=s1>&#39;cat&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
    <span class=s1>&#39;bird&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
    <span class=s1>&#39;zebra&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>  <span class=c1># Unseen during training</span>
<span class=p>}</span>

<span class=c1># Train on dog, cat, bird</span>
<span class=c1># Test on zebra using its attributes</span>
</code></pre></div> <p><strong>5. Siamese Networks:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>SiameseNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Learn similarity metric&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward_one</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute embeddings for pair&quot;&quot;&quot;</span>
        <span class=n>emb1</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward_one</span><span class=p>(</span><span class=n>x1</span><span class=p>)</span>
        <span class=n>emb2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward_one</span><span class=p>(</span><span class=n>x2</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>emb1</span><span class=p>,</span> <span class=n>emb2</span>

<span class=c1># Contrastive Loss</span>
<span class=k>class</span><span class=w> </span><span class=nc>ContrastiveLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>margin</span><span class=o>=</span><span class=mf>1.0</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>margin</span> <span class=o>=</span> <span class=n>margin</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb1</span><span class=p>,</span> <span class=n>emb2</span><span class=p>,</span> <span class=n>label</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        label: 1 if same class, 0 if different</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>distance</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>pairwise_distance</span><span class=p>(</span><span class=n>emb1</span><span class=p>,</span> <span class=n>emb2</span><span class=p>)</span>

        <span class=c1># Similar pairs: minimize distance</span>
        <span class=c1># Dissimilar pairs: maximize distance (up to margin)</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>label</span> <span class=o>*</span> <span class=n>distance</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>+</span> \
               <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>label</span><span class=p>)</span> <span class=o>*</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>margin</span> <span class=o>-</span> <span class=n>distance</span><span class=p>)</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

<span class=c1># For few-shot: compare query to support examples</span>
<span class=k>def</span><span class=w> </span><span class=nf>predict_few_shot</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>support_set</span><span class=p>,</span> <span class=n>support_labels</span><span class=p>,</span> <span class=n>query</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Predict by finding nearest neighbor in support&quot;&quot;&quot;</span>

    <span class=n>query_emb</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>forward_one</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>

    <span class=n>distances</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>support_sample</span> <span class=ow>in</span> <span class=n>support_set</span><span class=p>:</span>
        <span class=n>support_emb</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>forward_one</span><span class=p>(</span><span class=n>support_sample</span><span class=p>)</span>
        <span class=n>dist</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>pairwise_distance</span><span class=p>(</span><span class=n>query_emb</span><span class=p>,</span> <span class=n>support_emb</span><span class=p>)</span>
        <span class=n>distances</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dist</span><span class=p>)</span>

    <span class=n>nearest_idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>distances</span><span class=p>))</span>
    <span class=k>return</span> <span class=n>support_labels</span><span class=p>[</span><span class=n>nearest_idx</span><span class=p>]</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Type</th> <th>Key Idea</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Prototypical</td> <td>Few-shot</td> <td>Class prototypes</td> <td>Simple, effective</td> <td>Assumes clustered classes</td> </tr> <tr> <td>Matching</td> <td>Few-shot</td> <td>Attention over support</td> <td>Flexible</td> <td>More complex</td> </tr> <tr> <td>MAML</td> <td>Meta-learning</td> <td>Learn initialization</td> <td>General</td> <td>Slow, memory-intensive</td> </tr> <tr> <td>Attribute-based</td> <td>Zero-shot</td> <td>Semantic attributes</td> <td>True zero-shot</td> <td>Needs attribute annotations</td> </tr> <tr> <td>Siamese</td> <td>Metric learning</td> <td>Learn similarity</td> <td>Versatile</td> <td>Requires pairs</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced learning paradigms.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Few-shot: learn from K examples"</li> <li>"Zero-shot: unseen classes via attributes"</li> <li>Prototypical networks: class centroids</li> <li>MAML: meta-learning initialization</li> <li>"Metric learning: learn similarity function"</li> <li>Use cases: rare diseases, new products, low-resource languages</li> </ul> </div> </details> <hr> <h3 id=what-are-optimizers-in-neural-networks-most-tech-companies-interview-question>What are Optimizers in Neural Networks? - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Optimization</code>, <code>Neural Networks</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Optimizers:</strong></p> <p>Algorithms that update model weights to minimize loss function.</p> <p><strong>1. Stochastic Gradient Descent (SGD):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=c1># Basic SGD</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># With momentum (accelerate in consistent direction)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>

<span class=c1># With Nesterov momentum (look ahead)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>nesterov</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Training loop</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
    <span class=c1># Forward</span>
    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

    <span class=c1># Backward</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>  <span class=c1># Clear previous gradients</span>
    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>  <span class=c1># Compute gradients</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>  <span class=c1># Update weights</span>
</code></pre></div> <p><strong>2. Adam (Adaptive Moment Estimation):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Most popular optimizer</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>),</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>)</span>

<span class=c1># Custom implementation</span>
<span class=k>class</span><span class=w> </span><span class=nc>AdamOptimizer</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>beta1</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>beta2</span><span class=o>=</span><span class=mf>0.999</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>=</span> <span class=n>lr</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>beta1</span> <span class=o>=</span> <span class=n>beta1</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>beta2</span> <span class=o>=</span> <span class=n>beta2</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>

        <span class=c1># Initialize moment estimates</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>m</span> <span class=o>=</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v</span> <span class=o>=</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>t</span> <span class=o>=</span> <span class=mi>0</span>

    <span class=k>def</span><span class=w> </span><span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>t</span> <span class=o>+=</span> <span class=mi>1</span>

        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>):</span>
            <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
                <span class=k>continue</span>

            <span class=n>grad</span> <span class=o>=</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span>

            <span class=c1># Update biased first moment</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>m</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta1</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>m</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta1</span><span class=p>)</span> <span class=o>*</span> <span class=n>grad</span>

            <span class=c1># Update biased second moment</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta2</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>grad</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

            <span class=c1># Bias correction</span>
            <span class=n>m_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>m</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta1</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>t</span><span class=p>)</span>
            <span class=n>v_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta2</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>t</span><span class=p>)</span>

            <span class=c1># Update parameters</span>
            <span class=n>param</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>*</span> <span class=n>m_hat</span> <span class=o>/</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>v_hat</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</code></pre></div> <p><strong>3. RMSprop:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Good for RNNs</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>RMSprop</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>)</span>

<span class=c1># Implementation</span>
<span class=k>class</span><span class=w> </span><span class=nc>RMSpropOptimizer</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>=</span> <span class=n>lr</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>

        <span class=c1># Running average of squared gradients</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v</span> <span class=o>=</span> <span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>):</span>
            <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
                <span class=k>continue</span>

            <span class=n>grad</span> <span class=o>=</span> <span class=n>param</span><span class=o>.</span><span class=n>grad</span>

            <span class=c1># Update running average</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>grad</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

            <span class=c1># Update parameters</span>
            <span class=n>param</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>*</span> <span class=n>grad</span> <span class=o>/</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</code></pre></div> <p><strong>4. AdamW (Adam with Weight Decay):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Better regularization than Adam</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># Difference from Adam:</span>
<span class=c1># Adam: weight_decay is added to gradient</span>
<span class=c1># AdamW: weight_decay is applied directly to weights</span>
</code></pre></div> <p><strong>5. AdaGrad:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Adapts learning rate per parameter</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adagrad</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># Good for: sparse data, different feature scales</span>
<span class=c1># Issue: learning rate decays too aggressively</span>
</code></pre></div> <p><strong>6. Adadelta:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Extension of Adagrad (doesn&#39;t decay to zero)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adadelta</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>rho</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Compare optimizers on same problem</span>
<span class=n>optimizers</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;SGD&#39;</span><span class=p>:</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model1</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>),</span>
    <span class=s1>&#39;SGD+Momentum&#39;</span><span class=p>:</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model2</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>),</span>
    <span class=s1>&#39;RMSprop&#39;</span><span class=p>:</span> <span class=n>optim</span><span class=o>.</span><span class=n>RMSprop</span><span class=p>(</span><span class=n>model3</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>),</span>
    <span class=s1>&#39;Adam&#39;</span><span class=p>:</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model4</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>),</span>
    <span class=s1>&#39;AdamW&#39;</span><span class=p>:</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model5</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>histories</span> <span class=o>=</span> <span class=p>{</span><span class=n>name</span><span class=p>:</span> <span class=p>[]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>optimizers</span><span class=o>.</span><span class=n>keys</span><span class=p>()}</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>optimizer</span> <span class=ow>in</span> <span class=n>optimizers</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=p>[</span><span class=n>name</span><span class=p>]</span>
    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>50</span><span class=p>):</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>)</span>
        <span class=n>histories</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>losses</span> <span class=ow>in</span> <span class=n>histories</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>losses</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>name</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Optimizer Comparison&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>yscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Learning Rate Scheduling:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Combine optimizer with learning rate scheduler</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>

<span class=c1># Step decay</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Cosine annealing</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>CosineAnnealingLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>

<span class=c1># Reduce on plateau</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;min&#39;</span><span class=p>,</span> 
                                                 <span class=n>factor</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># One cycle</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>OneCycleLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>max_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> 
                                         <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>),</span> 
                                         <span class=n>epochs</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>

<span class=c1># Training loop with scheduler</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>50</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>train_step</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

        <span class=c1># For OneCycleLR: step every batch</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>OneCycleLR</span><span class=p>):</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

    <span class=c1># For others: step every epoch</span>
    <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>OneCycleLR</span><span class=p>):</span>
        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>scheduler</span><span class=p>,</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>):</span>
            <span class=n>val_loss</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>val_loader</span><span class=p>)</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>val_loss</span><span class=p>)</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <table> <thead> <tr> <th>Optimizer</th> <th>Pros</th> <th>Cons</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td>SGD</td> <td>Simple, proven</td> <td>Slow, needs tuning</td> <td>Baseline, small models</td> </tr> <tr> <td>SGD+Momentum</td> <td>Faster convergence</td> <td>Still needs tuning</td> <td>CNNs, proven recipes</td> </tr> <tr> <td>RMSprop</td> <td>Adaptive LR</td> <td>Can be unstable</td> <td>RNNs</td> </tr> <tr> <td>Adam</td> <td>Fast, adaptive, popular</td> <td>Can overfit</td> <td>Default choice</td> </tr> <tr> <td>AdamW</td> <td>Better regularization</td> <td>Slightly slower</td> <td>Large models, Transformers</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Training knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"SGD: simple but slow"</li> <li>"Adam: adaptive learning rates"</li> <li>"Momentum: accelerate consistent directions"</li> <li>"AdamW: decouple weight decay"</li> <li>"RMSprop: good for RNNs"</li> <li>Mentions learning rate scheduling</li> <li>"Adam default, but SGD+momentum for vision"</li> </ul> </div> </details> <hr> <h3 id=explain-class-imbalance-handling-techniques-most-tech-companies-interview-question>Explain Class Imbalance Handling Techniques - Most Tech Companies Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Imbalanced Data</code>, <code>Sampling</code>, <code>Loss Functions</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Class Imbalance:</strong></p> <p>When one class has significantly more samples than others (e.g., fraud: 0.1% positive).</p> <p><strong>1. Resampling:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>imblearn.over_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTE</span><span class=p>,</span> <span class=n>ADASYN</span><span class=p>,</span> <span class=n>RandomOverSampler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.under_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomUnderSampler</span><span class=p>,</span> <span class=n>TomekLinks</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.combine</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTETomek</span>

<span class=c1># Original imbalanced data</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original: </span><span class=si>{</span><span class=n>Counter</span><span class=p>(</span><span class=n>y_train</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=c1># {0: 9900, 1: 100}</span>

<span class=c1># 1a. Random Over-sampling</span>
<span class=n>ros</span> <span class=o>=</span> <span class=n>RandomOverSampler</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>ros</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Over-sampled: </span><span class=si>{</span><span class=n>Counter</span><span class=p>(</span><span class=n>y_resampled</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># 1b. SMOTE (Synthetic Minority Over-sampling)</span>
<span class=n>smote</span> <span class=o>=</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smote</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Custom SMOTE implementation</span>
<span class=k>def</span><span class=w> </span><span class=nf>simple_smote</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Generate synthetic samples&quot;&quot;&quot;</span>
    <span class=kn>from</span><span class=w> </span><span class=nn>sklearn.neighbors</span><span class=w> </span><span class=kn>import</span> <span class=n>NearestNeighbors</span>

    <span class=n>minority_class</span> <span class=o>=</span> <span class=mi>1</span>
    <span class=n>X_minority</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=n>minority_class</span><span class=p>]</span>

    <span class=c1># Find K nearest neighbors</span>
    <span class=n>nn</span> <span class=o>=</span> <span class=n>NearestNeighbors</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=n>k</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>nn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_minority</span><span class=p>)</span>

    <span class=n>synthetic_samples</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>sample</span> <span class=ow>in</span> <span class=n>X_minority</span><span class=p>:</span>
        <span class=c1># Get neighbors</span>
        <span class=n>neighbors</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>kneighbors</span><span class=p>([</span><span class=n>sample</span><span class=p>],</span> <span class=n>return_distance</span><span class=o>=</span><span class=kc>False</span><span class=p>)[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>:]</span>

        <span class=c1># Create synthetic samples</span>
        <span class=k>for</span> <span class=n>neighbor_idx</span> <span class=ow>in</span> <span class=n>neighbors</span><span class=p>:</span>
            <span class=c1># Random point between sample and neighbor</span>
            <span class=n>alpha</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span>
            <span class=n>synthetic</span> <span class=o>=</span> <span class=n>sample</span> <span class=o>+</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>X_minority</span><span class=p>[</span><span class=n>neighbor_idx</span><span class=p>]</span> <span class=o>-</span> <span class=n>sample</span><span class=p>)</span>
            <span class=n>synthetic_samples</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>synthetic</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X</span><span class=p>,</span> <span class=n>synthetic_samples</span><span class=p>])</span>

<span class=c1># 1c. Under-sampling</span>
<span class=n>rus</span> <span class=o>=</span> <span class=n>RandomUnderSampler</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>rus</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># 1d. Combined (SMOTETomek)</span>
<span class=n>smt</span> <span class=o>=</span> <span class=n>SMOTETomek</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smt</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>2. Class Weights:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.utils.class_weight</span><span class=w> </span><span class=kn>import</span> <span class=n>compute_class_weight</span>

<span class=c1># Compute weights inversely proportional to class frequency</span>
<span class=n>classes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y_train</span><span class=p>)</span>
<span class=n>class_weights</span> <span class=o>=</span> <span class=n>compute_class_weight</span><span class=p>(</span><span class=s1>&#39;balanced&#39;</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>classes</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y_train</span><span class=p>)</span>
<span class=n>class_weights_dict</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>classes</span><span class=p>,</span> <span class=n>class_weights</span><span class=p>))</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Class weights: </span><span class=si>{</span><span class=n>class_weights_dict</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=c1># {0: 0.505, 1: 50.5}  # Minority class weighted 100x more</span>

<span class=c1># Scikit-learn</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>class_weight</span><span class=o>=</span><span class=s1>&#39;balanced&#39;</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># PyTorch</span>
<span class=n>class_weights_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.505</span><span class=p>,</span> <span class=mf>50.5</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>weight</span><span class=o>=</span><span class=n>class_weights_tensor</span><span class=p>)</span>

<span class=c1># Training</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>  <span class=c1># Weighted loss</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

<span class=c1># TensorFlow/Keras</span>
<span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span>
    <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span>
    <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span>
    <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>]</span>
<span class=p>)</span>

<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>class_weight</span><span class=o>=</span><span class=n>class_weights_dict</span><span class=p>)</span>
</code></pre></div> <p><strong>3. Focal Loss:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>

<span class=k>class</span><span class=w> </span><span class=nc>FocalLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Focus on hard examples&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>2.0</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        inputs: [batch_size, num_classes]</span>
<span class=sd>        targets: [batch_size]</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>ce_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>

        <span class=c1># Get probabilities</span>
        <span class=n>p</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>ce_loss</span><span class=p>)</span>

        <span class=c1># Focal term: (1-p)^gamma</span>
        <span class=n>focal_weight</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span>

        <span class=c1># Alpha term (class balancing)</span>
        <span class=n>alpha_t</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>targets</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>targets</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>

        <span class=n>loss</span> <span class=o>=</span> <span class=n>alpha_t</span> <span class=o>*</span> <span class=n>focal_weight</span> <span class=o>*</span> <span class=n>ce_loss</span>

        <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

<span class=c1># Usage</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>FocalLoss</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>2.0</span><span class=p>)</span>

<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>4. Ensemble Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>BalancedRandomForestClassifier</span><span class=p>,</span> <span class=n>BalancedBaggingClassifier</span><span class=p>,</span> <span class=n>EasyEnsembleClassifier</span>

<span class=c1># Balanced Random Forest (under-sample each tree)</span>
<span class=n>brf</span> <span class=o>=</span> <span class=n>BalancedRandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
<span class=n>brf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Balanced Bagging</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.tree</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>
<span class=n>bbc</span> <span class=o>=</span> <span class=n>BalancedBaggingClassifier</span><span class=p>(</span>
    <span class=n>base_estimator</span><span class=o>=</span><span class=n>DecisionTreeClassifier</span><span class=p>(),</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>bbc</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Easy Ensemble (multiple balanced subsets)</span>
<span class=n>eec</span> <span class=o>=</span> <span class=n>EasyEnsembleClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>eec</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>5. Threshold Adjustment:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>precision_recall_curve</span>

<span class=c1># Train model normally</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Get probabilities</span>
<span class=n>y_scores</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_val</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Find optimal threshold</span>
<span class=n>precision</span><span class=p>,</span> <span class=n>recall</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_val</span><span class=p>,</span> <span class=n>y_scores</span><span class=p>)</span>

<span class=c1># Optimize F1-score</span>
<span class=n>f1_scores</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>precision</span> <span class=o>*</span> <span class=n>recall</span> <span class=o>/</span> <span class=p>(</span><span class=n>precision</span> <span class=o>+</span> <span class=n>recall</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span>
<span class=n>optimal_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>f1_scores</span><span class=p>)</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>optimal_idx</span><span class=p>]</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Optimal threshold: </span><span class=si>{</span><span class=n>optimal_threshold</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (default: 0.5)&quot;</span><span class=p>)</span>

<span class=c1># Predict with optimal threshold</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_scores</span> <span class=o>&gt;=</span> <span class=n>optimal_threshold</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>

<span class=c1># Visualize</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>thresholds</span><span class=p>,</span> <span class=n>precision</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Precision&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>thresholds</span><span class=p>,</span> <span class=n>recall</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Recall&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>thresholds</span><span class=p>,</span> <span class=n>f1_scores</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;F1-score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=n>optimal_threshold</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Optimal&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Threshold&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Threshold Selection&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>6. Anomaly Detection Approach:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># When extreme imbalance (e.g., 0.01% positive)</span>
<span class=c1># Treat as anomaly/outlier detection</span>

<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>IsolationForest</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.svm</span><span class=w> </span><span class=kn>import</span> <span class=n>OneClassSVM</span>

<span class=c1># Train only on majority class</span>
<span class=n>X_normal</span> <span class=o>=</span> <span class=n>X_train</span><span class=p>[</span><span class=n>y_train</span> <span class=o>==</span> <span class=mi>0</span><span class=p>]</span>

<span class=c1># Isolation Forest</span>
<span class=n>iso_forest</span> <span class=o>=</span> <span class=n>IsolationForest</span><span class=p>(</span><span class=n>contamination</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>iso_forest</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_normal</span><span class=p>)</span>

<span class=c1># Predict: -1 for anomaly (minority), 1 for normal</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>iso_forest</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=n>y_pred_binary</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_pred</span> <span class=o>==</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>Pros</th> <th>Cons</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td>SMOTE</td> <td>Creates synthetic samples</td> <td>Can create noise</td> <td>Moderate imbalance (1:10)</td> </tr> <tr> <td>Class Weights</td> <td>Simple, fast</td> <td>May not help much</td> <td>Any imbalance</td> </tr> <tr> <td>Focal Loss</td> <td>Focus on hard examples</td> <td>Needs tuning</td> <td>Deep learning</td> </tr> <tr> <td>Under-sampling</td> <td>Fast training</td> <td>Loses information</td> <td>Lots of data</td> </tr> <tr> <td>Threshold Tuning</td> <td>No retraining needed</td> <td>Requires validation set</td> <td>After training</td> </tr> <tr> <td>Anomaly Detection</td> <td>Principled approach</td> <td>Different paradigm</td> <td>Extreme imbalance (1:1000+)</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical problem-solving.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Multiple techniques (sampling, weighting, loss)</li> <li>"SMOTE: synthetic samples"</li> <li>"Class weights: penalize errors differently"</li> <li>"Focal loss: focus on hard examples"</li> <li>"Adjust threshold after training"</li> <li>"Anomaly detection for extreme cases"</li> <li>Mentions evaluation metrics (PR-AUC, not accuracy)</li> </ul> </div> </details> <hr> <h3 id=explainable-ai-xai-google-amazon-microsoft-interview-question>Explainable AI (XAI) - Google, Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Interpretability</code>, <code>SHAP</code>, <code>LIME</code>, <code>Trust</code>, <code>Compliance</code> | <strong>Asked by:</strong> Google, Amazon, Microsoft, Meta</p> <p><strong>Question:</strong> What is Explainable AI (XAI) and why is it important? Explain different techniques for making machine learning models interpretable.</p> <details class=success> <summary>View Answer</summary> <p><strong>Explainable AI (XAI)</strong> refers to methods and techniques that make machine learning model predictions understandable to humans. It's crucial for trust, debugging, compliance, and ethical AI deployment.</p> <p><strong>Why XAI Matters:</strong></p> <ol> <li><strong>Trust</strong>: Stakeholders need to understand model decisions</li> <li><strong>Debugging</strong>: Identify model errors and biases</li> <li><strong>Compliance</strong>: Regulations (GDPR, etc.) require explanations</li> <li><strong>Fairness</strong>: Detect and mitigate discrimination</li> <li><strong>Safety</strong>: Critical in healthcare, finance, autonomous systems</li> </ol> <p><strong>Main XAI Techniques:</strong></p> <p><strong>1. SHAP (SHapley Additive exPlanations)</strong></p> <ul> <li>Based on game theory (Shapley values)</li> <li>Provides consistent feature attributions</li> <li>Works for any model type</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>shap</span>
<span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>load_breast_cancer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Load data</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>load_breast_cancer</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>as_frame</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=c1># Train model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># SHAP explainer</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>shap</span><span class=o>.</span><span class=n>TreeExplainer</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
<span class=n>shap_values</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>shap_values</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=n>shap</span><span class=o>.</span><span class=n>summary_plot</span><span class=p>(</span><span class=n>shap_values</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>show</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;SHAP Feature Importance&quot;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;shap_summary.png&#39;</span><span class=p>)</span>

<span class=c1># Single prediction explanation</span>
<span class=n>instance_idx</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>shap</span><span class=o>.</span><span class=n>force_plot</span><span class=p>(</span>
    <span class=n>explainer</span><span class=o>.</span><span class=n>expected_value</span><span class=p>,</span>
    <span class=n>shap_values</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>],</span>
    <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>],</span>
    <span class=n>matplotlib</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>show</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;shap_force.png&#39;</span><span class=p>)</span>

<span class=c1># Waterfall plot for one prediction</span>
<span class=n>shap</span><span class=o>.</span><span class=n>plots</span><span class=o>.</span><span class=n>waterfall</span><span class=p>(</span>
    <span class=n>shap</span><span class=o>.</span><span class=n>Explanation</span><span class=p>(</span>
        <span class=n>values</span><span class=o>=</span><span class=n>shap_values</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>],</span>
        <span class=n>base_values</span><span class=o>=</span><span class=n>explainer</span><span class=o>.</span><span class=n>expected_value</span><span class=p>,</span>
        <span class=n>data</span><span class=o>=</span><span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>],</span>
        <span class=n>feature_names</span><span class=o>=</span><span class=n>X_test</span><span class=o>.</span><span class=n>columns</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
    <span class=p>),</span>
    <span class=n>show</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;shap_waterfall.png&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>2. LIME (Local Interpretable Model-agnostic Explanations)</strong></p> <ul> <li>Explains individual predictions</li> <li>Creates local linear approximations</li> <li>Works for any black-box model</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>lime</span><span class=w> </span><span class=kn>import</span> <span class=n>lime_tabular</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># LIME explainer</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>lime_tabular</span><span class=o>.</span><span class=n>LimeTabularExplainer</span><span class=p>(</span>
    <span class=n>X_train</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>feature_names</span><span class=o>=</span><span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=o>.</span><span class=n>tolist</span><span class=p>(),</span>
    <span class=n>class_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;Malignant&#39;</span><span class=p>,</span> <span class=s1>&#39;Benign&#39;</span><span class=p>],</span>
    <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;classification&#39;</span>
<span class=p>)</span>

<span class=c1># Explain a prediction</span>
<span class=n>instance_idx</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>exp</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>explain_instance</span><span class=p>(</span>
    <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>,</span>
    <span class=n>num_features</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=c1># Show explanation</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Prediction:&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=n>instance_idx</span><span class=p>]])[</span><span class=mi>0</span><span class=p>])</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Feature contributions:&quot;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>feature</span><span class=p>,</span> <span class=n>weight</span> <span class=ow>in</span> <span class=n>exp</span><span class=o>.</span><span class=n>as_list</span><span class=p>():</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>weight</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize</span>
<span class=n>fig</span> <span class=o>=</span> <span class=n>exp</span><span class=o>.</span><span class=n>as_pyplot_figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;lime_explanation.png&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>3. Permutation Feature Importance</strong></p> <ul> <li>Measures feature importance by shuffling</li> <li>Model-agnostic</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.inspection</span><span class=w> </span><span class=kn>import</span> <span class=n>permutation_importance</span>

<span class=c1># Calculate permutation importance</span>
<span class=n>perm_importance</span> <span class=o>=</span> <span class=n>permutation_importance</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span>
    <span class=n>n_repeats</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span>
<span class=p>)</span>

<span class=c1># Sort and display</span>
<span class=n>importance_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X_test</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;importance&#39;</span><span class=p>:</span> <span class=n>perm_importance</span><span class=o>.</span><span class=n>importances_mean</span><span class=p>,</span>
    <span class=s1>&#39;std&#39;</span><span class=p>:</span> <span class=n>perm_importance</span><span class=o>.</span><span class=n>importances_std</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=n>importance_df</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span>
    <span class=n>importance_df</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)[</span><span class=s1>&#39;feature&#39;</span><span class=p>],</span>
    <span class=n>importance_df</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)[</span><span class=s1>&#39;importance&#39;</span><span class=p>]</span>
<span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Permutation Importance&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Top 10 Features&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>gca</span><span class=p>()</span><span class=o>.</span><span class=n>invert_yaxis</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;permutation_importance.png&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>4. Partial Dependence Plots (PDP)</strong></p> <ul> <li>Shows marginal effect of features</li> <li>Visualizes feature-target relationships</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.inspection</span><span class=w> </span><span class=kn>import</span> <span class=n>partial_dependence</span><span class=p>,</span> <span class=n>PartialDependenceDisplay</span>

<span class=c1># Select features to analyze</span>
<span class=n>features_to_plot</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)]</span>  <span class=c1># Individual and interaction</span>

<span class=c1># Create PDP</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
<span class=n>display</span> <span class=o>=</span> <span class=n>PartialDependenceDisplay</span><span class=o>.</span><span class=n>from_estimator</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span>
    <span class=n>X_train</span><span class=p>,</span>
    <span class=n>features_to_plot</span><span class=p>,</span>
    <span class=n>feature_names</span><span class=o>=</span><span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=n>ax</span><span class=o>=</span><span class=n>ax</span>
<span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;partial_dependence.png&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>5. Integrated Gradients (for Neural Networks)</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

<span class=k>def</span><span class=w> </span><span class=nf>integrated_gradients</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_tensor</span><span class=p>,</span> <span class=n>baseline</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=mi>50</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Calculate integrated gradients.&quot;&quot;&quot;</span>
    <span class=k>if</span> <span class=n>baseline</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>baseline</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>input_tensor</span><span class=p>)</span>

    <span class=c1># Generate interpolated inputs</span>
    <span class=n>alphas</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>steps</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>interpolated</span> <span class=o>=</span> <span class=n>baseline</span> <span class=o>+</span> <span class=n>alphas</span> <span class=o>*</span> <span class=p>(</span><span class=n>input_tensor</span> <span class=o>-</span> <span class=n>baseline</span><span class=p>)</span>
    <span class=n>interpolated</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>

    <span class=c1># Calculate gradients</span>
    <span class=n>gradients</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>interp</span> <span class=ow>in</span> <span class=n>interpolated</span><span class=p>:</span>
        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>interp</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
        <span class=n>output</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>gradients</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>interp</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>clone</span><span class=p>())</span>
        <span class=n>interp</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>

    <span class=c1># Average gradients and scale</span>
    <span class=n>avg_gradients</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>gradients</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
    <span class=n>integrated_grads</span> <span class=o>=</span> <span class=p>(</span><span class=n>input_tensor</span> <span class=o>-</span> <span class=n>baseline</span><span class=p>)</span> <span class=o>*</span> <span class=n>avg_gradients</span>

    <span class=k>return</span> <span class=n>integrated_grads</span>

<span class=c1># Example usage</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>SimpleNN</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>
<span class=n>input_sample</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>30</span><span class=p>)</span>
<span class=n>attributions</span> <span class=o>=</span> <span class=n>integrated_gradients</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_sample</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Feature attributions:&quot;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>attr</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>attributions</span><span class=p>):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Feature </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>attr</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison of XAI Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Scope</th> <th>Model-Agnostic</th> <th>Consistency</th> <th>Speed</th> </tr> </thead> <tbody> <tr> <td>SHAP</td> <td>Local/Global</td> <td>Yes</td> <td>High</td> <td>Slow</td> </tr> <tr> <td>LIME</td> <td>Local</td> <td>Yes</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td>Permutation</td> <td>Global</td> <td>Yes</td> <td>High</td> <td>Slow</td> </tr> <tr> <td>PDP</td> <td>Global</td> <td>Yes</td> <td>High</td> <td>Medium</td> </tr> <tr> <td>Integrated Gradients</td> <td>Local</td> <td>No (NN only)</td> <td>High</td> <td>Fast</td> </tr> </tbody> </table> <p><strong>When to Use Each:</strong></p> <ul> <li><strong>SHAP</strong>: Best for comprehensive, theoretically-grounded explanations</li> <li><strong>LIME</strong>: Quick local explanations for any model</li> <li><strong>Permutation</strong>: Global feature importance without assumptions</li> <li><strong>PDP</strong>: Understanding feature effects and interactions</li> <li><strong>Integrated Gradients</strong>: Deep learning models</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Multiple XAI techniques (SHAP, LIME, permutation)</li> <li>"SHAP: game theory, Shapley values"</li> <li>"LIME: local linear approximations"</li> <li>"Model-agnostic vs model-specific"</li> <li>Practical considerations (speed vs accuracy)</li> <li>"Compliance and trust requirements"</li> <li>Trade-offs between methods</li> <li>Real-world deployment challenges</li> </ul> </div> </details> <hr> <h3 id=curriculum-learning-deepmind-openai-meta-ai-interview-question>Curriculum Learning - DeepMind, OpenAI, Meta AI Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Training Strategy</code>, <code>Deep Learning</code>, <code>Sample Ordering</code> | <strong>Asked by:</strong> DeepMind, OpenAI, Meta AI, Google Research</p> <p><strong>Question:</strong> What is curriculum learning and how can it improve model training? Provide examples and implementation strategies.</p> <details class=success> <summary>View Answer</summary> <p><strong>Curriculum Learning</strong> is a training strategy where a model learns from examples organized from easy to hard, mimicking how humans learn. It can accelerate training, improve generalization, and enable learning of complex tasks.</p> <p><strong>Key Concepts:</strong></p> <ol> <li><strong>Difficulty Scoring</strong>: Quantify example difficulty</li> <li><strong>Ordering Strategy</strong>: Sequence examples by difficulty</li> <li><strong>Pacing</strong>: Gradually increase task complexity</li> <li><strong>Self-Paced</strong>: Let model determine its own curriculum</li> </ol> <p><strong>Implementation Example:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>Dataset</span><span class=p>,</span> <span class=n>DataLoader</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>

<span class=k>class</span><span class=w> </span><span class=nc>CurriculumDataset</span><span class=p>(</span><span class=n>Dataset</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Dataset with curriculum learning support.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>difficulty_scores</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>

        <span class=c1># Calculate difficulty if not provided</span>
        <span class=k>if</span> <span class=n>difficulty_scores</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_calculate_difficulty</span><span class=p>()</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span> <span class=o>=</span> <span class=n>difficulty_scores</span>

        <span class=c1># Sort by difficulty (easy first)</span>
        <span class=n>sorted_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>X</span><span class=p>[</span><span class=n>sorted_indices</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>y</span><span class=p>[</span><span class=n>sorted_indices</span><span class=p>]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span><span class=p>[</span><span class=n>sorted_indices</span><span class=p>]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>_calculate_difficulty</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Estimate difficulty (e.g., distance to decision boundary).&quot;&quot;&quot;</span>
        <span class=c1># Simple heuristic: variance in features</span>
        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__getitem__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>X</span><span class=p>[</span><span class=n>idx</span><span class=p>],</span> <span class=bp>self</span><span class=o>.</span><span class=n>y</span><span class=p>[</span><span class=n>idx</span><span class=p>],</span> <span class=bp>self</span><span class=o>.</span><span class=n>difficulty</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>

<span class=k>class</span><span class=w> </span><span class=nc>CurriculumTrainer</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Trainer with curriculum learning strategies.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cpu&#39;</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_vanilla_curriculum</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>dataset</span><span class=p>,</span>
        <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
        <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span>
    <span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Simple curriculum: easy to hard.&quot;&quot;&quot;</span>
        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>

        <span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
            <span class=n>dataset</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
            <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span>  <span class=c1># Keep difficulty order</span>
        <span class=p>)</span>

        <span class=n>history</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
            <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>

            <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
                <span class=n>X_batch</span> <span class=o>=</span> <span class=n>X_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
                <span class=n>y_batch</span> <span class=o>=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
                <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

                <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=n>predicted</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>y_batch</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

            <span class=n>acc</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>
            <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>loader</span><span class=p>)</span>
            <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s1>&#39;epoch&#39;</span><span class=p>:</span> <span class=n>epoch</span><span class=p>,</span> <span class=s1>&#39;loss&#39;</span><span class=p>:</span> <span class=n>avg_loss</span><span class=p>,</span> <span class=s1>&#39;acc&#39;</span><span class=p>:</span> <span class=n>acc</span><span class=p>})</span>

            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc=</span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>history</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_self_paced_curriculum</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>dataset</span><span class=p>,</span>
        <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
        <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
        <span class=n>lambda_init</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
        <span class=n>lambda_growth</span><span class=o>=</span><span class=mf>1.1</span>
    <span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Self-paced learning: model chooses examples.&quot;&quot;&quot;</span>
        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>

        <span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
            <span class=n>dataset</span><span class=p>,</span>
            <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
            <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
        <span class=p>)</span>

        <span class=n>lambda_param</span> <span class=o>=</span> <span class=n>lambda_init</span>
        <span class=n>history</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
            <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>selected_count</span> <span class=o>=</span> <span class=mi>0</span>

            <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
                <span class=n>X_batch</span> <span class=o>=</span> <span class=n>X_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
                <span class=n>y_batch</span> <span class=o>=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
                <span class=n>losses</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

                <span class=c1># Self-paced weighting: select easy examples</span>
                <span class=n>weights</span> <span class=o>=</span> <span class=p>(</span><span class=n>losses</span> <span class=o>&lt;=</span> <span class=n>lambda_param</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
                <span class=n>selected_count</span> <span class=o>+=</span> <span class=n>weights</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

                <span class=c1># Weighted loss</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>weights</span> <span class=o>*</span> <span class=n>losses</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
                <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

                <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=n>predicted</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>y_batch</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

            <span class=c1># Increase lambda to include harder examples</span>
            <span class=n>lambda_param</span> <span class=o>*=</span> <span class=n>lambda_growth</span>

            <span class=n>acc</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>
            <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>loader</span><span class=p>)</span>
            <span class=n>selection_rate</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>selected_count</span> <span class=o>/</span> <span class=n>total</span>

            <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
                <span class=s1>&#39;epoch&#39;</span><span class=p>:</span> <span class=n>epoch</span><span class=p>,</span>
                <span class=s1>&#39;loss&#39;</span><span class=p>:</span> <span class=n>avg_loss</span><span class=p>,</span>
                <span class=s1>&#39;acc&#39;</span><span class=p>:</span> <span class=n>acc</span><span class=p>,</span>
                <span class=s1>&#39;selection_rate&#39;</span><span class=p>:</span> <span class=n>selection_rate</span><span class=p>,</span>
                <span class=s1>&#39;lambda&#39;</span><span class=p>:</span> <span class=n>lambda_param</span>
            <span class=p>})</span>

            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc=</span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%, &quot;</span>
                  <span class=sa>f</span><span class=s2>&quot;Selected=</span><span class=si>{</span><span class=n>selection_rate</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%, Œª=</span><span class=si>{</span><span class=n>lambda_param</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>history</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_with_difficulty_stages</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>dataset</span><span class=p>,</span>
        <span class=n>stages</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
        <span class=n>epochs_per_stage</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
        <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span>
    <span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Train in stages with increasing difficulty.&quot;&quot;&quot;</span>
        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>

        <span class=c1># Divide dataset into difficulty stages</span>
        <span class=n>dataset_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
        <span class=n>stage_size</span> <span class=o>=</span> <span class=n>dataset_size</span> <span class=o>//</span> <span class=n>stages</span>

        <span class=n>history</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>stage</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>stages</span><span class=p>):</span>
            <span class=n>start_idx</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># Always start from beginning (easy)</span>
            <span class=n>end_idx</span> <span class=o>=</span> <span class=p>(</span><span class=n>stage</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>stage_size</span>

            <span class=c1># Create subset for this stage</span>
            <span class=n>indices</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=n>end_idx</span><span class=p>,</span> <span class=n>dataset_size</span><span class=p>)))</span>
            <span class=n>subset</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Subset</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>indices</span><span class=p>)</span>
            <span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>subset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>=== Stage </span><span class=si>{</span><span class=n>stage</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>stages</span><span class=si>}</span><span class=s2>: Training on &quot;</span>
                  <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>subset</span><span class=p>)</span><span class=si>}</span><span class=s2> examples ===&quot;</span><span class=p>)</span>

            <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs_per_stage</span><span class=p>):</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
                <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
                <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
                <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>

                <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
                    <span class=n>X_batch</span> <span class=o>=</span> <span class=n>X_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
                    <span class=n>y_batch</span> <span class=o>=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

                    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
                    <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
                    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
                    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
                    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

                    <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
                    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
                    <span class=n>total</span> <span class=o>+=</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
                    <span class=n>correct</span> <span class=o>+=</span> <span class=n>predicted</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>y_batch</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

                <span class=n>acc</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>
                <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>loader</span><span class=p>)</span>

                <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
                    <span class=s1>&#39;stage&#39;</span><span class=p>:</span> <span class=n>stage</span><span class=p>,</span>
                    <span class=s1>&#39;epoch&#39;</span><span class=p>:</span> <span class=n>epoch</span><span class=p>,</span>
                    <span class=s1>&#39;loss&#39;</span><span class=p>:</span> <span class=n>avg_loss</span><span class=p>,</span>
                    <span class=s1>&#39;acc&#39;</span><span class=p>:</span> <span class=n>acc</span>
                <span class=p>})</span>

                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc=</span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>history</span>

<span class=c1># Example: Compare curriculum vs random training</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_classification</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>

<span class=c1># Generate synthetic data</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_classification</span><span class=p>(</span>
    <span class=n>n_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
    <span class=n>n_features</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
    <span class=n>n_informative</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span>
    <span class=n>n_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=c1># Simple model</span>
<span class=k>class</span><span class=w> </span><span class=nc>SimpleClassifier</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.3</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

<span class=c1># Create curriculum dataset</span>
<span class=n>curriculum_dataset</span> <span class=o>=</span> <span class=n>CurriculumDataset</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Train with curriculum</span>
<span class=n>model_curriculum</span> <span class=o>=</span> <span class=n>SimpleClassifier</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>trainer</span> <span class=o>=</span> <span class=n>CurriculumTrainer</span><span class=p>(</span><span class=n>model_curriculum</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training with Vanilla Curriculum:&quot;</span><span class=p>)</span>
<span class=n>history_curriculum</span> <span class=o>=</span> <span class=n>trainer</span><span class=o>.</span><span class=n>train_vanilla_curriculum</span><span class=p>(</span>
    <span class=n>curriculum_dataset</span><span class=p>,</span>
    <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span>
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training with Self-Paced Curriculum:&quot;</span><span class=p>)</span>
<span class=n>model_self_paced</span> <span class=o>=</span> <span class=n>SimpleClassifier</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>trainer_sp</span> <span class=o>=</span> <span class=n>CurriculumTrainer</span><span class=p>(</span><span class=n>model_self_paced</span><span class=p>)</span>
<span class=n>history_sp</span> <span class=o>=</span> <span class=n>trainer_sp</span><span class=o>.</span><span class=n>train_self_paced_curriculum</span><span class=p>(</span>
    <span class=n>curriculum_dataset</span><span class=p>,</span>
    <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span>
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training with Staged Curriculum:&quot;</span><span class=p>)</span>
<span class=n>model_staged</span> <span class=o>=</span> <span class=n>SimpleClassifier</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>trainer_staged</span> <span class=o>=</span> <span class=n>CurriculumTrainer</span><span class=p>(</span><span class=n>model_staged</span><span class=p>)</span>
<span class=n>history_staged</span> <span class=o>=</span> <span class=n>trainer_staged</span><span class=o>.</span><span class=n>train_with_difficulty_stages</span><span class=p>(</span>
    <span class=n>curriculum_dataset</span><span class=p>,</span>
    <span class=n>stages</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>epochs_per_stage</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Curriculum Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>Description</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td>Vanilla</td> <td>Easy‚ÜíHard ordering</td> <td>Stable, predictable training</td> </tr> <tr> <td>Self-Paced</td> <td>Model selects examples</td> <td>Noisy data, robust learning</td> </tr> <tr> <td>Staged</td> <td>Train in phases</td> <td>Complex multi-task learning</td> </tr> <tr> <td>Transfer</td> <td>Pre-train‚ÜíFine-tune</td> <td>Domain adaptation</td> </tr> </tbody> </table> <p><strong>Benefits:</strong></p> <ul> <li><strong>Faster Convergence</strong>: Reach good solutions quicker</li> <li><strong>Better Generalization</strong>: Avoid local minima</li> <li><strong>Stability</strong>: Smoother training dynamics</li> <li><strong>Sample Efficiency</strong>: Learn more from less data</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Easy-to-hard ordering, like human learning"</li> <li>"Difficulty scoring mechanisms"</li> <li>"Self-paced: model chooses curriculum"</li> <li>"Staged training for complex tasks"</li> <li>"Faster convergence, better generalization"</li> <li>Implementation strategies</li> <li>Trade-offs and challenges</li> <li>Real-world applications (robotics, NLP)</li> </ul> </div> </details> <hr> <h3 id=active-learning-google-research-snorkel-ai-scale-ai-interview-question>Active Learning - Google Research, Snorkel AI, Scale AI Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Labeling</code>, <code>Sample Selection</code>, <code>Uncertainty</code>, <code>Human-in-the-Loop</code> | <strong>Asked by:</strong> Google Research, Snorkel AI, Scale AI, Microsoft Research</p> <p><strong>Question:</strong> Explain active learning and how it reduces labeling costs. What are different query strategies for selecting samples to label?</p> <details class=success> <summary>View Answer</summary> <p><strong>Active Learning</strong> is a machine learning paradigm where the model iteratively selects the most informative unlabeled samples for human annotation, minimizing labeling costs while maximizing model performance.</p> <p><strong>Core Concept:</strong></p> <p>Instead of randomly labeling data, actively select samples that provide maximum information gain for the model.</p> <p><strong>Active Learning Cycle:</strong></p> <ol> <li>Train model on labeled data</li> <li>Select most informative unlabeled samples</li> <li>Get human labels for selected samples</li> <li>Add to training set</li> <li>Retrain and repeat</li> </ol> <p><strong>Query Strategies:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_classification</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>accuracy_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=k>class</span><span class=w> </span><span class=nc>ActiveLearner</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Active learning framework with multiple query strategies.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>X_labeled</span><span class=p>,</span> <span class=n>y_labeled</span><span class=p>,</span> <span class=n>X_unlabeled</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span> <span class=o>=</span> <span class=n>X_labeled</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span> <span class=o>=</span> <span class=n>y_labeled</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span> <span class=o>=</span> <span class=n>X_unlabeled</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>history</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>uncertainty_sampling</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Select samples with highest prediction uncertainty.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span>
        <span class=n>probs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span>

        <span class=c1># Least confident: 1 - max(prob)</span>
        <span class=n>uncertainty</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Select top uncertain samples</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>uncertainty</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>uncertainty</span>

    <span class=k>def</span><span class=w> </span><span class=nf>margin_sampling</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Select samples with smallest margin between top 2 classes.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span>
        <span class=n>probs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span>

        <span class=c1># Sort probabilities</span>
        <span class=n>sorted_probs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Margin: difference between top 2</span>
        <span class=n>margin</span> <span class=o>=</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>2</span><span class=p>]</span>

        <span class=c1># Select smallest margins (most uncertain)</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>margin</span><span class=p>)[:</span><span class=n>n_samples</span><span class=p>]</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>margin</span>

    <span class=k>def</span><span class=w> </span><span class=nf>entropy_sampling</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Select samples with highest entropy.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span>
        <span class=n>probs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span>

        <span class=c1># Calculate entropy</span>
        <span class=n>epsilon</span> <span class=o>=</span> <span class=mf>1e-10</span>  <span class=c1># Avoid log(0)</span>
        <span class=n>entropy</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>probs</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>probs</span> <span class=o>+</span> <span class=n>epsilon</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Select highest entropy</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>entropy</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>entropy</span>

    <span class=k>def</span><span class=w> </span><span class=nf>query_by_committee</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>n_committee</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Use committee of models to measure disagreement.&quot;&quot;&quot;</span>
        <span class=c1># Train committee of models with bootstrap</span>
        <span class=n>committee_predictions</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_committee</span><span class=p>):</span>
            <span class=c1># Bootstrap sample</span>
            <span class=n>indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span>
                <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>),</span>
                <span class=n>size</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>),</span>
                <span class=n>replace</span><span class=o>=</span><span class=kc>True</span>
            <span class=p>)</span>
            <span class=n>X_boot</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>
            <span class=n>y_boot</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>

            <span class=c1># Train committee member</span>
            <span class=n>model_copy</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span>
                <span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
                <span class=n>random_state</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1000</span><span class=p>)</span>
            <span class=p>)</span>
            <span class=n>model_copy</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_boot</span><span class=p>,</span> <span class=n>y_boot</span><span class=p>)</span>
            <span class=n>preds</span> <span class=o>=</span> <span class=n>model_copy</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span>
            <span class=n>committee_predictions</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>preds</span><span class=p>)</span>

        <span class=c1># Calculate vote entropy (disagreement)</span>
        <span class=n>committee_predictions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>committee_predictions</span><span class=p>)</span>
        <span class=n>disagreement</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)):</span>
            <span class=n>votes</span> <span class=o>=</span> <span class=n>committee_predictions</span><span class=p>[:,</span> <span class=n>i</span><span class=p>]</span>
            <span class=n>unique</span><span class=p>,</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>votes</span><span class=p>,</span> <span class=n>return_counts</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
            <span class=n>vote_dist</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>votes</span><span class=p>)</span>
            <span class=n>entropy</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>vote_dist</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>vote_dist</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>))</span>
            <span class=n>disagreement</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>entropy</span><span class=p>)</span>

        <span class=n>disagreement</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>disagreement</span><span class=p>)</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>disagreement</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>disagreement</span>

    <span class=k>def</span><span class=w> </span><span class=nf>expected_model_change</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Select samples that would change model most.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span>

        <span class=c1># Get current model parameters (for linear models)</span>
        <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=s1>&#39;feature_importances_&#39;</span><span class=p>):</span>
            <span class=n>current_importances</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>feature_importances_</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>current_importances</span> <span class=o>=</span> <span class=kc>None</span>

        <span class=n>changes</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)):</span>
            <span class=c1># Simulate adding this sample with predicted label</span>
            <span class=n>pred_label</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>[</span><span class=n>i</span><span class=p>]])[</span><span class=mi>0</span><span class=p>]</span>

            <span class=n>X_temp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>[</span><span class=n>i</span><span class=p>]])</span>
            <span class=n>y_temp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>,</span> <span class=n>pred_label</span><span class=p>)</span>

            <span class=c1># Retrain</span>
            <span class=n>temp_model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
            <span class=n>temp_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_temp</span><span class=p>,</span> <span class=n>y_temp</span><span class=p>)</span>

            <span class=c1># Calculate parameter change</span>
            <span class=k>if</span> <span class=n>current_importances</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                <span class=n>new_importances</span> <span class=o>=</span> <span class=n>temp_model</span><span class=o>.</span><span class=n>feature_importances_</span>
                <span class=n>change</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>new_importances</span> <span class=o>-</span> <span class=n>current_importances</span><span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>change</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span>  <span class=c1># Fallback</span>

            <span class=n>changes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>change</span><span class=p>)</span>

        <span class=n>changes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>changes</span><span class=p>)</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>changes</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>changes</span>

    <span class=k>def</span><span class=w> </span><span class=nf>diversity_sampling</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Select diverse samples using clustering.&quot;&quot;&quot;</span>
        <span class=kn>from</span><span class=w> </span><span class=nn>sklearn.cluster</span><span class=w> </span><span class=kn>import</span> <span class=n>KMeans</span>

        <span class=c1># Cluster unlabeled data</span>
        <span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>n_samples</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
        <span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span>

        <span class=c1># Select samples closest to cluster centers</span>
        <span class=n>selected_indices</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>center</span> <span class=ow>in</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>cluster_centers_</span><span class=p>:</span>
            <span class=n>distances</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span> <span class=o>-</span> <span class=n>center</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
            <span class=n>closest_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>distances</span><span class=p>)</span>
            <span class=k>if</span> <span class=n>closest_idx</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>selected_indices</span><span class=p>:</span>
                <span class=n>selected_indices</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>closest_idx</span><span class=p>)</span>

        <span class=n>selected_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>selected_indices</span><span class=p>[:</span><span class=n>n_samples</span><span class=p>])</span>

        <span class=k>return</span> <span class=n>selected_indices</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>label_samples</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>indices</span><span class=p>,</span> <span class=n>oracle_labels</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Add labeled samples to training set.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>[</span><span class=n>indices</span><span class=p>]])</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>,</span> <span class=n>oracle_labels</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>delete</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>,</span> <span class=n>indices</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>evaluate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Evaluate current model.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_labeled</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
        <span class=n>acc</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>acc</span>

<span class=c1># Compare active learning strategies</span>
<span class=k>def</span><span class=w> </span><span class=nf>compare_strategies</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>n_iterations</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>samples_per_iteration</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Compare different active learning strategies.&quot;&quot;&quot;</span>

    <span class=c1># Start with small labeled set</span>
    <span class=n>initial_size</span> <span class=o>=</span> <span class=mi>50</span>
    <span class=n>X_labeled</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=n>initial_size</span><span class=p>]</span>
    <span class=n>y_labeled</span> <span class=o>=</span> <span class=n>y</span><span class=p>[:</span><span class=n>initial_size</span><span class=p>]</span>
    <span class=n>X_unlabeled</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>initial_size</span><span class=p>:]</span>
    <span class=n>y_unlabeled</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>initial_size</span><span class=p>:]</span>  <span class=c1># Oracle labels (hidden)</span>

    <span class=n>strategies</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;Random&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
        <span class=s1>&#39;Uncertainty&#39;</span><span class=p>:</span> <span class=s1>&#39;uncertainty_sampling&#39;</span><span class=p>,</span>
        <span class=s1>&#39;Margin&#39;</span><span class=p>:</span> <span class=s1>&#39;margin_sampling&#39;</span><span class=p>,</span>
        <span class=s1>&#39;Entropy&#39;</span><span class=p>:</span> <span class=s1>&#39;entropy_sampling&#39;</span><span class=p>,</span>
        <span class=s1>&#39;Committee&#39;</span><span class=p>:</span> <span class=s1>&#39;query_by_committee&#39;</span>
    <span class=p>}</span>

    <span class=n>results</span> <span class=o>=</span> <span class=p>{</span><span class=n>name</span><span class=p>:</span> <span class=p>[]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>strategies</span><span class=o>.</span><span class=n>keys</span><span class=p>()}</span>

    <span class=k>for</span> <span class=n>strategy_name</span><span class=p>,</span> <span class=n>strategy_method</span> <span class=ow>in</span> <span class=n>strategies</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Running </span><span class=si>{</span><span class=n>strategy_name</span><span class=si>}</span><span class=s2> strategy...&quot;</span><span class=p>)</span>

        <span class=c1># Reset data</span>
        <span class=n>X_lab</span> <span class=o>=</span> <span class=n>X_labeled</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
        <span class=n>y_lab</span> <span class=o>=</span> <span class=n>y_labeled</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
        <span class=n>X_unlab</span> <span class=o>=</span> <span class=n>X_unlabeled</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
        <span class=n>y_unlab</span> <span class=o>=</span> <span class=n>y_unlabeled</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>

        <span class=n>learner</span> <span class=o>=</span> <span class=n>ActiveLearner</span><span class=p>(</span>
            <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
            <span class=n>X_lab</span><span class=p>,</span> <span class=n>y_lab</span><span class=p>,</span> <span class=n>X_unlab</span>
        <span class=p>)</span>

        <span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_iterations</span><span class=p>):</span>
            <span class=c1># Evaluate</span>
            <span class=n>acc</span> <span class=o>=</span> <span class=n>learner</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
            <span class=n>results</span><span class=p>[</span><span class=n>strategy_name</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>acc</span><span class=p>)</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Iteration </span><span class=si>{</span><span class=n>iteration</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Accuracy = </span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, &quot;</span>
                  <span class=sa>f</span><span class=s2>&quot;Labeled = </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>learner</span><span class=o>.</span><span class=n>y_labeled</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>learner</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>samples_per_iteration</span><span class=p>:</span>
                <span class=k>break</span>

            <span class=c1># Select samples</span>
            <span class=k>if</span> <span class=n>strategy_name</span> <span class=o>==</span> <span class=s1>&#39;Random&#39;</span><span class=p>:</span>
                <span class=n>indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span>
                    <span class=nb>len</span><span class=p>(</span><span class=n>learner</span><span class=o>.</span><span class=n>X_unlabeled</span><span class=p>),</span>
                    <span class=n>size</span><span class=o>=</span><span class=n>samples_per_iteration</span><span class=p>,</span>
                    <span class=n>replace</span><span class=o>=</span><span class=kc>False</span>
                <span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>method</span> <span class=o>=</span> <span class=nb>getattr</span><span class=p>(</span><span class=n>learner</span><span class=p>,</span> <span class=n>strategy_method</span><span class=p>)</span>
                <span class=n>indices</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>method</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=n>samples_per_iteration</span><span class=p>)</span>

            <span class=c1># Get oracle labels</span>
            <span class=n>oracle_labels</span> <span class=o>=</span> <span class=n>y_unlab</span><span class=p>[</span><span class=n>indices</span><span class=p>]</span>
            <span class=n>y_unlab</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>delete</span><span class=p>(</span><span class=n>y_unlab</span><span class=p>,</span> <span class=n>indices</span><span class=p>)</span>

            <span class=c1># Update learner</span>
            <span class=n>learner</span><span class=o>.</span><span class=n>label_samples</span><span class=p>(</span><span class=n>indices</span><span class=p>,</span> <span class=n>oracle_labels</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>results</span>

<span class=c1># Generate data and compare</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_classification</span><span class=p>(</span>
    <span class=n>n_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
    <span class=n>n_features</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
    <span class=n>n_informative</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span>
    <span class=n>n_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>X_pool</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_pool</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>results</span> <span class=o>=</span> <span class=n>compare_strategies</span><span class=p>(</span>
    <span class=n>X_pool</span><span class=p>,</span> <span class=n>y_pool</span><span class=p>,</span>
    <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span>
    <span class=n>n_iterations</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>samples_per_iteration</span><span class=o>=</span><span class=mi>20</span>
<span class=p>)</span>

<span class=c1># Plot results</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=k>for</span> <span class=n>strategy</span><span class=p>,</span> <span class=n>accuracies</span> <span class=ow>in</span> <span class=n>results</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>labeled_sizes</span> <span class=o>=</span> <span class=p>[</span><span class=mi>50</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>20</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>accuracies</span><span class=p>))]</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>labeled_sizes</span><span class=p>,</span> <span class=n>accuracies</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>strategy</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Labeled Samples&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Test Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Active Learning Strategy Comparison&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;active_learning_comparison.png&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Query Strategy Comparison:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>Principle</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Uncertainty</td> <td>Max prediction uncertainty</td> <td>Simple, effective</td> <td>Can select outliers</td> </tr> <tr> <td>Margin</td> <td>Min margin between classes</td> <td>Robust to noise</td> <td>Only for classifiers</td> </tr> <tr> <td>Entropy</td> <td>Max entropy</td> <td>Theoretically grounded</td> <td>Computationally expensive</td> </tr> <tr> <td>QBC</td> <td>Committee disagreement</td> <td>Reduces overfitting</td> <td>Requires multiple models</td> </tr> <tr> <td>Expected Change</td> <td>Max parameter change</td> <td>Direct optimization</td> <td>Very slow</td> </tr> <tr> <td>Diversity</td> <td>Cover feature space</td> <td>Good coverage</td> <td>May miss informative samples</td> </tr> </tbody> </table> <p><strong>Real-World Applications:</strong></p> <ul> <li><strong>Medical imaging</strong>: Label only diagnostically uncertain cases</li> <li><strong>NLP</strong>: Annotate ambiguous text samples</li> <li><strong>Autonomous driving</strong>: Label edge cases and rare scenarios</li> <li><strong>Fraud detection</strong>: Investigate suspicious transactions</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Iterative: select ‚Üí label ‚Üí retrain"</li> <li>"Minimize labeling costs"</li> <li>Multiple query strategies (uncertainty, margin, entropy, QBC)</li> <li>"Uncertainty sampling: most uncertain predictions"</li> <li>"Query by committee: model disagreement"</li> <li>"Cold start problem: initial labeled set"</li> <li>"Stopping criteria: performance plateau"</li> <li>Real-world cost-benefit analysis</li> <li>Implementation challenges (outliers, class imbalance)</li> </ul> </div> </details> <hr> <h3 id=meta-learning-learning-to-learn-deepmind-openai-interview-question>Meta-Learning (Learning to Learn) - DeepMind, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Meta-Learning</code>, <code>MAML</code>, <code>Few-Shot</code>, <code>Transfer Learning</code> | <strong>Asked by:</strong> DeepMind, OpenAI, Meta AI, Google Research</p> <p><strong>Question:</strong> What is meta-learning and how does it differ from traditional transfer learning? Explain MAML (Model-Agnostic Meta-Learning) and its advantages.</p> <details class=success> <summary>View Answer</summary> <p><strong>Meta-Learning</strong> (learning to learn) trains models to quickly adapt to new tasks with minimal data by learning a good initialization or learning strategy across multiple related tasks.</p> <p><strong>Key Difference from Transfer Learning:</strong></p> <ul> <li><strong>Transfer Learning</strong>: Learn features from task A, apply to task B</li> <li><strong>Meta-Learning</strong>: Learn <em>how to learn</em> across many tasks, rapidly adapt to new tasks</li> </ul> <p><strong>MAML (Model-Agnostic Meta-Learning):</strong></p> <p>Finds model parameters that are easily adaptable to new tasks with just a few gradient steps.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=k>class</span><span class=w> </span><span class=nc>MAMLModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Simple neural network for MAML.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>output_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>clone</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Create a copy with same architecture.&quot;&quot;&quot;</span>
        <span class=n>cloned</span> <span class=o>=</span> <span class=n>MAMLModel</span><span class=p>(</span>
            <span class=n>input_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>in_features</span><span class=p>,</span>
            <span class=n>hidden_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=o>.</span><span class=n>out_features</span><span class=p>,</span>
            <span class=n>output_dim</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=o>.</span><span class=n>out_features</span>
        <span class=p>)</span>
        <span class=n>cloned</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
        <span class=k>return</span> <span class=n>cloned</span>

<span class=k>class</span><span class=w> </span><span class=nc>MAML</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Model-Agnostic Meta-Learning implementation.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>model</span><span class=p>,</span>
        <span class=n>meta_lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
        <span class=n>inner_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
        <span class=n>inner_steps</span><span class=o>=</span><span class=mi>5</span>
    <span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>meta_lr</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>inner_lr</span> <span class=o>=</span> <span class=n>inner_lr</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>inner_steps</span> <span class=o>=</span> <span class=n>inner_steps</span>

    <span class=k>def</span><span class=w> </span><span class=nf>inner_loop</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>task_data</span><span class=p>,</span> <span class=n>task_labels</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Adapt model to a specific task (inner loop).&quot;&quot;&quot;</span>
        <span class=c1># Clone model for task-specific adaptation</span>
        <span class=n>adapted_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>

        <span class=c1># Task-specific optimizer</span>
        <span class=n>task_optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span>
            <span class=n>adapted_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
            <span class=n>lr</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>inner_lr</span>
        <span class=p>)</span>

        <span class=c1># Adapt on support set</span>
        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>inner_steps</span><span class=p>):</span>
            <span class=n>task_optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
            <span class=n>predictions</span> <span class=o>=</span> <span class=n>adapted_model</span><span class=p>(</span><span class=n>task_data</span><span class=p>)</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>task_labels</span><span class=p>)</span>
            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
            <span class=n>task_optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>adapted_model</span>

    <span class=k>def</span><span class=w> </span><span class=nf>meta_train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tasks</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Meta-training step across multiple tasks.</span>

<span class=sd>        Args:</span>
<span class=sd>            tasks: List of (support_x, support_y, query_x, query_y) tuples</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=n>meta_loss</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>for</span> <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>query_x</span><span class=p>,</span> <span class=n>query_y</span> <span class=ow>in</span> <span class=n>tasks</span><span class=p>:</span>
            <span class=c1># Inner loop: adapt to task</span>
            <span class=n>adapted_model</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>inner_loop</span><span class=p>(</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>

            <span class=c1># Outer loop: evaluate on query set</span>
            <span class=n>query_pred</span> <span class=o>=</span> <span class=n>adapted_model</span><span class=p>(</span><span class=n>query_x</span><span class=p>)</span>
            <span class=n>task_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>query_pred</span><span class=p>,</span> <span class=n>query_y</span><span class=p>)</span>
            <span class=n>meta_loss</span> <span class=o>+=</span> <span class=n>task_loss</span>

        <span class=c1># Meta-update</span>
        <span class=n>meta_loss</span> <span class=o>/=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tasks</span><span class=p>)</span>
        <span class=n>meta_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>meta_optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>meta_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>adapt_to_new_task</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Quickly adapt to a new task.&quot;&quot;&quot;</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>inner_loop</span><span class=p>(</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>

<span class=c1># Example: Sine wave regression tasks</span>
<span class=k>def</span><span class=w> </span><span class=nf>generate_sine_task</span><span class=p>(</span><span class=n>amplitude</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>phase</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Generate a sine wave regression task.&quot;&quot;&quot;</span>
    <span class=k>if</span> <span class=n>amplitude</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>amplitude</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>5.0</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>phase</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>phase</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span><span class=p>)</span>

    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>amplitude</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>phase</span><span class=p>)</span>

    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>amplitude</span><span class=p>,</span> <span class=n>phase</span>

<span class=k>def</span><span class=w> </span><span class=nf>train_maml</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Train MAML on sine wave tasks.&quot;&quot;&quot;</span>
    <span class=c1># Initialize model and MAML</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>MAMLModel</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>output_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>maml</span> <span class=o>=</span> <span class=n>MAML</span><span class=p>(</span>
        <span class=n>model</span><span class=p>,</span>
        <span class=n>meta_lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
        <span class=n>inner_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
        <span class=n>inner_steps</span><span class=o>=</span><span class=mi>5</span>
    <span class=p>)</span>

    <span class=n>n_meta_iterations</span> <span class=o>=</span> <span class=mi>1000</span>
    <span class=n>tasks_per_iteration</span> <span class=o>=</span> <span class=mi>5</span>

    <span class=n>meta_losses</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Meta-Training MAML...&quot;</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_meta_iterations</span><span class=p>):</span>
        <span class=c1># Sample batch of tasks</span>
        <span class=n>tasks</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>tasks_per_iteration</span><span class=p>):</span>
            <span class=c1># Support set (for adaptation)</span>
            <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>generate_sine_task</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
            <span class=c1># Query set (for meta-update)</span>
            <span class=n>query_x</span><span class=p>,</span> <span class=n>query_y</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>generate_sine_task</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
            <span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>query_x</span><span class=p>,</span> <span class=n>query_y</span><span class=p>))</span>

        <span class=c1># Meta-train step</span>
        <span class=n>meta_loss</span> <span class=o>=</span> <span class=n>maml</span><span class=o>.</span><span class=n>meta_train_step</span><span class=p>(</span><span class=n>tasks</span><span class=p>)</span>
        <span class=n>meta_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>meta_loss</span><span class=p>)</span>

        <span class=k>if</span> <span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Iteration </span><span class=si>{</span><span class=n>iteration</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Meta-loss = </span><span class=si>{</span><span class=n>meta_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>maml</span><span class=p>,</span> <span class=n>meta_losses</span>

<span class=k>def</span><span class=w> </span><span class=nf>compare_maml_vs_scratch</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Compare MAML adaptation vs training from scratch.&quot;&quot;&quot;</span>
    <span class=c1># Train MAML</span>
    <span class=n>maml</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>train_maml</span><span class=p>()</span>

    <span class=c1># Test on a new task</span>
    <span class=n>test_amplitude</span><span class=p>,</span> <span class=n>test_phase</span> <span class=o>=</span> <span class=mf>2.0</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span><span class=o>/</span><span class=mi>4</span>
    <span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>generate_sine_task</span><span class=p>(</span>
        <span class=n>amplitude</span><span class=o>=</span><span class=n>test_amplitude</span><span class=p>,</span>
        <span class=n>phase</span><span class=o>=</span><span class=n>test_phase</span><span class=p>,</span>
        <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span>
    <span class=p>)</span>

    <span class=c1># Adapt MAML model</span>
    <span class=n>adapted_model</span> <span class=o>=</span> <span class=n>maml</span><span class=o>.</span><span class=n>adapt_to_new_task</span><span class=p>(</span><span class=n>support_x</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>

    <span class=c1># Train model from scratch</span>
    <span class=n>scratch_model</span> <span class=o>=</span> <span class=n>MAMLModel</span><span class=p>(</span><span class=n>input_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>output_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>scratch_optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>scratch_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>  <span class=c1># More steps than MAML inner loop</span>
        <span class=n>scratch_optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>pred</span> <span class=o>=</span> <span class=n>scratch_model</span><span class=p>(</span><span class=n>support_x</span><span class=p>)</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>pred</span><span class=p>,</span> <span class=n>support_y</span><span class=p>)</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>scratch_optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

    <span class=c1># Evaluate both</span>
    <span class=n>test_x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>test_y</span> <span class=o>=</span> <span class=n>test_amplitude</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>test_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span> <span class=o>+</span> <span class=n>test_phase</span><span class=p>)</span>

    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
        <span class=n>maml_pred</span> <span class=o>=</span> <span class=n>adapted_model</span><span class=p>(</span><span class=n>test_x</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
        <span class=n>scratch_pred</span> <span class=o>=</span> <span class=n>scratch_model</span><span class=p>(</span><span class=n>test_x</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>

    <span class=c1># Plot comparison</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>support_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>support_y</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> 
               <span class=n>c</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Support Set&#39;</span><span class=p>,</span> <span class=n>zorder</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>test_y</span><span class=p>,</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;True Function&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>maml_pred</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;MAML (5 steps)&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;y&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;MAML: Fast Adaptation&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>support_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>support_y</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> 
               <span class=n>c</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Support Set&#39;</span><span class=p>,</span> <span class=n>zorder</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>test_y</span><span class=p>,</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;True Function&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>test_x</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>scratch_pred</span><span class=p>,</span> <span class=s1>&#39;g-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;From Scratch (100 steps)&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;y&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training from Scratch&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;maml_comparison.png&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

    <span class=c1># Calculate MSE</span>
    <span class=n>maml_mse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>((</span><span class=n>maml_pred</span> <span class=o>-</span> <span class=n>test_y</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>scratch_mse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>((</span><span class=n>scratch_pred</span> <span class=o>-</span> <span class=n>test_y</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>MAML MSE (after 5 steps): </span><span class=si>{</span><span class=n>maml_mse</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;From Scratch MSE (after 100 steps): </span><span class=si>{</span><span class=n>scratch_mse</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Run comparison</span>
<span class=n>compare_maml_vs_scratch</span><span class=p>()</span>

<span class=c1># Prototypical Networks (alternative meta-learning approach)</span>
<span class=k>class</span><span class=w> </span><span class=nc>PrototypicalNetwork</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Prototypical networks for few-shot classification.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=o>=</span><span class=mi>64</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>compute_prototypes</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>support_embeddings</span><span class=p>,</span> <span class=n>support_labels</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute class prototypes (mean of support embeddings).&quot;&quot;&quot;</span>
        <span class=n>unique_labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>support_labels</span><span class=p>)</span>
        <span class=n>prototypes</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>unique_labels</span><span class=p>:</span>
            <span class=n>class_embeddings</span> <span class=o>=</span> <span class=n>support_embeddings</span><span class=p>[</span><span class=n>support_labels</span> <span class=o>==</span> <span class=n>label</span><span class=p>]</span>
            <span class=n>prototype</span> <span class=o>=</span> <span class=n>class_embeddings</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
            <span class=n>prototypes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>prototype</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>prototypes</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>classify</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query_embeddings</span><span class=p>,</span> <span class=n>prototypes</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Classify based on nearest prototype.&quot;&quot;&quot;</span>
        <span class=c1># Euclidean distance to prototypes</span>
        <span class=n>distances</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cdist</span><span class=p>(</span><span class=n>query_embeddings</span><span class=p>,</span> <span class=n>prototypes</span><span class=p>)</span>
        <span class=c1># Negative distance as logits</span>
        <span class=k>return</span> <span class=o>-</span><span class=n>distances</span>
</code></pre></div> <p><strong>Meta-Learning Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Key Idea</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>MAML</td> <td>Good initialization</td> <td>Fast adaptation</td> <td>Expensive meta-training</td> </tr> <tr> <td>Prototypical</td> <td>Prototype-based classification</td> <td>Simple, effective</td> <td>Limited to classification</td> </tr> <tr> <td>Meta-SGD</td> <td>Learn learning rates</td> <td>Flexible</td> <td>More parameters</td> </tr> <tr> <td>Reptile</td> <td>First-order MAML</td> <td>Computationally cheaper</td> <td>Slightly worse performance</td> </tr> </tbody> </table> <p><strong>Applications:</strong></p> <ul> <li><strong>Few-shot learning</strong>: Learn from 1-5 examples</li> <li><strong>Robotics</strong>: Quick adaptation to new tasks</li> <li><strong>Drug discovery</strong>: Transfer across molecular tasks</li> <li><strong>Personalization</strong>: Adapt to individual users</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Learning to learn across tasks"</li> <li>"MAML: good initialization for fast adaptation"</li> <li>"Inner loop: task adaptation, outer loop: meta-update"</li> <li>"Few-shot learning: learn from 1-5 examples"</li> <li>vs. "Transfer learning: feature reuse"</li> <li>"Prototypical networks: prototype-based"</li> <li>Applications (robotics, personalization)</li> <li>"Expensive meta-training, fast adaptation"</li> </ul> </div> </details> <hr> <h3 id=continuallifelong-learning-deepmind-meta-ai-interview-question>Continual/Lifelong Learning - DeepMind, Meta AI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Catastrophic Forgetting</code>, <code>Continual Learning</code>, <code>EWC</code>, <code>Replay</code> | <strong>Asked by:</strong> DeepMind, Meta AI, Google Research, Microsoft Research</p> <p><strong>Question:</strong> What is catastrophic forgetting and how can we enable continual learning? Explain strategies like Elastic Weight Consolidation (EWC) and experience replay.</p> <details class=success> <summary>View Answer</summary> <p><strong>Catastrophic Forgetting</strong> occurs when a neural network forgets previously learned tasks upon learning new ones. <strong>Continual/Lifelong Learning</strong> aims to learn sequential tasks without forgetting.</p> <p><strong>The Problem:</strong></p> <p>Standard neural networks trained on Task B will overwrite weights learned for Task A, losing performance on A.</p> <p><strong>Solutions:</strong></p> <p><strong>1. Elastic Weight Consolidation (EWC)</strong></p> <p>Protects important weights for old tasks using Fisher Information Matrix.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>DataLoader</span><span class=p>,</span> <span class=n>TensorDataset</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=k>class</span><span class=w> </span><span class=nc>EWC</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Elastic Weight Consolidation for continual learning.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>dataset</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cpu&#39;</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=p>{</span><span class=n>n</span><span class=p>:</span> <span class=n>p</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span> <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>}</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fisher</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_compute_fisher</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>_compute_fisher</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dataset</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute Fisher Information Matrix diagonal.&quot;&quot;&quot;</span>
        <span class=n>fisher</span> <span class=o>=</span> <span class=p>{</span><span class=n>n</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span> <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>}</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
        <span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
            <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

            <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>

            <span class=c1># Accumulate squared gradients</span>
            <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
                <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=ow>and</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
                    <span class=n>fisher</span><span class=p>[</span><span class=n>n</span><span class=p>]</span> <span class=o>+=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>

        <span class=c1># Normalize</span>
        <span class=n>n_samples</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
        <span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=n>fisher</span><span class=p>:</span>
            <span class=n>fisher</span><span class=p>[</span><span class=n>n</span><span class=p>]</span> <span class=o>/=</span> <span class=n>n_samples</span>

        <span class=k>return</span> <span class=n>fisher</span>

    <span class=k>def</span><span class=w> </span><span class=nf>penalty</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Calculate EWC penalty term.&quot;&quot;&quot;</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
            <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>:</span>
                <span class=n>loss</span> <span class=o>+=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fisher</span><span class=p>[</span><span class=n>n</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>p</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>[</span><span class=n>n</span><span class=p>])</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
        <span class=k>return</span> <span class=n>loss</span>

<span class=k>class</span><span class=w> </span><span class=nc>ContinualLearner</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Framework for continual learning experiments.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cpu&#39;</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>ewc_list</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_task</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>train_loader</span><span class=p>,</span>
        <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
        <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
        <span class=n>ewc_lambda</span><span class=o>=</span><span class=mi>5000</span><span class=p>,</span>
        <span class=n>use_ewc</span><span class=o>=</span><span class=kc>True</span>
    <span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Train on a new task with optional EWC.&quot;&quot;&quot;</span>
        <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
        <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>

        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
            <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>

            <span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
                <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>

                <span class=c1># Standard loss</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>

                <span class=c1># Add EWC penalty for previous tasks</span>
                <span class=k>if</span> <span class=n>use_ewc</span><span class=p>:</span>
                    <span class=k>for</span> <span class=n>ewc</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>ewc_list</span><span class=p>:</span>
                        <span class=n>loss</span> <span class=o>+=</span> <span class=n>ewc_lambda</span> <span class=o>*</span> <span class=n>ewc</span><span class=o>.</span><span class=n>penalty</span><span class=p>()</span>

                <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

                <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=n>predicted</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

            <span class=n>acc</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>
            <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>5</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>total_loss</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Acc=</span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>add_ewc_constraint</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dataset</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Add EWC constraint for current task.&quot;&quot;&quot;</span>
        <span class=n>ewc</span> <span class=o>=</span> <span class=n>EWC</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=n>dataset</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>ewc_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ewc</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>evaluate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>test_loader</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Evaluate on test set.&quot;&quot;&quot;</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
        <span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
                <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
                <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
                <span class=n>total</span> <span class=o>+=</span> <span class=n>labels</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
                <span class=n>correct</span> <span class=o>+=</span> <span class=n>predicted</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

        <span class=k>return</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>correct</span> <span class=o>/</span> <span class=n>total</span>

<span class=c1># 2. Experience Replay</span>
<span class=k>class</span><span class=w> </span><span class=nc>ReplayBuffer</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Store and replay past experiences.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>capacity</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>capacity</span> <span class=o>=</span> <span class=n>capacity</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>add_task_samples</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dataset</span><span class=p>,</span> <span class=n>samples_per_task</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Add samples from current task to buffer.&quot;&quot;&quot;</span>
        <span class=n>indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>),</span> 
                                  <span class=nb>min</span><span class=p>(</span><span class=n>samples_per_task</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)),</span> 
                                  <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>indices</span><span class=p>:</span>
            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>)</span> <span class=o>&gt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>capacity</span><span class=p>:</span>
                <span class=c1># Remove oldest samples</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dataset</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>

    <span class=k>def</span><span class=w> </span><span class=nf>get_replay_loader</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Get data loader for replay samples.&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>:</span>
            <span class=k>return</span> <span class=kc>None</span>

        <span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>([</span><span class=n>x</span> <span class=k>for</span> <span class=n>x</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>])</span>
        <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>y</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>y</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>buffer</span><span class=p>])</span>
        <span class=n>dataset</span> <span class=o>=</span> <span class=n>TensorDataset</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># 3. Progressive Neural Networks</span>
<span class=k>class</span><span class=w> </span><span class=nc>ProgressiveNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Progressive Neural Networks: add new columns for new tasks.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dims</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>columns</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>adapters</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>()</span>

        <span class=c1># First column</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>add_column</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dims</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

    <span class=k>def</span><span class=w> </span><span class=nf>add_column</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Add a new column for a new task.&quot;&quot;&quot;</span>
        <span class=n>column</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>column</span><span class=p>)</span>

        <span class=c1># Add lateral connections from previous columns</span>
        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
            <span class=n>adapters</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span> 
                <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
            <span class=p>])</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>adapters</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>adapters</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>task_id</span><span class=o>=-</span><span class=mi>1</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Forward pass through column for task_id.&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=n>task_id</span> <span class=o>==</span> <span class=o>-</span><span class=mi>1</span><span class=p>:</span>
            <span class=n>task_id</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>

        <span class=c1># Compute activations from previous columns</span>
        <span class=n>prev_activations</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>task_id</span><span class=p>):</span>
            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>  <span class=c1># Freeze previous columns</span>
                <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>i</span><span class=p>][:</span><span class=o>-</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>)</span>  <span class=c1># All but last layer</span>
                <span class=n>prev_activations</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>

        <span class=c1># Current column with lateral connections</span>
        <span class=n>h</span> <span class=o>=</span> <span class=n>x</span>
        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>task_id</span><span class=p>][:</span><span class=o>-</span><span class=mi>1</span><span class=p>]):</span>
            <span class=n>h</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>

            <span class=c1># Add lateral connections</span>
            <span class=k>if</span> <span class=n>task_id</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>i</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
                <span class=k>for</span> <span class=n>j</span><span class=p>,</span> <span class=n>prev_h</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>prev_activations</span><span class=p>):</span>
                    <span class=n>h</span> <span class=o>=</span> <span class=n>h</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>adapters</span><span class=p>[</span><span class=n>task_id</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>](</span><span class=n>prev_h</span><span class=p>)</span>

        <span class=c1># Final output layer</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>task_id</span><span class=p>][</span><span class=o>-</span><span class=mi>1</span><span class=p>](</span><span class=n>h</span><span class=p>)</span>

<span class=c1># Comparison experiment</span>
<span class=k>def</span><span class=w> </span><span class=nf>compare_continual_methods</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Compare different continual learning strategies.&quot;&quot;&quot;</span>
    <span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_classification</span>

    <span class=c1># Create 3 different tasks</span>
    <span class=n>tasks</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>seed</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>42</span><span class=p>,</span> <span class=mi>43</span><span class=p>,</span> <span class=mi>44</span><span class=p>]:</span>
        <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_classification</span><span class=p>(</span>
            <span class=n>n_samples</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span>
            <span class=n>n_features</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>
            <span class=n>n_informative</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span>
            <span class=n>n_classes</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
            <span class=n>random_state</span><span class=o>=</span><span class=n>seed</span>
        <span class=p>)</span>
        <span class=n>X_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>y_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
        <span class=n>dataset</span> <span class=o>=</span> <span class=n>TensorDataset</span><span class=p>(</span><span class=n>X_tensor</span><span class=p>,</span> <span class=n>y_tensor</span><span class=p>)</span>
        <span class=n>tasks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>

    <span class=c1># Simple model</span>
    <span class=k>class</span><span class=w> </span><span class=nc>SimpleNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
        <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
            <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
                <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
            <span class=p>)</span>

        <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

    <span class=n>methods</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;Naive (No Protection)&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;use_replay&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>},</span>
        <span class=s1>&#39;EWC&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;use_replay&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>},</span>
        <span class=s1>&#39;Experience Replay&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span> <span class=s1>&#39;use_replay&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>},</span>
        <span class=s1>&#39;EWC + Replay&#39;</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;use_replay&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>}</span>
    <span class=p>}</span>

    <span class=n>results</span> <span class=o>=</span> <span class=p>{</span><span class=n>method</span><span class=p>:</span> <span class=p>[]</span> <span class=k>for</span> <span class=n>method</span> <span class=ow>in</span> <span class=n>methods</span><span class=p>}</span>

    <span class=k>for</span> <span class=n>method_name</span><span class=p>,</span> <span class=n>config</span> <span class=ow>in</span> <span class=n>methods</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=si>{</span><span class=s1>&#39;=&#39;</span><span class=o>*</span><span class=mi>60</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Method: </span><span class=si>{</span><span class=n>method_name</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;=&#39;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>

        <span class=n>model</span> <span class=o>=</span> <span class=n>SimpleNet</span><span class=p>()</span>
        <span class=n>learner</span> <span class=o>=</span> <span class=n>ContinualLearner</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
        <span class=n>replay_buffer</span> <span class=o>=</span> <span class=n>ReplayBuffer</span><span class=p>(</span><span class=n>capacity</span><span class=o>=</span><span class=mi>500</span><span class=p>)</span> <span class=k>if</span> <span class=n>config</span><span class=p>[</span><span class=s1>&#39;use_replay&#39;</span><span class=p>]</span> <span class=k>else</span> <span class=kc>None</span>

        <span class=n>task_accuracies</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>for</span> <span class=n>task_id</span><span class=p>,</span> <span class=n>task_dataset</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>tasks</span><span class=p>):</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Training on Task </span><span class=si>{</span><span class=n>task_id</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>...&quot;</span><span class=p>)</span>

            <span class=c1># Prepare data loader</span>
            <span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>task_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

            <span class=c1># Add replay samples</span>
            <span class=k>if</span> <span class=n>replay_buffer</span> <span class=ow>and</span> <span class=n>replay_buffer</span><span class=o>.</span><span class=n>buffer</span><span class=p>:</span>
                <span class=c1># Mix current task with replay</span>
                <span class=n>replay_loader</span> <span class=o>=</span> <span class=n>replay_buffer</span><span class=o>.</span><span class=n>get_replay_loader</span><span class=p>()</span>
                <span class=c1># For simplicity, train on task then replay</span>
                <span class=n>learner</span><span class=o>.</span><span class=n>train_task</span><span class=p>(</span><span class=n>train_loader</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> 
                                  <span class=n>use_ewc</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>])</span>
                <span class=n>learner</span><span class=o>.</span><span class=n>train_task</span><span class=p>(</span><span class=n>replay_loader</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> 
                                  <span class=n>use_ewc</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>])</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>learner</span><span class=o>.</span><span class=n>train_task</span><span class=p>(</span><span class=n>train_loader</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> 
                                  <span class=n>use_ewc</span><span class=o>=</span><span class=n>config</span><span class=p>[</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>])</span>

            <span class=c1># Add EWC constraint</span>
            <span class=k>if</span> <span class=n>config</span><span class=p>[</span><span class=s1>&#39;use_ewc&#39;</span><span class=p>]:</span>
                <span class=n>learner</span><span class=o>.</span><span class=n>add_ewc_constraint</span><span class=p>(</span><span class=n>task_dataset</span><span class=p>)</span>

            <span class=c1># Add to replay buffer</span>
            <span class=k>if</span> <span class=n>replay_buffer</span><span class=p>:</span>
                <span class=n>replay_buffer</span><span class=o>.</span><span class=n>add_task_samples</span><span class=p>(</span><span class=n>task_dataset</span><span class=p>,</span> <span class=n>samples_per_task</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

            <span class=c1># Evaluate on all previous tasks</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Evaluation after Task </span><span class=si>{</span><span class=n>task_id</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>
            <span class=k>for</span> <span class=n>eval_task_id</span><span class=p>,</span> <span class=n>eval_dataset</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>tasks</span><span class=p>[:</span><span class=n>task_id</span><span class=o>+</span><span class=mi>1</span><span class=p>]):</span>
                <span class=n>eval_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>eval_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>)</span>
                <span class=n>acc</span> <span class=o>=</span> <span class=n>learner</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>eval_loader</span><span class=p>)</span>
                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Task </span><span class=si>{</span><span class=n>eval_task_id</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2> Accuracy: </span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>%&quot;</span><span class=p>)</span>

                <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>task_accuracies</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=n>eval_task_id</span><span class=p>:</span>
                    <span class=n>task_accuracies</span><span class=o>.</span><span class=n>append</span><span class=p>([])</span>
                <span class=n>task_accuracies</span><span class=p>[</span><span class=n>eval_task_id</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>acc</span><span class=p>)</span>

        <span class=n>results</span><span class=p>[</span><span class=n>method_name</span><span class=p>]</span> <span class=o>=</span> <span class=n>task_accuracies</span>

    <span class=c1># Plot forgetting</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>method_name</span><span class=p>,</span> <span class=n>task_accs</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>items</span><span class=p>()):</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
        <span class=k>for</span> <span class=n>task_id</span><span class=p>,</span> <span class=n>accs</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>task_accs</span><span class=p>):</span>
            <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>task_id</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>accs</span><span class=p>)</span><span class=o>+</span><span class=n>task_id</span><span class=p>),</span> <span class=n>accs</span><span class=p>,</span> 
                    <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>method_name</span><span class=si>}</span><span class=s1> - Task </span><span class=si>{</span><span class=n>task_id</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;After Training Task&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy (%)&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Task Performance Over Time&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>bbox_to_anchor</span><span class=o>=</span><span class=p>(</span><span class=mf>1.05</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>loc</span><span class=o>=</span><span class=s1>&#39;upper left&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=c1># Average forgetting</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
    <span class=n>avg_forgetting</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>method_name</span><span class=p>,</span> <span class=n>task_accs</span> <span class=ow>in</span> <span class=n>results</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=c1># Calculate average forgetting</span>
        <span class=n>forgetting</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>task_id</span><span class=p>,</span> <span class=n>accs</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>task_accs</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]):</span>
            <span class=n>forgetting</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>accs</span><span class=p>[</span><span class=n>task_id</span><span class=p>]</span> <span class=o>-</span> <span class=n>accs</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># Initial - Final</span>
        <span class=n>avg_f</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>forgetting</span><span class=p>)</span> <span class=k>if</span> <span class=n>forgetting</span> <span class=k>else</span> <span class=mi>0</span>
        <span class=n>avg_forgetting</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_f</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>methods</span><span class=p>)),</span> <span class=n>avg_forgetting</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>methods</span><span class=p>)),</span> <span class=n>methods</span><span class=o>.</span><span class=n>keys</span><span class=p>(),</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;right&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Average Forgetting (%)&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Catastrophic Forgetting Comparison&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;continual_learning_comparison.png&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Run comparison</span>
<span class=n>compare_continual_methods</span><span class=p>()</span>
</code></pre></div> <p><strong>Continual Learning Strategies:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Key Idea</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>EWC</td> <td>Protect important weights</td> <td>No memory overhead</td> <td>Hyperparameter sensitive</td> </tr> <tr> <td>Experience Replay</td> <td>Store past examples</td> <td>Simple, effective</td> <td>Memory overhead</td> </tr> <tr> <td>Progressive NN</td> <td>New network per task</td> <td>No forgetting</td> <td>Network grows unbounded</td> </tr> <tr> <td>PackNet</td> <td>Prune and freeze</td> <td>Compact</td> <td>Requires pruning</td> </tr> <tr> <td>GEM</td> <td>Constrained optimization</td> <td>Strong guarantees</td> <td>Computationally expensive</td> </tr> </tbody> </table> <p><strong>Applications:</strong></p> <ul> <li><strong>Robotics</strong>: Learn new skills without forgetting old ones</li> <li><strong>Personalization</strong>: Adapt to user preferences over time</li> <li><strong>Edge AI</strong>: Update models without retraining from scratch</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Catastrophic forgetting: forgetting old tasks"</li> <li>"EWC: protect important weights using Fisher information"</li> <li>"Experience replay: store and replay past samples"</li> <li>"Progressive networks: add columns for new tasks"</li> <li>"Trade-off: memory vs computation vs forgetting"</li> <li>"Stability-plasticity dilemma"</li> <li>Real-world constraints (memory, compute)</li> <li>Evaluation metrics (average accuracy, forgetting)</li> </ul> </div> </details> <hr> <h3 id=graph-neural-networks-gnns-deepmind-meta-ai-twitter-interview-question>Graph Neural Networks (GNNs) - DeepMind, Meta AI, Twitter Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Graph Learning</code>, <code>GCN</code>, <code>Message Passing</code>, <code>Node Embeddings</code> | <strong>Asked by:</strong> DeepMind, Meta AI, Twitter, Pinterest, Uber</p> <p><strong>Question:</strong> What are Graph Neural Networks and how do they work? Explain message passing and different GNN architectures (GCN, GAT, GraphSAGE).</p> <details class=success> <summary>View Answer</summary> <p><strong>Graph Neural Networks (GNNs)</strong> learn representations for graph-structured data by iteratively aggregating information from neighbors through message passing.</p> <p><strong>Key Concepts:</strong></p> <ul> <li><strong>Nodes</strong>: Entities (users, molecules, papers)</li> <li><strong>Edges</strong>: Relationships (friendships, bonds, citations)</li> <li><strong>Features</strong>: Node/edge attributes</li> <li><strong>Message Passing</strong>: Nodes exchange and aggregate neighbor information</li> </ul> <p><strong>Core GNN Operation:</strong></p> <div class=arithmatex>\[h_v^{(k+1)} = \text{UPDATE}(h_v^{(k)}, \text{AGGREGATE}(\{h_u^{(k)} : u \in \mathcal{N}(v)\}))\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>networkx</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nx</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>from</span><span class=w> </span><span class=nn>typing</span><span class=w> </span><span class=kn>import</span> <span class=n>List</span><span class=p>,</span> <span class=n>Tuple</span>

<span class=c1># 1. Graph Convolutional Network (GCN)</span>
<span class=k>class</span><span class=w> </span><span class=nc>GCNLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Graph Convolutional Layer.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Args:</span>
<span class=sd>            X: Node features (N x in_features)</span>
<span class=sd>            adj: Adjacency matrix (N x N)</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=c1># Normalize adjacency: D^{-1/2} A D^{-1/2}</span>
        <span class=n>D</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>adj</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=o>-</span><span class=mf>0.5</span><span class=p>))</span>
        <span class=n>adj_norm</span> <span class=o>=</span> <span class=n>D</span> <span class=o>@</span> <span class=n>adj</span> <span class=o>@</span> <span class=n>D</span>

        <span class=c1># Aggregate neighbors and transform</span>
        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>adj_norm</span> <span class=o>@</span> <span class=n>X</span><span class=p>))</span>

<span class=k>class</span><span class=w> </span><span class=nc>GCN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Multi-layer GCN.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_features</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>()</span>

        <span class=c1># Input layer</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>GCNLayer</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>))</span>

        <span class=c1># Hidden layers</span>
        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span> <span class=o>-</span> <span class=mi>2</span><span class=p>):</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>GCNLayer</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>))</span>

        <span class=c1># Output layer</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>GCNLayer</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>out_features</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>):</span>
        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
            <span class=n>X</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>)</span>
            <span class=n>X</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>](</span><span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>)</span>

<span class=c1># 2. Graph Attention Network (GAT)</span>
<span class=k>class</span><span class=w> </span><span class=nc>GATLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Graph Attention Layer with multi-head attention.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.6</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span> <span class=o>=</span> <span class=n>out_features</span>

        <span class=c1># Linear transformations</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span> <span class=o>*</span> <span class=n>num_heads</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>a</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>out_features</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>leaky_relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>reset_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_uniform_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=p>)</span>
        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_uniform_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>a</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Args:</span>
<span class=sd>            X: Node features (N x in_features)</span>
<span class=sd>            adj: Adjacency matrix (N x N)</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>N</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Linear transformation</span>
        <span class=n>H</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W</span>  <span class=c1># (N x out_features*num_heads)</span>
        <span class=n>H</span> <span class=o>=</span> <span class=n>H</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span><span class=p>)</span>  <span class=c1># (N x heads x out)</span>

        <span class=c1># Attention mechanism</span>
        <span class=c1># Concatenate for all pairs</span>
        <span class=n>a_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span>
            <span class=n>H</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>N</span> <span class=o>*</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span><span class=p>),</span>
            <span class=n>H</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
        <span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span><span class=p>)</span>

        <span class=c1># Compute attention scores</span>
        <span class=n>e</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>leaky_relu</span><span class=p>((</span><span class=n>a_input</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>a</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_features</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>

        <span class=c1># Mask non-neighbors</span>
        <span class=n>zero_vec</span> <span class=o>=</span> <span class=o>-</span><span class=mf>9e15</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>e</span><span class=p>)</span>
        <span class=n>attention</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>adj</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=n>e</span><span class=p>,</span> <span class=n>zero_vec</span><span class=p>)</span>

        <span class=c1># Softmax</span>
        <span class=n>attention</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>attention</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>)</span>

        <span class=c1># Weighted sum</span>
        <span class=n>H_prime</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>H</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
        <span class=n>H_prime</span> <span class=o>=</span> <span class=n>H_prime</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># Average over heads</span>

        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>elu</span><span class=p>(</span><span class=n>H_prime</span><span class=p>)</span>

<span class=c1># 3. GraphSAGE</span>
<span class=k>class</span><span class=w> </span><span class=nc>GraphSAGELayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;GraphSAGE layer with sampling.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>,</span> <span class=n>aggregator</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>aggregator</span> <span class=o>=</span> <span class=n>aggregator</span>

        <span class=c1># Separate transforms for self and neighbors</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_self</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>W_neigh</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_features</span><span class=p>,</span> <span class=n>out_features</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>,</span> <span class=n>sample_size</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Args:</span>
<span class=sd>            X: Node features (N x in_features)</span>
<span class=sd>            adj: Adjacency matrix (N x N)</span>
<span class=sd>            sample_size: Number of neighbors to sample</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>N</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Sample neighbors</span>
        <span class=n>neighbor_features</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>):</span>
            <span class=n>neighbors</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>adj</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>

            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>neighbors</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>sample_size</span><span class=p>:</span>
                <span class=c1># Sample</span>
                <span class=n>sampled</span> <span class=o>=</span> <span class=n>neighbors</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>randperm</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>neighbors</span><span class=p>))[:</span><span class=n>sample_size</span><span class=p>]]</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>sampled</span> <span class=o>=</span> <span class=n>neighbors</span>

            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>sampled</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
                <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>aggregator</span> <span class=o>==</span> <span class=s1>&#39;mean&#39;</span><span class=p>:</span>
                    <span class=n>neigh_feat</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>sampled</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
                <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>aggregator</span> <span class=o>==</span> <span class=s1>&#39;max&#39;</span><span class=p>:</span>
                    <span class=n>neigh_feat</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>sampled</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
                <span class=k>elif</span> <span class=bp>self</span><span class=o>.</span><span class=n>aggregator</span> <span class=o>==</span> <span class=s1>&#39;lstm&#39;</span><span class=p>:</span>
                    <span class=c1># LSTM aggregator (simplified)</span>
                    <span class=n>neigh_feat</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>sampled</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
                <span class=k>else</span><span class=p>:</span>
                    <span class=n>neigh_feat</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>sampled</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>neigh_feat</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

            <span class=n>neighbor_features</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>neigh_feat</span><span class=p>)</span>

        <span class=n>neighbor_features</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>neighbor_features</span><span class=p>)</span>

        <span class=c1># Combine self and neighbor features</span>
        <span class=n>self_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_self</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>neigh_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_neigh</span><span class=p>(</span><span class=n>neighbor_features</span><span class=p>)</span>

        <span class=n>output</span> <span class=o>=</span> <span class=n>self_features</span> <span class=o>+</span> <span class=n>neigh_features</span>

        <span class=c1># L2 normalization</span>
        <span class=n>output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>normalize</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>

<span class=c1># Example: Node classification on Karate Club</span>
<span class=k>def</span><span class=w> </span><span class=nf>node_classification_example</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Node classification on Karate Club graph.&quot;&quot;&quot;</span>
    <span class=c1># Load Karate Club graph</span>
    <span class=n>G</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>karate_club_graph</span><span class=p>()</span>

    <span class=c1># Create features (degree, clustering coefficient, etc.)</span>
    <span class=n>n_nodes</span> <span class=o>=</span> <span class=n>G</span><span class=o>.</span><span class=n>number_of_nodes</span><span class=p>()</span>
    <span class=n>features</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>G</span><span class=o>.</span><span class=n>nodes</span><span class=p>():</span>
        <span class=n>features</span><span class=o>.</span><span class=n>append</span><span class=p>([</span>
            <span class=n>G</span><span class=o>.</span><span class=n>degree</span><span class=p>(</span><span class=n>node</span><span class=p>),</span>
            <span class=n>nx</span><span class=o>.</span><span class=n>clustering</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>node</span><span class=p>),</span>
            <span class=n>nx</span><span class=o>.</span><span class=n>closeness_centrality</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>node</span><span class=p>)</span>
        <span class=p>])</span>

    <span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>

    <span class=c1># Adjacency matrix</span>
    <span class=n>adj</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>to_numpy_array</span><span class=p>(</span><span class=n>G</span><span class=p>)</span>
    <span class=n>adj</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>adj</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>n_nodes</span><span class=p>))</span>  <span class=c1># Add self-loops</span>

    <span class=c1># Labels (which karate club each node joined)</span>
    <span class=n>labels</span> <span class=o>=</span> <span class=p>[</span><span class=n>G</span><span class=o>.</span><span class=n>nodes</span><span class=p>[</span><span class=n>node</span><span class=p>][</span><span class=s1>&#39;club&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=s1>&#39;Mr. Hi&#39;</span> <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>G</span><span class=o>.</span><span class=n>nodes</span><span class=p>()]</span>
    <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span>

    <span class=c1># Train/test split</span>
    <span class=n>train_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_nodes</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bool</span><span class=p>)</span>
    <span class=n>train_mask</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>randperm</span><span class=p>(</span><span class=n>n_nodes</span><span class=p>)[:</span><span class=nb>int</span><span class=p>(</span><span class=mf>0.6</span> <span class=o>*</span> <span class=n>n_nodes</span><span class=p>)]]</span> <span class=o>=</span> <span class=kc>True</span>
    <span class=n>test_mask</span> <span class=o>=</span> <span class=o>~</span><span class=n>train_mask</span>

    <span class=c1># Train GCN</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>GCN</span><span class=p>(</span><span class=n>in_features</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>out_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>5e-4</span><span class=p>)</span>

    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training GCN...&quot;</span><span class=p>)</span>
    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>200</span><span class=p>):</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>)</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>out</span><span class=p>[</span><span class=n>train_mask</span><span class=p>],</span> <span class=n>y</span><span class=p>[</span><span class=n>train_mask</span><span class=p>])</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>50</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
                <span class=n>pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>)</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
                <span class=n>train_acc</span> <span class=o>=</span> <span class=p>(</span><span class=n>pred</span><span class=p>[</span><span class=n>train_mask</span><span class=p>]</span> <span class=o>==</span> <span class=n>y</span><span class=p>[</span><span class=n>train_mask</span><span class=p>])</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
                <span class=n>test_acc</span> <span class=o>=</span> <span class=p>(</span><span class=n>pred</span><span class=p>[</span><span class=n>test_mask</span><span class=p>]</span> <span class=o>==</span> <span class=n>y</span><span class=p>[</span><span class=n>test_mask</span><span class=p>])</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Loss=</span><span class=si>{</span><span class=n>loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, &quot;</span>
                  <span class=sa>f</span><span class=s2>&quot;Train Acc=</span><span class=si>{</span><span class=n>train_acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Test Acc=</span><span class=si>{</span><span class=n>test_acc</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
            <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>

    <span class=c1># Visualize embeddings</span>
    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
        <span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>X</span><span class=p>,</span> <span class=n>adj</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>

    <span class=c1># Plot</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=n>pos</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>spring_layout</span><span class=p>(</span><span class=n>G</span><span class=p>)</span>
    <span class=n>nx</span><span class=o>.</span><span class=n>draw</span><span class=p>(</span><span class=n>G</span><span class=p>,</span> <span class=n>pos</span><span class=p>,</span> <span class=n>node_color</span><span class=o>=</span><span class=n>y</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;coolwarm&#39;</span><span class=p>,</span> 
           <span class=n>with_labels</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>node_size</span><span class=o>=</span><span class=mi>500</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Original Graph with True Labels&quot;</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>embeddings</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>embeddings</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;coolwarm&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y_coord</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>embeddings</span><span class=p>):</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>annotate</span><span class=p>(</span><span class=nb>str</span><span class=p>(</span><span class=n>i</span><span class=p>),</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y_coord</span><span class=p>),</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&quot;Embedding Dimension 1&quot;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&quot;Embedding Dimension 2&quot;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;GCN Node Embeddings&quot;</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;gnn_embeddings.png&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Run example</span>
<span class=n>node_classification_example</span><span class=p>()</span>

<span class=c1># Link prediction example</span>
<span class=k>def</span><span class=w> </span><span class=nf>link_prediction_example</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Link prediction using GNN embeddings.&quot;&quot;&quot;</span>
    <span class=c1># Create a graph</span>
    <span class=n>G</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>karate_club_graph</span><span class=p>()</span>

    <span class=c1># Remove some edges for testing</span>
    <span class=n>edges</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>G</span><span class=o>.</span><span class=n>edges</span><span class=p>())</span>
    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>edges</span><span class=p>)</span>
    <span class=n>n_test</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>edges</span><span class=p>)</span> <span class=o>//</span> <span class=mi>5</span>
    <span class=n>test_edges</span> <span class=o>=</span> <span class=n>edges</span><span class=p>[:</span><span class=n>n_test</span><span class=p>]</span>
    <span class=n>train_edges</span> <span class=o>=</span> <span class=n>edges</span><span class=p>[</span><span class=n>n_test</span><span class=p>:]</span>

    <span class=c1># Create negative samples</span>
    <span class=n>non_edges</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>nx</span><span class=o>.</span><span class=n>non_edges</span><span class=p>(</span><span class=n>G</span><span class=p>))</span>
    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>non_edges</span><span class=p>)</span>
    <span class=n>neg_test_edges</span> <span class=o>=</span> <span class=n>non_edges</span><span class=p>[:</span><span class=n>n_test</span><span class=p>]</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Train edges: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>train_edges</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Test edges: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>test_edges</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Negative test edges: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>neg_test_edges</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=c1># Train GNN on remaining graph</span>
    <span class=n>G_train</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>Graph</span><span class=p>()</span>
    <span class=n>G_train</span><span class=o>.</span><span class=n>add_nodes_from</span><span class=p>(</span><span class=n>G</span><span class=o>.</span><span class=n>nodes</span><span class=p>())</span>
    <span class=n>G_train</span><span class=o>.</span><span class=n>add_edges_from</span><span class=p>(</span><span class=n>train_edges</span><span class=p>)</span>

    <span class=c1># ... (train GNN and predict links)</span>

<span class=n>link_prediction_example</span><span class=p>()</span>
</code></pre></div> <p><strong>GNN Architecture Comparison:</strong></p> <table> <thead> <tr> <th>Architecture</th> <th>Aggregation</th> <th>Attention</th> <th>Sampling</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td>GCN</td> <td>Mean</td> <td>No</td> <td>No</td> <td>Transductive learning</td> </tr> <tr> <td>GAT</td> <td>Weighted</td> <td>Yes</td> <td>No</td> <td>Varying neighbor importance</td> </tr> <tr> <td>GraphSAGE</td> <td>Mean/Max/LSTM</td> <td>No</td> <td>Yes</td> <td>Inductive learning, large graphs</td> </tr> <tr> <td>GIN</td> <td>Sum</td> <td>No</td> <td>No</td> <td>Graph-level tasks</td> </tr> </tbody> </table> <p><strong>Applications:</strong></p> <ul> <li><strong>Social Networks</strong>: Friend recommendation, influence prediction</li> <li><strong>Molecules</strong>: Property prediction, drug discovery</li> <li><strong>Recommendation</strong>: User-item graphs</li> <li><strong>Knowledge Graphs</strong>: Link prediction, reasoning</li> <li><strong>Traffic</strong>: Traffic flow prediction</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Message passing: aggregate neighbor information"</li> <li>"GCN: spectral convolution on graphs"</li> <li>"GAT: attention-weighted aggregation"</li> <li>"GraphSAGE: sampling for scalability"</li> <li>"Inductive vs transductive learning"</li> <li>"Over-smoothing problem in deep GNNs"</li> <li>Applications (social networks, molecules)</li> <li>"Node, edge, and graph-level tasks"</li> </ul> </div> </details> <hr> <h3 id=reinforcement-learning-basics-deepmind-openai-tesla-interview-question>Reinforcement Learning Basics - DeepMind, OpenAI, Tesla Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>RL</code>, <code>Q-Learning</code>, <code>Policy Gradient</code>, <code>Markov Decision Process</code> | <strong>Asked by:</strong> DeepMind, OpenAI, Tesla, Cruise, Waymo</p> <p><strong>Question:</strong> Explain the basics of Reinforcement Learning. What are the differences between value-based (Q-Learning) and policy-based (Policy Gradient) methods?</p> <details class=success> <summary>View Answer</summary> <p><strong>Reinforcement Learning (RL)</strong> is learning optimal behavior through trial and error by interacting with an environment to maximize cumulative reward.</p> <p><strong>Core Components:</strong></p> <ul> <li><strong>Agent</strong>: Learner/decision maker</li> <li><strong>Environment</strong>: World the agent interacts with</li> <li><strong>State (s)</strong>: Current situation</li> <li><strong>Action (a)</strong>: Decision made by agent</li> <li><strong>Reward &reg;</strong>: Feedback signal</li> <li><strong>Policy (œÄ)</strong>: Strategy mapping states to actions</li> </ul> <p><strong>Markov Decision Process (MDP):</strong></p> <div class=arithmatex>\[(S, A, P, R, \gamma)\]</div> <ul> <li>S: State space</li> <li>A: Action space</li> <li>P: Transition probabilities</li> <li>R: Reward function</li> <li>Œ≥: Discount factor</li> </ul> <p><strong>Q-Learning (Value-Based):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>from</span><span class=w> </span><span class=nn>collections</span><span class=w> </span><span class=kn>import</span> <span class=n>defaultdict</span>

<span class=k>class</span><span class=w> </span><span class=nc>QLearning</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Q-Learning algorithm for discrete state/action spaces.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>n_states</span><span class=p>,</span>
        <span class=n>n_actions</span><span class=p>,</span>
        <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
        <span class=n>discount_factor</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span>
        <span class=n>epsilon</span><span class=o>=</span><span class=mf>0.1</span>
    <span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>n_states</span><span class=p>,</span> <span class=n>n_actions</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>=</span> <span class=n>learning_rate</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>discount_factor</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span> <span class=o>=</span> <span class=n>epsilon</span>

    <span class=k>def</span><span class=w> </span><span class=nf>select_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Epsilon-greedy action selection.&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>:</span>
            <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># Explore</span>
        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>])</span>  <span class=c1># Exploit</span>

    <span class=k>def</span><span class=w> </span><span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>,</span> <span class=n>done</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Q-Learning update rule.&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
            <span class=n>target</span> <span class=o>=</span> <span class=n>reward</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>target</span> <span class=o>=</span> <span class=n>reward</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>next_state</span><span class=p>])</span>

        <span class=c1># TD error</span>
        <span class=n>td_error</span> <span class=o>=</span> <span class=n>target</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>]</span>

        <span class=c1># Update Q-value</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>]</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lr</span> <span class=o>*</span> <span class=n>td_error</span>

        <span class=k>return</span> <span class=n>td_error</span>

<span class=c1># Simple Grid World environment</span>
<span class=k>class</span><span class=w> </span><span class=nc>GridWorld</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Simple grid world for RL.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>=</span> <span class=n>size</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span> <span class=o>=</span> <span class=n>size</span> <span class=o>*</span> <span class=n>size</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>n_actions</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># Up, Down, Left, Right</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>goal</span> <span class=o>=</span> <span class=p>(</span><span class=n>size</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span> <span class=o>=</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_state</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>_get_state</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>

    <span class=k>def</span><span class=w> </span><span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Execute action and return (next_state, reward, done).&quot;&quot;&quot;</span>
        <span class=n>row</span><span class=p>,</span> <span class=n>col</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span>

        <span class=c1># Actions: 0=Up, 1=Down, 2=Left, 3=Right</span>
        <span class=k>if</span> <span class=n>action</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>  <span class=c1># Up</span>
            <span class=n>row</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>row</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>  <span class=c1># Down</span>
            <span class=n>row</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>row</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>  <span class=c1># Left</span>
            <span class=n>col</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>col</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
        <span class=k>elif</span> <span class=n>action</span> <span class=o>==</span> <span class=mi>3</span><span class=p>:</span>  <span class=c1># Right</span>
            <span class=n>col</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>col</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span> <span class=o>=</span> <span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>)</span>

        <span class=c1># Reward</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>goal</span><span class=p>:</span>
            <span class=n>reward</span> <span class=o>=</span> <span class=mf>1.0</span>
            <span class=n>done</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>reward</span> <span class=o>=</span> <span class=o>-</span><span class=mf>0.01</span>  <span class=c1># Small penalty for each step</span>
            <span class=n>done</span> <span class=o>=</span> <span class=kc>False</span>

        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_state</span><span class=p>(),</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span>

    <span class=k>def</span><span class=w> </span><span class=nf>render</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Visualize grid and policy.&quot;&quot;&quot;</span>
        <span class=n>grid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=bp>self</span><span class=o>.</span><span class=n>size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>size</span><span class=p>))</span>
        <span class=n>grid</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>agent_pos</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.5</span>
        <span class=n>grid</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>goal</span><span class=p>]</span> <span class=o>=</span> <span class=mf>1.0</span>

        <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>grid</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>

        <span class=c1># Draw policy arrows</span>
        <span class=k>if</span> <span class=n>Q</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>arrow_map</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>:</span> <span class=s1>&#39;‚Üë&#39;</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span> <span class=s1>&#39;‚Üì&#39;</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span> <span class=s1>&#39;‚Üê&#39;</span><span class=p>,</span> <span class=mi>3</span><span class=p>:</span> <span class=s1>&#39;‚Üí&#39;</span><span class=p>}</span>
            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>size</span><span class=p>):</span>
                <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>size</span><span class=p>):</span>
                    <span class=n>state</span> <span class=o>=</span> <span class=n>i</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>+</span> <span class=n>j</span>
                    <span class=n>best_action</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>])</span>
                    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>arrow_map</span><span class=p>[</span><span class=n>best_action</span><span class=p>],</span> 
                           <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>

        <span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>([])</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>([])</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&quot;Grid World&quot;</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;gridworld_policy.png&#39;</span><span class=p>)</span>

<span class=c1># Train Q-Learning</span>
<span class=k>def</span><span class=w> </span><span class=nf>train_q_learning</span><span class=p>(</span><span class=n>n_episodes</span><span class=o>=</span><span class=mi>500</span><span class=p>):</span>
    <span class=n>env</span> <span class=o>=</span> <span class=n>GridWorld</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
    <span class=n>agent</span> <span class=o>=</span> <span class=n>QLearning</span><span class=p>(</span>
        <span class=n>n_states</span><span class=o>=</span><span class=n>env</span><span class=o>.</span><span class=n>n_states</span><span class=p>,</span>
        <span class=n>n_actions</span><span class=o>=</span><span class=n>env</span><span class=o>.</span><span class=n>n_actions</span><span class=p>,</span>
        <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
        <span class=n>discount_factor</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span>
        <span class=n>epsilon</span><span class=o>=</span><span class=mf>0.1</span>
    <span class=p>)</span>

    <span class=n>episode_rewards</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>episode_lengths</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>episode</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_episodes</span><span class=p>):</span>
        <span class=n>state</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
        <span class=n>total_reward</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>steps</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>while</span> <span class=n>steps</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>:</span>  <span class=c1># Max steps per episode</span>
            <span class=n>action</span> <span class=o>=</span> <span class=n>agent</span><span class=o>.</span><span class=n>select_action</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>
            <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>done</span> <span class=o>=</span> <span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
            <span class=n>agent</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>next_state</span><span class=p>,</span> <span class=n>done</span><span class=p>)</span>

            <span class=n>total_reward</span> <span class=o>+=</span> <span class=n>reward</span>
            <span class=n>steps</span> <span class=o>+=</span> <span class=mi>1</span>
            <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>

            <span class=k>if</span> <span class=n>done</span><span class=p>:</span>
                <span class=k>break</span>

        <span class=n>episode_rewards</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>total_reward</span><span class=p>)</span>
        <span class=n>episode_lengths</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>steps</span><span class=p>)</span>

        <span class=k>if</span> <span class=p>(</span><span class=n>episode</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>avg_reward</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>episode_rewards</span><span class=p>[</span><span class=o>-</span><span class=mi>100</span><span class=p>:])</span>
            <span class=n>avg_length</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>episode_lengths</span><span class=p>[</span><span class=o>-</span><span class=mi>100</span><span class=p>:])</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Episode </span><span class=si>{</span><span class=n>episode</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>: Avg Reward=</span><span class=si>{</span><span class=n>avg_reward</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>, &quot;</span>
                  <span class=sa>f</span><span class=s2>&quot;Avg Length=</span><span class=si>{</span><span class=n>avg_length</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>agent</span><span class=p>,</span> <span class=n>episode_rewards</span><span class=p>,</span> <span class=n>episode_lengths</span>

<span class=c1># Policy Gradient (REINFORCE)</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=k>class</span><span class=w> </span><span class=nc>PolicyNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Neural network for policy.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state_dim</span><span class=p>,</span> <span class=n>action_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>128</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>state_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>action_dim</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Softmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>state</span><span class=p>)</span>

<span class=k>class</span><span class=w> </span><span class=nc>PolicyGradient</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;REINFORCE algorithm.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state_dim</span><span class=p>,</span> <span class=n>action_dim</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.001</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>policy</span> <span class=o>=</span> <span class=n>PolicyNetwork</span><span class=p>(</span><span class=n>state_dim</span><span class=p>,</span> <span class=n>action_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>policy</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>select_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Sample action from policy.&quot;&quot;&quot;</span>
        <span class=n>state_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>state</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
        <span class=n>probs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>policy</span><span class=p>(</span><span class=n>state_tensor</span><span class=p>)</span>
        <span class=n>action_dist</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>Categorical</span><span class=p>(</span><span class=n>probs</span><span class=p>)</span>
        <span class=n>action</span> <span class=o>=</span> <span class=n>action_dist</span><span class=o>.</span><span class=n>sample</span><span class=p>()</span>
        <span class=n>log_prob</span> <span class=o>=</span> <span class=n>action_dist</span><span class=o>.</span><span class=n>log_prob</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>action</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>log_prob</span>

    <span class=k>def</span><span class=w> </span><span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>log_probs</span><span class=p>,</span> <span class=n>rewards</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.99</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Update policy using REINFORCE.&quot;&quot;&quot;</span>
        <span class=c1># Calculate discounted returns</span>
        <span class=n>returns</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>G</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=k>for</span> <span class=n>r</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=n>rewards</span><span class=p>):</span>
            <span class=n>G</span> <span class=o>=</span> <span class=n>r</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>G</span>
            <span class=n>returns</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>G</span><span class=p>)</span>

        <span class=n>returns</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>returns</span><span class=p>)</span>
        <span class=n>returns</span> <span class=o>=</span> <span class=p>(</span><span class=n>returns</span> <span class=o>-</span> <span class=n>returns</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span> <span class=o>/</span> <span class=p>(</span><span class=n>returns</span><span class=o>.</span><span class=n>std</span><span class=p>()</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span>

        <span class=c1># Policy gradient loss</span>
        <span class=n>policy_loss</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>log_prob</span><span class=p>,</span> <span class=n>G</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>log_probs</span><span class=p>,</span> <span class=n>returns</span><span class=p>):</span>
            <span class=n>policy_loss</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=o>-</span><span class=n>log_prob</span> <span class=o>*</span> <span class=n>G</span><span class=p>)</span>

        <span class=c1># Update</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>policy_loss</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

<span class=c1># Compare Q-Learning vs Policy Gradient</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training Q-Learning...&quot;</span><span class=p>)</span>
<span class=n>q_agent</span><span class=p>,</span> <span class=n>q_rewards</span><span class=p>,</span> <span class=n>q_lengths</span> <span class=o>=</span> <span class=n>train_q_learning</span><span class=p>(</span><span class=n>n_episodes</span><span class=o>=</span><span class=mi>500</span><span class=p>)</span>

<span class=c1># Visualize learned policy</span>
<span class=n>env</span> <span class=o>=</span> <span class=n>GridWorld</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>env</span><span class=o>.</span><span class=n>render</span><span class=p>(</span><span class=n>Q</span><span class=o>=</span><span class=n>q_agent</span><span class=o>.</span><span class=n>Q</span><span class=p>)</span>

<span class=c1># Plot learning curves</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>window</span> <span class=o>=</span> <span class=mi>20</span>
<span class=n>q_rewards_smooth</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>convolve</span><span class=p>(</span><span class=n>q_rewards</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>window</span><span class=p>)</span><span class=o>/</span><span class=n>window</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;valid&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>q_rewards_smooth</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Q-Learning&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Episode&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Average Reward&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Learning Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>q_lengths_smooth</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>convolve</span><span class=p>(</span><span class=n>q_lengths</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>window</span><span class=p>)</span><span class=o>/</span><span class=n>window</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;valid&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>q_lengths_smooth</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Q-Learning&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Episode&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Episode Length&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Episode Length Over Time&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;rl_learning_curves.png&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Q-Learning vs Policy Gradient:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Q-Learning (Value-Based)</th> <th>Policy Gradient</th> </tr> </thead> <tbody> <tr> <td><strong>Learns</strong></td> <td>Q-values (state-action values)</td> <td>Policy directly</td> </tr> <tr> <td><strong>Action Selection</strong></td> <td>Argmax over Q-values</td> <td>Sample from distribution</td> </tr> <tr> <td><strong>Continuous Actions</strong></td> <td>Difficult</td> <td>Natural</td> </tr> <tr> <td><strong>Convergence</strong></td> <td>Can diverge with function approx</td> <td>More stable</td> </tr> <tr> <td><strong>Sample Efficiency</strong></td> <td>More efficient</td> <td>Less efficient</td> </tr> <tr> <td><strong>Stochastic Policies</strong></td> <td>Difficult</td> <td>Natural</td> </tr> </tbody> </table> <p><strong>Key RL Algorithms:</strong></p> <ul> <li><strong>Value-Based</strong>: Q-Learning, DQN, Double DQN</li> <li><strong>Policy-Based</strong>: REINFORCE, PPO, TRPO</li> <li><strong>Actor-Critic</strong>: A3C, SAC, TD3 (combines both)</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"MDP: states, actions, rewards, transitions"</li> <li>"Q-Learning: learn Q(s,a), act greedily"</li> <li>"Policy Gradient: learn œÄ(a|s) directly"</li> <li>"Exploration vs exploitation trade-off"</li> <li>"Q-Learning: off-policy, sample efficient"</li> <li>"Policy Gradient: on-policy, handles continuous"</li> <li>"Actor-Critic: combines both approaches"</li> <li>Applications (robotics, games, recommendation)</li> </ul> </div> </details> <hr> <h3 id=variational-autoencoders-vaes-deepmind-openai-interview-question>Variational Autoencoders (VAEs) - DeepMind, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Generative Models</code>, <code>Latent Variables</code>, <code>VAE</code>, <code>ELBO</code> | <strong>Asked by:</strong> DeepMind, OpenAI, Meta AI, Stability AI</p> <p><strong>Question:</strong> Explain Variational Autoencoders (VAEs). How do they differ from regular autoencoders? What is the reparameterization trick and why is it needed?</p> <details class=success> <summary>View Answer</summary> <p><strong>Variational Autoencoders (VAEs)</strong> are generative models that learn a probabilistic mapping between data and a latent space, enabling generation of new samples.</p> <p><strong>Key Differences from Regular Autoencoders:</strong></p> <table> <thead> <tr> <th>Regular Autoencoder</th> <th>Variational Autoencoder</th> </tr> </thead> <tbody> <tr> <td>Deterministic encoding</td> <td>Probabilistic encoding</td> </tr> <tr> <td>Learns point in latent space</td> <td>Learns distribution in latent space</td> </tr> <tr> <td>Can't generate new samples reliably</td> <td>Can generate new samples</td> </tr> <tr> <td>Reconstruction loss only</td> <td>Reconstruction + KL divergence loss</td> </tr> </tbody> </table> <p><strong>VAE Architecture:</strong></p> <ul> <li><strong>Encoder</strong>: Maps x ‚Üí (Œº, œÉ) representing q(z|x)</li> <li><strong>Latent Space</strong>: Sample z ~ N(Œº, œÉ¬≤)</li> <li><strong>Decoder</strong>: Maps z ‚Üí xÃÇ representing p(x|z)</li> </ul> <p><strong>Loss Function (ELBO):</strong></p> <div class=arithmatex>\[\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))\]</div> <ul> <li>First term: Reconstruction loss</li> <li>Second term: KL divergence (regularization)</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn.functional</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>F</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>DataLoader</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=k>class</span><span class=w> </span><span class=nc>VAE</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Variational Autoencoder.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=o>=</span><span class=mi>784</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=o>=</span><span class=mi>400</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>20</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=c1># Encoder</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc_mu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc_logvar</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>

        <span class=c1># Decoder</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc4</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Encode input to latent distribution parameters.&quot;&quot;&quot;</span>
        <span class=n>h</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>mu</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_mu</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
        <span class=n>logvar</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_logvar</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span>

    <span class=k>def</span><span class=w> </span><span class=nf>reparameterize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>        Reparameterization trick: z = Œº + œÉ * Œµ where Œµ ~ N(0,1)</span>
<span class=sd>        This allows backpropagation through sampling.</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>std</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>logvar</span><span class=p>)</span>
        <span class=n>eps</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>std</span><span class=p>)</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>mu</span> <span class=o>+</span> <span class=n>eps</span> <span class=o>*</span> <span class=n>std</span>
        <span class=k>return</span> <span class=n>z</span>

    <span class=k>def</span><span class=w> </span><span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Decode latent variable to reconstruction.&quot;&quot;&quot;</span>
        <span class=n>h</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc4</span><span class=p>(</span><span class=n>h</span><span class=p>))</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Full forward pass.&quot;&quot;&quot;</span>
        <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>784</span><span class=p>))</span>
        <span class=n>z</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>reparameterize</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>)</span>
        <span class=n>recon_x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>recon_x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span>

<span class=k>def</span><span class=w> </span><span class=nf>vae_loss</span><span class=p>(</span><span class=n>recon_x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    VAE loss = Reconstruction loss + KL divergence.</span>

<span class=sd>    KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=c1># Reconstruction loss (binary cross-entropy)</span>
    <span class=n>BCE</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>binary_cross_entropy</span><span class=p>(</span>
        <span class=n>recon_x</span><span class=p>,</span>
        <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>784</span><span class=p>),</span>
        <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;sum&#39;</span>
    <span class=p>)</span>

    <span class=c1># KL divergence</span>
    <span class=c1># KL(N(Œº, œÉ¬≤) || N(0, 1))</span>
    <span class=n>KLD</span> <span class=o>=</span> <span class=o>-</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>logvar</span> <span class=o>-</span> <span class=n>mu</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>-</span> <span class=n>logvar</span><span class=o>.</span><span class=n>exp</span><span class=p>())</span>

    <span class=k>return</span> <span class=n>BCE</span> <span class=o>+</span> <span class=n>KLD</span><span class=p>,</span> <span class=n>BCE</span><span class=p>,</span> <span class=n>KLD</span>

<span class=c1># Train VAE on MNIST</span>
<span class=k>def</span><span class=w> </span><span class=nf>train_vae</span><span class=p>():</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Train VAE on MNIST dataset.&quot;&quot;&quot;</span>
    <span class=c1># Data</span>
    <span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()</span>
    <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span>
        <span class=s1>&#39;./data&#39;</span><span class=p>,</span>
        <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>transform</span><span class=o>=</span><span class=n>transform</span>
    <span class=p>)</span>
    <span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

    <span class=c1># Model</span>
    <span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span><span class=p>)</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>VAE</span><span class=p>(</span><span class=n>latent_dim</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>

    <span class=c1># Training</span>
    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
    <span class=n>train_losses</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>recon_losses</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>kl_losses</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=n>n_epochs</span> <span class=o>=</span> <span class=mi>10</span>

    <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Training VAE...&quot;</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_epochs</span><span class=p>):</span>
        <span class=n>epoch_loss</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>epoch_recon</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>epoch_kl</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
            <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
            <span class=n>recon_batch</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
            <span class=n>loss</span><span class=p>,</span> <span class=n>recon</span><span class=p>,</span> <span class=n>kl</span> <span class=o>=</span> <span class=n>vae_loss</span><span class=p>(</span><span class=n>recon_batch</span><span class=p>,</span> <span class=n>data</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>)</span>
            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

            <span class=n>epoch_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
            <span class=n>epoch_recon</span> <span class=o>+=</span> <span class=n>recon</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
            <span class=n>epoch_kl</span> <span class=o>+=</span> <span class=n>kl</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>

        <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>epoch_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>
        <span class=n>avg_recon</span> <span class=o>=</span> <span class=n>epoch_recon</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>
        <span class=n>avg_kl</span> <span class=o>=</span> <span class=n>epoch_kl</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>)</span>

        <span class=n>train_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_loss</span><span class=p>)</span>
        <span class=n>recon_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_recon</span><span class=p>)</span>
        <span class=n>kl_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_kl</span><span class=p>)</span>

        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>n_epochs</span><span class=si>}</span><span class=s2>: &quot;</span>
              <span class=sa>f</span><span class=s2>&quot;Loss=</span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Recon=</span><span class=si>{</span><span class=n>avg_recon</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, KL=</span><span class=si>{</span><span class=n>avg_kl</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

    <span class=k>return</span> <span class=n>model</span><span class=p>,</span> <span class=n>train_losses</span><span class=p>,</span> <span class=n>recon_losses</span><span class=p>,</span> <span class=n>kl_losses</span>

<span class=c1># Train model</span>
<span class=n>model</span><span class=p>,</span> <span class=n>train_losses</span><span class=p>,</span> <span class=n>recon_losses</span><span class=p>,</span> <span class=n>kl_losses</span> <span class=o>=</span> <span class=n>train_vae</span><span class=p>()</span>

<span class=c1># Visualize results</span>
<span class=k>def</span><span class=w> </span><span class=nf>visualize_vae</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>n_samples</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Visualize VAE reconstructions and generations.&quot;&quot;&quot;</span>
    <span class=n>device</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span><span class=o>.</span><span class=n>device</span>
    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>

    <span class=c1># Load test data</span>
    <span class=n>test_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span>
        <span class=s1>&#39;./data&#39;</span><span class=p>,</span>
        <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
        <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>transform</span><span class=o>=</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()</span>
    <span class=p>)</span>

    <span class=c1># Reconstructions</span>
    <span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>

    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_samples</span><span class=p>):</span>
        <span class=c1># Original</span>
        <span class=n>img</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>test_dataset</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
        <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Original&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

        <span class=c1># Reconstruction</span>
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=n>img_tensor</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
            <span class=n>recon</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>img_tensor</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
            <span class=n>recon_img</span> <span class=o>=</span> <span class=n>recon</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>

        <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>recon_img</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
        <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Reconstructed&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

    <span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=s1>&#39;VAE Reconstructions&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;vae_reconstructions.png&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

    <span class=c1># Generate new samples</span>
    <span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>

    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
        <span class=c1># Sample from prior N(0, 1)</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=n>generated</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_samples</span><span class=p>):</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>generated</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>

        <span class=c1># Interpolation between two samples</span>
        <span class=n>z1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=n>z2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_samples</span><span class=p>):</span>
            <span class=n>alpha</span> <span class=o>=</span> <span class=n>i</span> <span class=o>/</span> <span class=p>(</span><span class=n>n_samples</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
            <span class=n>z_interp</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>z1</span> <span class=o>+</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>z2</span>
            <span class=n>img_interp</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z_interp</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img_interp</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
            <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>

    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Random Samples&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Interpolation&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;vae_generations.png&#39;</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

    <span class=c1># Latent space visualization (2D)</span>
    <span class=k>if</span> <span class=n>model</span><span class=o>.</span><span class=n>fc_mu</span><span class=o>.</span><span class=n>out_features</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>
        <span class=c1># Encode test set</span>
        <span class=n>test_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>test_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
        <span class=n>latents</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=p>[]</span>

        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=k>for</span> <span class=n>data</span><span class=p>,</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
                <span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
                <span class=n>mu</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>784</span><span class=p>))</span>
                <span class=n>latents</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>mu</span><span class=o>.</span><span class=n>cpu</span><span class=p>())</span>
                <span class=n>labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>label</span><span class=p>)</span>

        <span class=n>latents</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>latents</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>

        <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
        <span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span>
            <span class=n>latents</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span>
            <span class=n>latents</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span>
            <span class=n>c</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
            <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;tab10&#39;</span><span class=p>,</span>
            <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span>
        <span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Latent Dimension 1&#39;</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Latent Dimension 2&#39;</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;VAE Latent Space (colored by digit)&#39;</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;vae_latent_space.png&#39;</span><span class=p>)</span>
        <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=n>visualize_vae</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>

<span class=c1># Plot training curves</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_losses</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Total Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>recon_losses</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Reconstruction Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Reconstruction Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>kl_losses</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;KL Divergence&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;KL Divergence&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;vae_training_curves.png&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>The Reparameterization Trick:</strong></p> <p><strong>Problem</strong>: Can't backpropagate through sampling operation z ~ N(Œº, œÉ¬≤)</p> <p><strong>Solution</strong>: Reparameterize as z = Œº + œÉ √ó Œµ where Œµ ~ N(0, 1)</p> <ul> <li>Randomness moved to Œµ (no parameters)</li> <li>Gradients flow through Œº and œÉ</li> <li>Enables end-to-end training</li> </ul> <p><strong>Applications:</strong></p> <ul> <li><strong>Image Generation</strong>: Generate realistic images</li> <li><strong>Data Augmentation</strong>: Synthetic training data</li> <li><strong>Anomaly Detection</strong>: Detect out-of-distribution samples</li> <li><strong>Representation Learning</strong>: Learn meaningful embeddings</li> <li><strong>Drug Discovery</strong>: Generate novel molecules</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Probabilistic encoder: outputs Œº and œÉ"</li> <li>"Reparameterization trick: z = Œº + œÉŒµ"</li> <li>"ELBO: reconstruction + KL divergence"</li> <li>"KL term: regularizes latent space"</li> <li>vs. "Regular AE: deterministic, can't generate"</li> <li>"Continuous latent space enables interpolation"</li> <li>"Œ≤-VAE: weighted KL for disentanglement"</li> <li>Applications and limitations</li> </ul> </div> </details> <hr> <h3 id=generative-adversarial-networks-gans-nvidia-openai-stability-ai-interview-question>Generative Adversarial Networks (GANs) - NVIDIA, OpenAI, Stability AI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Generative Models</code>, <code>GANs</code>, <code>Adversarial Training</code>, <code>Mode Collapse</code> | <strong>Asked by:</strong> NVIDIA, OpenAI, Stability AI, Meta AI, DeepMind</p> <p><strong>Question:</strong> Explain how Generative Adversarial Networks (GANs) work. What are common training challenges like mode collapse, and how can they be addressed?</p> <details class=success> <summary>View Answer</summary> <p><strong>Generative Adversarial Networks (GANs)</strong> consist of two neural networks‚Äîa Generator and a Discriminator‚Äîthat compete in a game-theoretic framework to generate realistic data.</p> <p><strong>Architecture:</strong></p> <ul> <li><strong>Generator (G)</strong>: Learns to create fake samples from noise</li> <li><strong>Discriminator (D)</strong>: Learns to distinguish real from fake</li> <li><strong>Training</strong>: Minimax game between G and D</li> </ul> <p><strong>Objective:</strong></p> <div class=arithmatex>\[\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torch.utils.data</span><span class=w> </span><span class=kn>import</span> <span class=n>DataLoader</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Generator Network</span>
<span class=k>class</span><span class=w> </span><span class=nc>Generator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Generator network for GAN.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>img_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>img_shape</span> <span class=o>=</span> <span class=n>img_shape</span>

        <span class=k>def</span><span class=w> </span><span class=nf>block</span><span class=p>(</span><span class=n>in_feat</span><span class=p>,</span> <span class=n>out_feat</span><span class=p>,</span> <span class=n>normalize</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
            <span class=n>layers</span> <span class=o>=</span> <span class=p>[</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>in_feat</span><span class=p>,</span> <span class=n>out_feat</span><span class=p>)]</span>
            <span class=k>if</span> <span class=n>normalize</span><span class=p>:</span>
                <span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=n>out_feat</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>))</span>
            <span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
            <span class=k>return</span> <span class=n>layers</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=o>*</span><span class=n>block</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>normalize</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
            <span class=o>*</span><span class=n>block</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=o>*</span><span class=n>block</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
            <span class=o>*</span><span class=n>block</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>1024</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=nb>int</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>prod</span><span class=p>(</span><span class=n>img_shape</span><span class=p>))),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
        <span class=n>img</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
        <span class=n>img</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>img</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>*</span><span class=bp>self</span><span class=o>.</span><span class=n>img_shape</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>img</span>

<span class=c1># Discriminator Network</span>
<span class=k>class</span><span class=w> </span><span class=nc>Discriminator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Discriminator network for GAN.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>)):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>prod</span><span class=p>(</span><span class=n>img_shape</span><span class=p>)),</span> <span class=mi>512</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img</span><span class=p>):</span>
        <span class=n>img_flat</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>img</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>validity</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>img_flat</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>validity</span>

<span class=c1># GAN Trainer</span>
<span class=k>class</span><span class=w> </span><span class=nc>GANTrainer</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Trainer for standard GAN.&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
        <span class=bp>self</span><span class=p>,</span>
        <span class=n>generator</span><span class=p>,</span>
        <span class=n>discriminator</span><span class=p>,</span>
        <span class=n>latent_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
        <span class=n>lr</span><span class=o>=</span><span class=mf>0.0002</span><span class=p>,</span>
        <span class=n>b1</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span>
        <span class=n>b2</span><span class=o>=</span><span class=mf>0.999</span><span class=p>,</span>
        <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cpu&#39;</span>
    <span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>generator</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>latent_dim</span> <span class=o>=</span> <span class=n>latent_dim</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>

        <span class=c1># Optimizers</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_G</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span>
            <span class=n>generator</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
            <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span>
            <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=n>b1</span><span class=p>,</span> <span class=n>b2</span><span class=p>)</span>
        <span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_D</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span>
            <span class=n>discriminator</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
            <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span>
            <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=n>b1</span><span class=p>,</span> <span class=n>b2</span><span class=p>)</span>
        <span class=p>)</span>

        <span class=c1># Loss</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>adversarial_loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCELoss</span><span class=p>()</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>real_imgs</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Single training step.&quot;&quot;&quot;</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>real_imgs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Adversarial ground truths</span>
        <span class=n>valid</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
        <span class=n>fake</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=n>real_imgs</span> <span class=o>=</span> <span class=n>real_imgs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># ---------------------</span>
        <span class=c1># Train Generator</span>
        <span class=c1># ---------------------</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_G</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Sample noise</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># Generate fake images</span>
        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>

        <span class=c1># Generator loss (fool discriminator)</span>
        <span class=n>g_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>adversarial_loss</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>),</span> <span class=n>valid</span><span class=p>)</span>

        <span class=n>g_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_G</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=c1># ---------------------</span>
        <span class=c1># Train Discriminator</span>
        <span class=c1># ---------------------</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_D</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Real images</span>
        <span class=n>real_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>adversarial_loss</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>real_imgs</span><span class=p>),</span> <span class=n>valid</span><span class=p>)</span>

        <span class=c1># Fake images</span>
        <span class=n>fake_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>adversarial_loss</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>gen_imgs</span><span class=o>.</span><span class=n>detach</span><span class=p>()),</span> <span class=n>fake</span><span class=p>)</span>

        <span class=c1># Total discriminator loss</span>
        <span class=n>d_loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>real_loss</span> <span class=o>+</span> <span class=n>fake_loss</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>

        <span class=n>d_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>g_loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>d_loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>gen_imgs</span>

<span class=c1># Wasserstein GAN (addresses training stability)</span>
<span class=k>class</span><span class=w> </span><span class=nc>WassersteinGAN</span><span class=p>(</span><span class=n>GANTrainer</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;WGAN with gradient penalty (WGAN-GP).&quot;&quot;&quot;</span>

    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lambda_gp</span> <span class=o>=</span> <span class=mi>10</span>  <span class=c1># Gradient penalty coefficient</span>

    <span class=k>def</span><span class=w> </span><span class=nf>compute_gradient_penalty</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>real_samples</span><span class=p>,</span> <span class=n>fake_samples</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Calculate gradient penalty for WGAN-GP.&quot;&quot;&quot;</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>real_samples</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Random weight term for interpolation</span>
        <span class=n>alpha</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># Interpolated samples</span>
        <span class=n>interpolates</span> <span class=o>=</span> <span class=p>(</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>real_samples</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha</span><span class=p>)</span> <span class=o>*</span> <span class=n>fake_samples</span><span class=p>)</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>

        <span class=n>d_interpolates</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>interpolates</span><span class=p>)</span>

        <span class=n>fake</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># Get gradients</span>
        <span class=n>gradients</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>grad</span><span class=p>(</span>
            <span class=n>outputs</span><span class=o>=</span><span class=n>d_interpolates</span><span class=p>,</span>
            <span class=n>inputs</span><span class=o>=</span><span class=n>interpolates</span><span class=p>,</span>
            <span class=n>grad_outputs</span><span class=o>=</span><span class=n>fake</span><span class=p>,</span>
            <span class=n>create_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
            <span class=n>retain_graph</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
            <span class=n>only_inputs</span><span class=o>=</span><span class=kc>True</span>
        <span class=p>)[</span><span class=mi>0</span><span class=p>]</span>

        <span class=n>gradients</span> <span class=o>=</span> <span class=n>gradients</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Calculate penalty</span>
        <span class=n>gradient_penalty</span> <span class=o>=</span> <span class=p>((</span><span class=n>gradients</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>gradient_penalty</span>

    <span class=k>def</span><span class=w> </span><span class=nf>train_step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>real_imgs</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;WGAN-GP training step.&quot;&quot;&quot;</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>real_imgs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
        <span class=n>real_imgs</span> <span class=o>=</span> <span class=n>real_imgs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># ---------------------</span>
        <span class=c1># Train Discriminator (Critic)</span>
        <span class=c1># ---------------------</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_D</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Sample noise</span>
        <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=c1># Generate fake images</span>
        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>

        <span class=c1># Discriminator outputs</span>
        <span class=n>real_validity</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>real_imgs</span><span class=p>)</span>
        <span class=n>fake_validity</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>gen_imgs</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>

        <span class=c1># Gradient penalty</span>
        <span class=n>gradient_penalty</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_gradient_penalty</span><span class=p>(</span><span class=n>real_imgs</span><span class=p>,</span> <span class=n>gen_imgs</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>

        <span class=c1># Wasserstein loss</span>
        <span class=n>d_loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>real_validity</span><span class=p>)</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>fake_validity</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>lambda_gp</span> <span class=o>*</span> <span class=n>gradient_penalty</span>

        <span class=n>d_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=c1># ---------------------</span>
        <span class=c1># Train Generator</span>
        <span class=c1># ---------------------</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_G</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>

        <span class=c1># Generate new fake images</span>
        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
        <span class=n>fake_validity</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>discriminator</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>)</span>

        <span class=n>g_loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>fake_validity</span><span class=p>)</span>

        <span class=n>g_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_G</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>

        <span class=k>return</span> <span class=n>g_loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>d_loss</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>gen_imgs</span>

<span class=c1># Train GAN</span>
<span class=k>def</span><span class=w> </span><span class=nf>train_gan</span><span class=p>(</span><span class=n>gan_type</span><span class=o>=</span><span class=s1>&#39;standard&#39;</span><span class=p>,</span> <span class=n>n_epochs</span><span class=o>=</span><span class=mi>50</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;Train GAN on MNIST.&quot;&quot;&quot;</span>
    <span class=c1># Data</span>
    <span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
        <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
        <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>([</span><span class=mf>0.5</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.5</span><span class=p>])</span>
    <span class=p>])</span>

    <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span>
        <span class=s1>&#39;./data&#39;</span><span class=p>,</span>
        <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>transform</span><span class=o>=</span><span class=n>transform</span>
    <span class=p>)</span>
    <span class=n>dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

    <span class=c1># Models</span>
    <span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span><span class=p>)</span>
    <span class=n>generator</span> <span class=o>=</span> <span class=n>Generator</span><span class=p>(</span><span class=n>latent_dim</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
    <span class=n>discriminator</span> <span class=o>=</span> <span class=n>Discriminator</span><span class=p>()</span>

    <span class=c1># Trainer</span>
    <span class=k>if</span> <span class=n>gan_type</span> <span class=o>==</span> <span class=s1>&#39;wgan&#39;</span><span class=p>:</span>
        <span class=n>trainer</span> <span class=o>=</span> <span class=n>WassersteinGAN</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>trainer</span> <span class=o>=</span> <span class=n>GANTrainer</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>

    <span class=c1># Training</span>
    <span class=n>g_losses</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>d_losses</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Training </span><span class=si>{</span><span class=n>gan_type</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span><span class=si>}</span><span class=s2>...&quot;</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_epochs</span><span class=p>):</span>
        <span class=n>epoch_g_loss</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>epoch_d_loss</span> <span class=o>=</span> <span class=mi>0</span>

        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>imgs</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dataloader</span><span class=p>):</span>
            <span class=n>g_loss</span><span class=p>,</span> <span class=n>d_loss</span><span class=p>,</span> <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>trainer</span><span class=o>.</span><span class=n>train_step</span><span class=p>(</span><span class=n>imgs</span><span class=p>)</span>

            <span class=n>epoch_g_loss</span> <span class=o>+=</span> <span class=n>g_loss</span>
            <span class=n>epoch_d_loss</span> <span class=o>+=</span> <span class=n>d_loss</span>

        <span class=n>avg_g_loss</span> <span class=o>=</span> <span class=n>epoch_g_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataloader</span><span class=p>)</span>
        <span class=n>avg_d_loss</span> <span class=o>=</span> <span class=n>epoch_d_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>dataloader</span><span class=p>)</span>

        <span class=n>g_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_g_loss</span><span class=p>)</span>
        <span class=n>d_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>avg_d_loss</span><span class=p>)</span>

        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>n_epochs</span><span class=si>}</span><span class=s2>: G_loss=</span><span class=si>{</span><span class=n>avg_g_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, D_loss=</span><span class=si>{</span><span class=n>avg_d_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

        <span class=c1># Save generated images</span>
        <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>10</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
            <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
                <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>25</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
                <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>

                <span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
                <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>ax</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>axes</span><span class=o>.</span><span class=n>flat</span><span class=p>):</span>
                    <span class=n>img</span> <span class=o>=</span> <span class=n>gen_imgs</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
                    <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
                    <span class=n>ax</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>

                <span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>gan_type</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span><span class=si>}</span><span class=s1> - Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
                <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
                <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>gan_type</span><span class=si>}</span><span class=s1>_epoch_</span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>.png&#39;</span><span class=p>)</span>
                <span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>

    <span class=k>return</span> <span class=n>generator</span><span class=p>,</span> <span class=n>g_losses</span><span class=p>,</span> <span class=n>d_losses</span>

<span class=c1># Train both types</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span> <span class=o>*</span> <span class=mi>60</span><span class=p>)</span>
<span class=n>gen_standard</span><span class=p>,</span> <span class=n>g_losses_std</span><span class=p>,</span> <span class=n>d_losses_std</span> <span class=o>=</span> <span class=n>train_gan</span><span class=p>(</span><span class=s1>&#39;standard&#39;</span><span class=p>,</span> <span class=n>n_epochs</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span> <span class=o>*</span> <span class=mi>60</span><span class=p>)</span>
<span class=n>gen_wgan</span><span class=p>,</span> <span class=n>g_losses_wgan</span><span class=p>,</span> <span class=n>d_losses_wgan</span> <span class=o>=</span> <span class=n>train_gan</span><span class=p>(</span><span class=s1>&#39;wgan&#39;</span><span class=p>,</span> <span class=n>n_epochs</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>

<span class=c1># Compare training curves</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>g_losses_std</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Standard GAN - Generator&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>d_losses_std</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Standard GAN - Discriminator&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Standard GAN Training&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>g_losses_wgan</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;WGAN-GP - Generator&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>d_losses_wgan</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;WGAN-GP - Critic&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epoch&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;WGAN-GP Training&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;gan_training_comparison.png&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Common GAN Challenges &amp; Solutions:</strong></p> <table> <thead> <tr> <th>Challenge</th> <th>Description</th> <th>Solutions</th> </tr> </thead> <tbody> <tr> <td><strong>Mode Collapse</strong></td> <td>G produces limited variety</td> <td>Minibatch discrimination, unrolled GAN</td> </tr> <tr> <td><strong>Training Instability</strong></td> <td>Oscillating losses</td> <td>WGAN, Spectral normalization</td> </tr> <tr> <td><strong>Vanishing Gradients</strong></td> <td>D too strong, G can't learn</td> <td>Feature matching, label smoothing</td> </tr> <tr> <td><strong>Hyperparameter Sensitivity</strong></td> <td>Hard to tune</td> <td>Progressive growing, StyleGAN</td> </tr> </tbody> </table> <p><strong>GAN Variants:</strong></p> <ul> <li><strong>DCGAN</strong>: Use convolutional layers</li> <li><strong>WGAN/WGAN-GP</strong>: Wasserstein distance + gradient penalty</li> <li><strong>StyleGAN</strong>: Style-based generation</li> <li><strong>CycleGAN</strong>: Unpaired image-to-image translation</li> <li><strong>Pix2Pix</strong>: Paired image-to-image translation</li> <li><strong>ProGAN</strong>: Progressive growing</li> </ul> <p><strong>Applications:</strong></p> <ul> <li><strong>Image Generation</strong>: Realistic faces, artwork</li> <li><strong>Super Resolution</strong>: Enhance image quality</li> <li><strong>Style Transfer</strong>: Artistic style application</li> <li><strong>Data Augmentation</strong>: Generate training data</li> <li><strong>Anomaly Detection</strong>: Identify unusual patterns</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>"Adversarial training: generator vs discriminator"</li> <li>"Minimax game: G fools D, D distinguishes real/fake"</li> <li>"Mode collapse: limited output diversity"</li> <li>"WGAN: Wasserstein distance for stability"</li> <li>"Gradient penalty enforces Lipschitz constraint"</li> <li>"Training tricks: label smoothing, feature matching"</li> <li>vs. "VAE: explicit likelihood, GAN: implicit"</li> <li>Applications and recent advances (StyleGAN)</li> </ul> </div> </details> <hr> <h3 id=transformer-architecture-google-openai-meta-interview-question>Transformer Architecture - Google, OpenAI, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Transformers</code>, <code>Self-Attention</code>, <code>BERT</code>, <code>GPT</code> | <strong>Asked by:</strong> Google, OpenAI, Meta, Anthropic, Cohere</p> <p><strong>Question:</strong> Explain the Transformer architecture. How does self-attention work? What are the key differences between BERT and GPT?</p> <details class=success> <summary>View Answer</summary> <p><strong>Transformers</strong> revolutionized NLP by replacing recurrence with self-attention mechanisms, enabling parallel processing and better long-range dependencies.</p> <p><strong>Key Components:</strong></p> <ol> <li><strong>Self-Attention</strong>: Compute relevance between all positions</li> <li><strong>Multi-Head Attention</strong>: Multiple attention patterns</li> <li><strong>Position Encoding</strong>: Inject sequence order information</li> <li><strong>Feed-Forward Networks</strong>: Process attended representations</li> <li><strong>Layer Normalization &amp; Residual Connections</strong>: Training stability</li> </ol> <p><strong>Self-Attention Mechanism:</strong></p> <div class=arithmatex>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div> <ul> <li>Q (Query), K (Key), V (Value) from input embeddings</li> <li>Scaled dot-product attention</li> <li>Output: weighted sum of values</li> </ul> <p><strong>BERT vs GPT:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>BERT</th> <th>GPT</th> </tr> </thead> <tbody> <tr> <td><strong>Architecture</strong></td> <td>Encoder-only</td> <td>Decoder-only</td> </tr> <tr> <td><strong>Training</strong></td> <td>Masked Language Modeling</td> <td>Causal Language Modeling</td> </tr> <tr> <td><strong>Attention</strong></td> <td>Bidirectional</td> <td>Unidirectional (causal)</td> </tr> <tr> <td><strong>Best For</strong></td> <td>Understanding tasks</td> <td>Generation tasks</td> </tr> <tr> <td><strong>Examples</strong></td> <td>Classification, NER, QA</td> <td>Text generation, completion</td> </tr> </tbody> </table> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Self-attention: compute relationships between all tokens"</li> <li>"Multi-head: multiple attention patterns"</li> <li>"Positional encoding: inject order information"</li> <li>"BERT: bidirectional, masked LM"</li> <li>"GPT: autoregressive, causal LM"</li> <li>"Transformers: parallelizable, better than RNNs"</li> <li>Applications in NLP, CV (ViT), multimodal</li> </ul> </div> </details> <hr> <h3 id=fine-tuning-vs-transfer-learning-google-meta-openai-interview-question>Fine-Tuning vs Transfer Learning - Google, Meta, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Transfer Learning</code>, <code>Fine-Tuning</code>, <code>Feature Extraction</code>, <code>Domain Adaptation</code> | <strong>Asked by:</strong> Google, Meta, OpenAI, Microsoft, Amazon</p> <p><strong>Question:</strong> What's the difference between fine-tuning and transfer learning? When would you use each approach? Explain domain adaptation.</p> <details class=success> <summary>View Answer</summary> <p><strong>Transfer Learning</strong>: Using knowledge from a source task to improve learning on a target task.</p> <p><strong>Fine-Tuning</strong>: Continuing training of a pre-trained model on new data, updating some or all weights.</p> <p><strong>Strategies:</strong></p> <table> <thead> <tr> <th>Approach</th> <th>When to Use</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td><strong>Feature Extraction</strong></td> <td>Small target dataset, similar domains</td> <td>Fast, prevents overfitting</td> <td>Limited adaptation</td> </tr> <tr> <td><strong>Fine-Tune Last Layers</strong></td> <td>Medium dataset, related domains</td> <td>Good balance</td> <td>Need to choose layers</td> </tr> <tr> <td><strong>Fine-Tune All Layers</strong></td> <td>Large dataset, different domains</td> <td>Maximum adaptation</td> <td>Risk overfitting, slow</td> </tr> </tbody> </table> <p><strong>Domain Adaptation</strong>: Transfer when source and target have different distributions.</p> <ul> <li><strong>Supervised</strong>: Labels in both domains</li> <li><strong>Unsupervised</strong>: Labels only in source</li> <li><strong>Semi-supervised</strong>: Few labels in target</li> </ul> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Transfer learning: reuse learned features"</li> <li>"Fine-tuning: continue training with lower LR"</li> <li>"Feature extraction: freeze base, train head"</li> <li>"Domain shift: source ‚â† target distribution"</li> <li>"Few-shot: adapt with minimal examples"</li> <li>Practical considerations (dataset size, compute)</li> </ul> </div> </details> <hr> <h3 id=handling-missing-data-google-amazon-microsoft-interview-question>Handling Missing Data - Google, Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Imputation</code>, <code>Missing Values</code> | <strong>Asked by:</strong> Google, Amazon, Microsoft, Meta, Apple</p> <p><strong>Question:</strong> What are different strategies for handling missing data? When would you use each approach?</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Missing Data:</strong></p> <ol> <li><strong>MCAR (Missing Completely At Random)</strong>: No pattern</li> <li><strong>MAR (Missing At Random)</strong>: Related to observed data</li> <li><strong>MNAR (Missing Not At Random)</strong>: Related to missing value itself</li> </ol> <p><strong>Strategies:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Use Case</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td><strong>Deletion</strong></td> <td>MCAR, &lt;5% missing</td> <td>Simple, no bias if MCAR</td> <td>Loses information</td> </tr> <tr> <td><strong>Mean/Median</strong></td> <td>Numerical, MCAR</td> <td>Fast, preserves size</td> <td>Reduces variance</td> </tr> <tr> <td><strong>Mode</strong></td> <td>Categorical</td> <td>Simple</td> <td>May create bias</td> </tr> <tr> <td><strong>Forward/Backward Fill</strong></td> <td>Time series</td> <td>Preserves trends</td> <td>Not for cross-sectional</td> </tr> <tr> <td><strong>Interpolation</strong></td> <td>Time series, ordered</td> <td>Smooth estimates</td> <td>Assumes continuity</td> </tr> <tr> <td><strong>KNN Imputation</strong></td> <td>Complex patterns</td> <td>Captures relationships</td> <td>Slow, sensitive to K</td> </tr> <tr> <td><strong>Model-Based</strong></td> <td>MAR, complex</td> <td>Most accurate</td> <td>Computationally expensive</td> </tr> <tr> <td><strong>Multiple Imputation</strong></td> <td>Uncertainty quantification</td> <td>Accounts for uncertainty</td> <td>Complex, slow</td> </tr> </tbody> </table> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>Understanding of MCAR/MAR/MNAR</li> <li>Multiple imputation strategies</li> <li>"Check missingness pattern first"</li> <li>"Mean for MCAR numerical"</li> <li>"KNN/model-based for complex patterns"</li> <li>"Consider creating 'is_missing' indicator"</li> <li>Impact on downstream models</li> </ul> </div> </details> <hr> <h3 id=feature-selection-techniques-google-amazon-meta-interview-question>Feature Selection Techniques - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Feature Selection</code>, <code>Dimensionality Reduction</code>, <code>Model Interpretability</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Airbnb, LinkedIn</p> <p><strong>Question:</strong> Compare different feature selection methods: filter, wrapper, and embedded methods. When would you use each?</p> <details class=success> <summary>View Answer</summary> <p><strong>Feature Selection Methods:</strong></p> <p><strong>1. Filter Methods</strong> (Independent of model): - Correlation coefficients - Chi-square test - Information gain - Variance threshold</p> <p><strong>2. Wrapper Methods</strong> (Model-dependent): - Forward selection - Backward elimination - Recursive Feature Elimination (RFE)</p> <p><strong>3. Embedded Methods</strong> (During training): - Lasso (L1 regularization) - Ridge (L2 regularization) - Tree-based feature importance - Elastic Net</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Speed</th> <th>Accuracy</th> <th>Model-Agnostic</th> </tr> </thead> <tbody> <tr> <td><strong>Filter</strong></td> <td>Fast</td> <td>Moderate</td> <td>Yes</td> </tr> <tr> <td><strong>Wrapper</strong></td> <td>Slow</td> <td>High</td> <td>No</td> </tr> <tr> <td><strong>Embedded</strong></td> <td>Medium</td> <td>High</td> <td>No</td> </tr> </tbody> </table> <p><strong>When to Use:</strong> - <strong>Filter</strong>: Quick exploration, many features - <strong>Wrapper</strong>: Small feature sets, need optimal subset - <strong>Embedded</strong>: During model training, automatic</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Filter: univariate, fast, model-agnostic"</li> <li>"Wrapper: search subsets, slow, accurate"</li> <li>"Embedded: during training, efficient"</li> <li>"Lasso for automatic selection"</li> <li>"RFE with cross-validation"</li> <li>"Consider domain knowledge"</li> <li>Trade-offs (speed vs accuracy)</li> </ul> </div> </details> <hr> <h3 id=time-series-forecasting-uber-airbnb-amazon-interview-question>Time Series Forecasting - Uber, Airbnb, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Time Series</code>, <code>ARIMA</code>, <code>LSTM</code>, <code>Seasonality</code> | <strong>Asked by:</strong> Uber, Airbnb, Amazon, Lyft, DoorDash</p> <p><strong>Question:</strong> Explain approaches to time series forecasting. Compare statistical methods (ARIMA) with deep learning (LSTM). How do you handle seasonality?</p> <details class=success> <summary>View Answer</summary> <p><strong>Time Series Components:</strong></p> <ol> <li><strong>Trend</strong>: Long-term direction</li> <li><strong>Seasonality</strong>: Regular patterns</li> <li><strong>Cyclic</strong>: Non-fixed frequency patterns</li> <li><strong>Residual</strong>: Random noise</li> </ol> <p><strong>Methods:</strong></p> <p><strong>Statistical:</strong> - <strong>ARIMA</strong>: AutoRegressive Integrated Moving Average - <strong>SARIMA</strong>: Seasonal ARIMA - <strong>Prophet</strong>: Facebook's additive model - <strong>Exponential Smoothing</strong>: Weighted averages</p> <p><strong>Deep Learning:</strong> - <strong>LSTM/GRU</strong>: Capture long-term dependencies - <strong>Temporal Convolutional Networks</strong>: Dilated convolutions - <strong>Transformer</strong>: Attention for time series</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>ARIMA</th> <th>LSTM</th> </tr> </thead> <tbody> <tr> <td><strong>Interpretability</strong></td> <td>High</td> <td>Low</td> </tr> <tr> <td><strong>Data Required</strong></td> <td>Small</td> <td>Large</td> </tr> <tr> <td><strong>Seasonality</strong></td> <td>SARIMA extension</td> <td>Learns automatically</td> </tr> <tr> <td><strong>Multiple Variables</strong></td> <td>VAR extension</td> <td>Native support</td> </tr> <tr> <td><strong>Non-linearity</strong></td> <td>Limited</td> <td>Excellent</td> </tr> </tbody> </table> <p><strong>Handling Seasonality:</strong> - Decomposition (additive/multiplicative) - Differencing - Seasonal dummy variables - Fourier features - SARIMA - Let deep learning learn it</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"ARIMA: linear, interpretable, small data"</li> <li>"LSTM: non-linear, large data, multivariate"</li> <li>"Stationarity check (ADF test)"</li> <li>"Seasonality: decomposition, differencing"</li> <li>"Walk-forward validation for evaluation"</li> <li>"Exogenous variables for forecasting"</li> <li>Prophet for business time series</li> </ul> </div> </details> <hr> <h3 id=ab-testing-in-ml-meta-uber-netflix-airbnb-interview-question>A/B Testing in ML - Meta, Uber, Netflix, Airbnb Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>A/B Testing</code>, <code>Experimentation</code>, <code>Statistical Significance</code>, <code>ML Systems</code> | <strong>Asked by:</strong> Meta, Uber, Netflix, Airbnb, Booking.com</p> <p><strong>Question:</strong> How do you A/B test machine learning models in production? What metrics would you track? How do you handle seasonality and confounding variables?</p> <details class=success> <summary>View Answer</summary> <p><strong>A/B Testing ML Models:</strong></p> <p><strong>Setup:</strong> 1. <strong>Control</strong>: Current model 2. <strong>Treatment</strong>: New model 3. <strong>Random assignment</strong>: Users ‚Üí groups 4. <strong>Measure</strong>: Business + ML metrics</p> <p><strong>Key Considerations:</strong></p> <p><strong>Metrics:</strong> - <strong>Business</strong>: Revenue, engagement, retention - <strong>ML</strong>: Accuracy, latency, throughput - <strong>User Experience</strong>: CTR, time on site, conversion</p> <p><strong>Challenges:</strong></p> <ol> <li><strong>Sample Size</strong>: Power analysis for detection</li> <li><strong>Duration</strong>: Account for day-of-week, seasonality</li> <li><strong>Network Effects</strong>: User interactions</li> <li><strong>Multiple Testing</strong>: Bonferroni correction</li> <li><strong>Novelty Effect</strong>: Users try new things initially</li> </ol> <p><strong>Statistical Tests:</strong> - T-test: Continuous metrics - Chi-square: Categorical metrics - Mann-Whitney U: Non-parametric - Bootstrap: Confidence intervals</p> <p><strong>Advanced Techniques:</strong> - <strong>Multi-armed Bandits</strong>: Dynamic allocation - <strong>Sequential Testing</strong>: Early stopping - <strong>Stratification</strong>: Control for confounders - <strong>CUPED</strong>: Variance reduction using pre-experiment data</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Random assignment for causal inference"</li> <li>"Business metrics + ML metrics"</li> <li>"Statistical power and sample size"</li> <li>"Run 1-2 weeks to capture seasonality"</li> <li>"Check A/A test first (sanity check)"</li> <li>"Guard rails: latency, error rates"</li> <li>"Novelty effect and long-term impact"</li> <li>Multi-armed bandits for exploration</li> </ul> </div> </details> <hr> <h3 id=model-monitoring-drift-detection-uber-netflix-airbnb-interview-question>Model Monitoring &amp; Drift Detection - Uber, Netflix, Airbnb Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>ML Ops</code>, <code>Model Monitoring</code>, <code>Data Drift</code>, <code>Concept Drift</code> | <strong>Asked by:</strong> Uber, Netflix, Airbnb, DoorDash, Instacart</p> <p><strong>Question:</strong> How do you monitor ML models in production? Explain data drift vs concept drift. What metrics and techniques would you use for drift detection?</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Drift:</strong></p> <p><strong>1. Data Drift (Covariate Shift):</strong> - Input distribution changes: P(X) changes - Features evolve over time - Example: User demographics shift</p> <p><strong>2. Concept Drift:</strong> - Relationship changes: P(Y|X) changes - Target definition evolves - Example: User preferences change</p> <p><strong>3. Label Drift:</strong> - Output distribution changes: P(Y) changes - Class balance shifts</p> <p><strong>Detection Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Type</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>KL Divergence</strong></td> <td>Statistical</td> <td>Distribution comparison</td> </tr> <tr> <td><strong>KS Test</strong></td> <td>Statistical</td> <td>Two-sample test</td> </tr> <tr> <td><strong>PSI (Population Stability Index)</strong></td> <td>Statistical</td> <td>Feature drift</td> </tr> <tr> <td><strong>Performance Monitoring</strong></td> <td>Model-based</td> <td>Concept drift</td> </tr> <tr> <td><strong>Feature Distribution</strong></td> <td>Statistical</td> <td>Data drift</td> </tr> </tbody> </table> <p><strong>Monitoring Metrics:</strong></p> <p><strong>Model Performance:</strong> - Accuracy, precision, recall - AUC, F1 score - Prediction distribution</p> <p><strong>Data Quality:</strong> - Missing values - Out-of-range values - New categorical values - Feature correlations</p> <p><strong>System Metrics:</strong> - Latency (p50, p95, p99) - Throughput (requests/sec) - Error rates - Resource utilization</p> <p><strong>Response Strategies:</strong> 1. <strong>Retrain</strong>: On recent data 2. <strong>Online Learning</strong>: Continuous updates 3. <strong>Ensemble</strong>: Combine old + new models 4. <strong>Rollback</strong>: Revert to previous version 5. <strong>Alert</strong>: Human intervention</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Data drift: P(X) changes"</li> <li>"Concept drift: P(Y|X) changes"</li> <li>"KS test, PSI for detection"</li> <li>"Monitor both performance and data"</li> <li>"Retrain triggers: performance drop, time-based"</li> <li>"Shadow mode: test new model safely"</li> <li>"Logging: predictions, features, outcomes"</li> <li>Feedback loop and continuous improvement</li> </ul> </div> </details> <hr> <h3 id=neural-architecture-search-nas-google-deepmind-interview-question>Neural Architecture Search (NAS) - Google, DeepMind Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>AutoML</code>, <code>NAS</code>, <code>Hyperparameter Optimization</code>, <code>Meta-Learning</code> | <strong>Asked by:</strong> Google, DeepMind, Microsoft Research, Meta AI</p> <p><strong>Question:</strong> What is Neural Architecture Search (NAS)? Explain different NAS methods and their trade-offs.</p> <details class=success> <summary>View Answer</summary> <p><strong>Neural Architecture Search (NAS)</strong>: Automated process of designing optimal neural network architectures.</p> <p><strong>Components:</strong></p> <ol> <li><strong>Search Space</strong>: Possible architectures</li> <li><strong>Search Strategy</strong>: How to explore space</li> <li><strong>Performance Estimation</strong>: Evaluate candidates</li> </ol> <p><strong>NAS Methods:</strong></p> <p><strong>1. Reinforcement Learning-based:</strong> - Controller RNN generates architectures - Train child network, use accuracy as reward - Very expensive (thousands of GPUs)</p> <p><strong>2. Evolutionary:</strong> - Population of architectures - Mutation and crossover - Natural selection based on performance</p> <p><strong>3. Gradient-based (DARTS):</strong> - Continuous relaxation of search space - Differentiate w.r.t. architecture - Much faster than RL/EA</p> <p><strong>4. One-Shot:</strong> - Train super-network once - Sample sub-networks for evaluation - Very efficient</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Speed</th> <th>Quality</th> <th>Cost</th> </tr> </thead> <tbody> <tr> <td><strong>RL-based</strong></td> <td>Slow</td> <td>High</td> <td>Very High</td> </tr> <tr> <td><strong>Evolutionary</strong></td> <td>Slow</td> <td>High</td> <td>High</td> </tr> <tr> <td><strong>DARTS</strong></td> <td>Fast</td> <td>Good</td> <td>Low</td> </tr> <tr> <td><strong>One-Shot</strong></td> <td>Very Fast</td> <td>Moderate</td> <td>Very Low</td> </tr> </tbody> </table> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"NAS: automate architecture design"</li> <li>"Search space, strategy, evaluation"</li> <li>"RL-based: expensive, high quality"</li> <li>"DARTS: gradient-based, efficient"</li> <li>"Transfer NAS: search once, use everywhere"</li> <li>"Hardware-aware NAS: optimize for deployment"</li> <li>Trade-offs (cost vs performance)</li> </ul> </div> </details> <hr> <h3 id=federated-learning-google-apple-microsoft-interview-question>Federated Learning - Google, Apple, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Distributed ML</code>, <code>Privacy</code>, <code>Federated Learning</code>, <code>Edge Computing</code> | <strong>Asked by:</strong> Google, Apple, Microsoft, NVIDIA, Meta</p> <p><strong>Question:</strong> Explain Federated Learning. How does it preserve privacy? What are the challenges and how do you address them?</p> <details class=success> <summary>View Answer</summary> <p><strong>Federated Learning</strong>: Train models across decentralized devices without collecting raw data centrally.</p> <p><strong>Process:</strong></p> <ol> <li>Server sends model to clients</li> <li>Clients train locally on private data</li> <li>Clients send updates (not data) to server</li> <li>Server aggregates updates</li> <li>Repeat</li> </ol> <p><strong>Key Algorithm: Federated Averaging (FedAvg)</strong></p> <ul> <li>Clients perform multiple SGD steps</li> <li>Server averages client models</li> <li>Reduces communication rounds</li> </ul> <p><strong>Privacy Preservation:</strong> - <strong>Data stays local</strong>: Never leaves device - <strong>Differential Privacy</strong>: Add noise to updates - <strong>Secure Aggregation</strong>: Encrypted aggregation - <strong>Homomorphic Encryption</strong>: Compute on encrypted data</p> <p><strong>Challenges:</strong></p> <table> <thead> <tr> <th>Challenge</th> <th>Description</th> <th>Solution</th> </tr> </thead> <tbody> <tr> <td><strong>Non-IID Data</strong></td> <td>Heterogeneous distributions</td> <td>FedProx, personalization</td> </tr> <tr> <td><strong>Communication Cost</strong></td> <td>Slow networks</td> <td>Compression, quantization</td> </tr> <tr> <td><strong>Systems Heterogeneity</strong></td> <td>Different devices</td> <td>Asynchronous FL</td> </tr> <tr> <td><strong>Privacy Leakage</strong></td> <td>Model inversion</td> <td>Differential privacy, secure aggregation</td> </tr> <tr> <td><strong>Stragglers</strong></td> <td>Slow devices</td> <td>Asynchronous updates, timeout</td> </tr> </tbody> </table> <p><strong>Applications:</strong> - <strong>Mobile Keyboards</strong>: Gboard, Apple Keyboard - <strong>Healthcare</strong>: Hospital collaboration without sharing patient data - <strong>Finance</strong>: Cross-bank fraud detection - <strong>IoT</strong>: Edge device learning</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Decentralized: data stays on device"</li> <li>"FedAvg: average model updates"</li> <li>"Non-IID data challenge"</li> <li>"Differential privacy for protection"</li> <li>"Communication efficiency critical"</li> <li>"Personalization: global + local models"</li> <li>Trade-offs (privacy vs accuracy)</li> </ul> </div> </details> <hr> <h3 id=model-compression-google-nvidia-apple-interview-question>Model Compression - Google, NVIDIA, Apple Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Compression</code>, <code>Pruning</code>, <code>Quantization</code>, <code>Distillation</code>, <code>Mobile ML</code> | <strong>Asked by:</strong> Google, NVIDIA, Apple, Qualcomm, Meta</p> <p><strong>Question:</strong> Explain different model compression techniques. How do you deploy large models on resource-constrained devices?</p> <details class=success> <summary>View Answer</summary> <p><strong>Model Compression Techniques:</strong></p> <p><strong>1. Quantization:</strong> - Reduce precision (FP32 ‚Üí INT8) - 4x smaller, 4x faster - Post-training or quantization-aware training</p> <p><strong>2. Pruning:</strong> - Remove unnecessary weights/neurons - Magnitude-based, gradient-based - Structured vs unstructured</p> <p><strong>3. Knowledge Distillation:</strong> - Teacher (large) ‚Üí Student (small) - Student learns from teacher's soft labels - Retains performance with fewer parameters</p> <p><strong>4. Low-Rank Factorization:</strong> - Decompose weight matrices - Reduce parameters - SVD, Tucker decomposition</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>Size Reduction</th> <th>Speed Up</th> <th>Accuracy Loss</th> </tr> </thead> <tbody> <tr> <td><strong>Quantization</strong></td> <td>4x</td> <td>2-4x</td> <td>Minimal</td> </tr> <tr> <td><strong>Pruning</strong></td> <td>2-10x</td> <td>2-3x</td> <td>Low-Medium</td> </tr> <tr> <td><strong>Distillation</strong></td> <td>Variable</td> <td>Variable</td> <td>Low</td> </tr> <tr> <td><strong>Low-Rank</strong></td> <td>2-5x</td> <td>2-3x</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>Combined Approach:</strong> - Distillation + Quantization + Pruning - Can achieve 10-100x compression - With &lt;1% accuracy loss</p> <p><strong>Deployment Strategies:</strong> - <strong>TensorFlow Lite</strong>: Mobile/embedded - <strong>ONNX Runtime</strong>: Cross-platform - <strong>TensorRT</strong>: NVIDIA GPUs - <strong>Core ML</strong>: Apple devices</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Quantization: INT8 for 4x compression"</li> <li>"Pruning: remove low-magnitude weights"</li> <li>"Distillation: student learns from teacher"</li> <li>"Combined techniques for best results"</li> <li>"Hardware-aware: target device constraints"</li> <li>"Quantization-aware training beats post-training"</li> <li>Trade-offs (size vs accuracy vs latency)</li> </ul> </div> </details> <hr> <h3 id=causal-inference-in-ml-linkedin-airbnb-uber-interview-question>Causal Inference in ML - LinkedIn, Airbnb, Uber Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Causal Inference</code>, <code>Treatment Effects</code>, <code>Confounding</code>, <code>Counterfactuals</code> | <strong>Asked by:</strong> LinkedIn, Airbnb, Uber, Microsoft, Meta</p> <p><strong>Question:</strong> Explain the difference between correlation and causation. How do you estimate causal effects in observational data? What are confounders?</p> <details class=success> <summary>View Answer</summary> <p><strong>Causation vs Correlation:</strong></p> <ul> <li><strong>Correlation</strong>: X and Y move together</li> <li><strong>Causation</strong>: X causes Y (interventional)</li> </ul> <p><strong>Causal Framework:</strong></p> <p><strong>Key Concepts:</strong></p> <ol> <li><strong>Treatment</strong>: Intervention (e.g., ad exposure)</li> <li><strong>Outcome</strong>: Effect (e.g., purchase)</li> <li><strong>Confounder</strong>: Affects both treatment &amp; outcome</li> <li><strong>Counterfactual</strong>: What would have happened?</li> </ol> <p><strong>Estimation Methods:</strong></p> <p><strong>1. Randomized Controlled Trials (RCTs):</strong> - Gold standard - Random assignment eliminates confounding - Not always feasible</p> <p><strong>2. Propensity Score Matching:</strong> - Match treated/control with similar propensity - Balance observed confounders - Doesn't handle unobserved confounders</p> <p><strong>3. Instrumental Variables:</strong> - Use instrument correlated with treatment - Not directly affecting outcome - Handles unobserved confounding</p> <p><strong>4. Difference-in-Differences:</strong> - Compare before/after treatment - Across treated/control groups - Parallel trends assumption</p> <p><strong>5. Regression Discontinuity:</strong> - Exploit cutoff for treatment assignment - Local randomization at threshold</p> <p><strong>Causal ML:</strong> - <strong>Uplift Modeling</strong>: Predict treatment effect - <strong>Meta-Learners</strong>: T-learner, S-learner, X-learner - <strong>CATE</strong>: Conditional Average Treatment Effect - <strong>Double ML</strong>: Debiased machine learning</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Correlation ‚â† Causation"</li> <li>"Confounders: affect both treatment and outcome"</li> <li>"RCT: randomization eliminates confounding"</li> <li>"Propensity scores: match similar units"</li> <li>"Counterfactuals: what if treatment not given"</li> <li>"Uplift modeling: predict treatment effect"</li> <li>Applications (marketing, policy, healthcare)</li> </ul> </div> </details> <hr> <h3 id=recommendation-systems-netflix-spotify-amazon-interview-question>Recommendation Systems - Netflix, Spotify, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Recommender Systems</code>, <code>Collaborative Filtering</code>, <code>Matrix Factorization</code>, <code>Two-Tower</code> | <strong>Asked by:</strong> Netflix, Spotify, Amazon, YouTube, Pinterest</p> <p><strong>Question:</strong> Explain different recommendation system approaches. Compare collaborative filtering, content-based, and hybrid methods. How do you handle cold start?</p> <details class=success> <summary>View Answer</summary> <p><strong>Recommendation Approaches:</strong></p> <p><strong>1. Collaborative Filtering:</strong> - <strong>User-based</strong>: Similar users like similar items - <strong>Item-based</strong>: Similar items liked by same users - <strong>Matrix Factorization</strong>: Latent factors (SVD, ALS)</p> <p><strong>2. Content-Based:</strong> - Recommend based on item features - User profile from past interactions - No cold start for new users</p> <p><strong>3. Hybrid:</strong> - Combine collaborative + content-based - Ensemble or integrated models</p> <p><strong>4. Deep Learning:</strong> - <strong>Two-Tower</strong>: User/item embeddings - <strong>Neural Collaborative Filtering</strong>: Deep CF - <strong>Sequence Models</strong>: RNN, Transformer for sessions</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Cold Start</th> <th>Diversity</th> <th>Scalability</th> </tr> </thead> <tbody> <tr> <td><strong>Collaborative</strong></td> <td>Poor</td> <td>Good</td> <td>Medium</td> </tr> <tr> <td><strong>Content-Based</strong></td> <td>Good</td> <td>Poor</td> <td>Good</td> </tr> <tr> <td><strong>Hybrid</strong></td> <td>Good</td> <td>Good</td> <td>Medium</td> </tr> <tr> <td><strong>Deep Learning</strong></td> <td>Medium</td> <td>Good</td> <td>Depends</td> </tr> </tbody> </table> <p><strong>Cold Start Solutions:</strong></p> <p><strong>New Users:</strong> - Onboarding questionnaire - Popular items - Demographic-based</p> <p><strong>New Items:</strong> - Content-based features - Transfer from similar items - Explore-exploit (bandits)</p> <p><strong>Metrics:</strong> - <strong>Accuracy</strong>: RMSE, MAE - <strong>Ranking</strong>: Precision@K, Recall@K, NDCG - <strong>Business</strong>: CTR, engagement, diversity - <strong>Coverage</strong>: % items recommended</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Collaborative filtering: user-item patterns"</li> <li>"Matrix factorization: latent factors"</li> <li>"Content-based: item features"</li> <li>"Cold start: no history for new users/items"</li> <li>"Two-tower models: user/item embeddings"</li> <li>"Explore-exploit for new items"</li> <li>"Metrics: accuracy + diversity + coverage"</li> <li>Production challenges (scalability, freshness)</li> </ul> </div> </details> <hr> <h3 id=imbalanced-learning-stripe-paypal-meta-interview-question>Imbalanced Learning - Stripe, PayPal, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Imbalanced Data</code>, <code>Class Imbalance</code>, <code>Sampling</code>, <code>Cost-Sensitive</code> | <strong>Asked by:</strong> Stripe, PayPal, Meta, Amazon, Google</p> <p><strong>Question:</strong> How do you handle highly imbalanced datasets (e.g., fraud detection)? Compare different resampling and algorithmic approaches.</p> <details class=success> <summary>View Answer</summary> <p><strong>Problem</strong>: When one class significantly outnumbers others (e.g., 99.9% non-fraud, 0.1% fraud).</p> <p><strong>Challenges:</strong> - Models biased toward majority class - Poor recall on minority class - Accuracy misleading (99% by predicting all negative)</p> <p><strong>Solutions:</strong></p> <p><strong>1. Resampling:</strong> - <strong>Oversampling</strong>: Duplicate minority (SMOTE) - <strong>Undersampling</strong>: Remove majority (Random, Tomek links) - <strong>Hybrid</strong>: Combine both (SMOTE + ENN)</p> <p><strong>2. Algorithmic:</strong> - <strong>Class Weights</strong>: Penalize errors differently - <strong>Threshold Tuning</strong>: Adjust decision boundary - <strong>Ensemble</strong>: Balanced bagging/boosting - <strong>Anomaly Detection</strong>: Treat as outlier detection</p> <p><strong>3. Evaluation Metrics:</strong> - <strong>Precision-Recall</strong>: Better than accuracy - <strong>F1-Score</strong>: Harmonic mean - <strong>AUC-ROC</strong>: Threshold-independent - <strong>PR-AUC</strong>: Better for imbalanced - <strong>Matthews Correlation Coefficient</strong>: Balanced measure</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Approach</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td><strong>SMOTE</strong></td> <td>Creates synthetic samples</td> <td>May create noise</td> </tr> <tr> <td><strong>Undersampling</strong></td> <td>Fast, reduces majority</td> <td>Loses information</td> </tr> <tr> <td><strong>Class Weights</strong></td> <td>No data modification</td> <td>Hyperparameter tuning</td> </tr> <tr> <td><strong>Anomaly Detection</strong></td> <td>Unsupervised</td> <td>Needs normal data</td> </tr> </tbody> </table> <p><strong>Best Practices:</strong> - Use stratified splitting - Focus on PR-AUC over accuracy - Combine multiple techniques - Cost-sensitive learning (business cost of errors)</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Class imbalance: majority drowns minority"</li> <li>"SMOTE: synthetic minority samples"</li> <li>"Class weights: penalize errors differently"</li> <li>"Accuracy misleading, use PR-AUC"</li> <li>"Threshold tuning: optimize for business metric"</li> <li>"Anomaly detection for extreme imbalance"</li> <li>Real-world context (fraud 1:1000, rare disease 1:10000)</li> </ul> </div> </details> <hr> <h3 id=embedding-techniques-google-meta-linkedin-interview-question>Embedding Techniques - Google, Meta, LinkedIn Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Embeddings</code>, <code>Word2Vec</code>, <code>Entity Embeddings</code>, <code>Representation Learning</code> | <strong>Asked by:</strong> Google, Meta, LinkedIn, Pinterest, Twitter</p> <p><strong>Question:</strong> Explain different embedding techniques. How do Word2Vec, GloVe, and contextual embeddings (BERT) differ? When would you use entity embeddings for categorical variables?</p> <details class=success> <summary>View Answer</summary> <p><strong>Embeddings</strong>: Dense vector representations that capture semantic relationships.</p> <p><strong>Word Embeddings:</strong></p> <p><strong>1. Word2Vec:</strong> - <strong>Skip-gram</strong>: Predict context from word - <strong>CBOW</strong>: Predict word from context - Static embeddings (one vector per word)</p> <p><strong>2. GloVe:</strong> - Global word co-occurrence statistics - Matrix factorization approach - Static embeddings</p> <p><strong>3. Contextual (BERT, GPT):</strong> - Different vectors for same word in different contexts - "bank" (river) vs "bank" (financial) - Captures polysemy</p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Contextual</th> <th>Training</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>Word2Vec</strong></td> <td>No</td> <td>Local context</td> <td>Fast, lightweight</td> </tr> <tr> <td><strong>GloVe</strong></td> <td>No</td> <td>Global stats</td> <td>Good for similarity</td> </tr> <tr> <td><strong>BERT</strong></td> <td>Yes</td> <td>Transformer</td> <td>State-of-the-art</td> </tr> </tbody> </table> <p><strong>Entity Embeddings:</strong> - For categorical variables (user_id, product_id) - Learn dense representations - Capture relationships (similar users, substitute products) - Better than one-hot encoding for high cardinality</p> <p><strong>Benefits:</strong> - Dimensionality reduction - Similarity computation - Transfer learning - Visualization</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Embeddings: dense vector representations"</li> <li>"Word2Vec: predict context or word"</li> <li>"BERT: contextual, different vectors per context"</li> <li>"Entity embeddings: for categorical features"</li> <li>"Cosine similarity for semantic search"</li> <li>"Pre-trained vs task-specific embeddings"</li> <li>Applications (search, recommendations, clustering)</li> </ul> </div> </details> <hr> <h3 id=bias-and-fairness-in-ml-google-microsoft-linkedin-interview-question>Bias and Fairness in ML - Google, Microsoft, LinkedIn Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>ML Ethics</code>, <code>Bias</code>, <code>Fairness</code>, <code>Responsible AI</code> | <strong>Asked by:</strong> Google, Microsoft, LinkedIn, Meta, IBM</p> <p><strong>Question:</strong> What are different types of bias in ML systems? How do you measure and mitigate bias? Explain fairness metrics.</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Bias:</strong></p> <p><strong>1. Data Bias:</strong> - <strong>Historical</strong>: Past discrimination in training data - <strong>Sampling</strong>: Non-representative samples - <strong>Label</strong>: Biased human annotations</p> <p><strong>2. Algorithmic Bias:</strong> - <strong>Representation</strong>: Model amplifies data bias - <strong>Aggregation</strong>: One model for heterogeneous groups - <strong>Evaluation</strong>: Biased metrics</p> <p><strong>3. Deployment Bias:</strong> - <strong>Feedback Loops</strong>: Predictions affect future data - <strong>User Interaction</strong>: Different groups use system differently</p> <p><strong>Fairness Definitions:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Description</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td><strong>Demographic Parity</strong></td> <td>Equal positive rate across groups</td> <td>Equal opportunity contexts</td> </tr> <tr> <td><strong>Equal Opportunity</strong></td> <td>Equal TPR across groups</td> <td>Lending, hiring</td> </tr> <tr> <td><strong>Equalized Odds</strong></td> <td>Equal TPR and FPR</td> <td>Criminal justice</td> </tr> <tr> <td><strong>Predictive Parity</strong></td> <td>Equal precision across groups</td> <td>Resource allocation</td> </tr> </tbody> </table> <p><strong>Note</strong>: Cannot satisfy all fairness criteria simultaneously (impossibility theorems).</p> <p><strong>Mitigation Strategies:</strong></p> <p><strong>Pre-processing:</strong> - Collect representative data - Balance datasets - Remove sensitive attributes (with care)</p> <p><strong>In-processing:</strong> - Fairness constraints during training - Adversarial debiasing - Fair representation learning</p> <p><strong>Post-processing:</strong> - Adjust decision thresholds per group - Calibration - Reject option</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Bias types: data, algorithmic, deployment"</li> <li>"Fairness metrics: demographic parity, equal opportunity"</li> <li>"Trade-offs: fairness vs accuracy"</li> <li>"Can't satisfy all fairness definitions"</li> <li>"Mitigation: pre/in/post-processing"</li> <li>"Feedback loops amplify bias"</li> <li>"Transparency and explainability"</li> <li>Real-world consequences</li> </ul> </div> </details> <hr> <h3 id=multi-task-learning-google-deepmind-meta-ai-interview-question>Multi-Task Learning - Google DeepMind, Meta AI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Multi-Task Learning</code>, <code>Transfer Learning</code>, <code>Hard/Soft Sharing</code> | <strong>Asked by:</strong> Google DeepMind, Meta AI, Microsoft Research, NVIDIA</p> <p><strong>Question:</strong> What is multi-task learning? Explain hard vs soft parameter sharing. When does MTL help vs hurt?</p> <details class=success> <summary>View Answer</summary> <p><strong>Multi-Task Learning (MTL)</strong>: Train one model on multiple related tasks simultaneously.</p> <p><strong>Benefits:</strong> - <strong>Regularization</strong>: Shared representations prevent overfitting - <strong>Data Efficiency</strong>: Learn from multiple signals - <strong>Transfer</strong>: Knowledge transfer across tasks - <strong>Faster Learning</strong>: Auxiliary tasks help main task</p> <p><strong>Parameter Sharing:</strong></p> <p><strong>Hard Sharing:</strong> - Shared hidden layers - Task-specific output layers - Most common approach</p> <p><strong>Soft Sharing:</strong> - Separate models per task - Encourage similarity via regularization - More flexible but complex</p> <p><strong>When MTL Helps:</strong> - Related tasks (sentiment, topic, intent) - Limited data per task - Shared underlying structure - Auxiliary tasks provide useful signal</p> <p><strong>When MTL Hurts:</strong> - Unrelated tasks (negative transfer) - One task dominates training - Tasks require different representations - Task conflicts</p> <p><strong>Challenges:</strong> - <strong>Task Balancing</strong>: Equal contribution to loss - <strong>Negative Transfer</strong>: Tasks hurt each other - <strong>Architecture Design</strong>: How much to share? - <strong>Optimization</strong>: Different convergence rates</p> <p><strong>Solutions:</strong> - <strong>Task Weighting</strong>: Learn task weights - <strong>Gradients</strong>: GradNorm, PCGrad (project conflicting gradients) - <strong>Architecture Search</strong>: Learn sharing structure - <strong>Uncertainty Weighting</strong>: Weight by task uncertainty</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"MTL: train multiple tasks together"</li> <li>"Hard sharing: shared layers"</li> <li>"Soft sharing: separate models, regularized"</li> <li>"Benefits: regularization, data efficiency"</li> <li>"Negative transfer: tasks hurt each other"</li> <li>"Task weighting: balance contributions"</li> <li>"Auxiliary tasks can improve main task"</li> <li>Applications (NLP: NER + POS + parsing)</li> </ul> </div> </details> <hr> <h3 id=adversarial-robustness-google-openai-deepmind-interview-question>Adversarial Robustness - Google, OpenAI, DeepMind Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Adversarial Examples</code>, <code>Robustness</code>, <code>Security</code>, <code>Adversarial Training</code> | <strong>Asked by:</strong> Google, OpenAI, DeepMind, Microsoft, Meta</p> <p><strong>Question:</strong> What are adversarial examples? How do you make models robust to adversarial attacks? Explain adversarial training.</p> <details class=success> <summary>View Answer</summary> <p><strong>Adversarial Examples</strong>: Inputs with small perturbations that cause misclassification.</p> <div class=arithmatex>\[x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(x, y))\]</div> <p><strong>Types of Attacks:</strong></p> <p><strong>White-Box</strong> (Full model access): - <strong>FGSM</strong>: Fast Gradient Sign Method - <strong>PGD</strong>: Projected Gradient Descent - <strong>C&amp;W</strong>: Carlini &amp; Wagner attack</p> <p><strong>Black-Box</strong> (No model access): - <strong>Transfer attacks</strong>: Use substitute model - <strong>Query-based</strong>: Test inputs iteratively</p> <p><strong>Defense Strategies:</strong></p> <p><strong>1. Adversarial Training:</strong> - Include adversarial examples in training - Most effective but expensive</p> <p><strong>2. Defensive Distillation:</strong> - Train student on soft labels from teacher - Smooths decision boundaries</p> <p><strong>3. Input Transformations:</strong> - Compression, denoising - Can be circumvented</p> <p><strong>4. Detection:</strong> - Identify adversarial inputs - Reject or handle specially</p> <p><strong>5. Certified Defense:</strong> - Mathematical guarantees of robustness - Randomized smoothing</p> <p><strong>Trade-offs:</strong> - Robust accuracy vs standard accuracy - Computational cost - Robustness vs interpretability</p> <p><strong>Applications:</strong> - <strong>Autonomous Vehicles</strong>: Stop sign attacks - <strong>Face Recognition</strong>: Evade detection - <strong>Malware Detection</strong>: Adversarial malware - <strong>Medical Imaging</strong>: Misdiagnosis</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Adversarial examples: imperceptible perturbations"</li> <li>"FGSM: gradient-based attack"</li> <li>"Adversarial training: train on adversarial examples"</li> <li>"Robust accuracy vs standard accuracy trade-off"</li> <li>"White-box vs black-box attacks"</li> <li>"Certified robustness: mathematical guarantees"</li> <li>Security implications in production</li> </ul> </div> </details> <hr> <h3 id=self-supervised-learning-meta-ai-google-research-interview-question>Self-Supervised Learning - Meta AI, Google Research Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Self-Supervised</code>, <code>Contrastive Learning</code>, <code>Pre-training</code>, <code>SimCLR</code> | <strong>Asked by:</strong> Meta AI, Google Research, DeepMind, OpenAI</p> <p><strong>Question:</strong> What is self-supervised learning? Explain contrastive learning methods like SimCLR. How does it differ from supervised and unsupervised learning?</p> <details class=success> <summary>View Answer</summary> <p><strong>Self-Supervised Learning</strong>: Learn representations from unlabeled data by creating pretext tasks.</p> <p><strong>Key Idea</strong>: Generate supervision signal from data itself.</p> <p><strong>Approaches:</strong></p> <p><strong>1. Contrastive Learning:</strong> - Pull similar samples together - Push dissimilar samples apart - <strong>SimCLR, MoCo, CLIP</strong></p> <p><strong>2. Predictive:</strong> - Predict missing parts - <strong>BERT (masked LM), GPT (next token)</strong></p> <p><strong>3. Generative:</strong> - Reconstruct input - <strong>Autoencoders, MAE</strong></p> <p><strong>SimCLR (Simple Contrastive Learning):</strong></p> <ol> <li>Augment same image twice (positive pair)</li> <li>Encode to embeddings</li> <li>Maximize agreement between positive pairs</li> <li>Minimize agreement with negative pairs</li> </ol> <p><strong>Loss (InfoNCE):</strong></p> <div class=arithmatex>\[\mathcal{L} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}\]</div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Paradigm</th> <th>Labels</th> <th>Pre-training</th> <th>Fine-tuning</th> </tr> </thead> <tbody> <tr> <td><strong>Supervised</strong></td> <td>Required</td> <td>With labels</td> <td>Optional</td> </tr> <tr> <td><strong>Unsupervised</strong></td> <td>None</td> <td>Clustering, PCA</td> <td>N/A</td> </tr> <tr> <td><strong>Self-Supervised</strong></td> <td>None</td> <td>Pretext tasks</td> <td>On downstream</td> </tr> </tbody> </table> <p><strong>Benefits:</strong> - Leverage unlabeled data (abundant) - Better transfer learning - Robust representations - Less label dependence</p> <p><strong>Applications:</strong> - <strong>Computer Vision</strong>: ImageNet pre-training - <strong>NLP</strong>: BERT, GPT pre-training - <strong>Multimodal</strong>: CLIP (image-text) - <strong>Speech</strong>: wav2vec, HuBERT</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Self-supervised: create tasks from data"</li> <li>"Contrastive: similar together, dissimilar apart"</li> <li>"SimCLR: augmentation + contrastive loss"</li> <li>"Leverage massive unlabeled data"</li> <li>"Pre-train then fine-tune paradigm"</li> <li>vs. "Supervised: needs labels"</li> <li>"BERT: masked language modeling"</li> <li>"CLIP: align images and text"</li> </ul> </div> </details> <hr> <h3 id=few-shot-learning-meta-ai-deepmind-openai-interview-question>Few-Shot Learning - Meta AI, DeepMind, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Few-Shot</code>, <code>Meta-Learning</code>, <code>Prototypical Networks</code>, <code>In-Context Learning</code> | <strong>Asked by:</strong> Meta AI, DeepMind, OpenAI, Google Research</p> <p><strong>Question:</strong> What is few-shot learning? Compare different approaches: metric learning, meta-learning, and large language model in-context learning.</p> <details class=success> <summary>View Answer</summary> <p><strong>Few-Shot Learning</strong>: Learn new concepts from very few examples (1-5).</p> <p><strong>Problem</strong>: Traditional ML needs 100s-1000s of examples per class.</p> <p><strong>Approaches:</strong></p> <p><strong>1. Metric Learning:</strong> - Learn similarity metric - Classify based on distance to prototypes - <strong>Siamese Networks, Prototypical Networks</strong></p> <p><strong>2. Meta-Learning:</strong> - Learn to learn across tasks - <strong>MAML, Reptile</strong> - Good initialization for fast adaptation</p> <p><strong>3. Data Augmentation:</strong> - Generate synthetic examples - Hallucination from base classes</p> <p><strong>4. LLM In-Context Learning:</strong> - Provide examples in prompt - No parameter updates - <strong>GPT-3, GPT-4</strong></p> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Training</th> <th>Adaptation</th> <th>Examples Needed</th> </tr> </thead> <tbody> <tr> <td><strong>Prototypical</strong></td> <td>Episode-based</td> <td>Compute prototypes</td> <td>1-5 per class</td> </tr> <tr> <td><strong>MAML</strong></td> <td>Meta-train</td> <td>Few gradient steps</td> <td>5-10 per task</td> </tr> <tr> <td><strong>In-Context</strong></td> <td>Pre-training</td> <td>Add to prompt</td> <td>0-5 per class</td> </tr> </tbody> </table> <p><strong>Evaluation:</strong> - <strong>N-way K-shot</strong>: N classes, K examples each - <strong>Support Set</strong>: Training examples - <strong>Query Set</strong>: Test examples</p> <p><strong>Applications:</strong> - <strong>Drug Discovery</strong>: Predict properties of new molecules - <strong>Robotics</strong>: Adapt to new objects quickly - <strong>Personalization</strong>: User-specific models - <strong>Rare Disease</strong>: Classify with few patients</p> <p><strong>Interview Insights:</strong></p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <ul> <li>"Few-shot: learn from 1-5 examples"</li> <li>"Prototypical: classify by distance to prototypes"</li> <li>"MAML: meta-learn good initialization"</li> <li>"In-context: LLMs with prompt examples"</li> <li>"N-way K-shot evaluation"</li> <li>"Support set vs query set"</li> <li>"Transfer from base classes"</li> <li>Applications in low-data scenarios</li> </ul> </div> </details> <hr> <h2 id=questions-asked-in-slack-interview>Questions asked in Slack interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Feature Engineering </li> <li>Transfer Learning </li> </ul> <h2 id=questions-asked-in-airbnb-interview>Questions asked in Airbnb interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Hyperparameter Tuning </li> <li>Transfer Learning </li> <li>Model Interpretability: SHAP and LIME </li> </ul> <hr> <p><em>Note:</em> The practice links are curated from reputable sources such as Machine Learning Mastery, Towards Data Science, Analytics Vidhya, and Scikit-learn. You can update/contribute to these lists or add new ones as more resources become available.</p> <hr> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2020 - <script>document.write(/\d{4}/.exec(Date())[0])</script> ‚Ä¢ <strong>Kuldeep Singh Sidhu</strong> ‚Ä¢ <u><a href=https://choosealicense.com/licenses/agpl-3.0/ target=‚Äù_blank‚Äù>License</a></u> ‚Ä¢ <u><a href=/privacy>Privacy Policy</a></u> ‚Ä¢ <u><a href=/contact>Contact</a></u> ‚Ä¢ </div> </div> <div class=md-social> <a href=https://github.com/singhsidhukuldeep/ target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.01 8.01 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27s-1.36.09-2 .27c-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8"/></svg> </a> <a href=https://linkedin.com/in/singhsidhukuldeep target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://twitter.com/kuldeep_s_s target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> <a href=https://stackoverflow.com/u/7182350/ target=_blank rel=noopener title=stackoverflow.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 300zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-200v39.7h200zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg> </a> <a href=https://huggingface.co/singhsidhukuldeep target=_blank rel=noopener title=huggingface.co class=md-social__link> <svg width=500 height=463 viewbox="0 0 500 463" fill=none xmlns=http://www.w3.org/2000/svg> <path fill=white d="M496.592 369.699C500.563 381.093 499.61 393.227 494.315 403.778C490.503 411.48 485.05 417.441 478.379 422.769C470.331 429.099 460.324 434.48 448.253 439.65C433.852 445.77 416.274 451.52 408.226 453.63C387.63 458.958 367.829 462.334 347.762 462.493C319.066 462.756 294.34 456.004 276.762 438.753C267.656 439.861 258.443 440.494 249.178 440.494C240.389 440.494 231.706 439.967 223.076 438.912C205.445 456.057 180.825 462.756 152.234 462.493C132.168 462.334 112.366 458.958 91.7177 453.63C83.7229 451.52 66.145 445.77 51.7439 439.65C39.6723 434.48 29.6656 429.099 21.6708 422.769C14.9467 417.441 9.49334 411.48 5.68127 403.778C0.439661 393.227 -0.566304 381.093 3.45755 369.699C-0.248631 360.994 -1.20165 351.024 1.71035 339.998C3.03399 334.987 5.20476 330.344 7.95792 326.229C7.37552 324.067 6.89901 321.851 6.58134 319.424C4.56941 304.97 9.59923 291.781 19.0765 281.547C23.7357 276.43 28.7655 272.895 34.0071 270.627C30.1421 254.273 28.1302 237.445 28.1302 220.247C28.1302 98.5969 127.085 0 249.178 0C291.111 0 330.343 11.6058 363.805 31.8633C369.84 35.5561 375.77 39.5126 381.436 43.7329C384.242 45.8431 387.048 48.006 389.748 50.2744C392.501 52.49 395.201 54.8112 397.796 57.1851C405.632 64.3069 412.991 71.9562 419.715 80.133C421.992 82.8235 424.163 85.6194 426.28 88.4681C430.569 94.1128 434.54 99.9685 438.193 106.035C443.752 115.109 448.623 124.604 452.859 134.469C455.665 141.064 458.101 147.816 460.271 154.727C463.501 165.067 465.99 175.723 467.684 186.696C468.213 190.336 468.69 194.028 469.06 197.721C469.802 205.107 470.225 212.598 470.225 220.247C470.225 237.234 468.213 253.904 464.454 269.994C470.278 272.262 475.784 275.955 480.92 281.547C490.397 291.781 495.427 305.022 493.415 319.477C493.098 321.851 492.621 324.067 492.039 326.229C494.792 330.344 496.963 334.987 498.286 339.998C501.198 351.024 500.245 360.994 496.592 369.699Z"/> <path fill=black d="M433.839 221.75C433.839 120.838 351.531 39.0323 250 39.0323C148.469 39.0323 66.1613 120.838 66.1613 221.75C66.1613 322.662 148.469 404.468 250 404.468C351.531 404.468 433.839 322.662 433.839 221.75ZM45 221.75C45 109.222 136.782 18 250 18C363.218 18 455 109.222 455 221.75C455 334.278 363.218 425.5 250 425.5C136.782 425.5 45 334.278 45 221.75Z"/> <path fill=white d="M250 405.5C352.173 405.5 435 323.232 435 221.75C435 120.268 352.173 38 250 38C147.827 38 65 120.268 65 221.75C65 323.232 147.827 405.5 250 405.5Z"/> <path fill=white d="M202.198 404.174C216.789 383.118 215.755 367.316 195.735 347.627C175.715 327.943 164.062 299.145 164.062 299.145C164.062 299.145 159.709 282.419 149.794 283.958C139.88 285.497 132.6 310.492 153.368 325.783C174.135 341.069 149.232 351.456 141.242 337.099C133.252 322.741 111.435 285.831 100.121 278.772C88.8117 271.713 80.8483 275.668 83.5151 290.218C86.182 304.769 133.48 340.036 128.878 347.668C124.276 355.296 108.058 338.7 108.058 338.7C108.058 338.7 57.3079 293.255 46.2587 305.097C35.2096 316.94 54.641 326.863 82.3328 343.359C110.03 359.85 112.177 364.206 108.248 370.446C104.314 376.685 43.1836 325.971 37.4417 347.47C31.705 368.969 99.8291 375.209 95.6247 390.051C91.4203 404.899 47.6372 361.958 38.6823 378.689C29.7221 395.425 100.465 415.088 101.038 415.234C123.889 421.067 181.924 433.426 202.198 404.174Z"/> <path fill=black d="M90.9935 255C82.4744 255 74.8603 258.477 69.551 264.784C66.2675 268.69 62.8367 274.986 62.5578 284.414C58.985 283.394 55.5489 282.824 52.3391 282.824C44.183 282.824 36.8163 285.93 31.6069 291.573C24.9137 298.815 21.9407 307.715 23.2351 316.62C23.8508 320.861 25.2768 324.663 27.4079 328.182C22.9142 331.795 19.6044 336.826 18.0047 342.876C16.7524 347.619 15.4685 357.497 22.1722 367.673C21.746 368.337 21.3461 369.027 20.9725 369.733C16.9418 377.336 16.684 385.927 20.2411 393.928C25.6346 406.054 39.0368 415.608 65.0625 425.863C81.2536 432.242 96.0661 436.321 96.1976 436.357C117.603 441.874 136.962 444.677 153.721 444.677C184.525 444.677 206.578 435.301 219.27 416.811C239.697 387.036 236.776 359.803 210.346 333.552C195.717 319.026 185.993 297.607 183.967 292.906C179.884 278.986 169.086 263.513 151.138 263.513H151.133C149.622 263.513 148.096 263.633 146.592 263.869C138.73 265.097 131.858 269.595 126.949 276.361C121.65 269.814 116.504 264.606 111.847 261.667C104.827 257.243 97.813 255 90.9935 255ZM90.9935 275.917C93.6771 275.917 96.9553 277.051 100.57 279.331C111.794 286.406 133.452 323.403 141.382 337.793C144.039 342.614 148.581 344.654 152.669 344.654C160.783 344.654 167.118 336.638 153.411 326.451C132.8 311.124 140.03 286.072 149.87 284.529C150.301 284.461 150.727 284.43 151.138 284.43C160.083 284.43 164.03 299.751 164.03 299.751C164.03 299.751 175.595 328.616 195.465 348.346C215.334 368.08 216.36 383.919 201.879 405.024C192.002 419.415 173.096 421.292 153.721 421.292C133.626 421.292 112.99 417.772 101.445 414.796C100.877 414.65 30.7019 396.255 39.5946 379.48C41.089 376.661 43.5516 375.532 46.6509 375.532C59.1744 375.532 81.9535 394.054 91.746 394.054C93.935 394.054 95.5662 392.371 96.1976 390.112C100.555 374.522 32.6646 369.738 38.3633 348.189C39.3683 344.377 42.094 342.829 45.9248 342.834C62.4737 342.834 99.6021 371.756 107.385 371.756C107.979 371.756 108.405 371.584 108.637 371.218C112.536 364.964 110.74 359.872 83.257 343.343C55.7738 326.808 36.1428 317.588 47.114 305.718C48.3768 304.347 50.1659 303.741 52.3391 303.741C69.0248 303.746 108.447 339.398 108.447 339.398C108.447 339.398 119.087 350.395 125.523 350.395C127.001 350.395 128.259 349.815 129.111 348.382C133.673 340.737 86.7366 305.388 84.0898 290.804C82.2955 280.921 85.3474 275.917 90.9935 275.917Z"/> <path fill=white d="M296.9 404.174C282.31 383.118 283.343 367.316 303.363 347.627C323.383 327.943 335.037 299.145 335.037 299.145C335.037 299.145 339.39 282.419 349.304 283.958C359.219 285.497 366.498 310.492 345.731 325.783C324.963 341.069 349.866 351.456 357.856 337.099C365.846 322.741 387.663 285.831 398.978 278.772C410.287 271.713 418.25 275.668 415.583 290.218C412.916 304.769 365.618 340.036 370.22 347.668C374.822 355.296 391.041 338.7 391.041 338.7C391.041 338.7 441.791 293.255 452.84 305.097C463.889 316.94 444.457 326.863 416.766 343.359C389.068 359.85 386.921 364.206 390.85 370.446C394.784 376.685 455.915 325.971 461.657 347.47C467.393 368.969 399.269 375.209 403.474 390.051C407.678 404.899 451.461 361.958 460.416 378.689C469.376 395.425 398.633 415.088 398.06 415.234C375.209 421.067 317.175 433.426 296.9 404.174Z"/> <path fill=black d="M408.105 255C416.624 255 424.238 258.477 429.547 264.784C432.831 268.69 436.262 274.986 436.541 284.414C440.113 283.394 443.549 282.824 446.759 282.824C454.915 282.824 462.282 285.93 467.491 291.573C474.185 298.815 477.158 307.715 475.863 316.62C475.248 320.861 473.822 324.663 471.69 328.182C476.184 331.795 479.494 336.826 481.094 342.876C482.346 347.619 483.63 357.497 476.926 367.673C477.352 368.337 477.752 369.027 478.126 369.733C482.157 377.336 482.414 385.927 478.857 393.928C473.464 406.054 460.062 415.608 434.036 425.863C417.845 432.242 403.032 436.321 402.901 436.357C381.495 441.874 362.136 444.677 345.377 444.677C314.573 444.677 292.52 435.301 279.829 416.811C259.402 387.036 262.322 359.803 288.753 333.552C303.381 319.026 313.105 297.607 315.131 292.906C319.214 278.986 330.012 263.513 347.961 263.513H347.966C349.476 263.513 351.002 263.633 352.507 263.869C360.368 265.097 367.24 269.595 372.15 276.361C377.449 269.814 382.595 264.606 387.252 261.667C394.271 257.243 401.285 255 408.105 255ZM408.105 275.917C405.421 275.917 402.143 277.051 398.528 279.331C387.304 286.406 365.646 323.403 357.716 337.793C355.059 342.614 350.518 344.654 346.429 344.654C338.315 344.654 331.98 336.638 345.687 326.451C366.299 311.124 359.069 286.072 349.229 284.529C348.797 284.461 348.371 284.43 347.961 284.43C339.015 284.43 335.069 299.751 335.069 299.751C335.069 299.751 323.503 328.616 303.634 348.346C283.764 368.08 282.738 383.919 297.219 405.024C307.096 419.415 326.002 421.292 345.377 421.292C365.472 421.292 386.108 417.772 397.653 414.796C398.221 414.65 468.397 396.255 459.504 379.48C458.009 376.661 455.547 375.532 452.447 375.532C439.924 375.532 417.145 394.054 407.352 394.054C405.163 394.054 403.532 392.371 402.901 390.112C398.543 374.522 466.434 369.738 460.735 348.189C459.73 344.377 457.004 342.829 453.174 342.834C436.625 342.834 399.496 371.756 391.714 371.756C391.119 371.756 390.693 371.584 390.461 371.218C386.562 364.964 388.358 359.872 415.841 343.343C443.325 326.808 462.956 317.588 451.984 305.718C450.722 304.347 448.932 303.741 446.759 303.741C430.074 303.746 390.651 339.398 390.651 339.398C390.651 339.398 380.011 350.395 373.576 350.395C372.097 350.395 370.84 349.815 369.987 348.382C365.425 340.737 412.362 305.388 415.009 290.804C416.803 280.921 413.751 275.917 408.105 275.917Z"/> <path fill=#0E1116 d="M319.277 228.901C319.277 205.236 288.585 241.304 250.637 241.465C212.692 241.306 182 205.238 182 228.901C182 244.591 189.507 270.109 209.669 285.591C213.681 271.787 235.726 260.729 238.877 262.317C243.364 264.578 243.112 270.844 250.637 276.365C258.163 270.844 257.911 264.58 262.398 262.317C265.551 260.729 287.594 271.787 291.605 285.591C311.767 270.109 319.275 244.591 319.275 228.903L319.277 228.901Z"/> <path fill=#FF323D d="M262.4 262.315C257.913 264.576 258.165 270.842 250.639 276.363C243.114 270.842 243.366 264.578 238.879 262.315C235.726 260.727 213.683 271.785 209.672 285.589C219.866 293.417 233.297 298.678 250.627 298.806C250.631 298.806 250.635 298.806 250.641 298.806C250.646 298.806 250.65 298.806 250.656 298.806C267.986 298.68 281.417 293.417 291.611 285.589C287.6 271.785 265.555 260.727 262.404 262.315H262.4Z"/> <path fill=black d="M373 196C382.389 196 390 188.389 390 179C390 169.611 382.389 162 373 162C363.611 162 356 169.611 356 179C356 188.389 363.611 196 373 196Z"/> <path fill=black d="M128 196C137.389 196 145 188.389 145 179C145 169.611 137.389 162 128 162C118.611 162 111 169.611 111 179C111 188.389 118.611 196 128 196Z"/> <path fill=#0E1116 d="M313.06 171.596C319.796 173.968 322.476 187.779 329.281 184.171C342.167 177.337 347.06 161.377 340.208 148.524C333.356 135.671 317.354 130.792 304.467 137.626C291.58 144.46 286.688 160.419 293.54 173.272C296.774 179.339 307.039 169.475 313.06 171.596Z"/> <path fill=#0E1116 d="M188.554 171.596C181.818 173.968 179.138 187.779 172.334 184.171C159.447 177.337 154.555 161.377 161.407 148.524C168.259 135.671 184.26 130.792 197.147 137.626C210.034 144.46 214.926 160.419 208.074 173.272C204.84 179.339 194.575 169.475 188.554 171.596Z"/> </svg> </a> <a href=http://kuldeepsinghsidhu.com target=_blank rel=noopener title=kuldeepsinghsidhu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0M5.78 8.75a9.64 9.64 0 0 0 1.363 4.177q.383.64.857 1.215c.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a10 10 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.51 6.51 0 0 0 4.666 5.5q-.184-.271-.352-.552c-.715-1.192-1.437-2.874-1.581-4.948m-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948q.18-.295.353-.552a6.51 6.51 0 0 0-4.666 5.5m10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948q-.18.296-.353.552a6.51 6.51 0 0 0 4.666-5.5Zm2.733-1.5a6.51 6.51 0 0 0-4.666-5.5q.184.272.353.552c.714 1.192 1.436 2.874 1.58 4.948Z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "search.highlight", "search.share", "search.suggest", "content.tooltips", "navigation.instant.progress", "navigation.path", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../javascripts/xfile.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>
<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A curated list of 100+ Machine Learning interview questions for cracking data science interviews at top tech companies"><meta name=author content="Kuldeep Singh Sidhu"><link href=../data-structures-algorithms/ rel=prev><link href=../System-design/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.50"><title>Machine Learning Interview Questions - Data Science Interview preparation</title><link rel=stylesheet href=../../assets/stylesheets/main.a40c8224.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-EVGNTG49J7"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-EVGNTG49J7",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-EVGNTG49J7",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-interview-questions class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <!-- Add announcement here, including arbitrary HTML --> üëÄ This project is in early stages of development. <strong> ü§ó Please <a href=/Contribute>contribute content</a> if possible! ü§ù</strong><br> <small>ü´µ You can <b> <a href=/Contribute>SUBMIT</a></b> simple text/markdown content, I will format it! üôå</small> <meta name=google-adsense-account content=ca-pub-4988388949365963> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4988388949365963" crossorigin=anonymous></script> </div> </aside> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Data Science Interview preparation" class="md-header__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Science Interview preparation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Machine Learning Interview Questions </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Data Science Interview preparation" class="md-nav__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> Data Science Interview preparation </label> <div class=md-nav__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> üè° Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> üë®üèø‚Äçüè´ Interview Questions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> üë®üèø‚Äçüè´ Interview Questions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../flashcards/ class=md-nav__link> <span class=md-ellipsis> üìá Flashcards </span> </a> </li> <li class=md-nav__item> <a href=../data-structures-algorithms/ class=md-nav__link> <span class=md-ellipsis> DSA (Data Structures & Algorithms) </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Machine Learning </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Machine Learning </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#premium-interview-questions class=md-nav__link> <span class=md-ellipsis> Premium Interview Questions </span> </a> <nav class=md-nav aria-label="Premium Interview Questions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-does-gradient-descent-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Does Gradient Descent Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-precision-recall-and-f1-score-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question class=md-nav__link> <span class=md-ellipsis> What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-neural-networks-and-backpropagation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Neural Networks and Backpropagation - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dropout-and-why-does-it-work-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dropout and Why Does It Work? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-transfer-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Transfer Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-roc-curve-and-auc-score-microsoft-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-k-means-clustering-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain K-Means Clustering - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-engineering-give-examples-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-missing-data-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-evaluate-regression-models-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Evaluate Regression Models? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-curse-of-dimensionality-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Curse of Dimensionality? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-categorical-features-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Categorical Features? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-calibration-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Calibration? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-semi-supervised-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Semi-Supervised Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-active-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Active Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-automl-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is AutoML? - Amazon, Google Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#quick-reference-100-interview-questions class=md-nav__link> <span class=md-ellipsis> Quick Reference: 100 Interview Questions </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-google-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Google interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-facebook-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Facebook interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-amazon-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Amazon interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-microsoft-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Microsoft interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-uber-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Uber interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-swiggy-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Swiggy interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-flipkart-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Flipkart interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-ola-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Ola interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-paytm-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Paytm interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-oyo-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in OYO interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-whatsapp-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in WhatsApp interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-slack-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Slack interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-airbnb-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Airbnb interview </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../System-design/ class=md-nav__link> <span class=md-ellipsis> System Design </span> </a> </li> <li class=md-nav__item> <a href=../Natural-Language-Processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing (NLP) </span> </a> </li> <li class=md-nav__item> <a href=../Probability/ class=md-nav__link> <span class=md-ellipsis> Probability </span> </a> </li> <li class=md-nav__item> <a href=../AB-testing/ class=md-nav__link> <span class=md-ellipsis> A/B Testing </span> </a> </li> <li class=md-nav__item> <a href=../SQL-Interview-Questions/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../Scikit-Learn/ class=md-nav__link> <span class=md-ellipsis> Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=../LangChain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../LangGraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> <li class=md-nav__item> <a href=../Interview-Question-Resources/ class=md-nav__link> <span class=md-ellipsis> Interview Question Resources </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> üìù Cheat Sheets </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> üìù Cheat Sheets </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Cheat-Sheets/Django/ class=md-nav__link> <span class=md-ellipsis> Django </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Flask/ class=md-nav__link> <span class=md-ellipsis> Flask </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Hypothesis-Tests/ class=md-nav__link> <span class=md-ellipsis> Hypothesis Tests </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Keras/ class=md-nav__link> <span class=md-ellipsis> Keras </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PySpark/ class=md-nav__link> <span class=md-ellipsis> PySpark </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PyTorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/RegEx/ class=md-nav__link> <span class=md-ellipsis> Regular Expressions (RegEx) </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Sk-learn/ class=md-nav__link> <span class=md-ellipsis> Scikit Learn </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/SQL/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/tensorflow/ class=md-nav__link> <span class=md-ellipsis> TensorFlow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> ‚Äçüéì ML Topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> ‚Äçüéì ML Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Machine-Learning/ARIMA/ class=md-nav__link> <span class=md-ellipsis> ARIMA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Activation%20functions/ class=md-nav__link> <span class=md-ellipsis> Activation functions </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Collaborative%20Filtering/ class=md-nav__link> <span class=md-ellipsis> Collaborative Filtering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Confusion%20Matrix/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/DBSCAN/ class=md-nav__link> <span class=md-ellipsis> DBSCAN </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Decision%20Trees/ class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Gradient%20Boosting/ class=md-nav__link> <span class=md-ellipsis> Gradient Boosting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/K-means%20clustering/ class=md-nav__link> <span class=md-ellipsis> K-means clustering </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Linear%20Regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Logistic%20Regression/ class=md-nav__link> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Loss%20Function%20MAE%2C%20RMSE/ class=md-nav__link> <span class=md-ellipsis> Loss Function MAE, RMSE </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Neural%20Networks/ class=md-nav__link> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normal%20Distribution/ class=md-nav__link> <span class=md-ellipsis> Normal Distribution </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Normalization%20Regularisation/ class=md-nav__link> <span class=md-ellipsis> Normalization Regularisation </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Overfitting%2C%20Underfitting/ class=md-nav__link> <span class=md-ellipsis> Overfitting, Underfitting </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/PCA/ class=md-nav__link> <span class=md-ellipsis> PCA </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Random%20Forest/ class=md-nav__link> <span class=md-ellipsis> Random Forest </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Support%20Vector%20Machines/ class=md-nav__link> <span class=md-ellipsis> Support Vector Machines </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/Unbalanced%2C%20Skewed%20data/ class=md-nav__link> <span class=md-ellipsis> Unbalanced, Skewed data </span> </a> </li> <li class=md-nav__item> <a href=../../Machine-Learning/kNN/ class=md-nav__link> <span class=md-ellipsis> kNN </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> üë®üèæ‚Äçüíª Online Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> üë®üèæ‚Äçüíª Online Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Online-Material/Online-Material-for-Learning/ class=md-nav__link> <span class=md-ellipsis> Online Study Material </span> </a> </li> <li class=md-nav__item> <a href=../../Online-Material/popular-resources/ class=md-nav__link> <span class=md-ellipsis> Popular Blogs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../projects/ class=md-nav__link> <span class=md-ellipsis> üì≥ Projects </span> </a> </li> <li class=md-nav__item> <a href=../../Contribute/ class=md-nav__link> <span class=md-ellipsis> ü§ù Contribute </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#premium-interview-questions class=md-nav__link> <span class=md-ellipsis> Premium Interview Questions </span> </a> <nav class=md-nav aria-label="Premium Interview Questions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-does-gradient-descent-work-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Does Gradient Descent Work? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-precision-recall-and-f1-score-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question class=md-nav__link> <span class=md-ellipsis> What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-neural-networks-and-backpropagation-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Neural Networks and Backpropagation - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dropout-and-why-does-it-work-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dropout and Why Does It Work? - Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-transfer-learning-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Transfer Learning? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-roc-curve-and-auc-score-microsoft-netflix-interview-question class=md-nav__link> <span class=md-ellipsis> Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-k-means-clustering-amazon-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> Explain K-Means Clustering - Amazon, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question class=md-nav__link> <span class=md-ellipsis> What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-engineering-give-examples-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-missing-data-amazon-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-evaluate-regression-models-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Evaluate Regression Models? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-curse-of-dimensionality-google-amazon-interview-question class=md-nav__link> <span class=md-ellipsis> What is the Curse of Dimensionality? - Google, Amazon Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#how-do-you-handle-categorical-features-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> How Do You Handle Categorical Features? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-model-calibration-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Model Calibration? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-online-learning-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is Online Learning? - Amazon, Google Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-semi-supervised-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Semi-Supervised Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-active-learning-google-meta-interview-question class=md-nav__link> <span class=md-ellipsis> What is Active Learning? - Google, Meta Interview Question </span> </a> </li> <li class=md-nav__item> <a href=#what-is-automl-amazon-google-interview-question class=md-nav__link> <span class=md-ellipsis> What is AutoML? - Amazon, Google Interview Question </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#quick-reference-100-interview-questions class=md-nav__link> <span class=md-ellipsis> Quick Reference: 100 Interview Questions </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-google-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Google interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-facebook-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Facebook interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-amazon-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Amazon interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-microsoft-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Microsoft interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-uber-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Uber interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-swiggy-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Swiggy interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-flipkart-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Flipkart interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-ola-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Ola interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-paytm-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Paytm interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-oyo-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in OYO interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-whatsapp-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in WhatsApp interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-slack-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Slack interview </span> </a> </li> <li class=md-nav__item> <a href=#questions-asked-in-airbnb-interview class=md-nav__link> <span class=md-ellipsis> Questions asked in Airbnb interview </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/edit/master/docs/Interview-Questions/Machine-Learning.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/raw/master/docs/Interview-Questions/Machine-Learning.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=machine-learning-interview-questions>Machine Learning Interview Questions</h1> <p>This comprehensive guide contains <strong>100+ Machine Learning interview questions</strong> commonly asked at top tech companies like Google, Amazon, Meta, Microsoft, and Netflix. Each premium question includes detailed explanations, code examples, and interviewer insights to help you ace your ML interviews.</p> <hr> <h2 id=premium-interview-questions>Premium Interview Questions</h2> <p>Master these frequently asked ML questions with detailed explanations, code examples, and insights into what interviewers really look for.</p> <hr> <h3 id=what-is-the-bias-variance-tradeoff-in-machine-learning-google-amazon-interview-question>What is the Bias-Variance Tradeoff in Machine Learning? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Evaluation</code>, <code>Generalization</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Concept:</strong></p> <p>The bias-variance tradeoff is a fundamental concept that describes the tension between two sources of error in machine learning models:</p> <ul> <li><strong>Bias</strong>: Error from overly simplistic assumptions. High bias ‚Üí underfitting.</li> <li><strong>Variance</strong>: Error from sensitivity to training data fluctuations. High variance ‚Üí overfitting.</li> </ul> <p><strong>Mathematical Formulation:</strong></p> <p>For a model's expected prediction error:</p> <div class=arithmatex>\[\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]</div> <div class=arithmatex>\[E[(y - \hat{f}(x))^2] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] + \sigma^2\]</div> <p><strong>Visual Understanding:</strong></p> <table> <thead> <tr> <th>Model Complexity</th> <th>Bias</th> <th>Variance</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Low (Linear)</td> <td>High</td> <td>Low</td> <td>Underfitting</td> </tr> <tr> <td>Optimal</td> <td>Balanced</td> <td>Balanced</td> <td>Good generalization</td> </tr> <tr> <td>High (Deep NN)</td> <td>Low</td> <td>High</td> <td>Overfitting</td> </tr> </tbody> </table> <p><strong>Practical Example:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestRegressor</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>PolynomialFeatures</span>

<span class=c1># High Bias Model (Underfitting)</span>
<span class=n>linear_model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>scores_linear</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>linear_model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Linear Model CV Score: </span><span class=si>{</span><span class=n>scores_linear</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_linear</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># Balanced Model</span>
<span class=n>rf_model</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>scores_rf</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>rf_model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Random Forest CV Score: </span><span class=si>{</span><span class=n>scores_rf</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_rf</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># High Variance Model (Overfitting risk)</span>
<span class=n>rf_deep</span> <span class=o>=</span> <span class=n>RandomForestRegressor</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>max_depth</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>scores_deep</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>rf_deep</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Deep RF CV Score: </span><span class=si>{</span><span class=n>scores_deep</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>scores_deep</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're really testing:</strong> Your ability to diagnose model performance issues and choose appropriate solutions.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can draw the classic U-shaped curve from memory</li> <li>Gives concrete examples: "Linear regression on non-linear data = high bias"</li> <li>Mentions solutions: cross-validation, regularization, ensemble methods</li> <li>Discusses real scenarios: "In production at scale, I often prefer slightly higher bias for stability"</li> </ul> </div> </details> <hr> <h3 id=explain-l1-lasso-vs-l2-ridge-regularization-amazon-microsoft-interview-question>Explain L1 (Lasso) vs L2 (Ridge) Regularization - Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Feature Selection</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Amazon, Microsoft, Google, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Core Difference:</strong></p> <p>Both add a penalty term to the loss function to prevent overfitting, but with different effects:</p> <table> <thead> <tr> <th>Aspect</th> <th>L1 (Lasso)</th> <th>L2 (Ridge)</th> </tr> </thead> <tbody> <tr> <td>Penalty</td> <td>$\lambda \sum</td> <td>w_i</td> </tr> <tr> <td>Effect on weights</td> <td>Drives weights to exactly 0</td> <td>Shrinks weights toward 0</td> </tr> <tr> <td>Feature selection</td> <td>Yes (sparse solutions)</td> <td>No (keeps all features)</td> </tr> <tr> <td>Geometry</td> <td>Diamond constraint</td> <td>Circular constraint</td> </tr> <tr> <td>Best for</td> <td>High-dimensional sparse data</td> <td>Multicollinearity</td> </tr> </tbody> </table> <p><strong>Mathematical Formulation:</strong></p> <div class=arithmatex>\[\text{L1 Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} |w_i|\]</div> <div class=arithmatex>\[\text{L2 Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} w_i^2\]</div> <p><strong>Why L1 Creates Sparsity (Geometric Intuition):</strong></p> <p>The L1 constraint region is a diamond shape. The optimal solution often occurs at corners where some weights = 0.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>ElasticNet</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_regression</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Generate data with some irrelevant features</span>
<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_regression</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>n_informative</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>noise</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># L1 Regularization - Feature Selection</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
<span class=n>lasso</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;L1 Non-zero coefficients: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>lasso</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
<span class=c1># Output: ~5 (identifies informative features)</span>

<span class=c1># L2 Regularization - All features kept</span>
<span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
<span class=n>ridge</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;L2 Non-zero coefficients: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>ridge</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
<span class=c1># Output: 20 (all features kept, but shrunk)</span>

<span class=c1># Elastic Net - Best of both worlds</span>
<span class=n>elastic</span> <span class=o>=</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=n>elastic</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Elastic Net Non-zero: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>elastic</span><span class=o>.</span><span class=n>coef_</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>/20&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep understanding of regularization mechanics, not just definitions.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains WHY L1 creates zeros (diamond geometry)</li> <li>Knows when to use each: "L1 for feature selection, L2 for correlated features"</li> <li>Mentions Elastic Net as hybrid solution</li> <li>Can discuss tuning Œª via cross-validation</li> </ul> </div> </details> <hr> <h3 id=how-does-gradient-descent-work-google-meta-interview-question>How Does Gradient Descent Work? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Optimization</code>, <code>Deep Learning</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Idea:</strong></p> <p>Gradient descent is an iterative optimization algorithm that finds the minimum of a function by repeatedly moving in the direction of steepest descent (negative gradient).</p> <p><strong>Update Rule:</strong></p> <div class=arithmatex>\[w_{t+1} = w_t - \eta \cdot \nabla L(w_t)\]</div> <p>Where: - <span class=arithmatex>\(w_t\)</span> = current weights - <span class=arithmatex>\(\eta\)</span> = learning rate (step size) - <span class=arithmatex>\(\nabla L(w_t)\)</span> = gradient of loss function</p> <p><strong>Variants Comparison:</strong></p> <table> <thead> <tr> <th>Variant</th> <th>Batch Size</th> <th>Speed</th> <th>Stability</th> <th>Memory</th> </tr> </thead> <tbody> <tr> <td>Batch GD</td> <td>All data</td> <td>Slow</td> <td>Very stable</td> <td>High</td> </tr> <tr> <td>Stochastic GD</td> <td>1 sample</td> <td>Fast</td> <td>Noisy</td> <td>Low</td> </tr> <tr> <td>Mini-batch GD</td> <td>32-512</td> <td>Balanced</td> <td>Balanced</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>Modern Optimizers:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=c1># Standard SGD</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># SGD with Momentum (accelerates convergence)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>

<span class=c1># Adam (adaptive learning rates per parameter)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>))</span>

<span class=c1># AdamW (Adam with proper weight decay)</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Adam's Magic Formula:</strong></p> <div class=arithmatex>\[m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$ $$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$ $$w_{t+1} = w_t - \eta \frac{m_t}{\sqrt{v_t} + \epsilon}\]</div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Can you explain optimization intuitively AND mathematically?</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Draws the loss landscape and shows how GD navigates it</li> <li>Knows why learning rate matters (too high = diverge, too low = slow)</li> <li>Can explain momentum: "Like a ball rolling downhill with inertia"</li> <li>Knows Adam is often the default: "Adaptive LR + momentum, works well out-of-box"</li> </ul> </div> </details> <hr> <h3 id=what-is-cross-validation-and-why-is-it-important-facebook-amazon-interview-question>What is Cross-Validation and Why Is It Important? - Facebook, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Model Evaluation</code>, <code>Validation</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Meta, Amazon, Google, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>The Problem It Solves:</strong></p> <p>A single train/test split can give misleading results due to random variation in how data is split. Cross-validation provides a more reliable estimate of model performance.</p> <p><strong>K-Fold Cross-Validation:</strong></p> <ol> <li>Split data into K equal folds</li> <li>For each fold i:<ul> <li>Train on all folds except i</li> <li>Validate on fold i</li> </ul> </li> <li>Average all K validation scores</li> </ol> <p><strong>Common Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>K</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>5-Fold</td> <td>5</td> <td>Standard, good balance</td> </tr> <tr> <td>10-Fold</td> <td>10</td> <td>More reliable, slower</td> </tr> <tr> <td>Leave-One-Out</td> <td>N</td> <td>Small datasets, expensive</td> </tr> <tr> <td>Stratified K-Fold</td> <td>K</td> <td>Imbalanced classification</td> </tr> <tr> <td>Time Series Split</td> <td>K</td> <td>Temporal data (no leakage)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>cross_val_score</span><span class=p>,</span> <span class=n>KFold</span><span class=p>,</span> <span class=n>StratifiedKFold</span><span class=p>,</span> <span class=n>TimeSeriesSplit</span>
<span class=p>)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Standard K-Fold</span>
<span class=n>cv_scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span>
<span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;CV Score: </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=o>*</span><span class=mi>2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># Stratified for imbalanced data</span>
<span class=n>stratified_cv</span> <span class=o>=</span> <span class=n>StratifiedKFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Time Series (prevents data leakage)</span>
<span class=n>tscv</span> <span class=o>=</span> <span class=n>TimeSeriesSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=k>for</span> <span class=n>train_idx</span><span class=p>,</span> <span class=n>test_idx</span> <span class=ow>in</span> <span class=n>tscv</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Train: </span><span class=si>{</span><span class=n>train_idx</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2>..., Test: </span><span class=si>{</span><span class=n>test_idx</span><span class=p>[:</span><span class=mi>3</span><span class=p>]</span><span class=si>}</span><span class=s2>...&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of model validation fundamentals.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to use stratified (imbalanced classes) vs regular</li> <li>Immediately mentions TimeSeriesSplit for temporal data (data leakage awareness)</li> <li>Can explain computational tradeoff: "10-fold is 2x slower but more reliable"</li> <li>Mentions nested CV for hyperparameter tuning</li> </ul> </div> </details> <hr> <h3 id=explain-precision-recall-and-f1-score-google-microsoft-interview-question>Explain Precision, Recall, and F1-Score - Google, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification Metrics</code>, <code>Model Evaluation</code>, <code>Imbalanced Data</code> | <strong>Asked by:</strong> Google, Microsoft, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Confusion Matrix Foundation:</strong></p> <table> <thead> <tr> <th></th> <th>Predicted Positive</th> <th>Predicted Negative</th> </tr> </thead> <tbody> <tr> <td><strong>Actual Positive</strong></td> <td>TP (True Positive)</td> <td>FN (False Negative)</td> </tr> <tr> <td><strong>Actual Negative</strong></td> <td>FP (False Positive)</td> <td>TN (True Negative)</td> </tr> </tbody> </table> <p><strong>The Metrics:</strong></p> <div class=arithmatex>\[\text{Precision} = \frac{TP}{TP + FP}\]</div> <p><em>"Of all positive predictions, how many were correct?"</em></p> <div class=arithmatex>\[\text{Recall} = \frac{TP}{TP + FN}\]</div> <p><em>"Of all actual positives, how many did we find?"</em></p> <div class=arithmatex>\[\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\]</div> <p><em>"Harmonic mean - penalizes extreme imbalances"</em></p> <p><strong>When to Prioritize Which:</strong></p> <table> <thead> <tr> <th>Scenario</th> <th>Priority</th> <th>Why</th> </tr> </thead> <tbody> <tr> <td>Spam detection</td> <td>Precision</td> <td>Don't want to lose important emails</td> </tr> <tr> <td>Cancer screening</td> <td>Recall</td> <td>Don't want to miss any cases</td> </tr> <tr> <td>Fraud detection</td> <td>F1 or Recall</td> <td>Balance matters, but missing fraud is costly</td> </tr> <tr> <td>Search ranking</td> <td>Precision@K</td> <td>Top results quality matters most</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>precision_score</span><span class=p>,</span> <span class=n>recall_score</span><span class=p>,</span> <span class=n>f1_score</span><span class=p>,</span>
    <span class=n>classification_report</span><span class=p>,</span> <span class=n>precision_recall_curve</span>
<span class=p>)</span>

<span class=c1># All metrics at once</span>
<span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>

<span class=c1># Adjust threshold for Precision-Recall tradeoff</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
<span class=n>precisions</span><span class=p>,</span> <span class=n>recalls</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>

<span class=c1># Find threshold for desired recall (e.g., 95%)</span>
<span class=n>target_recall</span> <span class=o>=</span> <span class=mf>0.95</span>
<span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>recalls</span> <span class=o>-</span> <span class=n>target_recall</span><span class=p>))</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Threshold for </span><span class=si>{</span><span class=n>target_recall</span><span class=si>}</span><span class=s2> recall: </span><span class=si>{</span><span class=n>optimal_threshold</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Can you choose the right metric for the business problem?</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Immediately asks: "What's the cost of false positives vs false negatives?"</li> <li>Knows accuracy is misleading for imbalanced data</li> <li>Can adjust classification threshold based on business needs</li> <li>Mentions AUC-PR for highly imbalanced datasets</li> </ul> </div> </details> <hr> <h3 id=what-is-a-decision-tree-and-how-does-it-work-amazon-facebook-interview-question>What is a Decision Tree and How Does It Work? - Amazon, Facebook Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Tree Models</code>, <code>Interpretability</code>, <code>Classification</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>How Decision Trees Work:</strong></p> <p>Decision trees recursively split the data based on feature values to create pure (homogeneous) leaf nodes.</p> <p><strong>Splitting Criteria:</strong></p> <p>For Classification (Information Gain / Gini):</p> <div class=arithmatex>\[\text{Gini} = 1 - \sum_{i=1}^{C} p_i^2\]</div> <div class=arithmatex>\[\text{Entropy} = -\sum_{i=1}^{C} p_i \log_2(p_i)\]</div> <p>For Regression (Variance Reduction):</p> <div class=arithmatex>\[\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2\]</div> <p><strong>Pros and Cons:</strong></p> <table> <thead> <tr> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Interpretable (white-box)</td> <td>Prone to overfitting</td> </tr> <tr> <td>No scaling needed</td> <td>Unstable (small data changes ‚Üí different tree)</td> </tr> <tr> <td>Handles non-linear relationships</td> <td>Greedy, not globally optimal</td> </tr> <tr> <td>Feature importance built-in</td> <td>Can't extrapolate beyond training range</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.tree</span><span class=w> </span><span class=kn>import</span> <span class=n>DecisionTreeClassifier</span><span class=p>,</span> <span class=n>plot_tree</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Create and train</span>
<span class=n>tree</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>           <span class=c1># Prevent overfitting</span>
    <span class=n>min_samples_split</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span>  <span class=c1># Minimum samples to split</span>
    <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>   <span class=c1># Minimum samples in leaf</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>tree</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Visualize the tree</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>plot_tree</span><span class=p>(</span><span class=n>tree</span><span class=p>,</span> <span class=n>feature_names</span><span class=o>=</span><span class=n>feature_names</span><span class=p>,</span> 
          <span class=n>class_names</span><span class=o>=</span><span class=n>class_names</span><span class=p>,</span> <span class=n>filled</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Feature importance</span>
<span class=n>importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>feature_names</span><span class=p>,</span>
    <span class=s1>&#39;importance&#39;</span><span class=p>:</span> <span class=n>tree</span><span class=o>.</span><span class=n>feature_importances_</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;importance&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>importance</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of interpretable ML and when to use simple models.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows trees are building blocks for Random Forest, XGBoost</li> <li>Can explain pruning techniques (pre-pruning vs post-pruning)</li> <li>Mentions when to use: "Interpretability required, e.g., credit decisioning"</li> <li>Knows limitation: "Single trees overfit; ensembles solve this"</li> </ul> </div> </details> <hr> <h3 id=random-forest-vs-gradient-boosting-when-to-use-which-google-netflix-interview-question>Random Forest vs Gradient Boosting: When to Use Which? - Google, Netflix Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>XGBoost</code>, <code>Model Selection</code> | <strong>Asked by:</strong> Google, Netflix, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Fundamental Difference:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Random Forest</th> <th>Gradient Boosting</th> </tr> </thead> <tbody> <tr> <td>Strategy</td> <td>Bagging (parallel)</td> <td>Boosting (sequential)</td> </tr> <tr> <td>Trees</td> <td>Independent</td> <td>Each fixes previous errors</td> </tr> <tr> <td>Bias-Variance</td> <td>Reduces variance</td> <td>Reduces bias</td> </tr> <tr> <td>Overfitting</td> <td>Resistant</td> <td>Can overfit if not tuned</td> </tr> <tr> <td>Training</td> <td>Parallelizable, fast</td> <td>Sequential, slower</td> </tr> <tr> <td>Tuning</td> <td>Easy</td> <td>Requires careful tuning</td> </tr> </tbody> </table> <p><strong>When to Use Which:</strong></p> <table> <thead> <tr> <th>Scenario</th> <th>Choice</th> <th>Reason</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Random Forest</td> <td>Works well with default params</td> </tr> <tr> <td>Maximum accuracy</td> <td>Gradient Boosting</td> <td>Better with tuning</td> </tr> <tr> <td>Large dataset</td> <td>Random Forest</td> <td>Faster training</td> </tr> <tr> <td>Kaggle competition</td> <td>XGBoost/LightGBM</td> <td>State-of-art tabular</td> </tr> <tr> <td>Production (simplicity)</td> <td>Random Forest</td> <td>More robust, less tuning</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span><span class=p>,</span> <span class=n>GradientBoostingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=kn>import</span> <span class=n>XGBClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>lightgbm</span><span class=w> </span><span class=kn>import</span> <span class=n>LGBMClassifier</span>

<span class=c1># Random Forest - Quick and robust</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>  <span class=c1># Parallel training</span>
<span class=p>)</span>

<span class=c1># Gradient Boosting (sklearn) - Good baseline</span>
<span class=n>gb</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>3</span>
<span class=p>)</span>

<span class=c1># XGBoost - Industry standard</span>
<span class=n>xgb</span> <span class=o>=</span> <span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>

<span class=c1># LightGBM - Fastest, handles large data</span>
<span class=n>lgbm</span> <span class=o>=</span> <span class=n>LGBMClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>num_leaves</span><span class=o>=</span><span class=mi>31</span><span class=p>,</span>
    <span class=n>feature_fraction</span><span class=o>=</span><span class=mf>0.8</span>
<span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical model selection skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains bagging vs boosting conceptually</li> <li>Knows XGBoost/LightGBM are gradient boosting implementations</li> <li>Can discuss tradeoffs: "RF is easier to deploy, GB needs more tuning"</li> <li>Mentions real experience: "In production, I often start with RF for baseline"</li> </ul> </div> </details> <hr> <h3 id=what-is-overfitting-and-how-do-you-prevent-it-amazon-meta-interview-question>What is Overfitting and How Do You Prevent It? - Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Generalization</code>, <code>Regularization</code>, <code>Model Evaluation</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Apple, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Definition:</strong></p> <p>Overfitting occurs when a model learns the training data too well, including noise and outliers, and fails to generalize to new data.</p> <p><strong>Signs of Overfitting:</strong></p> <ul> <li>High training accuracy, low test accuracy</li> <li>Large gap between training and validation loss</li> <li>Model complexity &gt;&gt; data complexity</li> </ul> <p><strong>Prevention Techniques:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>How It Helps</th> </tr> </thead> <tbody> <tr> <td>More data</td> <td>Reduces variance</td> </tr> <tr> <td>Regularization (L1/L2)</td> <td>Constrains model complexity</td> </tr> <tr> <td>Cross-validation</td> <td>Better estimate of generalization</td> </tr> <tr> <td>Early stopping</td> <td>Stops before overfitting</td> </tr> <tr> <td>Dropout</td> <td>Prevents co-adaptation in NNs</td> </tr> <tr> <td>Data augmentation</td> <td>Increases effective dataset size</td> </tr> <tr> <td>Ensemble methods</td> <td>Averages out individual model errors</td> </tr> <tr> <td>Feature selection</td> <td>Reduces irrelevant noise</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>learning_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Diagnose overfitting with learning curves</span>
<span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=p>,</span> <span class=n>val_scores</span> <span class=o>=</span> <span class=n>learning_curve</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span>
    <span class=n>train_sizes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span>
<span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>train_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>train_sizes</span><span class=p>,</span> <span class=n>val_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Training Size&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Learning Curve - Check for Overfitting&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Early stopping example (XGBoost)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=kn>import</span> <span class=n>XGBClassifier</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>
    <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>  <span class=c1># Stop if no improvement</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
    <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best iteration: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>best_iteration</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Core ML intuition and practical experience.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can draw learning curves and interpret them</li> <li>Mentions multiple techniques, not just one</li> <li>Knows underfitting is the opposite problem</li> <li>Gives real examples: "I use early stopping + regularization together"</li> </ul> </div> </details> <hr> <h3 id=explain-neural-networks-and-backpropagation-google-meta-interview-question>Explain Neural Networks and Backpropagation - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Neural Networks</code>, <code>Optimization</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Neural Network Architecture:</strong></p> <p>A neural network is a series of layers that transform input through weighted connections and non-linear activation functions:</p> <div class=arithmatex>\[z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$ $$a^{[l]} = g(z^{[l]})\]</div> <p>Where: - <span class=arithmatex>\(W^{[l]}\)</span> = weight matrix for layer <span class=arithmatex>\(l\)</span> - <span class=arithmatex>\(b^{[l]}\)</span> = bias vector - <span class=arithmatex>\(g\)</span> = activation function (ReLU, sigmoid, etc.)</p> <p><strong>Backpropagation (Chain Rule):</strong></p> <div class=arithmatex>\[\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial a^{[L]}} \cdot \frac{\partial a^{[L]}}{\partial z^{[L]}} \cdot ... \cdot \frac{\partial z^{[l]}}{\partial W^{[l]}}\]</div> <p><strong>Common Activation Functions:</strong></p> <table> <thead> <tr> <th>Function</th> <th>Formula</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>ReLU</td> <td><span class=arithmatex>\(\max(0, x)\)</span></td> <td>Hidden layers (default)</td> </tr> <tr> <td>Sigmoid</td> <td><span class=arithmatex>\(\frac{1}{1+e^{-x}}\)</span></td> <td>Binary output</td> </tr> <tr> <td>Softmax</td> <td><span class=arithmatex>\(\frac{e^{x_i}}{\sum e^{x_j}}\)</span></td> <td>Multi-class output</td> </tr> <tr> <td>Tanh</td> <td><span class=arithmatex>\(\frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span></td> <td>Hidden layers (centered)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Non-linearity is crucial!</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Training loop with backprop</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>SimpleNN</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>dataloader</span><span class=p>:</span>
        <span class=c1># Forward pass</span>
        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

        <span class=c1># Backward pass (backpropagation)</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>  <span class=c1># Clear old gradients</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>        <span class=c1># Compute gradients</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>       <span class=c1># Update weights</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep understanding of DL fundamentals.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can explain why non-linearity is essential (stacked linear = just linear)</li> <li>Knows vanishing gradient problem and solutions (ReLU, ResNets, LSTM)</li> <li>Can derive simple backprop by hand (at least for 1-layer)</li> <li>Mentions practical considerations: batch normalization, dropout</li> </ul> </div> </details> <hr> <h3 id=what-is-dropout-and-why-does-it-work-amazon-meta-interview-question>What is Dropout and Why Does It Work? - Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Deep Learning</code>, <code>Overfitting</code> | <strong>Asked by:</strong> Amazon, Meta, Google, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>How Dropout Works:</strong></p> <p>During training, randomly set a fraction <span class=arithmatex>\(p\)</span> of neuron outputs to zero:</p> <ol> <li>For each training batch:<ul> <li>Randomly select neurons to "drop" (output = 0)</li> <li>Scale remaining outputs by <span class=arithmatex>\(\frac{1}{1-p}\)</span> to maintain expected value</li> </ul> </li> <li>During inference:<ul> <li>Use all neurons (no dropout)</li> </ul> </li> </ol> <p><strong>Why It Works (Multiple Perspectives):</strong></p> <table> <thead> <tr> <th>Perspective</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Ensemble</td> <td>Training many sub-networks, averaging at test time</td> </tr> <tr> <td>Co-adaptation</td> <td>Prevents neurons from relying on specific other neurons</td> </tr> <tr> <td>Regularization</td> <td>Adds noise, similar to L2 regularization</td> </tr> <tr> <td>Bayesian</td> <td>Approximates Bayesian inference (variational)</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>DropoutNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 50% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>  <span class=c1># 30% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Applied during training</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Important: model.eval() disables dropout for inference</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
<span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
    <span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</code></pre></div> <p><strong>Common Dropout Rates:</strong></p> <ul> <li>Input layer: 0.2 (keep 80%)</li> <li>Hidden layers: 0.5 (keep 50%)</li> <li>After BatchNorm: Often not needed</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of regularization in deep learning.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows dropout is only active during training</li> <li>Can explain the scaling factor (<span class=arithmatex>\(\frac{1}{1-p}\)</span>)</li> <li>Mentions alternatives: DropConnect, Spatial Dropout for CNNs</li> <li>Knows practical tips: "Don't use after BatchNorm, less needed with modern architectures"</li> </ul> </div> </details> <hr> <h3 id=what-is-transfer-learning-google-amazon-interview-question>What is Transfer Learning? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Pretrained Models</code>, <code>Fine-tuning</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>The Core Idea:</strong></p> <p>Transfer learning leverages knowledge from a model trained on a large dataset (source task) to improve performance on a different but related task (target task).</p> <p><strong>Why It Works:</strong></p> <ul> <li>Lower layers learn general features (edges, textures, word patterns)</li> <li>Higher layers learn task-specific features</li> <li>General features transfer well across tasks</li> </ul> <p><strong>Transfer Learning Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>When to Use</th> <th>How</th> </tr> </thead> <tbody> <tr> <td>Feature extraction</td> <td>Small target dataset</td> <td>Freeze pretrained layers, train new head</td> </tr> <tr> <td>Fine-tuning</td> <td>Medium target dataset</td> <td>Unfreeze some layers, train with low LR</td> </tr> <tr> <td>Full fine-tuning</td> <td>Large target dataset</td> <td>Unfreeze all, train end-to-end</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=c1># Computer Vision (PyTorch)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>models</span>

<span class=c1># Load pretrained ResNet</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet50</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=c1># Strategy 1: Feature Extraction (freeze backbone)</span>
<span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>

<span class=c1># Replace final layer for our task</span>
<span class=n>model</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>in_features</span><span class=p>,</span> <span class=n>num_classes</span><span class=p>)</span>

<span class=c1># Strategy 2: Fine-tuning (unfreeze last block)</span>
<span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>layer4</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>

<span class=c1># NLP (Hugging Face Transformers)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span><span class=p>,</span> <span class=n>AutoTokenizer</span>

<span class=c1># Load pretrained BERT</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
    <span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>,</span>
    <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span>  <span class=c1># Binary classification</span>
<span class=p>)</span>
<span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>)</span>

<span class=c1># Fine-tune with lower learning rate for pretrained layers</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdamW</span><span class=p>([</span>
    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>bert</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>2e-5</span><span class=p>},</span>     <span class=c1># Pretrained</span>
    <span class=p>{</span><span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>classifier</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>}</span>  <span class=c1># New head</span>
<span class=p>])</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical deep learning experience.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to freeze vs fine-tune (data size matters)</li> <li>Mentions learning rate strategies (lower LR for pretrained)</li> <li>Can name popular pretrained models: ResNet, BERT, GPT</li> <li>Discusses domain shift: "Fine-tune more when source/target domains differ"</li> </ul> </div> </details> <hr> <h3 id=explain-roc-curve-and-auc-score-microsoft-netflix-interview-question>Explain ROC Curve and AUC Score - Microsoft, Netflix Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification Metrics</code>, <code>Model Evaluation</code>, <code>Binary Classification</code> | <strong>Asked by:</strong> Microsoft, Netflix, Google, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>ROC Curve (Receiver Operating Characteristic):</strong></p> <p>Plots True Positive Rate vs False Positive Rate at various classification thresholds:</p> <div class=arithmatex>\[TPR = \frac{TP}{TP + FN} = \text{Recall}\]</div> <div class=arithmatex>\[FPR = \frac{FP}{FP + TN}\]</div> <p><strong>AUC (Area Under Curve):</strong></p> <ul> <li><strong>AUC = 1.0</strong>: Perfect classifier</li> <li><strong>AUC = 0.5</strong>: Random guessing (diagonal line)</li> <li><strong>AUC &lt; 0.5</strong>: Worse than random (inverted predictions)</li> </ul> <p><strong>Interpretation:</strong></p> <p>AUC = Probability that a randomly chosen positive example ranks higher than a randomly chosen negative example.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>roc_curve</span><span class=p>,</span> <span class=n>auc</span><span class=p>,</span> <span class=n>roc_auc_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Get probabilities</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Calculate ROC curve</span>
<span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>roc_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
<span class=n>roc_auc</span> <span class=o>=</span> <span class=n>auc</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>)</span>

<span class=c1># Plot</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;ROC Curve (AUC = </span><span class=si>{</span><span class=n>roc_auc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Random&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;False Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;True Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;ROC Curve&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Quick AUC calculation</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;AUC Score: </span><span class=si>{</span><span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span><span class=w> </span><span class=n>y_proba</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Find optimal threshold (Youden&#39;s J statistic)</span>
<span class=n>optimal_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>tpr</span> <span class=o>-</span> <span class=n>fpr</span><span class=p>)</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>optimal_idx</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Optimal Threshold: </span><span class=si>{</span><span class=n>optimal_threshold</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>ROC-AUC vs PR-AUC:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Best For</th> <th>Why</th> </tr> </thead> <tbody> <tr> <td>ROC-AUC</td> <td>Balanced classes</td> <td>Considers both classes equally</td> </tr> <tr> <td>PR-AUC</td> <td>Imbalanced classes</td> <td>Focuses on positive class performance</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of evaluation metrics beyond accuracy.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows ROC-AUC can be misleading for imbalanced data</li> <li>Can interpret thresholds: "Moving along the curve = changing threshold"</li> <li>Mentions practical application: "I use AUC for model comparison, threshold tuning for deployment"</li> <li>Knows PR-AUC is better for highly imbalanced problems</li> </ul> </div> </details> <hr> <h3 id=what-is-dimensionality-reduction-explain-pca-google-amazon-interview-question>What is Dimensionality Reduction? Explain PCA - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Dimensionality Reduction</code>, <code>Feature Extraction</code>, <code>Unsupervised Learning</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Reduce Dimensions:</strong></p> <ul> <li>Curse of dimensionality (data becomes sparse)</li> <li>Reduce computation time</li> <li>Remove noise and redundant features</li> <li>Enable visualization (2D/3D)</li> </ul> <p><strong>PCA (Principal Component Analysis):</strong></p> <p>Finds orthogonal directions (principal components) that maximize variance in the data.</p> <p><strong>Steps:</strong> 1. Center the data (subtract mean) 2. Compute covariance matrix 3. Find eigenvectors and eigenvalues 4. Select top k eigenvectors 5. Project data onto new basis</p> <div class=arithmatex>\[\text{Maximize: } \sum_{i=1}^{k} \text{Var}(X \cdot w_i) = \sum_{i=1}^{k} \lambda_i\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=c1># Step 1: Always scale before PCA!</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Step 2: Determine optimal number of components</span>
<span class=n>pca_full</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>()</span>
<span class=n>pca_full</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=c1># Plot explained variance</span>
<span class=n>cumsum</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>pca_full</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>)</span>
<span class=n>n_95</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>cumsum</span> <span class=o>&gt;=</span> <span class=mf>0.95</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Components for 95% variance: </span><span class=si>{</span><span class=n>n_95</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Step 3: Apply PCA</span>
<span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=n>n_95</span><span class=p>)</span>
<span class=n>X_reduced</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=c1># Visualization (2D)</span>
<span class=n>pca_2d</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>X_2d</span> <span class=o>=</span> <span class=n>pca_2d</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_2d</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_2d</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;PC1 (</span><span class=si>{</span><span class=n>pca_2d</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>:</span><span class=s1>.1%</span><span class=si>}</span><span class=s1> var)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;PC2 (</span><span class=si>{</span><span class=n>pca_2d</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=si>:</span><span class=s1>.1%</span><span class=si>}</span><span class=s1> var)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <p><strong>Alternative Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Best For</th> <th>Preserves</th> </tr> </thead> <tbody> <tr> <td>PCA</td> <td>Linear relationships, variance</td> <td>Global structure</td> </tr> <tr> <td>t-SNE</td> <td>Visualization</td> <td>Local structure</td> </tr> <tr> <td>UMAP</td> <td>Large datasets, clustering</td> <td>Local + global</td> </tr> <tr> <td>LDA</td> <td>Classification</td> <td>Class separability</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of unsupervised learning and feature engineering.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows to scale data before PCA (otherwise high-variance features dominate)</li> <li>Can explain 95% variance retention heuristic</li> <li>Mentions limitations: "PCA assumes linear relationships"</li> <li>Knows alternatives: t-SNE for visualization, UMAP for clustering</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-imbalanced-datasets-netflix-meta-interview-question>How Do You Handle Imbalanced Datasets? - Netflix, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Imbalanced Data</code>, <code>Classification</code>, <code>Sampling</code> | <strong>Asked by:</strong> Netflix, Meta, Amazon, Google</p> <details class=success> <summary>View Answer</summary> <p><strong>The Problem:</strong></p> <p>When one class dominates (e.g., 99% negative, 1% positive), models tend to predict the majority class and achieve high accuracy while missing the minority class entirely.</p> <p><strong>Solutions Toolkit:</strong></p> <table> <thead> <tr> <th>Technique</th> <th>Category</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td>Class weights</td> <td>Cost-sensitive</td> <td>Always try first</td> </tr> <tr> <td>SMOTE</td> <td>Oversampling</td> <td>Moderate imbalance</td> </tr> <tr> <td>Random undersampling</td> <td>Undersampling</td> <td>Large dataset</td> </tr> <tr> <td>Threshold tuning</td> <td>Post-processing</td> <td>Quick fix</td> </tr> <tr> <td>Focal Loss</td> <td>Loss function</td> <td>Deep learning</td> </tr> <tr> <td>Ensemble methods</td> <td>Modeling</td> <td>Severe imbalance</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.utils.class_weight</span><span class=w> </span><span class=kn>import</span> <span class=n>compute_class_weight</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.over_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTE</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.under_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomUnderSampler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>

<span class=c1># Method 1: Class Weights (built into most algorithms)</span>
<span class=n>class_weights</span> <span class=o>=</span> <span class=n>compute_class_weight</span><span class=p>(</span><span class=s1>&#39;balanced&#39;</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y</span><span class=p>),</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>class_weight</span><span class=o>=</span><span class=s1>&#39;balanced&#39;</span><span class=p>)</span>

<span class=c1># Method 2: SMOTE (Synthetic Minority Over-sampling)</span>
<span class=n>smote</span> <span class=o>=</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smote</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Method 3: Combined Sampling Pipeline</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;under&#39;</span><span class=p>,</span> <span class=n>RandomUnderSampler</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;over&#39;</span><span class=p>,</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)),</span>
<span class=p>])</span>
<span class=n>X_balanced</span><span class=p>,</span> <span class=n>y_balanced</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Method 4: Threshold Tuning</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
<span class=c1># Lower threshold to catch more positives</span>
<span class=n>y_pred_adjusted</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_proba</span> <span class=o>&gt;=</span> <span class=mf>0.3</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>  <span class=c1># Instead of 0.5</span>

<span class=c1># Method 5: Focal Loss (PyTorch)</span>
<span class=k>class</span><span class=w> </span><span class=nc>FocalLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
        <span class=n>ce_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
        <span class=n>pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>ce_loss</span><span class=p>)</span>
        <span class=n>focal_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pt</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>ce_loss</span>
        <span class=k>return</span> <span class=n>focal_loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</code></pre></div> <p><strong>Evaluation for Imbalanced Data:</strong></p> <ul> <li>‚ùå Accuracy (misleading)</li> <li>‚úÖ Precision, Recall, F1</li> <li>‚úÖ PR-AUC (better than ROC-AUC)</li> <li>‚úÖ Confusion matrix</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Real-world ML problem-solving.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>First asks: "How imbalanced? 90-10 is different from 99.9-0.1"</li> <li>Knows class weights is usually the first approach</li> <li>Warns about SMOTE pitfalls: "Can overfit to synthetic examples"</li> <li>Mentions correct metrics: "Never use accuracy for imbalanced data"</li> </ul> </div> </details> <hr> <h3 id=explain-k-means-clustering-amazon-microsoft-interview-question>Explain K-Means Clustering - Amazon, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Clustering</code>, <code>Unsupervised Learning</code>, <code>K-Means</code> | <strong>Asked by:</strong> Amazon, Microsoft, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Algorithm Steps:</strong></p> <ol> <li>Initialize k centroids randomly</li> <li>Assign each point to nearest centroid</li> <li>Recalculate centroids as cluster means</li> <li>Repeat steps 2-3 until convergence</li> </ol> <p><strong>Objective Function (Inertia):</strong></p> <div class=arithmatex>\[J = \sum_{i=1}^{n} \min_{j} ||x_i - \mu_j||^2\]</div> <p>Minimize within-cluster sum of squares.</p> <p><strong>Choosing K (Elbow Method):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.cluster</span><span class=w> </span><span class=kn>import</span> <span class=n>KMeans</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>silhouette_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Elbow Method</span>
<span class=n>inertias</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>silhouettes</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>K_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>11</span><span class=p>)</span>

<span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K_range</span><span class=p>:</span>
    <span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
    <span class=n>kmeans</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>inertias</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>kmeans</span><span class=o>.</span><span class=n>inertia_</span><span class=p>)</span>
    <span class=n>silhouettes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>silhouette_score</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>labels_</span><span class=p>))</span>

<span class=c1># Plot</span>
<span class=n>fig</span><span class=p>,</span> <span class=p>(</span><span class=n>ax1</span><span class=p>,</span> <span class=n>ax2</span><span class=p>)</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>

<span class=n>ax1</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>inertias</span><span class=p>,</span> <span class=s1>&#39;bo-&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Clusters (K)&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Inertia&#39;</span><span class=p>)</span>
<span class=n>ax1</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Elbow Method&#39;</span><span class=p>)</span>

<span class=n>ax2</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>silhouettes</span><span class=p>,</span> <span class=s1>&#39;ro-&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of Clusters (K)&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Silhouette Score&#39;</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Silhouette Method&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Final model</span>
<span class=n>optimal_k</span> <span class=o>=</span> <span class=mi>5</span>  <span class=c1># From elbow analysis</span>
<span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>optimal_k</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>labels</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Limitations and Alternatives:</strong></p> <table> <thead> <tr> <th>Limitation</th> <th>Better Alternative</th> </tr> </thead> <tbody> <tr> <td>Assumes spherical clusters</td> <td>DBSCAN, GMM</td> </tr> <tr> <td>Sensitive to initialization</td> <td>KMeans++ (default)</td> </tr> <tr> <td>Must specify K</td> <td>DBSCAN (auto-detects)</td> </tr> <tr> <td>Sensitive to outliers</td> <td>DBSCAN, Robust clustering</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Basic unsupervised learning understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows K-means++ initialization (sklearn default)</li> <li>Can explain limitations: "Assumes spherical, equal-size clusters"</li> <li>Mentions silhouette score for validation</li> <li>Knows when to use alternatives: "DBSCAN for arbitrary shapes"</li> </ul> </div> </details> <hr> <h3 id=what-are-support-vector-machines-svms-when-should-you-use-them-google-amazon-meta-interview-question>What Are Support Vector Machines (SVMs)? When Should You Use Them? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Classification</code>, <code>Kernel Methods</code>, <code>Margin Maximization</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are SVMs?</strong></p> <p>Support Vector Machines are supervised learning models that find the optimal hyperplane to separate classes with maximum margin.</p> <p><strong>Key Concepts:</strong></p> <table> <thead> <tr> <th>Concept</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>Support Vectors</td> <td>Data points closest to decision boundary</td> </tr> <tr> <td>Margin</td> <td>Distance between boundary and nearest points</td> </tr> <tr> <td>Kernel Trick</td> <td>Maps data to higher dimensions for non-linear separation</td> </tr> </tbody> </table> <p><strong>Kernels:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.svm</span><span class=w> </span><span class=kn>import</span> <span class=n>SVC</span>

<span class=c1># Linear kernel - for linearly separable data</span>
<span class=n>svm_linear</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># RBF (Gaussian) - most common for non-linear</span>
<span class=n>svm_rbf</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;rbf&#39;</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=s1>&#39;scale&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># Polynomial kernel</span>
<span class=n>svm_poly</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;poly&#39;</span><span class=p>,</span> <span class=n>degree</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># Training</span>
<span class=n>svm_rbf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>svm_rbf</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>When to Use SVMs:</strong></p> <table> <thead> <tr> <th>Good for</th> <th>Not good for</th> </tr> </thead> <tbody> <tr> <td>High-dimensional data (text)</td> <td>Very large datasets (slow)</td> </tr> <tr> <td>Clear margin of separation</td> <td>Noisy data with overlapping classes</td> </tr> <tr> <td>Fewer samples than features</td> <td>Multi-class (needs one-vs-one)</td> </tr> </tbody> </table> <p><strong>Hyperparameters:</strong></p> <ul> <li><strong>C (Regularization)</strong>: Trade-off between margin and misclassification</li> <li><strong>gamma</strong>: Kernel coefficient - high = overfitting, low = underfitting</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of geometric intuition and kernel methods.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains margin maximization geometrically</li> <li>Knows when to use different kernels</li> <li>Mentions computational complexity O(n¬≤) to O(n¬≥)</li> <li>Knows SVMs work well for text classification</li> </ul> </div> </details> <hr> <h3 id=explain-convolutional-neural-networks-cnns-and-their-architecture-google-meta-amazon-interview-question>Explain Convolutional Neural Networks (CNNs) and Their Architecture - Google, Meta, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Computer Vision</code>, <code>Neural Networks</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple, NVIDIA</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are CNNs?</strong></p> <p>CNNs are neural networks designed for processing structured grid data (images, time series) using convolutional layers that detect spatial patterns.</p> <p><strong>Core Components:</strong></p> <table> <thead> <tr> <th>Layer</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Convolutional</td> <td>Extract features using learnable filters</td> </tr> <tr> <td>Pooling</td> <td>Downsample, reduce computation, add translation invariance</td> </tr> <tr> <td>Fully Connected</td> <td>Classification at the end</td> </tr> <tr> <td>Activation (ReLU)</td> <td>Add non-linearity</td> </tr> </tbody> </table> <p><strong>How Convolution Works:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>SimpleCNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=c1># Input: 3 channels (RGB), Output: 32 filters, 3x3 kernel</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># 2x2 pooling</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>  <span class=c1># After 2 pools: 32‚Üí16‚Üí8</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>  <span class=c1># 10 classes</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>  <span class=c1># 32x32 ‚Üí 16x16</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>  <span class=c1># 16x16 ‚Üí 8x8</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># Flatten</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>Key CNN Architectures:</strong></p> <table> <thead> <tr> <th>Architecture</th> <th>Year</th> <th>Innovation</th> </tr> </thead> <tbody> <tr> <td>LeNet</td> <td>1998</td> <td>First practical CNN</td> </tr> <tr> <td>AlexNet</td> <td>2012</td> <td>Deep CNNs, ReLU, Dropout</td> </tr> <tr> <td>VGG</td> <td>2014</td> <td>Small 3x3 filters, depth</td> </tr> <tr> <td>ResNet</td> <td>2015</td> <td>Skip connections (residual)</td> </tr> <tr> <td>EfficientNet</td> <td>2019</td> <td>Compound scaling</td> </tr> </tbody> </table> <p><strong>Calculations:</strong></p> <p>Output size: <span class=arithmatex>\((W - K + 2P) / S + 1\)</span></p> <p>Where: W = input, K = kernel, P = padding, S = stride</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep learning fundamentals and computer vision.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can calculate output dimensions</li> <li>Explains why pooling helps (translation invariance)</li> <li>Knows ResNet skip connections solve vanishing gradients</li> <li>Mentions transfer learning: "Use pretrained ImageNet models"</li> </ul> </div> </details> <hr> <h3 id=what-are-recurrent-neural-networks-rnns-and-lstms-google-amazon-meta-interview-question>What Are Recurrent Neural Networks (RNNs) and LSTMs? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Sequence Models</code>, <code>NLP</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are RNNs?</strong></p> <p>RNNs process sequential data by maintaining hidden state that captures information from previous time steps.</p> <p><strong>The Problem: Vanishing Gradients</strong></p> <p>Standard RNNs struggle with long sequences because gradients vanish/explode during backpropagation through time.</p> <p><strong>LSTM Solution:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>LSTMModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> 
                           <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> 
                           <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>bidirectional</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>  <span class=c1># *2 for bidirectional</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>output</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span><span class=p>(</span><span class=n>embedded</span><span class=p>)</span>
        <span class=c1># Concatenate final hidden states from both directions</span>
        <span class=n>hidden</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>],</span> <span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>hidden</span><span class=p>)</span>
</code></pre></div> <p><strong>LSTM Gates:</strong></p> <table> <thead> <tr> <th>Gate</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Forget</td> <td>Decide what to discard from cell state</td> </tr> <tr> <td>Input</td> <td>Decide what new info to store</td> </tr> <tr> <td>Output</td> <td>Decide what to output</td> </tr> </tbody> </table> <p><strong>GRU vs LSTM:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>LSTM</th> <th>GRU</th> </tr> </thead> <tbody> <tr> <td>Gates</td> <td>3 (forget, input, output)</td> <td>2 (reset, update)</td> </tr> <tr> <td>Parameters</td> <td>More</td> <td>Fewer</td> </tr> <tr> <td>Performance</td> <td>Better for longer sequences</td> <td>Often comparable</td> </tr> </tbody> </table> <p><strong>Modern Alternatives:</strong></p> <ul> <li><strong>Transformers</strong>: Now preferred for most NLP tasks</li> <li><strong>1D CNNs</strong>: Faster for some sequence tasks</li> <li><strong>Attention mechanisms</strong>: Can be added to RNNs</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of sequence modeling.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains vanishing gradient problem</li> <li>Draws LSTM cell diagram with gates</li> <li>Knows when to use bidirectional</li> <li>Mentions: "Transformers have largely replaced LSTMs for NLP"</li> </ul> </div> </details> <hr> <h3 id=what-is-batch-normalization-and-why-does-it-help-google-amazon-meta-interview-question>What is Batch Normalization and Why Does It Help? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Training</code>, <code>Regularization</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Batch Normalization?</strong></p> <p>Batch normalization normalizes layer inputs by re-centering and re-scaling, making training faster and more stable.</p> <p><strong>The Formula:</strong></p> <div class=arithmatex>\[\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y = \gamma \hat{x} + \beta\]</div> <p>Where <span class=arithmatex>\(\gamma\)</span> (scale) and <span class=arithmatex>\(\beta\)</span> (shift) are learnable parameters.</p> <p><strong>Benefits:</strong></p> <table> <thead> <tr> <th>Benefit</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>Faster training</td> <td>Enables higher learning rates</td> </tr> <tr> <td>Regularization</td> <td>Adds noise (mini-batch statistics)</td> </tr> <tr> <td>Reduces internal covariate shift</td> <td>Stable distributions</td> </tr> <tr> <td>Less sensitive to initialization</td> <td>Normalizes anyway</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>CNNWithBatchNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>  <span class=c1># After conv, before activation</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>bn_fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm1d</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>  <span class=c1># For fully connected</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Normalize</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Then activate</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>32</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>

<span class=c1># Training vs. inference mode matters!</span>
<span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>  <span class=c1># Uses batch statistics</span>
<span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>   <span class=c1># Uses running averages</span>
</code></pre></div> <p><strong>Layer Normalization (Alternative):</strong></p> <table> <thead> <tr> <th>BatchNorm</th> <th>LayerNorm</th> </tr> </thead> <tbody> <tr> <td>Normalizes across batch</td> <td>Normalizes across features</td> </tr> <tr> <td>Needs batch statistics</td> <td>Works with batch size 1</td> </tr> <tr> <td>Good for CNNs</td> <td>Good for RNNs, Transformers</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of deep learning training dynamics.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows position: after linear/conv, before activation</li> <li>Explains train vs eval mode difference</li> <li>Mentions Layer Norm for Transformers</li> <li>Knows it's less needed with skip connections (ResNet)</li> </ul> </div> </details> <hr> <h3 id=what-is-xgboost-and-how-does-it-differ-from-random-forest-amazon-google-microsoft-interview-question>What is XGBoost and How Does It Differ from Random Forest? - Amazon, Google, Microsoft Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>Boosting</code>, <code>Tabular Data</code> | <strong>Asked by:</strong> Amazon, Google, Microsoft, Netflix, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>XGBoost vs Random Forest:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Random Forest</th> <th>XGBoost</th> </tr> </thead> <tbody> <tr> <td>Method</td> <td>Bagging (parallel trees)</td> <td>Boosting (sequential trees)</td> </tr> <tr> <td>Error Focus</td> <td>Each tree is independent</td> <td>Each tree fixes previous errors</td> </tr> <tr> <td>Overfitting</td> <td>Resistant</td> <td>Needs regularization</td> </tr> <tr> <td>Speed</td> <td>Parallelizable</td> <td>Optimized (GPU support)</td> </tr> <tr> <td>Interpretability</td> <td>Feature importance</td> <td>Feature importance + SHAP</td> </tr> </tbody> </table> <p><strong>How XGBoost Works:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>

<span class=c1># Basic XGBoost</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>reg_alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>  <span class=c1># L1 regularization</span>
    <span class=n>reg_lambda</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>  <span class=c1># L2 regularization</span>
    <span class=n>use_label_encoder</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
    <span class=n>eval_metric</span><span class=o>=</span><span class=s1>&#39;logloss&#39;</span>
<span class=p>)</span>

<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=n>importance</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>feature_importances_</span>

<span class=c1># Cross-validation</span>
<span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Hyperparameters:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Effect</th> </tr> </thead> <tbody> <tr> <td>n_estimators</td> <td>Number of trees</td> </tr> <tr> <td>max_depth</td> <td>Tree depth (prevent overfitting)</td> </tr> <tr> <td>learning_rate</td> <td>Shrinkage (lower = more trees needed)</td> </tr> <tr> <td>subsample</td> <td>Row sampling per tree</td> </tr> <tr> <td>colsample_bytree</td> <td>Feature sampling per tree</td> </tr> <tr> <td>reg_alpha/lambda</td> <td>L1/L2 regularization</td> </tr> </tbody> </table> <p><strong>When to Use Which:</strong></p> <table> <thead> <tr> <th>Use Random Forest</th> <th>Use XGBoost</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Maximum accuracy</td> </tr> <tr> <td>Less tuning time</td> <td>Tabular competitions</td> </tr> <tr> <td>Reduce overfitting</td> <td>Handle missing values</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML knowledge for tabular data.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains bagging vs boosting difference</li> <li>Knows key hyperparameters to tune</li> <li>Mentions: "XGBoost handles missing values natively"</li> <li>Knows alternatives: LightGBM (faster), CatBoost (categorical)</li> </ul> </div> </details> <hr> <h3 id=explain-attention-mechanisms-and-transformers-google-meta-openai-interview-question>Explain Attention Mechanisms and Transformers - Google, Meta, OpenAI Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>NLP</code>, <code>Transformers</code> | <strong>Asked by:</strong> Google, Meta, OpenAI, Microsoft, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Attention?</strong></p> <p>Attention allows models to focus on relevant parts of the input when producing output, replacing the need for recurrence.</p> <p><strong>Self-Attention Formula:</strong></p> <div class=arithmatex>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div> <p><strong>Transformer Architecture:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
<span class=kn>import</span><span class=w> </span><span class=nn>math</span>

<span class=k>class</span><span class=w> </span><span class=nc>SelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>embed_dim</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Linear projections</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Reshape for multi-head</span>
        <span class=n>Q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>K</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>V</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>

        <span class=c1># Scaled dot-product attention</span>
        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>

        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>

        <span class=n>attention</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attention</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>

        <span class=c1># Concatenate heads</span>
        <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Multi-Head Attention</td> <td>Attend to different representation subspaces</td> </tr> <tr> <td>Position Encoding</td> <td>Inject sequence order information</td> </tr> <tr> <td>Layer Normalization</td> <td>Stabilize training</td> </tr> <tr> <td>Feed-Forward Network</td> <td>Non-linear transformation</td> </tr> </tbody> </table> <p><strong>Transformer Models:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Type</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>Encoder-only</td> <td>Classification, NER</td> </tr> <tr> <td>GPT</td> <td>Decoder-only</td> <td>Text generation</td> </tr> <tr> <td>T5</td> <td>Encoder-Decoder</td> <td>Translation, summarization</td> </tr> <tr> <td>ViT</td> <td>Vision</td> <td>Image classification</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Modern deep learning architecture understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Can explain Q, K, V analogy (query-key-value retrieval)</li> <li>Knows why scaling by ‚àöd_k (prevent softmax saturation)</li> <li>Understands positional encoding necessity</li> <li>Mentions computational complexity: O(n¬≤) for sequence length n</li> </ul> </div> </details> <hr> <h3 id=what-is-feature-engineering-give-examples-amazon-google-meta-interview-question>What is Feature Engineering? Give Examples - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Feature Engineering</code>, <code>ML Pipeline</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Feature Engineering?</strong></p> <p>Feature engineering is the process of creating, transforming, and selecting features to improve model performance.</p> <p><strong>Categories of Feature Engineering:</strong></p> <table> <thead> <tr> <th>Category</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>Creation</td> <td>Domain-specific features, aggregations</td> </tr> <tr> <td>Transformation</td> <td>Log, sqrt, polynomial features</td> </tr> <tr> <td>Encoding</td> <td>One-hot, target encoding, embeddings</td> </tr> <tr> <td>Scaling</td> <td>Standardization, normalization</td> </tr> <tr> <td>Selection</td> <td>Filter, wrapper, embedded methods</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>

<span class=c1># 1. Date/Time features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;day_of_week&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>dayofweek</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;hour&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>hour</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;is_weekend&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;day_of_week&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>isin</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>])</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;month_sin&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>month</span> <span class=o>/</span> <span class=mi>12</span><span class=p>)</span>  <span class=c1># Cyclical</span>

<span class=c1># 2. Aggregation features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_total_purchases&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;sum&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_avg_purchase&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;user_purchase_count&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;user_id&#39;</span><span class=p>)[</span><span class=s1>&#39;amount&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=s1>&#39;count&#39;</span><span class=p>)</span>

<span class=c1># 3. Text features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;text_length&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>len</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;word_count&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>split</span><span class=p>()</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>len</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;has_question&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>contains</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\?&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>

<span class=c1># 4. Interaction features</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;price_per_sqft&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;sqft&#39;</span><span class=p>]</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;bmi&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;weight&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;height&#39;</span><span class=p>]</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>

<span class=c1># 5. Binning</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;age_group&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>18</span><span class=p>,</span> <span class=mi>35</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>],</span> 
                         <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;child&#39;</span><span class=p>,</span> <span class=s1>&#39;young&#39;</span><span class=p>,</span> <span class=s1>&#39;middle&#39;</span><span class=p>,</span> <span class=s1>&#39;senior&#39;</span><span class=p>])</span>

<span class=c1># 6. Target encoding (for categorical)</span>
<span class=n>target_means</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>target_means</span><span class=p>)</span>

<span class=c1># 7. Log transformation (for skewed data)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;log_income&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>log1p</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>])</span>  <span class=c1># log1p handles zeros</span>
</code></pre></div> <p><strong>Domain-Specific Examples:</strong></p> <table> <thead> <tr> <th>Domain</th> <th>Feature Ideas</th> </tr> </thead> <tbody> <tr> <td>E-commerce</td> <td>Days since last purchase, cart abandonment rate</td> </tr> <tr> <td>Finance</td> <td>Moving averages, volatility, ratios</td> </tr> <tr> <td>NLP</td> <td>TF-IDF, n-grams, sentiment scores</td> </tr> <tr> <td>Healthcare</td> <td>BMI, age groups, risk scores</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical data science skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Gives domain-specific examples</li> <li>Knows cyclical encoding for time features</li> <li>Mentions target encoding for high-cardinality categoricals</li> <li>Warns about data leakage: "Always fit on train, transform on test"</li> </ul> </div> </details> <hr> <h3 id=what-is-model-interpretability-explain-shap-and-lime-google-amazon-meta-interview-question>What is Model Interpretability? Explain SHAP and LIME - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Explainability</code>, <code>Model Interpretation</code>, <code>XAI</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Interpretability Matters:</strong></p> <ul> <li>Regulatory compliance (GDPR, healthcare)</li> <li>Debug and improve models</li> <li>Build trust with stakeholders</li> <li>Detect bias and fairness issues</li> </ul> <p><strong>SHAP (SHapley Additive exPlanations):</strong></p> <p>Based on game theory - measures each feature's contribution to prediction.</p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>shap</span>

<span class=c1># Train model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Create explainer</span>
<span class=n>explainer</span> <span class=o>=</span> <span class=n>shap</span><span class=o>.</span><span class=n>TreeExplainer</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
<span class=n>shap_values</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>shap_values</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Summary plot (global importance)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>summary_plot</span><span class=p>(</span><span class=n>shap_values</span><span class=p>,</span> <span class=n>X_test</span><span class=p>)</span>

<span class=c1># Force plot (single prediction)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>force_plot</span><span class=p>(</span><span class=n>explainer</span><span class=o>.</span><span class=n>expected_value</span><span class=p>,</span> 
               <span class=n>shap_values</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>

<span class=c1># Dependence plot (feature interaction)</span>
<span class=n>shap</span><span class=o>.</span><span class=n>dependence_plot</span><span class=p>(</span><span class=s2>&quot;age&quot;</span><span class=p>,</span> <span class=n>shap_values</span><span class=p>,</span> <span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong></p> <p>Creates local linear approximations around individual predictions.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>lime</span><span class=w> </span><span class=kn>import</span> <span class=n>lime_tabular</span>

<span class=n>explainer</span> <span class=o>=</span> <span class=n>lime_tabular</span><span class=o>.</span><span class=n>LimeTabularExplainer</span><span class=p>(</span>
    <span class=n>X_train</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>feature_names</span><span class=o>=</span><span class=n>X_train</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=n>class_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;No&#39;</span><span class=p>,</span> <span class=s1>&#39;Yes&#39;</span><span class=p>],</span>
    <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;classification&#39;</span>
<span class=p>)</span>

<span class=c1># Explain single prediction</span>
<span class=n>exp</span> <span class=o>=</span> <span class=n>explainer</span><span class=o>.</span><span class=n>explain_instance</span><span class=p>(</span>
    <span class=n>X_test</span><span class=o>.</span><span class=n>iloc</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>values</span><span class=p>,</span>
    <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>,</span>
    <span class=n>num_features</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>exp</span><span class=o>.</span><span class=n>show_in_notebook</span><span class=p>()</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>SHAP</th> <th>LIME</th> </tr> </thead> <tbody> <tr> <td>Approach</td> <td>Game theory (Shapley values)</td> <td>Local linear models</td> </tr> <tr> <td>Consistency</td> <td>Theoretically guaranteed</td> <td>Approximate</td> </tr> <tr> <td>Speed</td> <td>Slower</td> <td>Faster</td> </tr> <tr> <td>Scope</td> <td>Global + local</td> <td>Local (per prediction)</td> </tr> </tbody> </table> <p><strong>Other Methods:</strong></p> <ul> <li><strong>Feature Importance</strong>: Built-in for tree models</li> <li><strong>Partial Dependence Plots</strong>: Show marginal effect</li> <li><strong>Permutation Importance</strong>: Model-agnostic</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of responsible AI.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows difference between global vs local explanations</li> <li>Can explain Shapley values intuitively</li> <li>Mentions use cases: debugging, compliance, bias detection</li> <li>Knows SHAP is theoretically grounded, LIME is approximate</li> </ul> </div> </details> <hr> <h3 id=what-is-hyperparameter-tuning-explain-grid-search-random-search-and-bayesian-optimization-amazon-google-interview-question>What is Hyperparameter Tuning? Explain Grid Search, Random Search, and Bayesian Optimization - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Model Optimization</code>, <code>Hyperparameter Tuning</code>, <code>AutoML</code> | <strong>Asked by:</strong> Amazon, Google, Microsoft, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What Are Hyperparameters?</strong></p> <p>Hyperparameters are external configurations set before training (unlike learned parameters).</p> <p><strong>Tuning Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Approach</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Grid Search</td> <td>Exhaustive search over parameter grid</td> <td>Complete</td> <td>Exponentially slow</td> </tr> <tr> <td>Random Search</td> <td>Random sampling from distributions</td> <td>Faster, finds good values</td> <td>May miss optimal</td> </tr> <tr> <td>Bayesian</td> <td>Probabilistic model of objective</td> <td>Efficient, smart</td> <td>More complex</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GridSearchCV</span><span class=p>,</span> <span class=n>RandomizedSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Grid Search</span>
<span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>300</span><span class=p>],</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=kc>None</span><span class=p>],</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>]</span>
<span class=p>}</span>

<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>param_grid</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;accuracy&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>
<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best params: </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Random Search (often better)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.stats</span><span class=w> </span><span class=kn>import</span> <span class=n>randint</span><span class=p>,</span> <span class=n>uniform</span>

<span class=n>param_dist</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
    <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
    <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=n>randint</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>random_search</span> <span class=o>=</span> <span class=n>RandomizedSearchCV</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>param_dist</span><span class=p>,</span>
    <span class=n>n_iter</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>  <span class=c1># Number of random combinations</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>random_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</code></pre></div> <p><strong>Bayesian Optimization (Optuna):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>optuna</span>

<span class=k>def</span><span class=w> </span><span class=nf>objective</span><span class=p>(</span><span class=n>trial</span><span class=p>):</span>
    <span class=n>params</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>500</span><span class=p>),</span>
        <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_int</span><span class=p>(</span><span class=s1>&#39;max_depth&#39;</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
        <span class=s1>&#39;learning_rate&#39;</span><span class=p>:</span> <span class=n>trial</span><span class=o>.</span><span class=n>suggest_float</span><span class=p>(</span><span class=s1>&#39;learning_rate&#39;</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=n>log</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=p>}</span>

    <span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=o>**</span><span class=n>params</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=k>return</span> <span class=n>score</span>

<span class=n>study</span> <span class=o>=</span> <span class=n>optuna</span><span class=o>.</span><span class=n>create_study</span><span class=p>(</span><span class=n>direction</span><span class=o>=</span><span class=s1>&#39;maximize&#39;</span><span class=p>)</span>
<span class=n>study</span><span class=o>.</span><span class=n>optimize</span><span class=p>(</span><span class=n>objective</span><span class=p>,</span> <span class=n>n_trials</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best params: </span><span class=si>{</span><span class=n>study</span><span class=o>.</span><span class=n>best_params</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Insight:</strong></p> <p>Random Search is often better than Grid Search because it explores more values of important hyperparameters.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML optimization skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows random search often beats grid search</li> <li>Can explain why (more coverage of important params)</li> <li>Mentions Optuna/Hyperopt for Bayesian optimization</li> <li>Uses cross-validation to avoid tuning to test set</li> </ul> </div> </details> <hr> <h3 id=what-is-data-leakage-how-do-you-prevent-it-amazon-google-meta-interview-question>What is Data Leakage? How Do You Prevent It? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>ML Best Practices</code>, <code>Data Leakage</code>, <code>Validation</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Data Leakage?</strong></p> <p>Data leakage occurs when information from outside the training set is used to create the model, causing overly optimistic validation scores that don't generalize.</p> <p><strong>Types of Leakage:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Example</th> <th>Solution</th> </tr> </thead> <tbody> <tr> <td>Target Leakage</td> <td>Using future data to predict past</td> <td>Respect time ordering</td> </tr> <tr> <td>Train-Test Contamination</td> <td>Scaling using full dataset stats</td> <td>Fit on train only</td> </tr> <tr> <td>Feature Leakage</td> <td>Feature derived from target</td> <td>Domain knowledge review</td> </tr> </tbody> </table> <p><strong>Common Examples:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># ‚ùå WRONG: Preprocessing before split</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Sees all data!</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>

<span class=c1># ‚úÖ CORRECT: Preprocess after split</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>  <span class=c1># Fit on train only</span>
<span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>  <span class=c1># Transform with train params</span>

<span class=c1># ‚úÖ BEST: Use Pipeline</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>

<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;scaler&#39;</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>()),</span>
    <span class=p>(</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>())</span>
<span class=p>])</span>

<span class=c1># Cross-validation respects the pipeline</span>
<span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <p><strong>Time Series Leakage:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># ‚ùå WRONG: Random split for time series</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># ‚úÖ CORRECT: Temporal split</span>
<span class=n>train</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&lt;</span> <span class=s1>&#39;2024-01-01&#39;</span><span class=p>]</span>
<span class=n>test</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=s1>&#39;2024-01-01&#39;</span><span class=p>]</span>

<span class=c1># Or use TimeSeriesSplit</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>TimeSeriesSplit</span>
<span class=n>tscv</span> <span class=o>=</span> <span class=n>TimeSeriesSplit</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div> <p><strong>Subtle Leakage Examples:</strong></p> <ul> <li>Customer ID that correlates with VIP status (target)</li> <li>Hospital department that indicates diagnosis</li> <li>Timestamp of transaction result recorded after outcome</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> ML engineering rigor.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Immediately mentions fit_transform on train only</li> <li>Uses sklearn Pipeline to avoid leakage</li> <li>Knows time series requires temporal splits</li> <li>Reviews features for target proxy patterns</li> </ul> </div> </details> <hr> <h3 id=what-is-ab-testing-in-the-context-of-ml-models-google-netflix-meta-interview-question>What is A/B Testing in the Context of ML Models? - Google, Netflix, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Experimentation</code>, <code>A/B Testing</code>, <code>Production ML</code> | <strong>Asked by:</strong> Google, Netflix, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why A/B Test ML Models?</strong></p> <p>Offline metrics don't always correlate with business metrics. A/B testing validates that a new model improves real user outcomes.</p> <p><strong>A/B Testing Framework:</strong></p> <table> <thead> <tr> <th>Step</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>1. Hypothesis</td> <td>New model improves metric X by Y%</td> </tr> <tr> <td>2. Randomization</td> <td>Users randomly assigned to control/treatment</td> </tr> <tr> <td>3. Sample Size</td> <td>Calculate required sample for statistical power</td> </tr> <tr> <td>4. Run Experiment</td> <td>Serve both models simultaneously</td> </tr> <tr> <td>5. Analysis</td> <td>Statistical significance test</td> </tr> </tbody> </table> <p><strong>Sample Size Calculation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>scipy</span><span class=w> </span><span class=kn>import</span> <span class=n>stats</span>

<span class=k>def</span><span class=w> </span><span class=nf>calculate_sample_size</span><span class=p>(</span><span class=n>baseline_rate</span><span class=p>,</span> <span class=n>mde</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>power</span><span class=o>=</span><span class=mf>0.8</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    baseline_rate: Current conversion rate</span>
<span class=sd>    mde: Minimum detectable effect (relative change)</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=n>effect_size</span> <span class=o>=</span> <span class=n>baseline_rate</span> <span class=o>*</span> <span class=n>mde</span>
    <span class=n>z_alpha</span> <span class=o>=</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>alpha</span><span class=o>/</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>z_power</span> <span class=o>=</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>ppf</span><span class=p>(</span><span class=n>power</span><span class=p>)</span>

    <span class=n>p</span> <span class=o>=</span> <span class=n>baseline_rate</span>
    <span class=n>p_hat</span> <span class=o>=</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>))</span> <span class=o>/</span> <span class=mi>2</span>

    <span class=n>n</span> <span class=o>=</span> <span class=p>(</span><span class=n>z_alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>p_hat</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p_hat</span><span class=p>))</span><span class=o>**</span><span class=mf>0.5</span> <span class=o>+</span> 
         <span class=n>z_power</span> <span class=o>*</span> <span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>p</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>effect_size</span><span class=p>)))</span><span class=o>**</span><span class=mf>0.5</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>/</span> <span class=n>effect_size</span><span class=o>**</span><span class=mi>2</span>

    <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>

<span class=c1># Example: 5% baseline, detect 10% relative improvement</span>
<span class=n>n</span> <span class=o>=</span> <span class=n>calculate_sample_size</span><span class=p>(</span><span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Need </span><span class=si>{</span><span class=n>n</span><span class=si>}</span><span class=s2> samples per group&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Statistical Significance:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>scipy</span><span class=w> </span><span class=kn>import</span> <span class=n>stats</span>

<span class=k>def</span><span class=w> </span><span class=nf>ab_test_significance</span><span class=p>(</span><span class=n>control_conversions</span><span class=p>,</span> <span class=n>control_total</span><span class=p>,</span>
                        <span class=n>treatment_conversions</span><span class=p>,</span> <span class=n>treatment_total</span><span class=p>):</span>
    <span class=n>control_rate</span> <span class=o>=</span> <span class=n>control_conversions</span> <span class=o>/</span> <span class=n>control_total</span>
    <span class=n>treatment_rate</span> <span class=o>=</span> <span class=n>treatment_conversions</span> <span class=o>/</span> <span class=n>treatment_total</span>

    <span class=c1># Two-proportion z-test</span>
    <span class=n>pooled</span> <span class=o>=</span> <span class=p>(</span><span class=n>control_conversions</span> <span class=o>+</span> <span class=n>treatment_conversions</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>control_total</span> <span class=o>+</span> <span class=n>treatment_total</span><span class=p>)</span>
    <span class=n>se</span> <span class=o>=</span> <span class=p>(</span><span class=n>pooled</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pooled</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=n>control_total</span> <span class=o>+</span> <span class=mi>1</span><span class=o>/</span><span class=n>treatment_total</span><span class=p>))</span> <span class=o>**</span> <span class=mf>0.5</span>
    <span class=n>z</span> <span class=o>=</span> <span class=p>(</span><span class=n>treatment_rate</span> <span class=o>-</span> <span class=n>control_rate</span><span class=p>)</span> <span class=o>/</span> <span class=n>se</span>
    <span class=n>p_value</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=o>.</span><span class=n>cdf</span><span class=p>(</span><span class=nb>abs</span><span class=p>(</span><span class=n>z</span><span class=p>)))</span>

    <span class=k>return</span> <span class=p>{</span>
        <span class=s1>&#39;control_rate&#39;</span><span class=p>:</span> <span class=n>control_rate</span><span class=p>,</span>
        <span class=s1>&#39;treatment_rate&#39;</span><span class=p>:</span> <span class=n>treatment_rate</span><span class=p>,</span>
        <span class=s1>&#39;lift&#39;</span><span class=p>:</span> <span class=p>(</span><span class=n>treatment_rate</span> <span class=o>-</span> <span class=n>control_rate</span><span class=p>)</span> <span class=o>/</span> <span class=n>control_rate</span><span class=p>,</span>
        <span class=s1>&#39;p_value&#39;</span><span class=p>:</span> <span class=n>p_value</span><span class=p>,</span>
        <span class=s1>&#39;significant&#39;</span><span class=p>:</span> <span class=n>p_value</span> <span class=o>&lt;</span> <span class=mf>0.05</span>
    <span class=p>}</span>
</code></pre></div> <p><strong>ML-Specific Considerations:</strong></p> <ul> <li><strong>Interleaving</strong>: Show both models' results mixed together</li> <li><strong>Multi-armed bandits</strong>: Adaptive allocation to better variants</li> <li><strong>Guardrail metrics</strong>: Ensure no degradation in key metrics</li> <li><strong>Novelty effects</strong>: New models may show initial boost that fades</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of production ML and experimentation.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows offline vs online metrics difference</li> <li>Can calculate sample size for desired power</li> <li>Mentions guardrail metrics and novelty effects</li> <li>Knows when to use bandits vs traditional A/B tests</li> </ul> </div> </details> <hr> <h3 id=explain-different-types-of-recommendation-systems-netflix-amazon-google-interview-question>Explain Different Types of Recommendation Systems - Netflix, Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Recommendation Systems</code>, <code>Collaborative Filtering</code>, <code>Content-Based</code> | <strong>Asked by:</strong> Netflix, Amazon, Google, Meta, Spotify</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Recommendation Systems:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Approach</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>Collaborative Filtering</td> <td>User-item interactions</td> <td>Discovers unexpected</td> <td>Cold start problem</td> </tr> <tr> <td>Content-Based</td> <td>Item features</td> <td>No cold start for items</td> <td>Limited novelty</td> </tr> <tr> <td>Hybrid</td> <td>Combines both</td> <td>Best of both</td> <td>More complex</td> </tr> </tbody> </table> <p><strong>Collaborative Filtering:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># User-based: Find similar users</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics.pairwise</span><span class=w> </span><span class=kn>import</span> <span class=n>cosine_similarity</span>

<span class=n>user_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=p>)</span>

<span class=c1># Item-based: Find similar items</span>
<span class=n>item_similarity</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>

<span class=c1># Matrix Factorization (SVD)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.sparse.linalg</span><span class=w> </span><span class=kn>import</span> <span class=n>svds</span>

<span class=n>U</span><span class=p>,</span> <span class=n>sigma</span><span class=p>,</span> <span class=n>Vt</span> <span class=o>=</span> <span class=n>svds</span><span class=p>(</span><span class=n>user_item_matrix</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
<span class=n>predicted_ratings</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>U</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>sigma</span><span class=p>)),</span> <span class=n>Vt</span><span class=p>)</span>
</code></pre></div> <p><strong>Deep Learning Approach:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>NeuralCollaborativeFiltering</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_users</span><span class=p>,</span> <span class=n>num_items</span><span class=p>,</span> <span class=n>embed_dim</span><span class=o>=</span><span class=mi>32</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>user_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_users</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>item_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_items</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
        <span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_ids</span><span class=p>,</span> <span class=n>item_ids</span><span class=p>):</span>
        <span class=n>user_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>user_embed</span><span class=p>(</span><span class=n>user_ids</span><span class=p>)</span>
        <span class=n>item_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>item_embed</span><span class=p>(</span><span class=n>item_ids</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>user_emb</span><span class=p>,</span> <span class=n>item_emb</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span>
</code></pre></div> <p><strong>Content-Based:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_extraction.text</span><span class=w> </span><span class=kn>import</span> <span class=n>TfidfVectorizer</span>

<span class=c1># Create item profiles from descriptions</span>
<span class=n>tfidf</span> <span class=o>=</span> <span class=n>TfidfVectorizer</span><span class=p>(</span><span class=n>stop_words</span><span class=o>=</span><span class=s1>&#39;english&#39;</span><span class=p>)</span>
<span class=n>item_features</span> <span class=o>=</span> <span class=n>tfidf</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>item_descriptions</span><span class=p>)</span>

<span class=c1># Create user profile from liked items</span>
<span class=n>user_profile</span> <span class=o>=</span> <span class=n>item_features</span><span class=p>[</span><span class=n>liked_items</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

<span class=c1># Recommend similar items</span>
<span class=n>similarities</span> <span class=o>=</span> <span class=n>cosine_similarity</span><span class=p>(</span><span class=n>user_profile</span><span class=p>,</span> <span class=n>item_features</span><span class=p>)</span>
</code></pre></div> <p><strong>Evaluation Metrics:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Measures</th> </tr> </thead> <tbody> <tr> <td>Precision@K</td> <td>Relevant items in top K</td> </tr> <tr> <td>Recall@K</td> <td>Coverage of relevant items</td> </tr> <tr> <td>NDCG</td> <td>Ranking quality</td> </tr> <tr> <td>MAP</td> <td>Mean average precision</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of personalization systems.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains cold start problem and solutions</li> <li>Knows matrix factorization vs deep learning trade-offs</li> <li>Mentions implicit vs explicit feedback</li> <li>Discusses evaluation: "We use NDCG because ranking matters"</li> </ul> </div> </details> <hr> <h3 id=what-is-imbalanced-data-how-do-you-handle-it-in-classification-amazon-google-meta-interview-question>What is Imbalanced Data? How Do You Handle It in Classification? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Class Imbalance</code>, <code>Classification</code>, <code>Sampling</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Imbalanced Data?</strong></p> <p>When one class significantly outnumbers others (e.g., 99% negative, 1% positive). Common in fraud detection, medical diagnosis, anomaly detection.</p> <p><strong>Why It's a Problem:</strong></p> <ul> <li>Model learns to predict majority class</li> <li>Accuracy is misleading (99% accuracy by predicting all negative)</li> <li>Minority class patterns not learned</li> </ul> <p><strong>Strategies:</strong></p> <table> <thead> <tr> <th>Level</th> <th>Technique</th> </tr> </thead> <tbody> <tr> <td>Data</td> <td>Oversampling, undersampling, SMOTE</td> </tr> <tr> <td>Algorithm</td> <td>Class weights, anomaly detection</td> </tr> <tr> <td>Evaluation</td> <td>Use F1, PR-AUC, not accuracy</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>imblearn.over_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>SMOTE</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.under_sampling</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomUnderSampler</span>
<span class=kn>from</span><span class=w> </span><span class=nn>imblearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span> <span class=k>as</span> <span class=n>ImbPipeline</span>

<span class=c1># SMOTE oversampling</span>
<span class=n>smote</span> <span class=o>=</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_resampled</span><span class=p>,</span> <span class=n>y_resampled</span> <span class=o>=</span> <span class=n>smote</span><span class=o>.</span><span class=n>fit_resample</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Combination: SMOTE + undersampling</span>
<span class=n>pipeline</span> <span class=o>=</span> <span class=n>ImbPipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;over&#39;</span><span class=p>,</span> <span class=n>SMOTE</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;under&#39;</span><span class=p>,</span> <span class=n>RandomUnderSampler</span><span class=p>(</span><span class=n>sampling_strategy</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>())</span>
<span class=p>])</span>

<span class=c1># Class weights (no resampling needed)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.utils.class_weight</span><span class=w> </span><span class=kn>import</span> <span class=n>compute_class_weight</span>

<span class=n>weights</span> <span class=o>=</span> <span class=n>compute_class_weight</span><span class=p>(</span><span class=s1>&#39;balanced&#39;</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>y</span><span class=p>),</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>class_weight</span><span class=o>=</span><span class=s1>&#39;balanced&#39;</span><span class=p>)</span>

<span class=c1># Or in XGBoost</span>
<span class=n>scale_pos_weight</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>[</span><span class=n>y_train</span><span class=o>==</span><span class=mi>0</span><span class=p>])</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>y_train</span><span class=p>[</span><span class=n>y_train</span><span class=o>==</span><span class=mi>1</span><span class=p>])</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>scale_pos_weight</span><span class=o>=</span><span class=n>scale_pos_weight</span><span class=p>)</span>
</code></pre></div> <p><strong>Threshold Tuning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>precision_recall_curve</span>

<span class=c1># Get probabilities</span>
<span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Find optimal threshold for F1</span>
<span class=n>precision</span><span class=p>,</span> <span class=n>recall</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>precision_recall_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
<span class=n>f1_scores</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>precision</span> <span class=o>*</span> <span class=n>recall</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>precision</span> <span class=o>+</span> <span class=n>recall</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>)</span>
<span class=n>optimal_threshold</span> <span class=o>=</span> <span class=n>thresholds</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>f1_scores</span><span class=p>)]</span>

<span class=c1># Use custom threshold</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_proba</span> <span class=o>&gt;=</span> <span class=n>optimal_threshold</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</code></pre></div> <p><strong>Evaluation for Imbalanced:</strong></p> <table> <thead> <tr> <th>Use</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Precision-Recall AUC</td> <td>Accuracy</td> </tr> <tr> <td>F1-Score</td> <td>ROC-AUC (can be misleading)</td> </tr> <tr> <td>Confusion Matrix</td> <td>Single metric alone</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical classification handling.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Never uses accuracy as primary metric</li> <li>Knows SMOTE and when to use it</li> <li>Suggests class weights as simpler alternative</li> <li>Mentions threshold tuning on PR curve</li> </ul> </div> </details> <hr> <h3 id=how-do-you-deploy-ml-models-to-production-amazon-google-meta-interview-question>How Do You Deploy ML Models to Production? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>MLOps</code>, <code>Deployment</code>, <code>Production ML</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Deployment Approaches:</strong></p> <table> <thead> <tr> <th>Approach</th> <th>Use Case</th> <th>Latency</th> </tr> </thead> <tbody> <tr> <td>Batch</td> <td>Periodic predictions, reports</td> <td>High (okay)</td> </tr> <tr> <td>Real-time API</td> <td>Interactive applications</td> <td>Low (critical)</td> </tr> <tr> <td>Edge</td> <td>Mobile, IoT, offline</td> <td>Very low</td> </tr> <tr> <td>Streaming</td> <td>Continuous data processing</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>Real-time API with FastAPI:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>fastapi</span><span class=w> </span><span class=kn>import</span> <span class=n>FastAPI</span>
<span class=kn>import</span><span class=w> </span><span class=nn>joblib</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=n>app</span> <span class=o>=</span> <span class=n>FastAPI</span><span class=p>()</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;model.joblib&#39;</span><span class=p>)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;scaler.joblib&#39;</span><span class=p>)</span>

<span class=nd>@app</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=s2>&quot;/predict&quot;</span><span class=p>)</span>
<span class=k>async</span> <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=n>features</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]):</span>
    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>features</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>prediction</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
    <span class=n>probability</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

    <span class=k>return</span> <span class=p>{</span>
        <span class=s2>&quot;prediction&quot;</span><span class=p>:</span> <span class=nb>int</span><span class=p>(</span><span class=n>prediction</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span>
        <span class=s2>&quot;probability&quot;</span><span class=p>:</span> <span class=nb>float</span><span class=p>(</span><span class=n>probability</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>())</span>
    <span class=p>}</span>
</code></pre></div> <p><strong>Docker Containerization:</strong></p> <div class=highlight><pre><span></span><code><span class=k>FROM</span><span class=w> </span><span class=s>python:3.10-slim</span>

<span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span>
<span class=k>COPY</span><span class=w> </span>requirements.txt<span class=w> </span>.
<span class=k>RUN</span><span class=w> </span>pip<span class=w> </span>install<span class=w> </span>-r<span class=w> </span>requirements.txt

<span class=k>COPY</span><span class=w> </span>model.joblib<span class=w> </span>.
<span class=k>COPY</span><span class=w> </span>app.py<span class=w> </span>.

<span class=k>CMD</span><span class=w> </span><span class=p>[</span><span class=s2>&quot;uvicorn&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;app:app&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;--host&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;0.0.0.0&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;--port&quot;</span><span class=p>,</span><span class=w> </span><span class=s2>&quot;8000&quot;</span><span class=p>]</span>
</code></pre></div> <p><strong>MLOps Considerations:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Tools</th> </tr> </thead> <tbody> <tr> <td>Model Registry</td> <td>MLflow, Weights &amp; Biases</td> </tr> <tr> <td>Serving</td> <td>TensorFlow Serving, Triton</td> </tr> <tr> <td>Monitoring</td> <td>Prometheus, Grafana</td> </tr> <tr> <td>Feature Store</td> <td>Feast, Tecton</td> </tr> <tr> <td>Pipeline</td> <td>Airflow, Kubeflow</td> </tr> </tbody> </table> <p><strong>Monitoring:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Track prediction drift</span>
<span class=kn>from</span><span class=w> </span><span class=nn>evidently</span><span class=w> </span><span class=kn>import</span> <span class=n>Report</span>
<span class=kn>from</span><span class=w> </span><span class=nn>evidently.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>DataDriftPreset</span>

<span class=n>report</span> <span class=o>=</span> <span class=n>Report</span><span class=p>(</span><span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=n>DataDriftPreset</span><span class=p>()])</span>
<span class=n>report</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>reference_data</span><span class=o>=</span><span class=n>train_df</span><span class=p>,</span> <span class=n>current_data</span><span class=o>=</span><span class=n>production_df</span><span class=p>)</span>
<span class=n>report</span><span class=o>.</span><span class=n>save_html</span><span class=p>(</span><span class=s2>&quot;drift_report.html&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Model Versioning:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>mlflow</span>

<span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>():</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_params</span><span class=p>(</span><span class=n>params</span><span class=p>)</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>(</span><span class=n>metrics</span><span class=p>)</span>
    <span class=n>mlflow</span><span class=o>.</span><span class=n>sklearn</span><span class=o>.</span><span class=n>log_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&quot;model&quot;</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Production ML engineering skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows batch vs real-time trade-offs</li> <li>Mentions containerization (Docker)</li> <li>Discusses monitoring for drift</li> <li>Knows model versioning and rollback strategies</li> </ul> </div> </details> <hr> <h3 id=what-is-linear-regression-explain-assumptions-and-diagnostics-google-amazon-interview-question>What is Linear Regression? Explain Assumptions and Diagnostics - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Regression</code>, <code>Statistics</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Linear Regression?</strong></p> <p>Linear regression models the relationship between a dependent variable and one or more independent variables using a linear function.</p> <p><strong>The Formula:</strong></p> <div class=arithmatex>\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=c1># Simple linear regression</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Coefficients: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Intercept: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>intercept_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;R¬≤ Score: </span><span class=si>{</span><span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span><span class=w> </span><span class=n>y_test</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Key Assumptions:</strong></p> <table> <thead> <tr> <th>Assumption</th> <th>Check Method</th> </tr> </thead> <tbody> <tr> <td>Linearity</td> <td>Residual vs fitted plot</td> </tr> <tr> <td>Independence</td> <td>Durbin-Watson test</td> </tr> <tr> <td>Homoscedasticity</td> <td>Residual spread plot</td> </tr> <tr> <td>Normality</td> <td>Q-Q plot of residuals</td> </tr> <tr> <td>No multicollinearity</td> <td>VIF (Variance Inflation Factor)</td> </tr> </tbody> </table> <p><strong>Diagnostics:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.stats.outliers_influence</span><span class=w> </span><span class=kn>import</span> <span class=n>variance_inflation_factor</span>

<span class=c1># Check multicollinearity</span>
<span class=n>vif</span> <span class=o>=</span> <span class=p>[</span><span class=n>variance_inflation_factor</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>values</span><span class=p>,</span> <span class=n>i</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])]</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;VIF:&quot;</span><span class=p>,</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span> <span class=n>vif</span><span class=p>)))</span>  <span class=c1># VIF &gt; 5 = problem</span>

<span class=c1># Residual analysis</span>
<span class=n>residuals</span> <span class=o>=</span> <span class=n>y_test</span> <span class=o>-</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Statistical foundation knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Lists assumptions without prompting</li> <li>Knows how to check each assumption</li> <li>Mentions VIF for multicollinearity</li> <li>Knows OLS minimizes squared residuals</li> </ul> </div> </details> <hr> <h3 id=what-is-logistic-regression-when-to-use-it-google-amazon-meta-interview-question>What is Logistic Regression? When to Use It? - Google, Amazon, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Classification</code>, <code>Probability</code>, <code>Fundamentals</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Logistic Regression?</strong></p> <p>Logistic regression is a linear model for binary classification that outputs probabilities using the sigmoid function.</p> <p><strong>The Sigmoid Function:</strong></p> <div class=arithmatex>\[P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}}\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;l2&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>solver</span><span class=o>=</span><span class=s1>&#39;lbfgs&#39;</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Probabilities</span>
<span class=n>probabilities</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Coefficients (log-odds)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Coefficients:&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>

<span class=c1># Odds ratio interpretation</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=n>odds_ratios</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Odds Ratios:&quot;</span><span class=p>,</span> <span class=n>odds_ratios</span><span class=p>)</span>
</code></pre></div> <p><strong>Interpretation:</strong></p> <table> <thead> <tr> <th>Coefficient</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>Positive</td> <td>Increases probability of class 1</td> </tr> <tr> <td>Negative</td> <td>Decreases probability of class 1</td> </tr> <tr> <td>Odds Ratio &gt; 1</td> <td>Feature increases odds</td> </tr> <tr> <td>Odds Ratio &lt; 1</td> <td>Feature decreases odds</td> </tr> </tbody> </table> <p><strong>When to Use:</strong></p> <table> <thead> <tr> <th>Use Logistic Regression</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Binary classification</td> <td>Complex non-linear relationships</td> </tr> <tr> <td>Need interpretability</td> <td>Multi-class (use softmax)</td> </tr> <tr> <td>Baseline model</td> <td>Very high dimensional</td> </tr> <tr> <td>Feature importance needed</td> <td></td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of probabilistic classification.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows it's called "regression" but used for classification</li> <li>Can interpret coefficients as log-odds</li> <li>Mentions maximum likelihood estimation</li> <li>Knows regularization prevents overfitting</li> </ul> </div> </details> <hr> <h3 id=what-is-naive-bayes-why-is-it-naive-amazon-google-interview-question>What is Naive Bayes? Why is it "Naive"? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Classification</code>, <code>Probability</code>, <code>Text Classification</code> | <strong>Asked by:</strong> Amazon, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Naive Bayes?</strong></p> <p>Naive Bayes is a probabilistic classifier based on Bayes' theorem with the "naive" assumption of feature independence.</p> <p><strong>Bayes' Theorem:</strong></p> <div class=arithmatex>\[P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}\]</div> <p><strong>The Naive Assumption:</strong></p> <p>Features are conditionally independent given the class: <span class=arithmatex>\(<span class=arithmatex>\(P(x_1, x_2, ..., x_n|C) = P(x_1|C) \cdot P(x_2|C) \cdot ... \cdot P(x_n|C)\)</span>\)</span></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.naive_bayes</span><span class=w> </span><span class=kn>import</span> <span class=n>GaussianNB</span><span class=p>,</span> <span class=n>MultinomialNB</span><span class=p>,</span> <span class=n>BernoulliNB</span>

<span class=c1># For continuous features</span>
<span class=n>gnb</span> <span class=o>=</span> <span class=n>GaussianNB</span><span class=p>()</span>

<span class=c1># For text/count data (most common)</span>
<span class=n>mnb</span> <span class=o>=</span> <span class=n>MultinomialNB</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>  <span class=c1># alpha = Laplace smoothing</span>

<span class=c1># For binary features</span>
<span class=n>bnb</span> <span class=o>=</span> <span class=n>BernoulliNB</span><span class=p>()</span>

<span class=c1># Text classification example</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_extraction.text</span><span class=w> </span><span class=kn>import</span> <span class=n>CountVectorizer</span>

<span class=n>vectorizer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>()</span>
<span class=n>X_train_counts</span> <span class=o>=</span> <span class=n>vectorizer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>train_texts</span><span class=p>)</span>

<span class=n>mnb</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_counts</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>predictions</span> <span class=o>=</span> <span class=n>mnb</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>vectorizer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>test_texts</span><span class=p>))</span>
</code></pre></div> <p><strong>Types:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Use Case</th> <th>Feature Type</th> </tr> </thead> <tbody> <tr> <td>Gaussian</td> <td>Continuous data</td> <td>Real numbers</td> </tr> <tr> <td>Multinomial</td> <td>Text, word counts</td> <td>Counts</td> </tr> <tr> <td>Bernoulli</td> <td>Binary features</td> <td>0/1</td> </tr> </tbody> </table> <p><strong>Why It Works Despite Being "Naive":</strong></p> <ul> <li>Classification only needs relative probabilities</li> <li>Works well with high-dimensional data</li> <li>Very fast training and prediction</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of probabilistic reasoning.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains the independence assumption and why it's unrealistic</li> <li>Knows it performs well for text classification</li> <li>Mentions Laplace smoothing for zero probabilities</li> <li>Compares to logistic regression: "Similar performance, faster"</li> </ul> </div> </details> <hr> <h3 id=what-is-feature-selection-compare-filter-wrapper-and-embedded-methods-amazon-google-interview-question>What is Feature Selection? Compare Filter, Wrapper, and Embedded Methods - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Feature Engineering</code>, <code>Model Optimization</code>, <code>Dimensionality</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Feature Selection?</strong></p> <ul> <li>Reduce overfitting</li> <li>Improve accuracy</li> <li>Reduce training time</li> <li>Improve interpretability</li> </ul> <p><strong>Three Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Speed</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Filter</td> <td>Statistical tests, independent of model</td> <td>Fast</td> <td>Lower</td> </tr> <tr> <td>Wrapper</td> <td>Evaluates subsets with model</td> <td>Slow</td> <td>Higher</td> </tr> <tr> <td>Embedded</td> <td>Selection during training</td> <td>Medium</td> <td>High</td> </tr> </tbody> </table> <p><strong>Filter Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>SelectKBest</span><span class=p>,</span> <span class=n>f_classif</span><span class=p>,</span> <span class=n>mutual_info_classif</span>

<span class=c1># ANOVA F-test (for classification)</span>
<span class=n>selector</span> <span class=o>=</span> <span class=n>SelectKBest</span><span class=p>(</span><span class=n>f_classif</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>X_selected</span> <span class=o>=</span> <span class=n>selector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># Correlation-based</span>
<span class=n>correlation_matrix</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>corr</span><span class=p>()</span>
<span class=n>high_corr_features</span> <span class=o>=</span> <span class=n>correlation_matrix</span><span class=p>[</span><span class=nb>abs</span><span class=p>(</span><span class=n>correlation_matrix</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.8</span><span class=p>]</span>

<span class=c1># Variance threshold</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>VarianceThreshold</span>
<span class=n>selector</span> <span class=o>=</span> <span class=n>VarianceThreshold</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Wrapper Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>RFE</span><span class=p>,</span> <span class=n>RFECV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Recursive Feature Elimination</span>
<span class=n>rfe</span> <span class=o>=</span> <span class=n>RFE</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span> <span class=n>n_features_to_select</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>rfe</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>selected_features</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>rfe</span><span class=o>.</span><span class=n>support_</span><span class=p>]</span>

<span class=c1># With cross-validation</span>
<span class=n>rfecv</span> <span class=o>=</span> <span class=n>RFECV</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>rfecv</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <p><strong>Embedded Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># L1 regularization (Lasso)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LassoCV</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>LassoCV</span><span class=p>(</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>selected</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>[</span><span class=n>lasso</span><span class=o>.</span><span class=n>coef_</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>]</span>

<span class=c1># Tree-based feature importance</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>importances</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>(</span><span class=n>rf</span><span class=o>.</span><span class=n>feature_importances_</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span>
<span class=n>top_features</span> <span class=o>=</span> <span class=n>importances</span><span class=o>.</span><span class=n>nlargest</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span><span class=o>.</span><span class=n>index</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical ML pipeline knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows trade-offs between methods</li> <li>Uses filter for large datasets, wrapper for smaller</li> <li>Mentions L1/Lasso as embedded selection</li> <li>Warns about target leakage in feature selection</li> </ul> </div> </details> <hr> <h3 id=what-is-ensemble-learning-explain-bagging-boosting-and-stacking-google-amazon-interview-question>What is Ensemble Learning? Explain Bagging, Boosting, and Stacking - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Ensemble Methods</code>, <code>Model Combination</code>, <code>Advanced</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Ensemble Learning?</strong></p> <p>Combining multiple models to produce better predictions than any single model.</p> <p><strong>Three Main Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Reduces</th> </tr> </thead> <tbody> <tr> <td>Bagging</td> <td>Parallel models on bootstrap samples</td> <td>Variance</td> </tr> <tr> <td>Boosting</td> <td>Sequential models fixing errors</td> <td>Bias</td> </tr> <tr> <td>Stacking</td> <td>Meta-model on base predictions</td> <td>Both</td> </tr> </tbody> </table> <p><strong>Bagging (Bootstrap Aggregating):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>BaggingClassifier</span><span class=p>,</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Random Forest is bagging + feature randomization</span>
<span class=n>rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>max_features</span><span class=o>=</span><span class=s1>&#39;sqrt&#39;</span><span class=p>)</span>

<span class=c1># Generic bagging</span>
<span class=n>bagging</span> <span class=o>=</span> <span class=n>BaggingClassifier</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>DecisionTreeClassifier</span><span class=p>(),</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
    <span class=n>max_samples</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>bootstrap</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Boosting:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>GradientBoostingClassifier</span><span class=p>,</span> <span class=n>AdaBoostClassifier</span>
<span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>
<span class=kn>import</span><span class=w> </span><span class=nn>lightgbm</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>lgb</span>

<span class=c1># Gradient Boosting</span>
<span class=n>gb</span> <span class=o>=</span> <span class=n>GradientBoostingClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># XGBoost</span>
<span class=n>xgb_model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># LightGBM (faster)</span>
<span class=n>lgb_model</span> <span class=o>=</span> <span class=n>lgb</span><span class=o>.</span><span class=n>LGBMClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</code></pre></div> <p><strong>Stacking:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>StackingClassifier</span>

<span class=n>estimators</span> <span class=o>=</span> <span class=p>[</span>
    <span class=p>(</span><span class=s1>&#39;rf&#39;</span><span class=p>,</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;xgb&#39;</span><span class=p>,</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>)),</span>
    <span class=p>(</span><span class=s1>&#39;lgb&#39;</span><span class=p>,</span> <span class=n>lgb</span><span class=o>.</span><span class=n>LGBMClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>))</span>
<span class=p>]</span>

<span class=n>stacking</span> <span class=o>=</span> <span class=n>StackingClassifier</span><span class=p>(</span>
    <span class=n>estimators</span><span class=o>=</span><span class=n>estimators</span><span class=p>,</span>
    <span class=n>final_estimator</span><span class=o>=</span><span class=n>LogisticRegression</span><span class=p>(),</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>Bagging</th> <th>Boosting</th> </tr> </thead> <tbody> <tr> <td>Training</td> <td>Parallel</td> <td>Sequential</td> </tr> <tr> <td>Goal</td> <td>Reduce variance</td> <td>Reduce bias</td> </tr> <tr> <td>Prone to overfitting</td> <td>Less</td> <td>More</td> </tr> <tr> <td>Example</td> <td>Random Forest</td> <td>XGBoost</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced ML knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains variance vs bias reduction</li> <li>Knows Random Forest = bagging + random features</li> <li>Mentions early stopping for boosting overfitting</li> <li>Can describe when to use each method</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-missing-data-amazon-google-meta-interview-question>How Do You Handle Missing Data? - Amazon, Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Data Preprocessing</code>, <code>Missing Data</code>, <code>Imputation</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Types of Missing Data:</strong></p> <table> <thead> <tr> <th>Type</th> <th>Description</th> <th>Handling</th> </tr> </thead> <tbody> <tr> <td>MCAR</td> <td>Missing Completely at Random</td> <td>Any method</td> </tr> <tr> <td>MAR</td> <td>Missing at Random (depends on observed)</td> <td>Model-based imputation</td> </tr> <tr> <td>MNAR</td> <td>Missing Not at Random</td> <td>Domain knowledge needed</td> </tr> </tbody> </table> <p><strong>Basic Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>SimpleImputer</span>

<span class=c1># Check missing</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>isnull</span><span class=p>()</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>

<span class=c1># Drop rows with missing</span>
<span class=n>df_clean</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>()</span>

<span class=c1># Drop columns with &gt; 50% missing</span>
<span class=n>df_clean</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>thresh</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>df</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.5</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Simple imputation</span>
<span class=n>imputer</span> <span class=o>=</span> <span class=n>SimpleImputer</span><span class=p>(</span><span class=n>strategy</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>  <span class=c1># or median, most_frequent</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Advanced Imputation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>KNNImputer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.experimental</span><span class=w> </span><span class=kn>import</span> <span class=n>enable_iterative_imputer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>IterativeImputer</span>

<span class=c1># KNN Imputation</span>
<span class=n>knn_imputer</span> <span class=o>=</span> <span class=n>KNNImputer</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>knn_imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># MICE (Multiple Imputation by Chained Equations)</span>
<span class=n>mice_imputer</span> <span class=o>=</span> <span class=n>IterativeImputer</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_imputed</span> <span class=o>=</span> <span class=n>mice_imputer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div> <p><strong>Indicator Variables:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Add missing indicator</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.impute</span><span class=w> </span><span class=kn>import</span> <span class=n>SimpleImputer</span><span class=p>,</span> <span class=n>MissingIndicator</span>

<span class=n>indicator</span> <span class=o>=</span> <span class=n>MissingIndicator</span><span class=p>()</span>
<span class=n>missing_flags</span> <span class=o>=</span> <span class=n>indicator</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Combine imputed data with indicators</span>
<span class=n>X_with_indicators</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>X_imputed</span><span class=p>,</span> <span class=n>missing_flags</span><span class=p>])</span>
</code></pre></div> <p><strong>Best Practices:</strong></p> <table> <thead> <tr> <th>Missing %</th> <th>Recommendation</th> </tr> </thead> <tbody> <tr> <td>&lt; 5%</td> <td>Simple imputation</td> </tr> <tr> <td>5-20%</td> <td>Advanced imputation (KNN, MICE)</td> </tr> <tr> <td>&gt; 20%</td> <td>Consider dropping or domain knowledge</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Data quality handling skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Asks about missing mechanism (MCAR, MAR, MNAR)</li> <li>Knows adding missing indicators can help</li> <li>Uses IterativeImputer/MICE for complex cases</li> <li>Warns: "Always impute after train/test split"</li> </ul> </div> </details> <hr> <h3 id=what-is-time-series-forecasting-explain-arima-and-its-components-amazon-google-interview-question>What is Time Series Forecasting? Explain ARIMA and Its Components - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Time Series</code>, <code>Forecasting</code>, <code>ARIMA</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>Time Series Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Trend</td> <td>Long-term increase/decrease</td> </tr> <tr> <td>Seasonality</td> <td>Regular periodic patterns</td> </tr> <tr> <td>Cyclical</td> <td>Non-fixed period fluctuations</td> </tr> <tr> <td>Noise</td> <td>Random variation</td> </tr> </tbody> </table> <p><strong>ARIMA (AutoRegressive Integrated Moving Average):</strong></p> <ul> <li><strong>AR(p)</strong>: AutoRegressive - uses past values</li> <li><strong>I(d)</strong>: Integrated - differencing for stationarity</li> <li><strong>MA(q)</strong>: Moving Average - uses past errors</li> </ul> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.tsa.arima.model</span><span class=w> </span><span class=kn>import</span> <span class=n>ARIMA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>statsmodels.tsa.stattools</span><span class=w> </span><span class=kn>import</span> <span class=n>adfuller</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>

<span class=c1># Check stationarity (ADF test)</span>
<span class=n>result</span> <span class=o>=</span> <span class=n>adfuller</span><span class=p>(</span><span class=n>series</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;ADF Statistic: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s2>, p-value: </span><span class=si>{</span><span class=n>result</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Fit ARIMA</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>ARIMA</span><span class=p>(</span><span class=n>series</span><span class=p>,</span> <span class=n>order</span><span class=o>=</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>  <span class=c1># (AR, differencing, MA)</span>
<span class=n>fitted</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>()</span>

<span class=c1># Forecast</span>
<span class=n>forecast</span> <span class=o>=</span> <span class=n>fitted</span><span class=o>.</span><span class=n>forecast</span><span class=p>(</span><span class=n>steps</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>

<span class=c1># Auto ARIMA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>pmdarima</span><span class=w> </span><span class=kn>import</span> <span class=n>auto_arima</span>
<span class=n>auto_model</span> <span class=o>=</span> <span class=n>auto_arima</span><span class=p>(</span><span class=n>series</span><span class=p>,</span> <span class=n>seasonal</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>12</span><span class=p>)</span>  <span class=c1># m=12 for monthly</span>
</code></pre></div> <p><strong>Choosing Parameters (p, d, q):</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>How to Choose</th> </tr> </thead> <tbody> <tr> <td>d</td> <td>Number of differences for stationarity</td> </tr> <tr> <td>p</td> <td>ACF cuts off, PACF decays</td> </tr> <tr> <td>q</td> <td>PACF cuts off, ACF decays</td> </tr> </tbody> </table> <p><strong>Modern Alternatives:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Prophet (Facebook)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>prophet</span><span class=w> </span><span class=kn>import</span> <span class=n>Prophet</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>Prophet</span><span class=p>(</span><span class=n>yearly_seasonality</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>  <span class=c1># df with &#39;ds&#39; and &#39;y&#39; columns</span>

<span class=c1># Deep Learning</span>
<span class=c1># LSTM, Transformer models for complex patterns</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Time series understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Checks stationarity first (ADF test)</li> <li>Knows ACF/PACF for parameter selection</li> <li>Mentions Prophet for quick results</li> <li>Uses walk-forward validation, not random split</li> </ul> </div> </details> <hr> <h3 id=what-is-gradient-boosted-trees-how-does-xgboost-work-amazon-google-interview-question>What is Gradient Boosted Trees? How Does XGBoost Work? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Boosting</code>, <code>XGBoost</code>, <code>Ensemble</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>How Gradient Boosting Works:</strong></p> <ol> <li>Fit initial model (e.g., mean)</li> <li>Calculate residuals (errors)</li> <li>Fit new tree to predict residuals</li> <li>Add new tree's predictions (with learning rate)</li> <li>Repeat</li> </ol> <p><strong>XGBoost Innovations:</strong></p> <table> <thead> <tr> <th>Feature</th> <th>Benefit</th> </tr> </thead> <tbody> <tr> <td>Regularization</td> <td>L1/L2 on leaf weights</td> </tr> <tr> <td>Sparsity awareness</td> <td>Efficient missing value handling</td> </tr> <tr> <td>Weighted quantile sketch</td> <td>Approximate tree learning</td> </tr> <tr> <td>Cache-aware access</td> <td>10x faster</td> </tr> <tr> <td>Block structure</td> <td>Parallelization</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>xgboost</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>xgb</span>

<span class=c1># Basic model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>xgb</span><span class=o>.</span><span class=n>XGBClassifier</span><span class=p>(</span>
    <span class=n>n_estimators</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
    <span class=n>max_depth</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
    <span class=n>subsample</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>colsample_bytree</span><span class=o>=</span><span class=mf>0.8</span><span class=p>,</span>
    <span class=n>reg_alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>  <span class=c1># L1</span>
    <span class=n>reg_lambda</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>  <span class=c1># L2</span>
    <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=c1># Training with early stopping</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span>
    <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>

<span class=c1># Feature importance</span>
<span class=n>xgb</span><span class=o>.</span><span class=n>plot_importance</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</code></pre></div> <p><strong>LightGBM vs XGBoost:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>XGBoost</th> <th>LightGBM</th> </tr> </thead> <tbody> <tr> <td>Tree growth</td> <td>Level-wise</td> <td>Leaf-wise</td> </tr> <tr> <td>Speed</td> <td>Fast</td> <td>Faster</td> </tr> <tr> <td>Memory</td> <td>Higher</td> <td>Lower</td> </tr> <tr> <td>Categorical</td> <td>Needs encoding</td> <td>Native support</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Practical tree ensemble knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains sequential fitting to residuals</li> <li>Knows key hyperparameters (learning_rate, max_depth)</li> <li>Uses early stopping to prevent overfitting</li> <li>Compares XGBoost vs LightGBM trade-offs</li> </ul> </div> </details> <hr> <h3 id=how-do-you-evaluate-regression-models-amazon-google-interview-question>How Do You Evaluate Regression Models? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü¢ Easy | <strong>Tags:</strong> <code>Evaluation</code>, <code>Regression</code>, <code>Metrics</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Common Regression Metrics:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Formula</th> <th>Interpretation</th> </tr> </thead> <tbody> <tr> <td>MAE</td> <td>$\frac{1}{n}\sum</td> <td>y_i - \hat{y}_i</td> </tr> <tr> <td>MSE</td> <td><span class=arithmatex>\(\frac{1}{n}\sum(y_i - \hat{y}_i)^2\)</span></td> <td>Penalizes large errors</td> </tr> <tr> <td>RMSE</td> <td><span class=arithmatex>\(\sqrt{MSE}\)</span></td> <td>Same scale as target</td> </tr> <tr> <td>R¬≤</td> <td><span class=arithmatex>\(1 - \frac{SS_{res}}{SS_{tot}}\)</span></td> <td>Variance explained</td> </tr> <tr> <td>MAPE</td> <td>$\frac{100}{n}\sum</td> <td>\frac{y_i - \hat{y}_i}{y_i}</td> </tr> </tbody> </table> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_absolute_error</span><span class=p>,</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=n>mae</span> <span class=o>=</span> <span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=n>mse</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
<span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mse</span><span class=p>)</span>
<span class=n>r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=c1># MAPE (handle zeros)</span>
<span class=n>mape</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>((</span><span class=n>y_test</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>/</span> <span class=n>y_test</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;MAE: </span><span class=si>{</span><span class=n>mae</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;RMSE: </span><span class=si>{</span><span class=n>rmse</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;R¬≤: </span><span class=si>{</span><span class=n>r2</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Choosing the Right Metric:</strong></p> <table> <thead> <tr> <th>Use Case</th> <th>Best Metric</th> </tr> </thead> <tbody> <tr> <td>Same units as target</td> <td>MAE, RMSE</td> </tr> <tr> <td>Penalize large errors</td> <td>RMSE, MSE</td> </tr> <tr> <td>Compare across scales</td> <td>MAPE, R¬≤</td> </tr> <tr> <td>Outlier-resistant</td> <td>MAE</td> </tr> </tbody> </table> <p><strong>Adjusted R¬≤:</strong></p> <div class=arithmatex>\[R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}\]</div> <p>Penalizes adding features that don't improve fit.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Evaluation metric knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows RMSE vs MAE trade-offs</li> <li>Uses adjusted R¬≤ when comparing models</li> <li>Mentions residual plots for diagnostics</li> <li>Warns about MAPE with values near zero</li> </ul> </div> </details> <hr> <h3 id=what-is-dimensionality-reduction-compare-pca-and-t-sne-google-amazon-interview-question>What is Dimensionality Reduction? Compare PCA and t-SNE - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Dimensionality Reduction</code>, <code>Visualization</code>, <code>PCA</code> | <strong>Asked by:</strong> Google, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Reduce Dimensions?</strong></p> <ul> <li>Combat curse of dimensionality</li> <li>Reduce noise</li> <li>Enable visualization (2D/3D)</li> <li>Speed up training</li> </ul> <p><strong>PCA (Principal Component Analysis):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span>

<span class=c1># Standardize first!</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Fit PCA</span>
<span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mf>0.95</span><span class=p>)</span>  <span class=c1># Keep 95% variance</span>
<span class=n>X_pca</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Components: </span><span class=si>{</span><span class=n>pca</span><span class=o>.</span><span class=n>n_components_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Explained variance: </span><span class=si>{</span><span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize variance explained</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>))</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Components&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Cumulative Variance&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>t-SNE (t-Distributed Stochastic Neighbor Embedding):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.manifold</span><span class=w> </span><span class=kn>import</span> <span class=n>TSNE</span>

<span class=c1># Usually for visualization only (2-3D)</span>
<span class=n>tsne</span> <span class=o>=</span> <span class=n>TSNE</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>perplexity</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=n>X_tsne</span> <span class=o>=</span> <span class=n>tsne</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_tsne</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_tsne</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Comparison:</strong></p> <table> <thead> <tr> <th>Aspect</th> <th>PCA</th> <th>t-SNE</th> </tr> </thead> <tbody> <tr> <td>Type</td> <td>Linear</td> <td>Non-linear</td> </tr> <tr> <td>Goal</td> <td>Maximize variance</td> <td>Preserve local structure</td> </tr> <tr> <td>Speed</td> <td>Fast</td> <td>Slow</td> </tr> <tr> <td>Deterministic</td> <td>Yes</td> <td>No</td> </tr> <tr> <td>Inverse transform</td> <td>Yes</td> <td>No</td> </tr> <tr> <td>Use case</td> <td>Feature reduction</td> <td>Visualization</td> </tr> </tbody> </table> <p><strong>UMAP (Modern Alternative):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>umap</span>
<span class=n>reducer</span> <span class=o>=</span> <span class=n>umap</span><span class=o>.</span><span class=n>UMAP</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>X_umap</span> <span class=o>=</span> <span class=n>reducer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=c1># Faster than t-SNE, preserves global structure better</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of data representation.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Standardizes data before PCA</li> <li>Knows PCA for features, t-SNE for visualization</li> <li>Mentions perplexity tuning for t-SNE</li> <li>Suggests UMAP as modern alternative</li> </ul> </div> </details> <hr> <h3 id=what-is-neural-network-optimization-explain-adam-and-learning-rate-schedules-google-meta-interview-question>What is Neural Network Optimization? Explain Adam and Learning Rate Schedules - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Deep Learning</code>, <code>Optimization</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Apple</p> <details class=success> <summary>View Answer</summary> <p><strong>Optimizers:</strong></p> <table> <thead> <tr> <th>Optimizer</th> <th>Description</th> <th>Use Case</th> </tr> </thead> <tbody> <tr> <td>SGD</td> <td>Basic gradient descent</td> <td>Large-scale, convex</td> </tr> <tr> <td>Momentum</td> <td>SGD with velocity</td> <td>Faster convergence</td> </tr> <tr> <td>RMSprop</td> <td>Adaptive learning rates</td> <td>Non-stationary</td> </tr> <tr> <td>Adam</td> <td>Momentum + RMSprop</td> <td>Default choice</td> </tr> <tr> <td>AdamW</td> <td>Adam + weight decay</td> <td>Transformers</td> </tr> </tbody> </table> <p><strong>Adam Optimizer:</strong></p> <div class=arithmatex>\[m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$ $$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$ $$\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>

<span class=c1># Adam with default parameters</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>))</span>

<span class=c1># AdamW for transformers</span>
<span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>5e-5</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</code></pre></div> <p><strong>Learning Rate Schedules:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim.lr_scheduler</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>StepLR</span><span class=p>,</span> <span class=n>ExponentialLR</span><span class=p>,</span> <span class=n>CosineAnnealingLR</span><span class=p>,</span> <span class=n>OneCycleLR</span>
<span class=p>)</span>

<span class=c1># Step decay</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>StepLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>step_size</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Cosine annealing (popular)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>CosineAnnealingLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>

<span class=c1># One cycle (fast training)</span>
<span class=n>scheduler</span> <span class=o>=</span> <span class=n>OneCycleLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>max_lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>))</span>

<span class=c1># Training loop</span>
<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>batch</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
        <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</code></pre></div> <p><strong>Learning Rate Finding:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Start low, increase exponentially, find where loss decreases fastest</span>
<span class=c1># Use lr_finder from pytorch-lightning or fastai</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Deep learning training expertise.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Uses Adam as default, knows when to use SGD</li> <li>Implements learning rate scheduling</li> <li>Knows warmup for transformers</li> <li>Can explain momentum and adaptive learning rates</li> </ul> </div> </details> <hr> <h3 id=what-is-regularization-compare-l1-l2-dropout-and-early-stopping-google-amazon-interview-question>What is Regularization? Compare L1, L2, Dropout, and Early Stopping - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Regularization</code>, <code>Overfitting</code>, <code>Training</code> | <strong>Asked by:</strong> Google, Amazon, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Why Regularization?</strong></p> <p>Prevents overfitting by constraining model complexity.</p> <p><strong>Types of Regularization:</strong></p> <table> <thead> <tr> <th>Method</th> <th>How It Works</th> <th>Effect</th> </tr> </thead> <tbody> <tr> <td>L1 (Lasso)</td> <td>Penalize sum of absolute weights</td> <td>Sparse weights (feature selection)</td> </tr> <tr> <td>L2 (Ridge)</td> <td>Penalize sum of squared weights</td> <td>Small weights (prevents extreme values)</td> </tr> <tr> <td>Dropout</td> <td>Randomly zero neurons during training</td> <td>Ensemble effect</td> </tr> <tr> <td>Early Stopping</td> <td>Stop when validation loss increases</td> <td>Limits training time</td> </tr> </tbody> </table> <p><strong>L1 vs L2:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span>

<span class=c1># L2 regularization</span>
<span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>

<span class=c1># L1 regularization (sparse coefficients)</span>
<span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Combination (Elastic Net)</span>
<span class=n>elastic</span> <span class=o>=</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>

<span class=c1># In neural networks</span>
<span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>  <span class=c1># L2</span>
</code></pre></div> <p><strong>Dropout:</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>Network</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 50% dropout</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># Only during training</span>
        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span>
</code></pre></div> <p><strong>Early Stopping:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># XGBoost</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> 
          <span class=n>eval_set</span><span class=o>=</span><span class=p>[(</span><span class=n>X_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>)],</span>
          <span class=n>early_stopping_rounds</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>

<span class=c1># PyTorch (manual)</span>
<span class=n>best_loss</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
<span class=n>patience</span> <span class=o>=</span> <span class=mi>10</span>
<span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>

<span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
    <span class=n>val_loss</span> <span class=o>=</span> <span class=n>validate</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>val_loss</span> <span class=o>&lt;</span> <span class=n>best_loss</span><span class=p>:</span>
        <span class=n>best_loss</span> <span class=o>=</span> <span class=n>val_loss</span>
        <span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=n>save_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>counter</span> <span class=o>+=</span> <span class=mi>1</span>
        <span class=k>if</span> <span class=n>counter</span> <span class=o>&gt;=</span> <span class=n>patience</span><span class=p>:</span>
            <span class=k>break</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of overfitting prevention.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows L1 leads to sparsity (feature selection)</li> <li>Uses dropout only during training</li> <li>Implements early stopping with patience</li> <li>Combines multiple regularization techniques</li> </ul> </div> </details> <hr> <h3 id=what-is-the-curse-of-dimensionality-google-amazon-interview-question>What is the Curse of Dimensionality? - Google, Amazon Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>High Dimensions</code>, <code>Feature Engineering</code>, <code>Theory</code> | <strong>Asked by:</strong> Google, Amazon, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is the Curse of Dimensionality?</strong></p> <p>As dimensions increase, data becomes increasingly sparse, making distance-based methods and density estimation unreliable.</p> <p><strong>Problems:</strong></p> <table> <thead> <tr> <th>Problem</th> <th>Implication</th> </tr> </thead> <tbody> <tr> <td>Data sparsity</td> <td>Need exponentially more data</td> </tr> <tr> <td>Distance concentration</td> <td>All points equidistant</td> </tr> <tr> <td>Computational cost</td> <td>Memory and time explode</td> </tr> <tr> <td>Overfitting</td> <td>More features = more noise</td> </tr> </tbody> </table> <p><strong>Distance Concentration:</strong></p> <p>As dimensions ‚Üí ‚àû, the ratio of nearest to farthest neighbor approaches 1:</p> <div class=arithmatex>\[\lim_{d \to \infty} \frac{dist_{max} - dist_{min}}{dist_{min}} = 0\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics.pairwise</span><span class=w> </span><span class=kn>import</span> <span class=n>euclidean_distances</span>

<span class=c1># Demonstrate distance concentration</span>
<span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>1000</span><span class=p>]:</span>
    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>d</span><span class=p>)</span>
    <span class=n>distances</span> <span class=o>=</span> <span class=n>euclidean_distances</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
    <span class=n>ratio</span> <span class=o>=</span> <span class=p>(</span><span class=n>distances</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>-</span> <span class=n>distances</span><span class=o>.</span><span class=n>min</span><span class=p>())</span> <span class=o>/</span> <span class=n>distances</span><span class=o>.</span><span class=n>min</span><span class=p>()</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Dimensions: </span><span class=si>{</span><span class=n>d</span><span class=si>}</span><span class=s2>, Max-Min Ratio: </span><span class=si>{</span><span class=n>ratio</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>Solutions:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Dimensionality reduction</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.decomposition</span><span class=w> </span><span class=kn>import</span> <span class=n>PCA</span>
<span class=n>X_reduced</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># 2. Feature selection</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.feature_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>SelectKBest</span>
<span class=n>X_selected</span> <span class=o>=</span> <span class=n>SelectKBest</span><span class=p>(</span><span class=n>k</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># 3. Use regularization</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LassoCV</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>LassoCV</span><span class=p>(</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># 4. Use tree-based models (less affected)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>
</code></pre></div> <p><strong>Rule of Thumb:</strong></p> <p>Need at least <span class=arithmatex>\(5^d\)</span> samples for <span class=arithmatex>\(d\)</span> dimensions to maintain data density.</p> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Understanding of high-dimensional data.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains why KNN fails in high dimensions</li> <li>Knows distance metrics become meaningless</li> <li>Suggests dimensionality reduction or regularization</li> <li>Mentions: "Tree models are less affected"</li> </ul> </div> </details> <hr> <h3 id=what-is-cross-entropy-loss-when-to-use-it-google-meta-interview-question>What is Cross-Entropy Loss? When to Use It? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Loss Functions</code>, <code>Classification</code>, <code>Deep Learning</code> | <strong>Asked by:</strong> Google, Meta, Amazon, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Cross-Entropy Loss?</strong></p> <p>Measures the distance between predicted probability distribution and true distribution.</p> <p><strong>Binary Cross-Entropy:</strong></p> <div class=arithmatex>\[L = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\]</div> <p><strong>Categorical Cross-Entropy:</strong></p> <div class=arithmatex>\[L = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})\]</div> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=c1># Binary classification</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCELoss</span><span class=p>()</span>  <span class=c1># With sigmoid output</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCEWithLogitsLoss</span><span class=p>()</span>  <span class=c1># Raw logits (preferred)</span>

<span class=c1># Multi-class classification</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>  <span class=c1># Raw logits (includes softmax)</span>

<span class=c1># Example</span>
<span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Shape: (batch_size, num_classes)</span>
<span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>  <span class=c1># targets: (batch_size,) - class indices</span>
</code></pre></div> <p><strong>Why Cross-Entropy?</strong></p> <table> <thead> <tr> <th>Loss</th> <th>Gradient</th> <th>Use</th> </tr> </thead> <tbody> <tr> <td>MSE</td> <td>Small when wrong</td> <td>Regression</td> </tr> <tr> <td>Cross-Entropy</td> <td>Large when wrong</td> <td>Classification</td> </tr> </tbody> </table> <p><strong>Label Smoothing:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Prevents overconfident predictions</span>
<span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>label_smoothing</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Soft targets: Instead of [0, 1, 0]</span>
<span class=c1># Use: [0.05, 0.9, 0.05]</span>
</code></pre></div> <p><strong>Focal Loss (Imbalanced Data):</strong></p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>FocalLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=mf>2.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.25</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>=</span> <span class=n>gamma</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
        <span class=n>ce_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
        <span class=n>pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>ce_loss</span><span class=p>)</span>
        <span class=n>focal_loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>pt</span><span class=p>)</span> <span class=o>**</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>ce_loss</span>
        <span class=k>return</span> <span class=n>focal_loss</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Loss function understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows cross-entropy for probabilities, MSE for values</li> <li>Uses BCEWithLogitsLoss for numerical stability</li> <li>Mentions label smoothing for regularization</li> <li>Knows focal loss for imbalanced data</li> </ul> </div> </details> <hr> <h3 id=how-do-you-handle-categorical-features-amazon-google-interview-question>How Do You Handle Categorical Features? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Feature Engineering</code>, <code>Encoding</code>, <code>Preprocessing</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Microsoft</p> <details class=success> <summary>View Answer</summary> <p><strong>Encoding Methods:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Use Case</th> <th>Cardinality</th> </tr> </thead> <tbody> <tr> <td>One-Hot</td> <td>Tree models, low cardinality</td> <td>&lt; 10-15</td> </tr> <tr> <td>Label Encoding</td> <td>Tree models</td> <td>Any</td> </tr> <tr> <td>Target Encoding</td> <td>High cardinality</td> <td>&gt; 15</td> </tr> <tr> <td>Frequency Encoding</td> <td>When frequency matters</td> <td>Any</td> </tr> <tr> <td>Embeddings</td> <td>Deep learning</td> <td>Very high</td> </tr> </tbody> </table> <p><strong>One-Hot Encoding:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>OneHotEncoder</span>

<span class=c1># Pandas</span>
<span class=n>df_encoded</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>])</span>

<span class=c1># Scikit-learn</span>
<span class=n>encoder</span> <span class=o>=</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>handle_unknown</span><span class=o>=</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
<span class=n>encoded</span> <span class=o>=</span> <span class=n>encoder</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>df</span><span class=p>[[</span><span class=s1>&#39;category&#39;</span><span class=p>]])</span>
</code></pre></div> <p><strong>Target Encoding:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>category_encoders</span><span class=w> </span><span class=kn>import</span> <span class=n>TargetEncoder</span>

<span class=n>encoder</span> <span class=o>=</span> <span class=n>TargetEncoder</span><span class=p>(</span><span class=n>smoothing</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>encoder</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>],</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>])</span>

<span class=c1># Manual with smoothing</span>
<span class=n>global_mean</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=n>smoothing</span> <span class=o>=</span> <span class=mi>10</span>

<span class=n>agg</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>agg</span><span class=p>([</span><span class=s1>&#39;mean&#39;</span><span class=p>,</span> <span class=s1>&#39;count&#39;</span><span class=p>])</span>
<span class=n>smoothed</span> <span class=o>=</span> <span class=p>(</span><span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;mean&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>smoothing</span> <span class=o>*</span> <span class=n>global_mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>smoothing</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>smoothed</span><span class=p>)</span>
</code></pre></div> <p><strong>Embedding (Deep Learning):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>

<span class=k>class</span><span class=w> </span><span class=nc>ModelWithEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_categories</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_categories</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim</span> <span class=o>+</span> <span class=n>n_numeric_features</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cat_features</span><span class=p>,</span> <span class=n>num_features</span><span class=p>):</span>
        <span class=n>cat_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>cat_features</span><span class=p>)</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>cat_embedded</span><span class=p>,</span> <span class=n>num_features</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</code></pre></div> <p><strong>Best Practices:</strong></p> <table> <thead> <tr> <th>Model Type</th> <th>Recommendation</th> </tr> </thead> <tbody> <tr> <td>Linear</td> <td>One-hot or target encoding</td> </tr> <tr> <td>Tree-based</td> <td>Label or target encoding</td> </tr> <tr> <td>Neural Net</td> <td>Embeddings</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Feature engineering skills.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Chooses encoding based on cardinality</li> <li>Knows target encoding needs CV to avoid leakage</li> <li>Uses embeddings for high cardinality in DL</li> <li>Mentions CatBoost handles categoricals natively</li> </ul> </div> </details> <hr> <h3 id=what-is-model-calibration-google-meta-interview-question>What is Model Calibration? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Probability</code>, <code>Calibration</code>, <code>Evaluation</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Calibration?</strong></p> <p>A model is well-calibrated if predicted probabilities match observed frequencies. If model says 70% probability, event should occur 70% of the time.</p> <p><strong>Why It Matters:</strong></p> <ul> <li>Probability thresholding</li> <li>Risk assessment</li> <li>Decision making</li> <li>Ensemble weighting</li> </ul> <p><strong>Checking Calibration:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>calibration_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=c1># Get probabilities</span>
<span class=n>y_prob</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>

<span class=c1># Calibration curve</span>
<span class=n>fraction_of_positives</span><span class=p>,</span> <span class=n>mean_predicted_value</span> <span class=o>=</span> <span class=n>calibration_curve</span><span class=p>(</span>
    <span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>,</span> <span class=n>n_bins</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>mean_predicted_value</span><span class=p>,</span> <span class=n>fraction_of_positives</span><span class=p>,</span> <span class=s1>&#39;s-&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;--&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Mean Predicted Probability&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Fraction of Positives&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Calibration Methods:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.calibration</span><span class=w> </span><span class=kn>import</span> <span class=n>CalibratedClassifierCV</span>

<span class=c1># Platt scaling (logistic regression on probabilities)</span>
<span class=n>calibrated</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>

<span class=c1># Isotonic regression (non-parametric)</span>
<span class=n>calibrated</span> <span class=o>=</span> <span class=n>CalibratedClassifierCV</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s1>&#39;isotonic&#39;</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>

<span class=n>calibrated</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>calibrated_probs</span> <span class=o>=</span> <span class=n>calibrated</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
</code></pre></div> <p><strong>Brier Score:</strong></p> <div class=arithmatex>\[BS = \frac{1}{N}\sum_{i=1}^{N}(p_i - y_i)^2\]</div> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>brier_score_loss</span>

<span class=n>brier</span> <span class=o>=</span> <span class=n>brier_score_loss</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_prob</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Brier Score: </span><span class=si>{</span><span class=n>brier</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>  <span class=c1># Lower is better</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Advanced probability understanding.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows neural networks are often overconfident</li> <li>Uses calibration curve for diagnosis</li> <li>Chooses Platt (low data) vs isotonic (more data)</li> <li>Mentions Brier score for evaluation</li> </ul> </div> </details> <hr> <h3 id=what-is-online-learning-amazon-google-interview-question>What is Online Learning? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üî¥ Hard | <strong>Tags:</strong> <code>Online Learning</code>, <code>Streaming</code>, <code>Production</code> | <strong>Asked by:</strong> Amazon, Google, Meta, Netflix</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Online Learning?</strong></p> <p>Updating model incrementally as new data arrives, instead of retraining on entire dataset.</p> <p><strong>Use Cases:</strong></p> <table> <thead> <tr> <th>Use Case</th> <th>Why Online</th> </tr> </thead> <tbody> <tr> <td>Streaming data</td> <td>Too much to store</td> </tr> <tr> <td>Concept drift</td> <td>Data distribution changes</td> </tr> <tr> <td>Real-time adaptation</td> <td>Need immediate updates</td> </tr> <tr> <td>Resource constraints</td> <td>Can't retrain frequently</td> </tr> </tbody> </table> <p><strong>Scikit-learn partial_fit:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>SGDClassifier</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>SGDClassifier</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;log_loss&#39;</span><span class=p>)</span>  <span class=c1># Logistic regression</span>

<span class=c1># Initial training</span>
<span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch1</span><span class=p>,</span> <span class=n>y_batch1</span><span class=p>,</span> <span class=n>classes</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>

<span class=c1># Incremental updates</span>
<span class=k>for</span> <span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</code></pre></div> <p><strong>Algorithms that Support Online Learning:</strong></p> <table> <thead> <tr> <th>Algorithm</th> <th>Scikit-learn Class</th> </tr> </thead> <tbody> <tr> <td>SGD Classifier</td> <td>SGDClassifier</td> </tr> <tr> <td>SGD Regressor</td> <td>SGDRegressor</td> </tr> <tr> <td>Naive Bayes</td> <td>MultinomialNB</td> </tr> <tr> <td>Perceptron</td> <td>Perceptron</td> </tr> <tr> <td>Mini-batch K-Means</td> <td>MiniBatchKMeans</td> </tr> </tbody> </table> <p><strong>River Library (Dedicated Online ML):</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>river</span><span class=w> </span><span class=kn>import</span> <span class=n>linear_model</span><span class=p>,</span> <span class=n>preprocessing</span>

<span class=n>model</span> <span class=o>=</span> <span class=p>(</span>
    <span class=n>preprocessing</span><span class=o>.</span><span class=n>StandardScaler</span><span class=p>()</span> <span class=o>|</span> 
    <span class=n>linear_model</span><span class=o>.</span><span class=n>LogisticRegression</span><span class=p>()</span>
<span class=p>)</span>

<span class=k>for</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=ow>in</span> <span class=n>stream</span><span class=p>:</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_one</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn_one</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div> <p><strong>Handling Concept Drift:</strong></p> <ul> <li><strong>Window-based</strong>: Train on recent N samples</li> <li><strong>Decay</strong>: Weight recent samples more</li> <li><strong>Drift detection</strong>: Monitor performance, reset when needed</li> </ul> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Streaming/production ML knowledge.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows when to use online vs batch</li> <li>Mentions concept drift</li> <li>Uses partial_fit in scikit-learn</li> <li>Knows decay/windowing strategies</li> </ul> </div> </details> <hr> <h3 id=what-is-semi-supervised-learning-google-meta-interview-question>What is Semi-Supervised Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Semi-Supervised</code>, <code>Label Propagation</code>, <code>Learning Paradigms</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Semi-Supervised Learning?</strong></p> <p>Uses both labeled and unlabeled data for training. Useful when labeling is expensive but data is abundant.</p> <p><strong>Approaches:</strong></p> <table> <thead> <tr> <th>Method</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Self-training</td> <td>Train, predict unlabeled, add confident predictions</td> </tr> <tr> <td>Co-training</td> <td>Two models teach each other</td> </tr> <tr> <td>Label propagation</td> <td>Spread labels through similarity graph</td> </tr> <tr> <td>Pseudo-labeling</td> <td>Use model predictions as labels</td> </tr> </tbody> </table> <p><strong>Self-Training:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.semi_supervised</span><span class=w> </span><span class=kn>import</span> <span class=n>SelfTrainingClassifier</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># -1 indicates unlabeled</span>
<span class=n>y_train_partial</span> <span class=o>=</span> <span class=n>y_train</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<span class=n>y_train_partial</span><span class=p>[</span><span class=n>unlabeled_mask</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>SelfTrainingClassifier</span><span class=p>(</span>
    <span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>threshold</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>  <span class=c1># Confidence threshold</span>
    <span class=n>max_iter</span><span class=o>=</span><span class=mi>10</span>
<span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train_partial</span><span class=p>)</span>
</code></pre></div> <p><strong>Label Propagation:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.semi_supervised</span><span class=w> </span><span class=kn>import</span> <span class=n>LabelPropagation</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>LabelPropagation</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;knn&#39;</span><span class=p>,</span> <span class=n>n_neighbors</span><span class=o>=</span><span class=mi>7</span><span class=p>)</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train_partial</span><span class=p>)</span>  <span class=c1># -1 for unlabeled</span>

<span class=c1># Get transduced labels</span>
<span class=n>transduced_labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transduction_</span>
</code></pre></div> <p><strong>Pseudo-Labeling (Deep Learning):</strong></p> <div class=highlight><pre><span></span><code><span class=c1># 1. Train on labeled data</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_labeled</span><span class=p>,</span> <span class=n>y_labeled</span><span class=p>)</span>

<span class=c1># 2. Predict unlabeled with confidence</span>
<span class=n>probs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>
<span class=n>confident_mask</span> <span class=o>=</span> <span class=n>probs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>0.95</span>
<span class=n>pseudo_labels</span> <span class=o>=</span> <span class=n>probs</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)[</span><span class=n>confident_mask</span><span class=p>]</span>

<span class=c1># 3. Add to training set</span>
<span class=n>X_combined</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X_labeled</span><span class=p>,</span> <span class=n>X_unlabeled</span><span class=p>[</span><span class=n>confident_mask</span><span class=p>]])</span>
<span class=n>y_combined</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>([</span><span class=n>y_labeled</span><span class=p>,</span> <span class=n>pseudo_labels</span><span class=p>])</span>

<span class=c1># 4. Retrain</span>
<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_combined</span><span class=p>,</span> <span class=n>y_combined</span><span class=p>)</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Knowledge of learning paradigms.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Explains when it's useful (expensive labeling)</li> <li>Knows confidence thresholding to avoid noise</li> <li>Mentions transductive vs inductive</li> <li>Compares to active learning</li> </ul> </div> </details> <hr> <h3 id=what-is-active-learning-google-meta-interview-question>What is Active Learning? - Google, Meta Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>Active Learning</code>, <code>Labeling</code>, <code>Human-in-the-Loop</code> | <strong>Asked by:</strong> Google, Meta, Amazon</p> <details class=success> <summary>View Answer</summary> <p><strong>What is Active Learning?</strong></p> <p>Model actively selects which samples to label, reducing labeling cost while maximizing performance.</p> <p><strong>Query Strategies:</strong></p> <table> <thead> <tr> <th>Strategy</th> <th>How It Works</th> </tr> </thead> <tbody> <tr> <td>Uncertainty Sampling</td> <td>Select least confident predictions</td> </tr> <tr> <td>Query by Committee</td> <td>Select where models disagree most</td> </tr> <tr> <td>Expected Model Change</td> <td>Select that would change model most</td> </tr> <tr> <td>Diversity Sampling</td> <td>Select diverse samples</td> </tr> </tbody> </table> <p><strong>Uncertainty Sampling:</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>modAL.uncertainty</span><span class=w> </span><span class=kn>import</span> <span class=n>uncertainty_sampling</span>
<span class=kn>from</span><span class=w> </span><span class=nn>modAL.models</span><span class=w> </span><span class=kn>import</span> <span class=n>ActiveLearner</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.ensemble</span><span class=w> </span><span class=kn>import</span> <span class=n>RandomForestClassifier</span>

<span class=c1># Initialize with few labeled samples</span>
<span class=n>learner</span> <span class=o>=</span> <span class=n>ActiveLearner</span><span class=p>(</span>
    <span class=n>estimator</span><span class=o>=</span><span class=n>RandomForestClassifier</span><span class=p>(),</span>
    <span class=n>query_strategy</span><span class=o>=</span><span class=n>uncertainty_sampling</span><span class=p>,</span>
    <span class=n>X_training</span><span class=o>=</span><span class=n>X_initial</span><span class=p>,</span>
    <span class=n>y_training</span><span class=o>=</span><span class=n>y_initial</span>
<span class=p>)</span>

<span class=c1># Active learning loop</span>
<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_queries</span><span class=p>):</span>
    <span class=n>query_idx</span><span class=p>,</span> <span class=n>query_instance</span> <span class=o>=</span> <span class=n>learner</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>

    <span class=c1># Get label from oracle (human)</span>
    <span class=n>label</span> <span class=o>=</span> <span class=n>get_label_from_human</span><span class=p>(</span><span class=n>query_instance</span><span class=p>)</span>

    <span class=n>learner</span><span class=o>.</span><span class=n>teach</span><span class=p>(</span><span class=n>query_instance</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>

    <span class=c1># Remove from unlabeled pool</span>
    <span class=n>X_unlabeled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>delete</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>,</span> <span class=n>query_idx</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div> <p><strong>Manual Implementation:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Uncertainty-based selection</span>
<span class=n>probs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_unlabeled</span><span class=p>)</span>

<span class=c1># Least confident</span>
<span class=n>uncertainty</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>probs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Margin (difference between top 2)</span>
<span class=n>sorted_probs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>margin</span> <span class=o>=</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>sorted_probs</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>2</span><span class=p>]</span>

<span class=c1># Entropy</span>
<span class=n>entropy</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>probs</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>probs</span> <span class=o>+</span> <span class=mf>1e-10</span><span class=p>),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=c1># Select top uncertain samples</span>
<span class=n>query_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>uncertainty</span><span class=p>)[</span><span class=o>-</span><span class=n>n_samples</span><span class=p>:]</span>
</code></pre></div> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Efficient labeling strategies.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows different query strategies</li> <li>Mentions exploration vs exploitation trade-off</li> <li>Uses margin or entropy for uncertainty</li> <li>Knows batch mode for efficiency</li> </ul> </div> </details> <hr> <h3 id=what-is-automl-amazon-google-interview-question>What is AutoML? - Amazon, Google Interview Question</h3> <p><strong>Difficulty:</strong> üü° Medium | <strong>Tags:</strong> <code>AutoML</code>, <code>Automation</code>, <code>Model Selection</code> | <strong>Asked by:</strong> Amazon, Google, Meta</p> <details class=success> <summary>View Answer</summary> <p><strong>What is AutoML?</strong></p> <p>Automated Machine Learning - automating model selection, hyperparameter tuning, and feature engineering.</p> <p><strong>AutoML Components:</strong></p> <table> <thead> <tr> <th>Component</th> <th>What It Automates</th> </tr> </thead> <tbody> <tr> <td>Data preprocessing</td> <td>Imputation, encoding, scaling</td> </tr> <tr> <td>Feature engineering</td> <td>Transformations, interactions</td> </tr> <tr> <td>Model selection</td> <td>Algorithm choice</td> </tr> <tr> <td>Hyperparameter tuning</td> <td>Parameter optimization</td> </tr> <tr> <td>Ensembling</td> <td>Combining models</td> </tr> </tbody> </table> <p><strong>Popular AutoML Tools:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Auto-sklearn</span>
<span class=kn>from</span><span class=w> </span><span class=nn>autosklearn.classification</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoSklearnClassifier</span>

<span class=n>automl</span> <span class=o>=</span> <span class=n>AutoSklearnClassifier</span><span class=p>(</span><span class=n>time_left_for_this_task</span><span class=o>=</span><span class=mi>3600</span><span class=p>)</span>
<span class=n>automl</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># H2O AutoML</span>
<span class=kn>import</span><span class=w> </span><span class=nn>h2o</span>
<span class=kn>from</span><span class=w> </span><span class=nn>h2o.automl</span><span class=w> </span><span class=kn>import</span> <span class=n>H2OAutoML</span>

<span class=n>h2o</span><span class=o>.</span><span class=n>init</span><span class=p>()</span>
<span class=n>aml</span> <span class=o>=</span> <span class=n>H2OAutoML</span><span class=p>(</span><span class=n>max_runtime_secs</span><span class=o>=</span><span class=mi>3600</span><span class=p>)</span>
<span class=n>aml</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=n>features</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>target</span><span class=p>,</span> <span class=n>training_frame</span><span class=o>=</span><span class=n>train</span><span class=p>)</span>

<span class=c1># TPOT</span>
<span class=kn>from</span><span class=w> </span><span class=nn>tpot</span><span class=w> </span><span class=kn>import</span> <span class=n>TPOTClassifier</span>

<span class=n>tpot</span> <span class=o>=</span> <span class=n>TPOTClassifier</span><span class=p>(</span><span class=n>generations</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>population_size</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>verbosity</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>tpot</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>tpot</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=s1>&#39;best_pipeline.py&#39;</span><span class=p>)</span>
</code></pre></div> <p><strong>Google Cloud AutoML:</strong></p> <div class=highlight><pre><span></span><code><span class=c1># Vertex AI AutoML</span>
<span class=kn>from</span><span class=w> </span><span class=nn>google.cloud</span><span class=w> </span><span class=kn>import</span> <span class=n>aiplatform</span>

<span class=n>dataset</span> <span class=o>=</span> <span class=n>aiplatform</span><span class=o>.</span><span class=n>TabularDataset</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
    <span class=n>display_name</span><span class=o>=</span><span class=s2>&quot;my_dataset&quot;</span><span class=p>,</span>
    <span class=n>gcs_source</span><span class=o>=</span><span class=s2>&quot;gs://bucket/data.csv&quot;</span>
<span class=p>)</span>

<span class=n>job</span> <span class=o>=</span> <span class=n>aiplatform</span><span class=o>.</span><span class=n>AutoMLTabularTrainingJob</span><span class=p>(</span>
    <span class=n>display_name</span><span class=o>=</span><span class=s2>&quot;my_model&quot;</span><span class=p>,</span>
    <span class=n>optimization_prediction_type</span><span class=o>=</span><span class=s2>&quot;classification&quot;</span>
<span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>job</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span> <span class=n>target_column</span><span class=o>=</span><span class=s2>&quot;label&quot;</span><span class=p>)</span>
</code></pre></div> <p><strong>When to Use AutoML:</strong></p> <table> <thead> <tr> <th>Use AutoML</th> <th>Don't Use</th> </tr> </thead> <tbody> <tr> <td>Quick baseline</td> <td>Need interpretability</td> </tr> <tr> <td>Limited ML expertise</td> <td>Complex domain constraints</td> </tr> <tr> <td>Standard ML problems</td> <td>Need custom architectures</td> </tr> </tbody> </table> <div class="admonition tip"> <p class=admonition-title>Interviewer's Insight</p> <p><strong>What they're testing:</strong> Awareness of automation tools.</p> <p><strong>Strong answer signals:</strong></p> <ul> <li>Knows popular tools (auto-sklearn, H2O, TPOT)</li> <li>Uses AutoML for baselines, then improves</li> <li>Mentions computational cost</li> <li>Knows when manual modeling is better</li> </ul> </div> </details> <hr> <h2 id=quick-reference-100-interview-questions>Quick Reference: 100 Interview Questions</h2> <table> <thead> <tr> <th>Sno</th> <th>Question Title</th> <th>Practice Links</th> <th>Companies Asking</th> <th>Difficulty</th> <th>Topics</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Bias-Variance Tradeoff</td> <td><a href=https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off/ >Machine Learning Mastery</a></td> <td>Google, Facebook, Amazon</td> <td>Medium</td> <td>Model Evaluation, Generalization</td> </tr> <tr> <td>2</td> <td>Regularization Techniques (L1, L2)</td> <td><a href=https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Overfitting, Generalization</td> </tr> <tr> <td>3</td> <td>Cross-Validation</td> <td><a href=https://scikit-learn.org/stable/modules/cross_validation.html>Scikit-Learn Cross Validation</a></td> <td>Google, Facebook, Amazon</td> <td>Easy</td> <td>Model Evaluation</td> </tr> <tr> <td>4</td> <td>Overfitting and Underfitting</td> <td><a href=https://www.analyticsvidhya.com/blog/2017/09/overfitting-underfitting-machine-learning/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Model Evaluation</td> </tr> <tr> <td>5</td> <td>Gradient Descent</td> <td><a href=https://towardsdatascience.com/gradient-descent-101-402f4b3a33f3>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Optimization</td> </tr> <tr> <td>6</td> <td>Supervised vs Unsupervised Learning</td> <td><a href=https://www.ibm.com/cloud/learn/supervised-vs-unsupervised-learning>IBM Cloud Learn</a></td> <td>Google, Facebook, Amazon</td> <td>Easy</td> <td>ML Basics</td> </tr> <tr> <td>7</td> <td>Classification vs Regression</td> <td><a href=https://towardsdatascience.com/classification-vs-regression-a-tutorial-4a2d123b9288>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>ML Basics</td> </tr> <tr> <td>8</td> <td>Evaluation Metrics: Precision, Recall, F1-score</td> <td><a href=https://towardsdatascience.com/accuracy-precision-recall-and-f1-score-5f728d4a57f0>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Model Evaluation</td> </tr> <tr> <td>9</td> <td>Decision Trees</td> <td><a href=https://machinelearningmastery.com/decision-trees-in-machine-learning/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Tree-based Models</td> </tr> <tr> <td>10</td> <td>Ensemble Learning: Bagging and Boosting</td> <td><a href=https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-9e17ce8b0b64>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Ensemble Methods</td> </tr> <tr> <td>11</td> <td>Random Forest</td> <td><a href=https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Ensemble, Decision Trees</td> </tr> <tr> <td>12</td> <td>Support Vector Machines (SVM)</td> <td><a href=https://machinelearningmastery.com/support-vector-machines-for-machine-learning/ >Machine Learning Mastery</a></td> <td>Google, Facebook, Amazon</td> <td>Hard</td> <td>Classification, Kernel Methods</td> </tr> <tr> <td>13</td> <td>k-Nearest Neighbors (k-NN)</td> <td><a href=https://towardsdatascience.com/k-nearest-neighbors-knn-algorithm-for-machine-learning-e883219c8f26>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Instance-based Learning</td> </tr> <tr> <td>14</td> <td>Dimensionality Reduction: PCA</td> <td><a href=https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Dimensionality Reduction</td> </tr> <tr> <td>15</td> <td>Handling Missing Data</td> <td><a href=https://machinelearningmastery.com/handle-missing-data-python/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Data Preprocessing</td> </tr> <tr> <td>16</td> <td>Parametric vs Non-Parametric Models</td> <td><a href=https://towardsdatascience.com/parametric-vs-non-parametric-models-825d1a0f5c2c>Towards Data Science</a></td> <td>Google, Amazon</td> <td>Medium</td> <td>Model Types</td> </tr> <tr> <td>17</td> <td>Neural Networks: Basics</td> <td><a href=https://towardsdatascience.com/a-beginners-guide-to-neural-networks-2cf4c3f9c9d0>Towards Data Science</a></td> <td>Google, Facebook, Amazon</td> <td>Medium</td> <td>Deep Learning</td> </tr> <tr> <td>18</td> <td>Convolutional Neural Networks (CNNs)</td> <td><a href=https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-for-computer-vision-2bda48ea1e50>Towards Data Science</a></td> <td>Google, Facebook, Amazon</td> <td>Hard</td> <td>Deep Learning, Computer Vision</td> </tr> <tr> <td>19</td> <td>Recurrent Neural Networks (RNNs) and LSTMs</td> <td><a href=https://towardsdatascience.com/recurrent-neural-networks-for-language-modeling-396f1d1659f2>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Sequence Models</td> </tr> <tr> <td>20</td> <td>Reinforcement Learning Basics</td> <td><a href=https://towardsdatascience.com/introduction-to-reinforcement-learning-6346f7f8c1ef>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Reinforcement Learning</td> </tr> <tr> <td>21</td> <td>Hyperparameter Tuning</td> <td><a href=https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Model Optimization</td> </tr> <tr> <td>22</td> <td>Feature Engineering</td> <td><a href=https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Data Preprocessing</td> </tr> <tr> <td>23</td> <td>ROC Curve and AUC</td> <td><a href=https://towardsdatascience.com/roc-curve-and-auc-using-python-and-scikit-learn-42da0fa0d0d>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Model Evaluation</td> </tr> <tr> <td>24</td> <td>Regression Evaluation Metrics</td> <td><a href=https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics>Scikit-Learn</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Model Evaluation, Regression</td> </tr> <tr> <td>25</td> <td>Curse of Dimensionality</td> <td><a href=https://machinelearningmastery.com/curse-of-dimensionality/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Data Preprocessing</td> </tr> <tr> <td>26</td> <td>Logistic Regression</td> <td><a href=https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Classification, Regression</td> </tr> <tr> <td>27</td> <td>Linear Regression</td> <td><a href=https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Regression</td> </tr> <tr> <td>28</td> <td>Loss Functions in ML</td> <td><a href=https://towardsdatascience.com/common-loss-functions-in-machine-learning-3b7af9f8bf2b>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Optimization, Model Evaluation</td> </tr> <tr> <td>29</td> <td>Gradient Descent Variants</td> <td><a href=https://machinelearningmastery.com/difference-between-batch-and-stochastic-gradient-descent/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Optimization</td> </tr> <tr> <td>30</td> <td>Data Normalization and Standardization</td> <td><a href=https://machinelearningmastery.com/normalize-standardize-machine-learning-data/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>Data Preprocessing</td> </tr> <tr> <td>31</td> <td>k-Means Clustering</td> <td><a href=https://towardsdatascience.com/introduction-to-k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Clustering</td> </tr> <tr> <td>32</td> <td>Other Clustering Techniques</td> <td><a href=https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Clustering</td> </tr> <tr> <td>33</td> <td>Anomaly Detection</td> <td><a href=https://towardsdatascience.com/anomaly-detection-techniques-in-python-50f650c75aaf>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Outlier Detection</td> </tr> <tr> <td>34</td> <td>Learning Rate in Optimization</td> <td><a href=https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Microsoft</td> <td>Medium</td> <td>Optimization</td> </tr> <tr> <td>35</td> <td>Deep Learning vs. Traditional ML</td> <td><a href=https://www.ibm.com/cloud/learn/deep-learning>IBM Cloud Learn</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Deep Learning, ML Basics</td> </tr> <tr> <td>36</td> <td>Dropout in Neural Networks</td> <td><a href=https://towardsdatascience.com/understanding-dropout-in-neural-networks-3c5da7a57f86>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Deep Learning, Regularization</td> </tr> <tr> <td>37</td> <td>Backpropagation</td> <td><a href=https://www.analyticsvidhya.com/blog/2017/05/implementation-neural-network-scratch-python/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Neural Networks</td> </tr> <tr> <td>38</td> <td>Role of Activation Functions</td> <td><a href=https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Neural Networks</td> </tr> <tr> <td>39</td> <td>Word Embeddings and Their Use</td> <td><a href=https://towardsdatascience.com/word-embeddings-6cb7d87c0f64>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>40</td> <td>Transfer Learning</td> <td><a href=https://machinelearningmastery.com/transfer-learning-for-deep-learning/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Deep Learning, Model Reuse</td> </tr> <tr> <td>41</td> <td>Bayesian Optimization for Hyperparameters</td> <td><a href=https://towardsdatascience.com/bayesian-optimization-explained-4f6c2e60731d>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Hard</td> <td>Hyperparameter Tuning, Optimization</td> </tr> <tr> <td>42</td> <td>Model Interpretability: SHAP and LIME</td> <td><a href=https://towardsdatascience.com/interpreting-machine-learning-models-using-shap-values-df04dc62fbd4>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Model Interpretability, Explainability</td> </tr> <tr> <td>43</td> <td>Ensemble Methods: Stacking and Blending</td> <td><a href=https://machinelearningmastery.com/ensemble-learning-stacking/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Microsoft</td> <td>Hard</td> <td>Ensemble Methods</td> </tr> <tr> <td>44</td> <td>Gradient Boosting Machines (GBM) Basics</td> <td><a href=https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Ensemble, Boosting</td> </tr> <tr> <td>45</td> <td>Extreme Gradient Boosting (XGBoost) Overview</td> <td><a href=https://towardsdatascience.com/xgboost-optimized-gradient-boosting-e3d7b32d27b1>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Ensemble, Boosting</td> </tr> <tr> <td>46</td> <td>LightGBM vs XGBoost Comparison</td> <td><a href=https://www.analyticsvidhya.com/blog/2019/06/lightgbm-vs-xgboost/ >Analytics Vidhya</a></td> <td>Google, Amazon</td> <td>Medium</td> <td>Ensemble, Boosting</td> </tr> <tr> <td>47</td> <td>CatBoost: Handling Categorical Features</td> <td><a href=https://towardsdatascience.com/catboost-for-beginners-d68638b78982>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Ensemble, Categorical Data</td> </tr> <tr> <td>48</td> <td>Time Series Forecasting with ARIMA</td> <td><a href=https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Time Series, Forecasting</td> </tr> <tr> <td>49</td> <td>Time Series Forecasting with LSTM</td> <td><a href=https://towardsdatascience.com/time-series-forecasting-using-lstm-3c6a39bfae39>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Time Series, Deep Learning</td> </tr> <tr> <td>50</td> <td>Robust Scaling Techniques</td> <td><a href=https://towardsdatascience.com/robust-scaling-why-when-and-how-3f2a67f1b0a3>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Data Preprocessing</td> </tr> <tr> <td>51</td> <td>Data Imputation Techniques in ML</td> <td><a href=https://machinelearningmastery.com/handle-missing-data-python/ >Machine Learning Mastery</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Data Preprocessing</td> </tr> <tr> <td>52</td> <td>Handling Imbalanced Datasets: SMOTE and Others</td> <td><a href=https://towardsdatascience.com/smote-oversampling-for-imbalanced-classification-6c2046f13447>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Data Preprocessing, Classification</td> </tr> <tr> <td>53</td> <td>Bias in Machine Learning: Fairness and Ethics</td> <td><a href=https://towardsdatascience.com/fairness-in-machine-learning-6e21a5c4d5db>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Ethics, Fairness</td> </tr> <tr> <td>54</td> <td>Model Deployment: From Prototype to Production</td> <td><a href=https://towardsdatascience.com/deploying-machine-learning-models-3f6e41013240>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Deployment</td> </tr> <tr> <td>55</td> <td>Online Learning Algorithms</td> <td><a href=https://towardsdatascience.com/online-learning-algorithms-40bf6c6d19de>Towards Data Science</a></td> <td>Google, Amazon, Microsoft</td> <td>Hard</td> <td>Online Learning</td> </tr> <tr> <td>56</td> <td>Concept Drift in Machine Learning</td> <td><a href=https://towardsdatascience.com/concept-drift-in-machine-learning-6b97e0f3f42d>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Model Maintenance</td> </tr> <tr> <td>57</td> <td>Transfer Learning in NLP: BERT, GPT</td> <td><a href=https://towardsdatascience.com/transfer-learning-in-nlp-9f26f10b2b96>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>58</td> <td>Natural Language Processing: Text Preprocessing</td> <td><a href=https://www.analyticsvidhya.com/blog/2020/07/text-preprocessing-techniques-in-python/ >Analytics Vidhya</a></td> <td>Google, Amazon, Facebook</td> <td>Easy</td> <td>NLP, Data Preprocessing</td> </tr> <tr> <td>59</td> <td>Text Vectorization: TF-IDF vs Word2Vec</td> <td><a href=https://towardsdatascience.com/text-vectorization-methods-6fd1d1a74a66>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>NLP, Feature Extraction</td> </tr> <tr> <td>60</td> <td>Transformer Architecture and Self-Attention</td> <td><a href=https://towardsdatascience.com/transformers-141e32e69591>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>61</td> <td>Understanding BERT for NLP Tasks</td> <td><a href=https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>62</td> <td>Understanding GPT Models</td> <td><a href=https://towardsdatascience.com/what-is-gpt-3-and-why-is-it-so-important-95b9acb9d0a3>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>63</td> <td>Data Augmentation Techniques in ML</td> <td><a href=https://towardsdatascience.com/data-augmentation-for-deep-learning-8e2f37e59a1b>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Data Preprocessing</td> </tr> <tr> <td>64</td> <td>Adversarial Machine Learning: Attack and Defense</td> <td><a href=https://towardsdatascience.com/adversarial-attacks-on-machine-learning-models-8a91b4a6a9a3>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Security, ML</td> </tr> <tr> <td>65</td> <td>Explainable AI (XAI) in Practice</td> <td><a href=https://towardsdatascience.com/explainable-ai-a-survey-of-methods-4d9e35597b0c>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Model Interpretability</td> </tr> <tr> <td>66</td> <td>Federated Learning: Concepts and Challenges</td> <td><a href=https://towardsdatascience.com/federated-learning-explained-d9e99d16ef57>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Distributed Learning</td> </tr> <tr> <td>67</td> <td>Multi-Task Learning in Neural Networks</td> <td><a href=https://towardsdatascience.com/multi-task-learning-for-neural-networks-6e4e2fcb5d3a>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Multi-Task</td> </tr> <tr> <td>68</td> <td>Metric Learning and Siamese Networks</td> <td><a href=https://towardsdatascience.com/siamese-networks-for-one-shot-learning-60b2c8c9b71>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Metric Learning</td> </tr> <tr> <td>69</td> <td>Deep Reinforcement Learning: DQN Overview</td> <td><a href=https://towardsdatascience.com/deep-q-learning-dqn-1b5f8bb83d11>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Reinforcement Learning, Deep Learning</td> </tr> <tr> <td>70</td> <td>Policy Gradient Methods in Reinforcement Learning</td> <td><a href=https://towardsdatascience.com/policy-gradient-methods-in-reinforcement-learning-713f77dceb79>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Reinforcement Learning</td> </tr> <tr> <td>71</td> <td>Actor-Critic Methods in RL</td> <td><a href=https://towardsdatascience.com/actor-critic-methods-in-reinforcement-learning-49cfa6403a5e>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Reinforcement Learning</td> </tr> <tr> <td>72</td> <td>Monte Carlo Methods in Machine Learning</td> <td><a href=https://towardsdatascience.com/monte-carlo-methods-in-machine-learning-8f7f0e9ad0e9>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Optimization, Probabilistic Methods</td> </tr> <tr> <td>73</td> <td>Expectation-Maximization Algorithm</td> <td><a href=https://towardsdatascience.com/expectation-maximization-algorithm-for-gaussian-mixture-models-ef96d0e98729>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Clustering, Probabilistic Models</td> </tr> <tr> <td>74</td> <td>Gaussian Mixture Models (GMM)</td> <td><a href=https://towardsdatascience.com/gaussian-mixture-models-in-python-6b85679b5a4>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Clustering, Probabilistic Models</td> </tr> <tr> <td>75</td> <td>Bayesian Inference in ML</td> <td><a href=https://towardsdatascience.com/introduction-to-bayesian-inference-7f72a56c97c>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Bayesian Methods</td> </tr> <tr> <td>76</td> <td>Markov Chain Monte Carlo (MCMC) Methods</td> <td><a href=https://towardsdatascience.com/markov-chain-monte-carlo-methods-a-tutorial-d3e4a14c6a1f>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Bayesian Methods, Probabilistic Models</td> </tr> <tr> <td>77</td> <td>Variational Autoencoders (VAEs)</td> <td><a href=https://towardsdatascience.com/variational-autoencoders-explained-8f7f0e9ad0e9>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Generative Models</td> </tr> <tr> <td>78</td> <td>Generative Adversarial Networks (GANs)</td> <td><a href=https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Generative Models</td> </tr> <tr> <td>79</td> <td>Conditional GANs for Data Generation</td> <td><a href=https://towardsdatascience.com/conditional-gans-explained-9f2b30d3e5e3>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Generative Models</td> </tr> <tr> <td>80</td> <td>Sequence-to-Sequence Models in NLP</td> <td><a href=https://towardsdatascience.com/sequence-to-sequence-models-for-machine-translation-873b51b65f0f>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>81</td> <td>Attention Mechanisms in Seq2Seq Models</td> <td><a href=https://towardsdatascience.com/attention-mechanisms-in-deep-learning-a-tutorial-3d9b62f341d>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>NLP, Deep Learning</td> </tr> <tr> <td>82</td> <td>Capsule Networks: An Introduction</td> <td><a href=https://towardsdatascience.com/capsule-networks-an-introduction-4d2b2a7dbd5>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Neural Networks</td> </tr> <tr> <td>83</td> <td>Self-Supervised Learning in Deep Learning</td> <td><a href=https://towardsdatascience.com/self-supervised-learning-explained-7e0e4a2f8b8>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Unsupervised Learning</td> </tr> <tr> <td>84</td> <td>Zero-Shot and Few-Shot Learning</td> <td><a href=https://towardsdatascience.com/zero-shot-learning-in-deep-learning-8f3e8c8e9a2b>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Transfer Learning</td> </tr> <tr> <td>85</td> <td>Meta-Learning: Learning to Learn</td> <td><a href=https://towardsdatascience.com/meta-learning-what-is-it-and-why-it-matters-7a1a1e9d9e3>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Deep Learning, Optimization</td> </tr> <tr> <td>86</td> <td>Hyperparameter Sensitivity Analysis</td> <td><a href=https://towardsdatascience.com/hyperparameter-sensitivity-analysis-123456789>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Hyperparameter Tuning</td> </tr> <tr> <td>87</td> <td>High-Dimensional Feature Selection Techniques</td> <td><a href=https://towardsdatascience.com/feature-selection-methods-abc123>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Feature Engineering, Dimensionality Reduction</td> </tr> <tr> <td>88</td> <td>Multi-Label Classification Techniques</td> <td><a href=https://towardsdatascience.com/multi-label-classification-methods-456def>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Classification, Multi-Output</td> </tr> <tr> <td>89</td> <td>Ordinal Regression in Machine Learning</td> <td><a href=https://towardsdatascience.com/ordinal-regression-explained-789ghi>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Regression, Classification</td> </tr> <tr> <td>90</td> <td>Survival Analysis in ML</td> <td><a href=https://towardsdatascience.com/survival-analysis-in-machine-learning-abc789>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Statistics, ML</td> </tr> <tr> <td>91</td> <td>Semi-Supervised Learning Methods</td> <td><a href=https://towardsdatascience.com/semi-supervised-learning-101-123abc>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Unsupervised Learning, ML Basics</td> </tr> <tr> <td>92</td> <td>Unsupervised Feature Learning</td> <td><a href=https://towardsdatascience.com/unsupervised-feature-learning-abc456>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Unsupervised Learning, Feature Extraction</td> </tr> <tr> <td>93</td> <td>Clustering Evaluation Metrics: Silhouette, Davies-Bouldin</td> <td><a href=https://towardsdatascience.com/clustering-evaluation-metrics-789jkl>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Clustering, Evaluation</td> </tr> <tr> <td>94</td> <td>Dimensionality Reduction: t-SNE and UMAP</td> <td><a href=https://towardsdatascience.com/t-sne-and-umap-789mno>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Dimensionality Reduction</td> </tr> <tr> <td>95</td> <td>Probabilistic Graphical Models: Bayesian Networks</td> <td><a href=https://towardsdatascience.com/bayesian-networks-123jkl>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Probabilistic Models, Graphical Models</td> </tr> <tr> <td>96</td> <td>Hidden Markov Models (HMMs) in ML</td> <td><a href=https://towardsdatascience.com/hidden-markov-models-101-456mno>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Probabilistic Models, Sequence Modeling</td> </tr> <tr> <td>97</td> <td>Recommender Systems: Collaborative Filtering</td> <td><a href=https://towardsdatascience.com/collaborative-filtering-789pqr>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Recommender Systems</td> </tr> <tr> <td>98</td> <td>Recommender Systems: Content-Based Filtering</td> <td><a href=https://towardsdatascience.com/content-based-recommender-systems-123stu>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Recommender Systems</td> </tr> <tr> <td>99</td> <td>Anomaly Detection in Time Series Data</td> <td><a href=https://towardsdatascience.com/anomaly-detection-in-time-series-data-456vwx>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Hard</td> <td>Time Series, Anomaly Detection</td> </tr> <tr> <td>100</td> <td>Optimization Algorithms Beyond Gradient Descent (Adam, RMSProp, etc.)</td> <td><a href=https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6f8eb4c5b0e4>Towards Data Science</a></td> <td>Google, Amazon, Facebook</td> <td>Medium</td> <td>Optimization, Deep Learning</td> </tr> </tbody> </table> <hr> <h2 id=questions-asked-in-google-interview>Questions asked in Google interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Gradient Descent </li> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Reinforcement Learning Basics </li> <li>Hyperparameter Tuning </li> <li>Transfer Learning </li> </ul> <h2 id=questions-asked-in-facebook-interview>Questions asked in Facebook interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Support Vector Machines (SVM) </li> <li>k-Nearest Neighbors (k-NN) </li> <li>Feature Engineering </li> <li>Dropout in Neural Networks </li> <li>Backpropagation </li> </ul> <h2 id=questions-asked-in-amazon-interview>Questions asked in Amazon interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Regularization Techniques (L1, L2) </li> <li>Cross-Validation </li> <li>Overfitting and Underfitting </li> <li>Decision Trees </li> <li>Ensemble Learning: Bagging and Boosting </li> <li>Random Forest </li> <li>Support Vector Machines (SVM) </li> <li>Neural Networks: Basics </li> <li>Hyperparameter Tuning </li> <li>ROC Curve and AUC </li> <li>Logistic Regression </li> <li>Data Normalization and Standardization </li> <li>k-Means Clustering </li> </ul> <h2 id=questions-asked-in-microsoft-interview>Questions asked in Microsoft interview</h2> <ul> <li>Regularization Techniques (L1, L2) </li> <li>Gradient Descent </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Support Vector Machines (SVM) </li> <li>Hyperparameter Tuning </li> <li>ROC Curve and AUC </li> <li>Loss Functions in ML </li> <li>Learning Rate in Optimization </li> <li>Bayesian Optimization for Hyperparameters </li> </ul> <h2 id=questions-asked-in-uber-interview>Questions asked in Uber interview</h2> <ul> <li>Reinforcement Learning Basics </li> <li>Anomaly Detection </li> <li>Gradient Descent Variants </li> <li>Model Deployment: From Prototype to Production </li> </ul> <h2 id=questions-asked-in-swiggy-interview>Questions asked in Swiggy interview</h2> <ul> <li>Handling Missing Data </li> <li>Data Imputation Techniques in ML </li> <li>Feature Engineering </li> <li>Model Interpretability: SHAP and LIME </li> </ul> <h2 id=questions-asked-in-flipkart-interview>Questions asked in Flipkart interview</h2> <ul> <li>Ensemble Methods: Stacking and Blending </li> <li>Time Series Forecasting with ARIMA </li> <li>Time Series Forecasting with LSTM </li> <li>Model Deployment: From Prototype to Production </li> </ul> <h2 id=questions-asked-in-ola-interview>Questions asked in Ola interview</h2> <ul> <li>Time Series Forecasting with LSTM </li> <li>Data Normalization and Standardization </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> </ul> <h2 id=questions-asked-in-paytm-interview>Questions asked in Paytm interview</h2> <ul> <li>Model Deployment: From Prototype to Production </li> <li>Online Learning Algorithms </li> <li>Handling Imbalanced Datasets: SMOTE and Others </li> </ul> <h2 id=questions-asked-in-oyo-interview>Questions asked in OYO interview</h2> <ul> <li>Data Preprocessing Techniques </li> <li>Ensemble Learning: Bagging and Boosting </li> <li>Regularization Techniques (L1, L2) </li> </ul> <h2 id=questions-asked-in-whatsapp-interview>Questions asked in WhatsApp interview</h2> <ul> <li>Neural Networks: Basics </li> <li>Convolutional Neural Networks (CNNs) </li> <li>Recurrent Neural Networks (RNNs) and LSTMs </li> <li>Dropout in Neural Networks </li> </ul> <h2 id=questions-asked-in-slack-interview>Questions asked in Slack interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Cross-Validation </li> <li>Feature Engineering </li> <li>Transfer Learning </li> </ul> <h2 id=questions-asked-in-airbnb-interview>Questions asked in Airbnb interview</h2> <ul> <li>Bias-Variance Tradeoff </li> <li>Hyperparameter Tuning </li> <li>Transfer Learning </li> <li>Model Interpretability: SHAP and LIME </li> </ul> <hr> <p><em>Note:</em> The practice links are curated from reputable sources such as Machine Learning Mastery, Towards Data Science, Analytics Vidhya, and Scikit-learn. You can update/contribute to these lists or add new ones as more resources become available.</p> <hr> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2020 - <script>document.write(/\d{4}/.exec(Date())[0])</script> ‚Ä¢ <strong>Kuldeep Singh Sidhu</strong> ‚Ä¢ <u><a href=https://choosealicense.com/licenses/agpl-3.0/ target=‚Äù_blank‚Äù>License</a></u> ‚Ä¢ <u><a href=/privacy>Privacy Policy</a></u> ‚Ä¢ <u><a href=/contact>Contact</a></u> ‚Ä¢ </div> </div> <div class=md-social> <a href=https://github.com/singhsidhukuldeep/ target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.01 8.01 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27s-1.36.09-2 .27c-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8"/></svg> </a> <a href=https://linkedin.com/in/singhsidhukuldeep target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://twitter.com/kuldeep_s_s target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> <a href=https://stackoverflow.com/u/7182350/ target=_blank rel=noopener title=stackoverflow.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 300zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-200v39.7h200zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg> </a> <a href=https://huggingface.co/singhsidhukuldeep target=_blank rel=noopener title=huggingface.co class=md-social__link> <svg width=500 height=463 viewbox="0 0 500 463" fill=none xmlns=http://www.w3.org/2000/svg> <path fill=white d="M496.592 369.699C500.563 381.093 499.61 393.227 494.315 403.778C490.503 411.48 485.05 417.441 478.379 422.769C470.331 429.099 460.324 434.48 448.253 439.65C433.852 445.77 416.274 451.52 408.226 453.63C387.63 458.958 367.829 462.334 347.762 462.493C319.066 462.756 294.34 456.004 276.762 438.753C267.656 439.861 258.443 440.494 249.178 440.494C240.389 440.494 231.706 439.967 223.076 438.912C205.445 456.057 180.825 462.756 152.234 462.493C132.168 462.334 112.366 458.958 91.7177 453.63C83.7229 451.52 66.145 445.77 51.7439 439.65C39.6723 434.48 29.6656 429.099 21.6708 422.769C14.9467 417.441 9.49334 411.48 5.68127 403.778C0.439661 393.227 -0.566304 381.093 3.45755 369.699C-0.248631 360.994 -1.20165 351.024 1.71035 339.998C3.03399 334.987 5.20476 330.344 7.95792 326.229C7.37552 324.067 6.89901 321.851 6.58134 319.424C4.56941 304.97 9.59923 291.781 19.0765 281.547C23.7357 276.43 28.7655 272.895 34.0071 270.627C30.1421 254.273 28.1302 237.445 28.1302 220.247C28.1302 98.5969 127.085 0 249.178 0C291.111 0 330.343 11.6058 363.805 31.8633C369.84 35.5561 375.77 39.5126 381.436 43.7329C384.242 45.8431 387.048 48.006 389.748 50.2744C392.501 52.49 395.201 54.8112 397.796 57.1851C405.632 64.3069 412.991 71.9562 419.715 80.133C421.992 82.8235 424.163 85.6194 426.28 88.4681C430.569 94.1128 434.54 99.9685 438.193 106.035C443.752 115.109 448.623 124.604 452.859 134.469C455.665 141.064 458.101 147.816 460.271 154.727C463.501 165.067 465.99 175.723 467.684 186.696C468.213 190.336 468.69 194.028 469.06 197.721C469.802 205.107 470.225 212.598 470.225 220.247C470.225 237.234 468.213 253.904 464.454 269.994C470.278 272.262 475.784 275.955 480.92 281.547C490.397 291.781 495.427 305.022 493.415 319.477C493.098 321.851 492.621 324.067 492.039 326.229C494.792 330.344 496.963 334.987 498.286 339.998C501.198 351.024 500.245 360.994 496.592 369.699Z"/> <path fill=black d="M433.839 221.75C433.839 120.838 351.531 39.0323 250 39.0323C148.469 39.0323 66.1613 120.838 66.1613 221.75C66.1613 322.662 148.469 404.468 250 404.468C351.531 404.468 433.839 322.662 433.839 221.75ZM45 221.75C45 109.222 136.782 18 250 18C363.218 18 455 109.222 455 221.75C455 334.278 363.218 425.5 250 425.5C136.782 425.5 45 334.278 45 221.75Z"/> <path fill=white d="M250 405.5C352.173 405.5 435 323.232 435 221.75C435 120.268 352.173 38 250 38C147.827 38 65 120.268 65 221.75C65 323.232 147.827 405.5 250 405.5Z"/> <path fill=white d="M202.198 404.174C216.789 383.118 215.755 367.316 195.735 347.627C175.715 327.943 164.062 299.145 164.062 299.145C164.062 299.145 159.709 282.419 149.794 283.958C139.88 285.497 132.6 310.492 153.368 325.783C174.135 341.069 149.232 351.456 141.242 337.099C133.252 322.741 111.435 285.831 100.121 278.772C88.8117 271.713 80.8483 275.668 83.5151 290.218C86.182 304.769 133.48 340.036 128.878 347.668C124.276 355.296 108.058 338.7 108.058 338.7C108.058 338.7 57.3079 293.255 46.2587 305.097C35.2096 316.94 54.641 326.863 82.3328 343.359C110.03 359.85 112.177 364.206 108.248 370.446C104.314 376.685 43.1836 325.971 37.4417 347.47C31.705 368.969 99.8291 375.209 95.6247 390.051C91.4203 404.899 47.6372 361.958 38.6823 378.689C29.7221 395.425 100.465 415.088 101.038 415.234C123.889 421.067 181.924 433.426 202.198 404.174Z"/> <path fill=black d="M90.9935 255C82.4744 255 74.8603 258.477 69.551 264.784C66.2675 268.69 62.8367 274.986 62.5578 284.414C58.985 283.394 55.5489 282.824 52.3391 282.824C44.183 282.824 36.8163 285.93 31.6069 291.573C24.9137 298.815 21.9407 307.715 23.2351 316.62C23.8508 320.861 25.2768 324.663 27.4079 328.182C22.9142 331.795 19.6044 336.826 18.0047 342.876C16.7524 347.619 15.4685 357.497 22.1722 367.673C21.746 368.337 21.3461 369.027 20.9725 369.733C16.9418 377.336 16.684 385.927 20.2411 393.928C25.6346 406.054 39.0368 415.608 65.0625 425.863C81.2536 432.242 96.0661 436.321 96.1976 436.357C117.603 441.874 136.962 444.677 153.721 444.677C184.525 444.677 206.578 435.301 219.27 416.811C239.697 387.036 236.776 359.803 210.346 333.552C195.717 319.026 185.993 297.607 183.967 292.906C179.884 278.986 169.086 263.513 151.138 263.513H151.133C149.622 263.513 148.096 263.633 146.592 263.869C138.73 265.097 131.858 269.595 126.949 276.361C121.65 269.814 116.504 264.606 111.847 261.667C104.827 257.243 97.813 255 90.9935 255ZM90.9935 275.917C93.6771 275.917 96.9553 277.051 100.57 279.331C111.794 286.406 133.452 323.403 141.382 337.793C144.039 342.614 148.581 344.654 152.669 344.654C160.783 344.654 167.118 336.638 153.411 326.451C132.8 311.124 140.03 286.072 149.87 284.529C150.301 284.461 150.727 284.43 151.138 284.43C160.083 284.43 164.03 299.751 164.03 299.751C164.03 299.751 175.595 328.616 195.465 348.346C215.334 368.08 216.36 383.919 201.879 405.024C192.002 419.415 173.096 421.292 153.721 421.292C133.626 421.292 112.99 417.772 101.445 414.796C100.877 414.65 30.7019 396.255 39.5946 379.48C41.089 376.661 43.5516 375.532 46.6509 375.532C59.1744 375.532 81.9535 394.054 91.746 394.054C93.935 394.054 95.5662 392.371 96.1976 390.112C100.555 374.522 32.6646 369.738 38.3633 348.189C39.3683 344.377 42.094 342.829 45.9248 342.834C62.4737 342.834 99.6021 371.756 107.385 371.756C107.979 371.756 108.405 371.584 108.637 371.218C112.536 364.964 110.74 359.872 83.257 343.343C55.7738 326.808 36.1428 317.588 47.114 305.718C48.3768 304.347 50.1659 303.741 52.3391 303.741C69.0248 303.746 108.447 339.398 108.447 339.398C108.447 339.398 119.087 350.395 125.523 350.395C127.001 350.395 128.259 349.815 129.111 348.382C133.673 340.737 86.7366 305.388 84.0898 290.804C82.2955 280.921 85.3474 275.917 90.9935 275.917Z"/> <path fill=white d="M296.9 404.174C282.31 383.118 283.343 367.316 303.363 347.627C323.383 327.943 335.037 299.145 335.037 299.145C335.037 299.145 339.39 282.419 349.304 283.958C359.219 285.497 366.498 310.492 345.731 325.783C324.963 341.069 349.866 351.456 357.856 337.099C365.846 322.741 387.663 285.831 398.978 278.772C410.287 271.713 418.25 275.668 415.583 290.218C412.916 304.769 365.618 340.036 370.22 347.668C374.822 355.296 391.041 338.7 391.041 338.7C391.041 338.7 441.791 293.255 452.84 305.097C463.889 316.94 444.457 326.863 416.766 343.359C389.068 359.85 386.921 364.206 390.85 370.446C394.784 376.685 455.915 325.971 461.657 347.47C467.393 368.969 399.269 375.209 403.474 390.051C407.678 404.899 451.461 361.958 460.416 378.689C469.376 395.425 398.633 415.088 398.06 415.234C375.209 421.067 317.175 433.426 296.9 404.174Z"/> <path fill=black d="M408.105 255C416.624 255 424.238 258.477 429.547 264.784C432.831 268.69 436.262 274.986 436.541 284.414C440.113 283.394 443.549 282.824 446.759 282.824C454.915 282.824 462.282 285.93 467.491 291.573C474.185 298.815 477.158 307.715 475.863 316.62C475.248 320.861 473.822 324.663 471.69 328.182C476.184 331.795 479.494 336.826 481.094 342.876C482.346 347.619 483.63 357.497 476.926 367.673C477.352 368.337 477.752 369.027 478.126 369.733C482.157 377.336 482.414 385.927 478.857 393.928C473.464 406.054 460.062 415.608 434.036 425.863C417.845 432.242 403.032 436.321 402.901 436.357C381.495 441.874 362.136 444.677 345.377 444.677C314.573 444.677 292.52 435.301 279.829 416.811C259.402 387.036 262.322 359.803 288.753 333.552C303.381 319.026 313.105 297.607 315.131 292.906C319.214 278.986 330.012 263.513 347.961 263.513H347.966C349.476 263.513 351.002 263.633 352.507 263.869C360.368 265.097 367.24 269.595 372.15 276.361C377.449 269.814 382.595 264.606 387.252 261.667C394.271 257.243 401.285 255 408.105 255ZM408.105 275.917C405.421 275.917 402.143 277.051 398.528 279.331C387.304 286.406 365.646 323.403 357.716 337.793C355.059 342.614 350.518 344.654 346.429 344.654C338.315 344.654 331.98 336.638 345.687 326.451C366.299 311.124 359.069 286.072 349.229 284.529C348.797 284.461 348.371 284.43 347.961 284.43C339.015 284.43 335.069 299.751 335.069 299.751C335.069 299.751 323.503 328.616 303.634 348.346C283.764 368.08 282.738 383.919 297.219 405.024C307.096 419.415 326.002 421.292 345.377 421.292C365.472 421.292 386.108 417.772 397.653 414.796C398.221 414.65 468.397 396.255 459.504 379.48C458.009 376.661 455.547 375.532 452.447 375.532C439.924 375.532 417.145 394.054 407.352 394.054C405.163 394.054 403.532 392.371 402.901 390.112C398.543 374.522 466.434 369.738 460.735 348.189C459.73 344.377 457.004 342.829 453.174 342.834C436.625 342.834 399.496 371.756 391.714 371.756C391.119 371.756 390.693 371.584 390.461 371.218C386.562 364.964 388.358 359.872 415.841 343.343C443.325 326.808 462.956 317.588 451.984 305.718C450.722 304.347 448.932 303.741 446.759 303.741C430.074 303.746 390.651 339.398 390.651 339.398C390.651 339.398 380.011 350.395 373.576 350.395C372.097 350.395 370.84 349.815 369.987 348.382C365.425 340.737 412.362 305.388 415.009 290.804C416.803 280.921 413.751 275.917 408.105 275.917Z"/> <path fill=#0E1116 d="M319.277 228.901C319.277 205.236 288.585 241.304 250.637 241.465C212.692 241.306 182 205.238 182 228.901C182 244.591 189.507 270.109 209.669 285.591C213.681 271.787 235.726 260.729 238.877 262.317C243.364 264.578 243.112 270.844 250.637 276.365C258.163 270.844 257.911 264.58 262.398 262.317C265.551 260.729 287.594 271.787 291.605 285.591C311.767 270.109 319.275 244.591 319.275 228.903L319.277 228.901Z"/> <path fill=#FF323D d="M262.4 262.315C257.913 264.576 258.165 270.842 250.639 276.363C243.114 270.842 243.366 264.578 238.879 262.315C235.726 260.727 213.683 271.785 209.672 285.589C219.866 293.417 233.297 298.678 250.627 298.806C250.631 298.806 250.635 298.806 250.641 298.806C250.646 298.806 250.65 298.806 250.656 298.806C267.986 298.68 281.417 293.417 291.611 285.589C287.6 271.785 265.555 260.727 262.404 262.315H262.4Z"/> <path fill=black d="M373 196C382.389 196 390 188.389 390 179C390 169.611 382.389 162 373 162C363.611 162 356 169.611 356 179C356 188.389 363.611 196 373 196Z"/> <path fill=black d="M128 196C137.389 196 145 188.389 145 179C145 169.611 137.389 162 128 162C118.611 162 111 169.611 111 179C111 188.389 118.611 196 128 196Z"/> <path fill=#0E1116 d="M313.06 171.596C319.796 173.968 322.476 187.779 329.281 184.171C342.167 177.337 347.06 161.377 340.208 148.524C333.356 135.671 317.354 130.792 304.467 137.626C291.58 144.46 286.688 160.419 293.54 173.272C296.774 179.339 307.039 169.475 313.06 171.596Z"/> <path fill=#0E1116 d="M188.554 171.596C181.818 173.968 179.138 187.779 172.334 184.171C159.447 177.337 154.555 161.377 161.407 148.524C168.259 135.671 184.26 130.792 197.147 137.626C210.034 144.46 214.926 160.419 208.074 173.272C204.84 179.339 194.575 169.475 188.554 171.596Z"/> </svg> </a> <a href=http://kuldeepsinghsidhu.com target=_blank rel=noopener title=kuldeepsinghsidhu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0M5.78 8.75a9.64 9.64 0 0 0 1.363 4.177q.383.64.857 1.215c.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a10 10 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.51 6.51 0 0 0 4.666 5.5q-.184-.271-.352-.552c-.715-1.192-1.437-2.874-1.581-4.948m-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948q.18-.295.353-.552a6.51 6.51 0 0 0-4.666 5.5m10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948q-.18.296-.353.552a6.51 6.51 0 0 0 4.666-5.5Zm2.733-1.5a6.51 6.51 0 0 0-4.666-5.5q.184.272.353.552c.714 1.192 1.436 2.874 1.58 4.948Z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "search.highlight", "search.share", "search.suggest", "content.tooltips", "navigation.instant.progress", "navigation.path", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../javascripts/xfile.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>
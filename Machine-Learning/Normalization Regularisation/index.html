<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Comprehensive guide to feature normalization and model regularization techniques with mathematical foundations, implementations, and interview questions."><meta name=author content="Kuldeep Singh Sidhu"><link href=../Normal%20Distribution/ rel=prev><link href=../Overfitting%2C%20Underfitting/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.50"><title>Normalization and Regularisation - Data Science Interview preparation</title><link rel=stylesheet href=../../assets/stylesheets/main.a40c8224.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-EVGNTG49J7"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-EVGNTG49J7",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-EVGNTG49J7",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#normalization-and-regularisation class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <!-- Add announcement here, including arbitrary HTML --> üëÄ This project is in early stages of development. <strong> ü§ó Please <a href=/Contribute>contribute content</a> if possible! ü§ù</strong><br> <small>ü´µ You can <b> <a href=/Contribute>SUBMIT</a></b> simple text/markdown content, I will format it! üôå</small> <meta name=google-adsense-account content=ca-pub-4988388949365963> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4988388949365963" crossorigin=anonymous></script> </div> </aside> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Data Science Interview preparation" class="md-header__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Science Interview preparation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Normalization and Regularisation </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=deep-purple data-md-color-accent=purple aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Data Science Interview preparation" class="md-nav__button md-logo" aria-label="Data Science Interview preparation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> Data Science Interview preparation </label> <div class=md-nav__source> <a href=https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> singhsidhukuldeep/singhsidhukuldeep.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> üè° Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> üë®üèø‚Äçüè´ Interview Questions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> üë®üèø‚Äçüè´ Interview Questions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../flashcards/ class=md-nav__link> <span class=md-ellipsis> üìá Flashcards </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Interview-Question-Resources/ class=md-nav__link> <span class=md-ellipsis> üìö Interview Question Resources </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/data-structures-algorithms/ class=md-nav__link> <span class=md-ellipsis> DSA (Data Structures & Algorithms) </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Machine-Learning/ class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/System-design/ class=md-nav__link> <span class=md-ellipsis> System Design </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Natural-Language-Processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing (NLP) </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Probability/ class=md-nav__link> <span class=md-ellipsis> Probability </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/AB-testing/ class=md-nav__link> <span class=md-ellipsis> A/B Testing </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/SQL-Interview-Questions/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/Scikit-Learn/ class=md-nav__link> <span class=md-ellipsis> Scikit-Learn </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/LangChain/ class=md-nav__link> <span class=md-ellipsis> LangChain </span> </a> </li> <li class=md-nav__item> <a href=../../Interview-Questions/LangGraph/ class=md-nav__link> <span class=md-ellipsis> LangGraph </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> üìù Cheat Sheets </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> üìù Cheat Sheets </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Cheat-Sheets/Django/ class=md-nav__link> <span class=md-ellipsis> Django </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Flask/ class=md-nav__link> <span class=md-ellipsis> Flask </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Hypothesis-Tests/ class=md-nav__link> <span class=md-ellipsis> Hypothesis Tests </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Keras/ class=md-nav__link> <span class=md-ellipsis> Keras </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/NumPy/ class=md-nav__link> <span class=md-ellipsis> NumPy </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Pandas/ class=md-nav__link> <span class=md-ellipsis> Pandas </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PySpark/ class=md-nav__link> <span class=md-ellipsis> PySpark </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/PyTorch/ class=md-nav__link> <span class=md-ellipsis> PyTorch </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Python/ class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/RegEx/ class=md-nav__link> <span class=md-ellipsis> Regular Expressions (RegEx) </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/Sk-learn/ class=md-nav__link> <span class=md-ellipsis> Scikit Learn </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/SQL/ class=md-nav__link> <span class=md-ellipsis> SQL </span> </a> </li> <li class=md-nav__item> <a href=../../Cheat-Sheets/tensorflow/ class=md-nav__link> <span class=md-ellipsis> TensorFlow </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> ‚Äçüéì ML Topics </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> ‚Äçüéì ML Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ARIMA/ class=md-nav__link> <span class=md-ellipsis> ARIMA </span> </a> </li> <li class=md-nav__item> <a href=../Activation%20functions/ class=md-nav__link> <span class=md-ellipsis> Activation functions </span> </a> </li> <li class=md-nav__item> <a href=../Collaborative%20Filtering/ class=md-nav__link> <span class=md-ellipsis> Collaborative Filtering </span> </a> </li> <li class=md-nav__item> <a href=../Confusion%20Matrix/ class=md-nav__link> <span class=md-ellipsis> Confusion Matrix </span> </a> </li> <li class=md-nav__item> <a href=../DBSCAN/ class=md-nav__link> <span class=md-ellipsis> DBSCAN </span> </a> </li> <li class=md-nav__item> <a href=../Decision%20Trees/ class=md-nav__link> <span class=md-ellipsis> Decision Trees </span> </a> </li> <li class=md-nav__item> <a href=../Gradient%20Boosting/ class=md-nav__link> <span class=md-ellipsis> Gradient Boosting </span> </a> </li> <li class=md-nav__item> <a href=../K-means%20clustering/ class=md-nav__link> <span class=md-ellipsis> K-means clustering </span> </a> </li> <li class=md-nav__item> <a href=../Linear%20Regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class=md-nav__item> <a href=../Logistic%20Regression/ class=md-nav__link> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../Loss%20Function%20MAE%2C%20RMSE/ class=md-nav__link> <span class=md-ellipsis> Loss Function MAE, RMSE </span> </a> </li> <li class=md-nav__item> <a href=../Neural%20Networks/ class=md-nav__link> <span class=md-ellipsis> Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../Normal%20Distribution/ class=md-nav__link> <span class=md-ellipsis> Normal Distribution </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Normalization Regularisation </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Normalization Regularisation </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> üìä Summary </span> </a> <nav class=md-nav aria-label="üìä Summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization class=md-nav__link> <span class=md-ellipsis> Normalization </span> </a> </li> <li class=md-nav__item> <a href=#regularisation class=md-nav__link> <span class=md-ellipsis> Regularisation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#intuition class=md-nav__link> <span class=md-ellipsis> üß† Intuition </span> </a> <nav class=md-nav aria-label="üß† Intuition"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-intuition class=md-nav__link> <span class=md-ellipsis> Normalization Intuition </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-intuition class=md-nav__link> <span class=md-ellipsis> Regularisation Intuition </span> </a> </li> <li class=md-nav__item> <a href=#mathematical-foundation class=md-nav__link> <span class=md-ellipsis> Mathematical Foundation </span> </a> <nav class=md-nav aria-label="Mathematical Foundation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-normalization-techniques class=md-nav__link> <span class=md-ellipsis> 1. Normalization Techniques </span> </a> </li> <li class=md-nav__item> <a href=#2-regularisation-mathematics class=md-nav__link> <span class=md-ellipsis> 2. Regularisation Mathematics </span> </a> </li> <li class=md-nav__item> <a href=#3-effect-on-gradients class=md-nav__link> <span class=md-ellipsis> 3. Effect on Gradients </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#implementation-using-libraries class=md-nav__link> <span class=md-ellipsis> üõ†Ô∏è Implementation using Libraries </span> </a> <nav class=md-nav aria-label="üõ†Ô∏è Implementation using Libraries"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-with-scikit-learn class=md-nav__link> <span class=md-ellipsis> Normalization with Scikit-learn </span> </a> </li> <li class=md-nav__item> <a href=#advanced-normalization-techniques class=md-nav__link> <span class=md-ellipsis> Advanced Normalization Techniques </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-implementation class=md-nav__link> <span class=md-ellipsis> Regularisation Implementation </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-path-analysis class=md-nav__link> <span class=md-ellipsis> Regularisation Path Analysis </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#from-scratch-implementation class=md-nav__link> <span class=md-ellipsis> Œª From Scratch Implementation </span> </a> <nav class=md-nav aria-label="Œª From Scratch Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#custom-scalers-implementation class=md-nav__link> <span class=md-ellipsis> Custom Scalers Implementation </span> </a> </li> <li class=md-nav__item> <a href=#custom-regularised-regression class=md-nav__link> <span class=md-ellipsis> Custom Regularised Regression </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Œª Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Œª Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Normalization Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Normalization Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-assumptions class=md-nav__link> <span class=md-ellipsis> Key Assumptions </span> </a> </li> <li class=md-nav__item> <a href=#limitations class=md-nav__link> <span class=md-ellipsis> Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#regularisation-assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Regularisation Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Regularisation Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-assumptions_1 class=md-nav__link> <span class=md-ellipsis> Key Assumptions </span> </a> </li> <li class=md-nav__item> <a href=#limitations_1 class=md-nav__link> <span class=md-ellipsis> Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#comparison-of-techniques class=md-nav__link> <span class=md-ellipsis> Comparison of Techniques </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#interview-questions class=md-nav__link> <span class=md-ellipsis> ‚ùì Interview Questions </span> </a> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> üìù Examples </span> </a> <nav class=md-nav aria-label="üìù Examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#real-world-example-customer-churn-prediction class=md-nav__link> <span class=md-ellipsis> Real-world Example: Customer Churn Prediction </span> </a> </li> <li class=md-nav__item> <a href=#financial-risk-assessment-example class=md-nav__link> <span class=md-ellipsis> Financial Risk Assessment Example </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> <span class=md-ellipsis> üìö References </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../Overfitting%2C%20Underfitting/ class=md-nav__link> <span class=md-ellipsis> Overfitting, Underfitting </span> </a> </li> <li class=md-nav__item> <a href=../PCA/ class=md-nav__link> <span class=md-ellipsis> PCA </span> </a> </li> <li class=md-nav__item> <a href=../Random%20Forest/ class=md-nav__link> <span class=md-ellipsis> Random Forest </span> </a> </li> <li class=md-nav__item> <a href=../Support%20Vector%20Machines/ class=md-nav__link> <span class=md-ellipsis> Support Vector Machines </span> </a> </li> <li class=md-nav__item> <a href=../Unbalanced%2C%20Skewed%20data/ class=md-nav__link> <span class=md-ellipsis> Unbalanced, Skewed data </span> </a> </li> <li class=md-nav__item> <a href=../kNN/ class=md-nav__link> <span class=md-ellipsis> kNN </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> üë®üèæ‚Äçüíª Online Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> üë®üèæ‚Äçüíª Online Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Online-Material/Online-Material-for-Learning/ class=md-nav__link> <span class=md-ellipsis> Online Study Material </span> </a> </li> <li class=md-nav__item> <a href=../../Online-Material/popular-resources/ class=md-nav__link> <span class=md-ellipsis> Popular Blogs </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../projects/ class=md-nav__link> <span class=md-ellipsis> üì≥ Projects </span> </a> </li> <li class=md-nav__item> <a href=../../Contribute/ class=md-nav__link> <span class=md-ellipsis> ü§ù Contribute </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> üìä Summary </span> </a> <nav class=md-nav aria-label="üìä Summary"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization class=md-nav__link> <span class=md-ellipsis> Normalization </span> </a> </li> <li class=md-nav__item> <a href=#regularisation class=md-nav__link> <span class=md-ellipsis> Regularisation </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#intuition class=md-nav__link> <span class=md-ellipsis> üß† Intuition </span> </a> <nav class=md-nav aria-label="üß† Intuition"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-intuition class=md-nav__link> <span class=md-ellipsis> Normalization Intuition </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-intuition class=md-nav__link> <span class=md-ellipsis> Regularisation Intuition </span> </a> </li> <li class=md-nav__item> <a href=#mathematical-foundation class=md-nav__link> <span class=md-ellipsis> Mathematical Foundation </span> </a> <nav class=md-nav aria-label="Mathematical Foundation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-normalization-techniques class=md-nav__link> <span class=md-ellipsis> 1. Normalization Techniques </span> </a> </li> <li class=md-nav__item> <a href=#2-regularisation-mathematics class=md-nav__link> <span class=md-ellipsis> 2. Regularisation Mathematics </span> </a> </li> <li class=md-nav__item> <a href=#3-effect-on-gradients class=md-nav__link> <span class=md-ellipsis> 3. Effect on Gradients </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#implementation-using-libraries class=md-nav__link> <span class=md-ellipsis> üõ†Ô∏è Implementation using Libraries </span> </a> <nav class=md-nav aria-label="üõ†Ô∏è Implementation using Libraries"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-with-scikit-learn class=md-nav__link> <span class=md-ellipsis> Normalization with Scikit-learn </span> </a> </li> <li class=md-nav__item> <a href=#advanced-normalization-techniques class=md-nav__link> <span class=md-ellipsis> Advanced Normalization Techniques </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-implementation class=md-nav__link> <span class=md-ellipsis> Regularisation Implementation </span> </a> </li> <li class=md-nav__item> <a href=#regularisation-path-analysis class=md-nav__link> <span class=md-ellipsis> Regularisation Path Analysis </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#from-scratch-implementation class=md-nav__link> <span class=md-ellipsis> Œª From Scratch Implementation </span> </a> <nav class=md-nav aria-label="Œª From Scratch Implementation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#custom-scalers-implementation class=md-nav__link> <span class=md-ellipsis> Custom Scalers Implementation </span> </a> </li> <li class=md-nav__item> <a href=#custom-regularised-regression class=md-nav__link> <span class=md-ellipsis> Custom Regularised Regression </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Œª Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Œª Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#normalization-assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Normalization Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Normalization Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-assumptions class=md-nav__link> <span class=md-ellipsis> Key Assumptions </span> </a> </li> <li class=md-nav__item> <a href=#limitations class=md-nav__link> <span class=md-ellipsis> Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#regularisation-assumptions-and-limitations class=md-nav__link> <span class=md-ellipsis> Regularisation Assumptions and Limitations </span> </a> <nav class=md-nav aria-label="Regularisation Assumptions and Limitations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-assumptions_1 class=md-nav__link> <span class=md-ellipsis> Key Assumptions </span> </a> </li> <li class=md-nav__item> <a href=#limitations_1 class=md-nav__link> <span class=md-ellipsis> Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#comparison-of-techniques class=md-nav__link> <span class=md-ellipsis> Comparison of Techniques </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#interview-questions class=md-nav__link> <span class=md-ellipsis> ‚ùì Interview Questions </span> </a> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> üìù Examples </span> </a> <nav class=md-nav aria-label="üìù Examples"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#real-world-example-customer-churn-prediction class=md-nav__link> <span class=md-ellipsis> Real-world Example: Customer Churn Prediction </span> </a> </li> <li class=md-nav__item> <a href=#financial-risk-assessment-example class=md-nav__link> <span class=md-ellipsis> Financial Risk Assessment Example </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> <span class=md-ellipsis> üìö References </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href="https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/edit/master/docs/Machine-Learning/Normalization Regularisation.md" title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href="https://github.com/singhsidhukuldeep/singhsidhukuldeep.github.io/raw/master/docs/Machine-Learning/Normalization Regularisation.md" title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <h1 id=normalization-and-regularisation>üìä Normalization and Regularisation</h1> <p>Normalization and regularisation are fundamental techniques in machine learning: normalization ensures features are on similar scales for optimal algorithm performance, while regularisation prevents overfitting by constraining model complexity.</p> <p><strong>Resources:</strong> <a href=https://scikit-learn.org/stable/modules/preprocessing.html>Scikit-learn Preprocessing</a> | <a href=https://www.deeplearningbook.org/contents/regularization.html>Regularization in Deep Learning</a> | <a href=https://web.stanford.edu/~hastie/ElemStatLearn/ >Elements of Statistical Learning - Chapter 3</a></p> <h2 id=summary>üìä Summary</h2> <p><strong>Normalization</strong> (Feature Scaling) transforms features to similar scales, ensuring no single feature dominates due to its scale. <strong>Regularisation</strong> adds penalty terms to the loss function to prevent overfitting by constraining model complexity.</p> <h3 id=normalization>Normalization</h3> <p>Feature scaling is crucial when features have different units, ranges, or variances. Without normalization, algorithms like gradient descent, SVM, and k-NN can be severely affected by scale differences.</p> <p><strong>Common normalization techniques:</strong> - <strong>Standardization (Z-score)</strong>: Mean = 0, Standard deviation = 1 - <strong>Min-Max scaling</strong>: Scale to [0,1] range - <strong>Robust scaling</strong>: Uses median and IQR, resistant to outliers - <strong>Unit vector scaling</strong>: Scale to unit norm - <strong>Quantile transformation</strong>: Map to uniform or normal distribution</p> <h3 id=regularisation>Regularisation</h3> <p>Regularisation prevents overfitting by adding penalty terms that discourage complex models, leading to better generalization on unseen data.</p> <p><strong>Common regularisation techniques:</strong> - <strong>L1 Regularization (Lasso)</strong>: Promotes sparsity, feature selection - <strong>L2 Regularization (Ridge)</strong>: Shrinks coefficients, handles multicollinearity - <strong>Elastic Net</strong>: Combines L1 and L2 penalties - <strong>Dropout</strong>: Randomly deactivates neurons (neural networks) - <strong>Early stopping</strong>: Stop training before overfitting occurs</p> <p><strong>Applications:</strong> - Feature preprocessing for all ML algorithms - Linear models (Ridge, Lasso, Elastic Net) - Neural networks (dropout, batch normalization) - Tree-based models (pruning) - Computer vision and NLP pipelines</p> <h2 id=intuition>üß† Intuition</h2> <h3 id=normalization-intuition>Normalization Intuition</h3> <p>Imagine you're comparing houses using price (in hundreds of thousands) and square footage (in thousands). Without normalization, price variations (20-800) might overshadow square footage variations (1-5), causing algorithms to ignore the latter feature entirely.</p> <p><strong>Example</strong>: In k-NN, Euclidean distance between houses: - Without normalization: Distance dominated by price differences - With normalization: Both features contribute meaningfully to distance</p> <h3 id=regularisation-intuition>Regularisation Intuition</h3> <p>Think of regularisation like speed limits on roads. Without limits (regularisation), drivers (models) might go too fast (overfit) and crash. Regularisation enforces "speed limits" on model complexity, ensuring safer (more generalizable) performance.</p> <p><strong>Analogy</strong>: - <strong>No regularisation</strong>: Memorizing exam answers Œª fails on new questions - <strong>With regularisation</strong>: Understanding concepts Œª succeeds on new questions</p> <h3 id=mathematical-foundation>Mathematical Foundation</h3> <h4 id=1-normalization-techniques>1. Normalization Techniques</h4> <p><strong>Standardization (Z-score normalization)</strong>: <span class=arithmatex>\(<span class=arithmatex>\(z = \frac{x - \mu}{\sigma}\)</span>\)</span></p> <p>Where <span class=arithmatex>\(\mu\)</span> is mean and <span class=arithmatex>\(\sigma\)</span> is standard deviation.</p> <p><strong>Min-Max scaling</strong>: <span class=arithmatex>\(<span class=arithmatex>\(x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}\)</span>\)</span></p> <p><strong>Robust scaling</strong>: <span class=arithmatex>\(<span class=arithmatex>\(x_{robust} = \frac{x - \text{median}(x)}{\text{IQR}(x)}\)</span>\)</span></p> <p>Where IQR is the interquartile range.</p> <p><strong>Unit vector scaling</strong>: <span class=arithmatex>\(<span class=arithmatex>\(x_{unit} = \frac{x}{||x||_2}\)</span>\)</span></p> <h4 id=2-regularisation-mathematics>2. Regularisation Mathematics</h4> <p><strong>L1 Regularization (Lasso)</strong>: <span class=arithmatex>\(<span class=arithmatex>\(\text{Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} |w_i|\)</span>\)</span></p> <p><strong>L2 Regularization (Ridge)</strong>: <span class=arithmatex>\(<span class=arithmatex>\(\text{Loss} = \text{MSE} + \lambda \sum_{i=1}^{n} w_i^2\)</span>\)</span></p> <p><strong>Elastic Net</strong>: <span class=arithmatex>\(<span class=arithmatex>\(\text{Loss} = \text{MSE} + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2\)</span>\)</span></p> <p>Where <span class=arithmatex>\(\lambda\)</span> controls regularisation strength.</p> <h4 id=3-effect-on-gradients>3. Effect on Gradients</h4> <p><strong>L1 gradient</strong> (creates sparsity): <span class=arithmatex>\(<span class=arithmatex>\(\frac{\partial}{\partial w_i} \lambda |w_i| = \lambda \cdot \text{sign}(w_i)\)</span>\)</span></p> <p><strong>L2 gradient</strong> (shrinks coefficients): <span class=arithmatex>\(<span class=arithmatex>\(\frac{\partial}{\partial w_i} \lambda w_i^2 = 2\lambda w_i\)</span>\)</span></p> <h2 id=implementation-using-libraries>üõ†Ô∏è Implementation using Libraries</h2> <h3 id=normalization-with-scikit-learn>Normalization with Scikit-learn</h3> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=p>(</span>
    <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>MinMaxScaler</span><span class=p>,</span> <span class=n>RobustScaler</span><span class=p>,</span> 
    <span class=n>Normalizer</span><span class=p>,</span> <span class=n>QuantileTransformer</span><span class=p>,</span> <span class=n>PowerTransformer</span>
<span class=p>)</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.datasets</span><span class=w> </span><span class=kn>import</span> <span class=n>make_classification</span><span class=p>,</span> <span class=n>load_boston</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>accuracy_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.neighbors</span><span class=w> </span><span class=kn>import</span> <span class=n>KNeighborsClassifier</span>

<span class=c1># Generate sample dataset with different scales</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>n_samples</span> <span class=o>=</span> <span class=mi>1000</span>

<span class=c1># Create features with vastly different scales</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;income&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>50000</span><span class=p>,</span> <span class=mi>15000</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>      <span class=c1># Mean ~50k</span>
    <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>35</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>               <span class=c1># Mean ~35</span>
    <span class=s1>&#39;debt_ratio&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>         <span class=c1># Range [0,1]</span>
    <span class=s1>&#39;credit_score&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>700</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>),</span>    <span class=c1># Mean ~700</span>
    <span class=s1>&#39;num_accounts&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>poisson</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>)</span>           <span class=c1># Count data</span>
<span class=p>}</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Feature Scaling Demonstration&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Original data statistics:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>describe</span><span class=p>())</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Feature ranges:&quot;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>col</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>columns</span><span class=p>:</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>col</span><span class=si>:</span><span class=s2>15</span><span class=si>}</span><span class=s2>: [</span><span class=si>{</span><span class=n>df</span><span class=p>[</span><span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, </span><span class=si>{</span><span class=n>df</span><span class=p>[</span><span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>]&quot;</span><span class=p>)</span>

<span class=c1># Visualize original distributions</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>axes</span> <span class=o>=</span> <span class=n>axes</span><span class=o>.</span><span class=n>ravel</span><span class=p>()</span>

<span class=n>original_data</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span>

<span class=n>scalers</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Original&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
    <span class=s1>&#39;StandardScaler&#39;</span><span class=p>:</span> <span class=n>StandardScaler</span><span class=p>(),</span>
    <span class=s1>&#39;MinMaxScaler&#39;</span><span class=p>:</span> <span class=n>MinMaxScaler</span><span class=p>(),</span>
    <span class=s1>&#39;RobustScaler&#39;</span><span class=p>:</span> <span class=n>RobustScaler</span><span class=p>(),</span>
    <span class=s1>&#39;Normalizer&#39;</span><span class=p>:</span> <span class=n>Normalizer</span><span class=p>(),</span>
    <span class=s1>&#39;QuantileTransformer&#39;</span><span class=p>:</span> <span class=n>QuantileTransformer</span><span class=p>(</span><span class=n>output_distribution</span><span class=o>=</span><span class=s1>&#39;normal&#39;</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>scaled_data</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>scaler</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>scalers</span><span class=o>.</span><span class=n>items</span><span class=p>()):</span>
    <span class=k>if</span> <span class=n>scaler</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>data_transformed</span> <span class=o>=</span> <span class=n>original_data</span>
        <span class=n>title_stats</span> <span class=o>=</span> <span class=s2>&quot;Original Data&quot;</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>data_transformed</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>original_data</span><span class=p>)</span>
        <span class=n>title_stats</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;Mean: </span><span class=si>{</span><span class=n>data_transformed</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, Std: </span><span class=si>{</span><span class=n>data_transformed</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&quot;</span>

    <span class=n>scaled_data</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>data_transformed</span>

    <span class=c1># Plot first feature (income) for each transformation</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_transformed</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> 
                <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=se>\n</span><span class=si>{</span><span class=n>title_stats</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Income (transformed)&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Density&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Compare performance impact on different algorithms</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>values</span>
<span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;income&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>median</span><span class=p>())</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>  <span class=c1># Binary target</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>stratify</span><span class=o>=</span><span class=n>y</span>
<span class=p>)</span>

<span class=c1># Test different scalers with k-NN (scale-sensitive algorithm)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Impact of normalization on k-NN classifier:&quot;</span><span class=p>)</span>

<span class=n>results</span> <span class=o>=</span> <span class=p>{}</span>
<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>scaler</span> <span class=ow>in</span> <span class=n>scalers</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=k>if</span> <span class=n>scaler</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>X_train</span>
        <span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>X_test</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
        <span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

    <span class=c1># Train k-NN classifier</span>
    <span class=n>knn</span> <span class=o>=</span> <span class=n>KNeighborsClassifier</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
    <span class=n>knn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_scaled</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

    <span class=c1># Evaluate</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>knn</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test_scaled</span><span class=p>)</span>
    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

    <span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>accuracy</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>name</span><span class=si>:</span><span class=s2>20</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>accuracy</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize results</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>methods</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
<span class=n>accuracies</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>

<span class=n>bars</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>methods</span><span class=p>,</span> <span class=n>accuracies</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;skyblue&#39;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;navy&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;k-NN Performance with Different Scaling Methods&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Scaling Method&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>

<span class=c1># Add value labels on bars</span>
<span class=k>for</span> <span class=n>bar</span><span class=p>,</span> <span class=n>acc</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>bars</span><span class=p>,</span> <span class=n>accuracies</span><span class=p>):</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>bar</span><span class=o>.</span><span class=n>get_x</span><span class=p>()</span> <span class=o>+</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_width</span><span class=p>()</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_height</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.001</span><span class=p>,</span> 
             <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>acc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;bottom&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=advanced-normalization-techniques>Advanced Normalization Techniques</h3> <div class=highlight><pre><span></span><code><span class=c1># Outlier-robust scaling comparison</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create data with outliers</span>
<span class=n>normal_data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
<span class=n>outliers</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>  <span class=c1># Extreme outliers</span>
<span class=n>data_with_outliers</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>normal_data</span><span class=p>,</span> <span class=n>outliers</span><span class=p>])</span>

<span class=c1># Compare different scalers on data with outliers</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>

<span class=c1># Original data</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_with_outliers</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Original Data (with outliers)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Frequency&#39;</span><span class=p>)</span>

<span class=c1># StandardScaler (sensitive to outliers)</span>
<span class=n>standard_scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>data_standard</span> <span class=o>=</span> <span class=n>standard_scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data_with_outliers</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_standard</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;StandardScaler</span><span class=se>\n</span><span class=s1>Mean: </span><span class=si>{</span><span class=n>data_standard</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>, Std: </span><span class=si>{</span><span class=n>data_standard</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=c1># RobustScaler (resistant to outliers)</span>
<span class=n>robust_scaler</span> <span class=o>=</span> <span class=n>RobustScaler</span><span class=p>()</span>
<span class=n>data_robust</span> <span class=o>=</span> <span class=n>robust_scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data_with_outliers</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_robust</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;RobustScaler</span><span class=se>\n</span><span class=s1>Median: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>data_robust</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>, IQR: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>data_robust</span><span class=p>,</span><span class=w> </span><span class=mi>75</span><span class=p>)</span><span class=w> </span><span class=o>-</span><span class=w> </span><span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>data_robust</span><span class=p>,</span><span class=w> </span><span class=mi>25</span><span class=p>)</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Frequency&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Scaled Values&#39;</span><span class=p>)</span>

<span class=c1># QuantileTransformer (maps to uniform distribution)</span>
<span class=n>quantile_transformer</span> <span class=o>=</span> <span class=n>QuantileTransformer</span><span class=p>(</span><span class=n>output_distribution</span><span class=o>=</span><span class=s1>&#39;uniform&#39;</span><span class=p>)</span>
<span class=n>data_quantile</span> <span class=o>=</span> <span class=n>quantile_transformer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data_with_outliers</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_quantile</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;QuantileTransformer (Uniform)</span><span class=se>\n</span><span class=s1>Range: [</span><span class=si>{</span><span class=n>data_quantile</span><span class=o>.</span><span class=n>min</span><span class=p>()</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>, </span><span class=si>{</span><span class=n>data_quantile</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>]&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Scaled Values&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Demonstrate PowerTransformer for non-normal data</span>
<span class=kn>from</span><span class=w> </span><span class=nn>scipy.stats</span><span class=w> </span><span class=kn>import</span> <span class=n>skew</span>

<span class=c1># Create skewed data</span>
<span class=n>skewed_data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>exponential</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original skewness: </span><span class=si>{</span><span class=n>skew</span><span class=p>(</span><span class=n>skewed_data</span><span class=p>)</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Apply different transformations</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=c1># Original</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>skewed_data</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Original Data</span><span class=se>\n</span><span class=s1>Skewness: </span><span class=si>{</span><span class=n>skew</span><span class=p>(</span><span class=n>skewed_data</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Density&#39;</span><span class=p>)</span>

<span class=c1># Box-Cox transformation</span>
<span class=n>power_transformer_box</span> <span class=o>=</span> <span class=n>PowerTransformer</span><span class=p>(</span><span class=n>method</span><span class=o>=</span><span class=s1>&#39;box-cox&#39;</span><span class=p>)</span>
<span class=n>data_box_cox</span> <span class=o>=</span> <span class=n>power_transformer_box</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>skewed_data</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_box_cox</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Box-Cox Transform</span><span class=se>\n</span><span class=s1>Skewness: </span><span class=si>{</span><span class=n>skew</span><span class=p>(</span><span class=n>data_box_cox</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=c1># Yeo-Johnson transformation (can handle negative values)</span>
<span class=n>power_transformer_yj</span> <span class=o>=</span> <span class=n>PowerTransformer</span><span class=p>(</span><span class=n>method</span><span class=o>=</span><span class=s1>&#39;yeo-johnson&#39;</span><span class=p>)</span>
<span class=n>data_yj</span> <span class=o>=</span> <span class=n>power_transformer_yj</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>skewed_data</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_yj</span><span class=p>,</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Yeo-Johnson Transform</span><span class=se>\n</span><span class=s1>Skewness: </span><span class=si>{</span><span class=n>skew</span><span class=p>(</span><span class=n>data_yj</span><span class=p>)</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

<span class=k>for</span> <span class=n>ax</span> <span class=ow>in</span> <span class=n>axes</span><span class=p>:</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Values&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=regularisation-implementation>Regularisation Implementation</h3> <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span><span class=p>,</span> <span class=n>LinearRegression</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>PolynomialFeatures</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>validation_curve</span>
<span class=kn>import</span><span class=w> </span><span class=nn>warnings</span>
<span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>

<span class=c1># Generate regression dataset with potential for overfitting</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>n_samples</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>n_features</span> <span class=o>=</span> <span class=mi>50</span>

<span class=n>X_reg</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span><span class=p>)</span>
<span class=c1># Create target with only few features actually relevant</span>
<span class=n>true_coef</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
<span class=n>true_coef</span><span class=p>[:</span><span class=mi>5</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># Only first 5 features matter</span>
<span class=n>y_reg</span> <span class=o>=</span> <span class=n>X_reg</span> <span class=o>@</span> <span class=n>true_coef</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>)</span>

<span class=n>X_train_reg</span><span class=p>,</span> <span class=n>X_test_reg</span><span class=p>,</span> <span class=n>y_train_reg</span><span class=p>,</span> <span class=n>y_test_reg</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X_reg</span><span class=p>,</span> <span class=n>y_reg</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Regularization Comparison&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Dataset: </span><span class=si>{</span><span class=n>n_samples</span><span class=si>}</span><span class=s2> samples, </span><span class=si>{</span><span class=n>n_features</span><span class=si>}</span><span class=s2> features&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;True non-zero coefficients: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>true_coef</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Compare different regularization techniques</span>
<span class=n>models</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Linear Regression&#39;</span><span class=p>:</span> <span class=n>LinearRegression</span><span class=p>(),</span>
    <span class=s1>&#39;Ridge (L2)&#39;</span><span class=p>:</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>),</span>
    <span class=s1>&#39;Lasso (L1)&#39;</span><span class=p>:</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>),</span>
    <span class=s1>&#39;Elastic Net&#39;</span><span class=p>:</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>results</span> <span class=o>=</span> <span class=p>{}</span>
<span class=n>coefficients</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>models</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=c1># Train model</span>
    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_reg</span><span class=p>,</span> <span class=n>y_train_reg</span><span class=p>)</span>

    <span class=c1># Predictions</span>
    <span class=n>y_pred_train</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_train_reg</span><span class=p>)</span>
    <span class=n>y_pred_test</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test_reg</span><span class=p>)</span>

    <span class=c1># Evaluate</span>
    <span class=n>train_mse</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_train_reg</span><span class=p>,</span> <span class=n>y_pred_train</span><span class=p>)</span>
    <span class=n>test_mse</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test_reg</span><span class=p>,</span> <span class=n>y_pred_test</span><span class=p>)</span>
    <span class=n>train_r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_train_reg</span><span class=p>,</span> <span class=n>y_pred_train</span><span class=p>)</span>
    <span class=n>test_r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test_reg</span><span class=p>,</span> <span class=n>y_pred_test</span><span class=p>)</span>

    <span class=c1># Store results</span>
    <span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;train_mse&#39;</span><span class=p>:</span> <span class=n>train_mse</span><span class=p>,</span>
        <span class=s1>&#39;test_mse&#39;</span><span class=p>:</span> <span class=n>test_mse</span><span class=p>,</span>
        <span class=s1>&#39;train_r2&#39;</span><span class=p>:</span> <span class=n>train_r2</span><span class=p>,</span>
        <span class=s1>&#39;test_r2&#39;</span><span class=p>:</span> <span class=n>test_r2</span><span class=p>,</span>
        <span class=s1>&#39;overfitting&#39;</span><span class=p>:</span> <span class=n>train_r2</span> <span class=o>-</span> <span class=n>test_r2</span>  <span class=c1># Measure of overfitting</span>
    <span class=p>}</span>

    <span class=c1># Store coefficients</span>
    <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s1>&#39;coef_&#39;</span><span class=p>):</span>
        <span class=n>coefficients</span><span class=p>[</span><span class=n>name</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>coef_</span>
        <span class=n>non_zero</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span>
        <span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>][</span><span class=s1>&#39;non_zero_coef&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>non_zero</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Train R¬≤: </span><span class=si>{</span><span class=n>train_r2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>, Test R¬≤: </span><span class=si>{</span><span class=n>test_r2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Train MSE: </span><span class=si>{</span><span class=n>train_mse</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>, Test MSE: </span><span class=si>{</span><span class=n>test_mse</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Overfitting gap: </span><span class=si>{</span><span class=n>train_r2</span><span class=w> </span><span class=o>-</span><span class=w> </span><span class=n>test_r2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s1>&#39;coef_&#39;</span><span class=p>):</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Non-zero coefficients: </span><span class=si>{</span><span class=n>non_zero</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>n_features</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize results</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>12</span><span class=p>))</span>

<span class=c1># Performance comparison</span>
<span class=n>model_names</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>results</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
<span class=n>train_r2s</span> <span class=o>=</span> <span class=p>[</span><span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>][</span><span class=s1>&#39;train_r2&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
<span class=n>test_r2s</span> <span class=o>=</span> <span class=p>[</span><span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>][</span><span class=s1>&#39;test_r2&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>

<span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model_names</span><span class=p>))</span>
<span class=n>width</span> <span class=o>=</span> <span class=mf>0.35</span>

<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>width</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>train_r2s</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train R¬≤&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>width</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>test_r2s</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Test R¬≤&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Model&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;R¬≤ Score&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Train vs Test Performance&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xticklabels</span><span class=p>(</span><span class=n>model_names</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Overfitting comparison</span>
<span class=n>overfitting_gaps</span> <span class=o>=</span> <span class=p>[</span><span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>][</span><span class=s1>&#39;overfitting&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>model_names</span><span class=p>,</span> <span class=n>overfitting_gaps</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Overfitting Gap (Train R¬≤ - Test R¬≤)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Overfitting Comparison&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Coefficient plots</span>
<span class=c1># True coefficients vs estimated</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>true_coef</span><span class=p>,</span> <span class=s1>&#39;ko-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;True coefficients&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>markersize</span><span class=o>=</span><span class=mi>6</span><span class=p>)</span>
<span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;Ridge (L2)&#39;</span><span class=p>,</span> <span class=s1>&#39;Lasso (L1)&#39;</span><span class=p>,</span> <span class=s1>&#39;Elastic Net&#39;</span><span class=p>]:</span>
    <span class=k>if</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>coefficients</span><span class=p>:</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>coefficients</span><span class=p>[</span><span class=n>name</span><span class=p>],</span> <span class=s1>&#39;o-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=n>name</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Feature Index&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Coefficient Comparison&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Sparsity comparison (number of non-zero coefficients)</span>
<span class=n>sparsity_names</span> <span class=o>=</span> <span class=p>[</span><span class=n>name</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>model_names</span> <span class=k>if</span> <span class=n>name</span> <span class=o>!=</span> <span class=s1>&#39;Linear Regression&#39;</span><span class=p>]</span>
<span class=n>non_zero_coefs</span> <span class=o>=</span> <span class=p>[</span><span class=n>results</span><span class=p>[</span><span class=n>name</span><span class=p>][</span><span class=s1>&#39;non_zero_coef&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>sparsity_names</span><span class=p>]</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>sparsity_names</span><span class=p>,</span> <span class=n>non_zero_coefs</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>y</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;True non-zero features&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Number of Non-zero Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Feature Selection (Sparsity)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h3 id=regularisation-path-analysis>Regularisation Path Analysis</h3> <div class=highlight><pre><span></span><code><span class=c1># Analyze how regularization strength affects coefficients</span>
<span class=n>alphas</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>50</span><span class=p>)</span>

<span class=c1># Ridge regression path</span>
<span class=n>ridge_coefs</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>ridge_scores</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>for</span> <span class=n>alpha</span> <span class=ow>in</span> <span class=n>alphas</span><span class=p>:</span>
    <span class=n>ridge</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>)</span>
    <span class=n>ridge</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_reg</span><span class=p>,</span> <span class=n>y_train_reg</span><span class=p>)</span>
    <span class=n>ridge_coefs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>ridge</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>ridge</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test_reg</span><span class=p>,</span> <span class=n>y_test_reg</span><span class=p>)</span>
    <span class=n>ridge_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>

<span class=n>ridge_coefs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>ridge_coefs</span><span class=p>)</span>

<span class=c1># Lasso regression path</span>
<span class=n>lasso_coefs</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>lasso_scores</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>for</span> <span class=n>alpha</span> <span class=ow>in</span> <span class=n>alphas</span><span class=p>:</span>
    <span class=n>lasso</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>
    <span class=n>lasso</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_reg</span><span class=p>,</span> <span class=n>y_train_reg</span><span class=p>)</span>
    <span class=n>lasso_coefs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>lasso</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
    <span class=n>score</span> <span class=o>=</span> <span class=n>lasso</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test_reg</span><span class=p>,</span> <span class=n>y_test_reg</span><span class=p>)</span>
    <span class=n>lasso_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>

<span class=n>lasso_coefs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>lasso_coefs</span><span class=p>)</span>

<span class=c1># Plot regularization paths</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>12</span><span class=p>))</span>

<span class=c1># Ridge coefficients path</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>n_features</span><span class=p>)):</span>  <span class=c1># Plot first 10 features</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>ridge_coefs</span><span class=p>[:,</span> <span class=n>i</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;Feature </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s1>&#39;</span> <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>5</span> <span class=k>else</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Ridge Regression Path&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Lasso coefficients path</span>
<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>n_features</span><span class=p>)):</span>  <span class=c1># Plot first 10 features</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>lasso_coefs</span><span class=p>[:,</span> <span class=n>i</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;Feature </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s1>&#39;</span> <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>5</span> <span class=k>else</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Lasso Regression Path&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Performance vs regularization strength</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>ridge_scores</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Ridge&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>lasso_scores</span><span class=p>,</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Lasso&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;R¬≤ Score&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Performance vs Regularization&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Number of non-zero coefficients (sparsity)</span>
<span class=n>ridge_sparsity</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>coef</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span> <span class=k>for</span> <span class=n>coef</span> <span class=ow>in</span> <span class=n>ridge_coefs</span><span class=p>]</span>
<span class=n>lasso_sparsity</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>coef</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span> <span class=k>for</span> <span class=n>coef</span> <span class=ow>in</span> <span class=n>lasso_coefs</span><span class=p>]</span>

<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>ridge_sparsity</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Ridge&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>lasso_sparsity</span><span class=p>,</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Lasso&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>y</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;True non-zero features&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Number of Non-zero Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Sparsity vs Regularization&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Key Insights:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;- Ridge: Shrinks coefficients but rarely makes them exactly zero&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;- Lasso: Creates sparse solutions by setting coefficients to exactly zero&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;- Optimal Œª for Ridge: </span><span class=si>{</span><span class=n>alphas</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>ridge_scores</span><span class=p>)]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;- Optimal Œª for Lasso: </span><span class=si>{</span><span class=n>alphas</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>lasso_scores</span><span class=p>)]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
</code></pre></div> <h2 id=from-scratch-implementation>Œª From Scratch Implementation</h2> <h3 id=custom-scalers-implementation>Custom Scalers Implementation</h3> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>

<span class=k>class</span><span class=w> </span><span class=nc>StandardScalerFromScratch</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>False</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute mean and standard deviation for later scaling&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>ddof</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Sample standard deviation</span>

        <span class=c1># Handle zero variance features</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>==</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mf>1.0</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span><span class=w> </span><span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Scale features using computed statistics&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;Scaler has not been fitted yet.&quot;</span><span class=p>)</span>

        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=k>return</span> <span class=p>(</span><span class=n>X</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit_transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fit and transform in one step&quot;&quot;&quot;</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>inverse_transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X_scaled</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Convert scaled features back to original scale&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;Scaler has not been fitted yet.&quot;</span><span class=p>)</span>

        <span class=n>X_scaled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>X_scaled</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_</span>

<span class=k>class</span><span class=w> </span><span class=nc>MinMaxScalerFromScratch</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>feature_range</span><span class=o>=</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>feature_range</span> <span class=o>=</span> <span class=n>feature_range</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>min_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data_min_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data_max_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>False</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute min and max for later scaling&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data_min_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>data_max_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Compute scaling parameters</span>
        <span class=n>data_range</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_max_</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_min_</span>
        <span class=n>data_range</span><span class=p>[</span><span class=n>data_range</span> <span class=o>==</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mf>1.0</span>  <span class=c1># Handle constant features</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>feature_range</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>feature_range</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=o>/</span> <span class=n>data_range</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>min_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feature_range</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>data_min_</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span><span class=w> </span><span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Scale features to specified range&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;Scaler has not been fitted yet.&quot;</span><span class=p>)</span>

        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>X</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit_transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fit and transform in one step&quot;&quot;&quot;</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>inverse_transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X_scaled</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Convert scaled features back to original scale&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;Scaler has not been fitted yet.&quot;</span><span class=p>)</span>

        <span class=n>X_scaled</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
        <span class=k>return</span> <span class=p>(</span><span class=n>X_scaled</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>min_</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span>

<span class=k>class</span><span class=w> </span><span class=nc>RobustScalerFromScratch</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>center_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>False</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Compute median and IQR for later scaling&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>center_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>median</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Compute IQR (75th percentile - 25th percentile)</span>
        <span class=n>q75</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>75</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=n>q25</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>=</span> <span class=n>q75</span> <span class=o>-</span> <span class=n>q25</span>

        <span class=c1># Handle zero IQR</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>scale_</span> <span class=o>==</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mf>1.0</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span> <span class=o>=</span> <span class=kc>True</span>
        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span><span class=w> </span><span class=nf>transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Scale features using median and IQR&quot;&quot;&quot;</span>
        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>fitted</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;Scaler has not been fitted yet.&quot;</span><span class=p>)</span>

        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=k>return</span> <span class=p>(</span><span class=n>X</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>center_</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale_</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit_transform</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fit and transform in one step&quot;&quot;&quot;</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>

<span class=c1># Demonstration with synthetic data</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Create data with outliers</span>
<span class=n>normal_data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
<span class=n>outliers</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>50</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>15</span><span class=p>],</span> <span class=p>[</span><span class=mi>60</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>20</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mi>20</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>]])</span>  <span class=c1># Add outliers</span>
<span class=n>data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>normal_data</span><span class=p>,</span> <span class=n>outliers</span><span class=p>])</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Custom Scalers Demonstration&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original data shape: </span><span class=si>{</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Original data statistics:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Mean: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>data</span><span class=p>,</span><span class=w> </span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Std: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>data</span><span class=p>,</span><span class=w> </span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Min: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>data</span><span class=p>,</span><span class=w> </span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Max: </span><span class=si>{</span><span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>data</span><span class=p>,</span><span class=w> </span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Test custom scalers</span>
<span class=n>scalers_custom</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Custom StandardScaler&#39;</span><span class=p>:</span> <span class=n>StandardScalerFromScratch</span><span class=p>(),</span>
    <span class=s1>&#39;Custom MinMaxScaler&#39;</span><span class=p>:</span> <span class=n>MinMaxScalerFromScratch</span><span class=p>(),</span>
    <span class=s1>&#39;Custom RobustScaler&#39;</span><span class=p>:</span> <span class=n>RobustScalerFromScratch</span><span class=p>()</span>
<span class=p>}</span>

<span class=c1># Compare with sklearn</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>MinMaxScaler</span><span class=p>,</span> <span class=n>RobustScaler</span>

<span class=n>scalers_sklearn</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Sklearn StandardScaler&#39;</span><span class=p>:</span> <span class=n>StandardScaler</span><span class=p>(),</span>
    <span class=s1>&#39;Sklearn MinMaxScaler&#39;</span><span class=p>:</span> <span class=n>MinMaxScaler</span><span class=p>(),</span>
    <span class=s1>&#39;Sklearn RobustScaler&#39;</span><span class=p>:</span> <span class=n>RobustScaler</span><span class=p>()</span>
<span class=p>}</span>

<span class=c1># Test and compare</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>scaler</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>scalers_custom</span><span class=o>.</span><span class=n>items</span><span class=p>()):</span>
    <span class=c1># Apply custom scaler</span>
    <span class=n>data_scaled_custom</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

    <span class=c1># Apply sklearn scaler for comparison</span>
    <span class=n>sklearn_name</span> <span class=o>=</span> <span class=n>name</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s1>&#39;Custom&#39;</span><span class=p>,</span> <span class=s1>&#39;Sklearn&#39;</span><span class=p>)</span>
    <span class=n>sklearn_scaler</span> <span class=o>=</span> <span class=n>scalers_sklearn</span><span class=p>[</span><span class=n>sklearn_name</span><span class=p>]</span>
    <span class=n>data_scaled_sklearn</span> <span class=o>=</span> <span class=n>sklearn_scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

    <span class=c1># Plot comparison for first feature</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_scaled_custom</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> 
                    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Custom&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>data_scaled_sklearn</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> 
                    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Sklearn&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>density</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=se>\n</span><span class=s1>(Feature 1)&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=c1># Check numerical differences</span>
    <span class=n>max_diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>data_scaled_custom</span> <span class=o>-</span> <span class=n>data_scaled_sklearn</span><span class=p>))</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>data_scaled_custom</span><span class=o>.</span><span class=n>flatten</span><span class=p>(),</span> 
                      <span class=n>data_scaled_sklearn</span><span class=o>.</span><span class=n>flatten</span><span class=p>(),</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=n>data_scaled_custom</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>data_scaled_custom</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span>
                    <span class=p>[</span><span class=n>data_scaled_custom</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>data_scaled_custom</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=s1>&#39;r--&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Custom Scaler&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Sklearn Scaler&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Comparison</span><span class=se>\n</span><span class=s1>Max diff: </span><span class=si>{</span><span class=n>max_diff</span><span class=si>:</span><span class=s1>.2e</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Test inverse transform</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Inverse Transform Test:&quot;</span><span class=p>)</span>
<span class=n>scaler_test</span> <span class=o>=</span> <span class=n>StandardScalerFromScratch</span><span class=p>()</span>
<span class=n>data_scaled</span> <span class=o>=</span> <span class=n>scaler_test</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=n>data_reconstructed</span> <span class=o>=</span> <span class=n>scaler_test</span><span class=o>.</span><span class=n>inverse_transform</span><span class=p>(</span><span class=n>data_scaled</span><span class=p>)</span>

<span class=n>reconstruction_error</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>data</span> <span class=o>-</span> <span class=n>data_reconstructed</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Max reconstruction error: </span><span class=si>{</span><span class=n>reconstruction_error</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot; Inverse transform working correctly&quot;</span> <span class=k>if</span> <span class=n>reconstruction_error</span> <span class=o>&lt;</span> <span class=mf>1e-10</span> <span class=k>else</span> <span class=s2>&quot; Inverse transform failed&quot;</span><span class=p>)</span>
</code></pre></div> <h3 id=custom-regularised-regression>Custom Regularised Regression</h3> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>RidgeRegressionFromScratch</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>fit_intercept</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fit_intercept</span> <span class=o>=</span> <span class=n>fit_intercept</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span> <span class=o>=</span> <span class=kc>None</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fit Ridge regression using closed-form solution&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit_intercept</span><span class=p>:</span>
            <span class=c1># Add intercept term</span>
            <span class=n>X_with_intercept</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>column_stack</span><span class=p>([</span><span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=n>X</span><span class=p>])</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=n>X_with_intercept</span> <span class=o>=</span> <span class=n>X</span>

        <span class=n>n_features</span> <span class=o>=</span> <span class=n>X_with_intercept</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>

        <span class=c1># Ridge regression closed-form solution: (X&#39;X + ŒªI)^(-1)X&#39;y</span>
        <span class=c1># Don&#39;t regularize the intercept term</span>
        <span class=n>I</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit_intercept</span><span class=p>:</span>
            <span class=n>I</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># Don&#39;t regularize intercept</span>

        <span class=n>XTX_plus_alphaI</span> <span class=o>=</span> <span class=n>X_with_intercept</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>X_with_intercept</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>I</span>
        <span class=n>XTy</span> <span class=o>=</span> <span class=n>X_with_intercept</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>y</span>

        <span class=c1># Solve the system</span>
        <span class=n>params</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>solve</span><span class=p>(</span><span class=n>XTX_plus_alphaI</span><span class=p>,</span> <span class=n>XTy</span><span class=p>)</span>

        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>fit_intercept</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span> <span class=o>=</span> <span class=n>params</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>=</span> <span class=n>params</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>=</span> <span class=n>params</span>

        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Make predictions&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span>

    <span class=k>def</span><span class=w> </span><span class=nf>score</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Calculate R¬≤ score&quot;&quot;&quot;</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>ss_res</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>y</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>ss_tot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>y</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=p>))</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
        <span class=k>return</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>ss_res</span> <span class=o>/</span> <span class=n>ss_tot</span><span class=p>)</span>

<span class=k>class</span><span class=w> </span><span class=nc>LassoRegressionFromScratch</span><span class=p>:</span>
    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>tol</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>max_iter</span> <span class=o>=</span> <span class=n>max_iter</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>tol</span> <span class=o>=</span> <span class=n>tol</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span> <span class=o>=</span> <span class=kc>None</span>

    <span class=k>def</span><span class=w> </span><span class=nf>_soft_threshold</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>alpha</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Soft thresholding operator for L1 regularization&quot;&quot;&quot;</span>
        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>-</span> <span class=n>alpha</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>

    <span class=k>def</span><span class=w> </span><span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Fit Lasso regression using coordinate descent&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>

        <span class=c1># Center the data</span>
        <span class=n>X_mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
        <span class=n>y_mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
        <span class=n>X_centered</span> <span class=o>=</span> <span class=n>X</span> <span class=o>-</span> <span class=n>X_mean</span>
        <span class=n>y_centered</span> <span class=o>=</span> <span class=n>y</span> <span class=o>-</span> <span class=n>y_mean</span>

        <span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span> <span class=o>=</span> <span class=n>X_centered</span><span class=o>.</span><span class=n>shape</span>

        <span class=c1># Initialize coefficients</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>

        <span class=c1># Precompute X&#39;X diagonal for coordinate descent</span>
        <span class=n>XTX_diag</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>X_centered</span> <span class=o>**</span> <span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>

        <span class=c1># Coordinate descent</span>
        <span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>max_iter</span><span class=p>):</span>
            <span class=n>coef_old</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>

            <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_features</span><span class=p>):</span>
                <span class=k>if</span> <span class=n>XTX_diag</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
                    <span class=k>continue</span>

                <span class=c1># Compute residual without j-th feature</span>
                <span class=n>residual</span> <span class=o>=</span> <span class=n>y_centered</span> <span class=o>-</span> <span class=n>X_centered</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>*</span> <span class=n>X_centered</span><span class=p>[:,</span> <span class=n>j</span><span class=p>]</span>

                <span class=c1># Update coefficient using soft thresholding</span>
                <span class=n>rho</span> <span class=o>=</span> <span class=n>X_centered</span><span class=p>[:,</span> <span class=n>j</span><span class=p>]</span> <span class=o>@</span> <span class=n>residual</span>
                <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_soft_threshold</span><span class=p>(</span><span class=n>rho</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>n_samples</span><span class=p>)</span> <span class=o>/</span> <span class=n>XTX_diag</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>

            <span class=c1># Check convergence</span>
            <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>-</span> <span class=n>coef_old</span><span class=p>))</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>tol</span><span class=p>:</span>
                <span class=k>break</span>

        <span class=c1># Calculate intercept</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span> <span class=o>=</span> <span class=n>y_mean</span> <span class=o>-</span> <span class=n>X_mean</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span>

        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span><span class=w> </span><span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Make predictions&quot;&quot;&quot;</span>
        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>coef_</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span>

    <span class=k>def</span><span class=w> </span><span class=nf>score</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;Calculate R¬≤ score&quot;&quot;&quot;</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
        <span class=n>ss_res</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>y</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
        <span class=n>ss_tot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=n>y</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=p>))</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
        <span class=k>return</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>ss_res</span> <span class=o>/</span> <span class=n>ss_tot</span><span class=p>)</span>

<span class=c1># Test custom regularized regression</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Generate test data</span>
<span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span> <span class=o>=</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>20</span>
<span class=n>X_test</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span><span class=p>)</span>
<span class=n>true_coef</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
<span class=n>true_coef</span><span class=p>[</span><span class=mi>10</span><span class=p>:]</span> <span class=o>=</span> <span class=mi>0</span>  <span class=c1># Make last 10 coefficients zero</span>
<span class=n>y_test</span> <span class=o>=</span> <span class=n>X_test</span> <span class=o>@</span> <span class=n>true_coef</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_samples</span><span class=p>)</span>

<span class=c1># Split data</span>
<span class=n>X_train_test</span><span class=p>,</span> <span class=n>X_val_test</span><span class=p>,</span> <span class=n>y_train_test</span><span class=p>,</span> <span class=n>y_val_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Custom Regularized Regression Test&quot;</span><span class=p>)</span>

<span class=c1># Test Ridge regression</span>
<span class=n>ridge_custom</span> <span class=o>=</span> <span class=n>RidgeRegressionFromScratch</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
<span class=n>ridge_custom</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_test</span><span class=p>,</span> <span class=n>y_train_test</span><span class=p>)</span>

<span class=n>ridge_sklearn</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
<span class=n>ridge_sklearn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_test</span><span class=p>,</span> <span class=n>y_train_test</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Ridge Regression Comparison:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Custom Ridge R¬≤: </span><span class=si>{</span><span class=n>ridge_custom</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val_test</span><span class=p>,</span><span class=w> </span><span class=n>y_val_test</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Sklearn Ridge R¬≤: </span><span class=si>{</span><span class=n>ridge_sklearn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val_test</span><span class=p>,</span><span class=w> </span><span class=n>y_val_test</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=n>coef_diff_ridge</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span> <span class=o>-</span> <span class=n>ridge_sklearn</span><span class=o>.</span><span class=n>coef_</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Max coefficient difference: </span><span class=si>{</span><span class=n>coef_diff_ridge</span><span class=si>:</span><span class=s2>.2e</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Test Lasso regression</span>
<span class=n>lasso_custom</span> <span class=o>=</span> <span class=n>LassoRegressionFromScratch</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>
<span class=n>lasso_custom</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_test</span><span class=p>,</span> <span class=n>y_train_test</span><span class=p>)</span>

<span class=n>lasso_sklearn</span> <span class=o>=</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>
<span class=n>lasso_sklearn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_test</span><span class=p>,</span> <span class=n>y_train_test</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Lasso Regression Comparison:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Custom Lasso R¬≤: </span><span class=si>{</span><span class=n>lasso_custom</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val_test</span><span class=p>,</span><span class=w> </span><span class=n>y_val_test</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Sklearn Lasso R¬≤: </span><span class=si>{</span><span class=n>lasso_sklearn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_val_test</span><span class=p>,</span><span class=w> </span><span class=n>y_val_test</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Compare sparsity</span>
<span class=n>custom_nonzero</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span>
<span class=n>sklearn_nonzero</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>lasso_sklearn</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Custom Lasso non-zero coefficients: </span><span class=si>{</span><span class=n>custom_nonzero</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Sklearn Lasso non-zero coefficients: </span><span class=si>{</span><span class=n>sklearn_nonzero</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize coefficient comparison</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=c1># Ridge coefficients</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>ridge_sklearn</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span>
             <span class=p>[</span><span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>ridge_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=s1>&#39;r--&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Custom Ridge Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Sklearn Ridge Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Ridge Regression Coefficients Comparison&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Lasso coefficients</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>lasso_sklearn</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span>
             <span class=p>[</span><span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>lasso_custom</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=s1>&#39;r--&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Custom Lasso Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Sklearn Lasso Coefficients&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Lasso Regression Coefficients Comparison&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h2 id=assumptions-and-limitations>Œª Assumptions and Limitations</h2> <h3 id=normalization-assumptions-and-limitations>Normalization Assumptions and Limitations</h3> <h4 id=key-assumptions>Key Assumptions</h4> <ol> <li><strong>Feature independence</strong>: Different scaling methods assume features are independent</li> <li><strong>Distribution stability</strong>: Scaling parameters computed on training data apply to test data</li> <li><strong>Outlier handling</strong>: StandardScaler assumes roughly normal distribution</li> <li><strong>Missing values</strong>: Most scalers require handling missing values beforehand</li> </ol> <h4 id=limitations>Limitations</h4> <ol> <li><strong>StandardScaler limitations</strong>:</li> <li><strong>Sensitive to outliers</strong>: Outliers heavily influence mean and standard deviation</li> <li><strong>Assumes normal distribution</strong>: Works best with normally distributed data</li> <li> <p><strong>Solution</strong>: Use RobustScaler for data with outliers</p> </li> <li> <p><strong>MinMaxScaler limitations</strong>:</p> </li> <li><strong>Very sensitive to outliers</strong>: Single outlier can compress all other values</li> <li><strong>Fixed range assumption</strong>: Assumes test data falls within training range</li> <li> <p><strong>Solution</strong>: Use robust scaling or outlier detection</p> </li> <li> <p><strong>RobustScaler limitations</strong>:</p> </li> <li><strong>Less efficient</strong>: May not utilize full feature range</li> <li><strong>Assumes symmetric distribution around median</strong></li> <li> <p><strong>Assessment</strong>: Check if IQR-based scaling is appropriate</p> </li> <li> <p><strong>Data leakage risk</strong>: Fitting scaler on entire dataset before train/test split</p> </li> <li><strong>Critical error</strong>: Using test data to compute scaling parameters</li> <li><strong>Solution</strong>: Always fit scaler only on training data</li> </ol> <h3 id=regularisation-assumptions-and-limitations>Regularisation Assumptions and Limitations</h3> <h4 id=key-assumptions_1>Key Assumptions</h4> <ol> <li><strong>Smooth coefficient penalty</strong>: Assumes large coefficients are undesirable</li> <li><strong>Feature relevance</strong>: L1 assumes many features are irrelevant (sparsity assumption)</li> <li><strong>Linear relationship</strong>: Regularization assumes linear model structure</li> <li><strong>Homoscedastic errors</strong>: Assumes constant error variance</li> </ol> <h4 id=limitations_1>Limitations</h4> <ol> <li><strong>L1 (Lasso) limitations</strong>:</li> <li><strong>Arbitrary feature selection</strong>: With correlated features, randomly picks one</li> <li><strong>Bias introduction</strong>: Can be overly aggressive in shrinking coefficients</li> <li> <p><strong>Solution</strong>: Use Elastic Net to combine L1 and L2</p> </li> <li> <p><strong>L2 (Ridge) limitations</strong>:</p> </li> <li><strong>No feature selection</strong>: Shrinks but doesn't eliminate features</li> <li><strong>Multicollinearity</strong>: Distributes weight among correlated features</li> <li> <p><strong>Alternative</strong>: Use Lasso for feature selection</p> </li> <li> <p><strong>Hyperparameter sensitivity</strong>: Performance heavily depends on regularization strength</p> </li> <li><strong>Challenge</strong>: Requires careful tuning using cross-validation</li> <li> <p><strong>Solution</strong>: Use automated hyperparameter optimization</p> </li> <li> <p><strong>Computational complexity</strong>: Some regularization methods scale poorly</p> </li> <li><strong>Impact</strong>: Lasso coordinate descent can be slow on very high-dimensional data</li> <li><strong>Solution</strong>: Use specialized libraries or approximate methods</li> </ol> <h3 id=comparison-of-techniques>Comparison of Techniques</h3> <table> <thead> <tr> <th>Aspect</th> <th>StandardScaler</th> <th>MinMaxScaler</th> <th>RobustScaler</th> <th>L1 (Lasso)</th> <th>L2 (Ridge)</th> </tr> </thead> <tbody> <tr> <td><strong>Outlier Sensitivity</strong></td> <td>High</td> <td>Very High</td> <td>Low</td> <td>Medium</td> <td>Medium</td> </tr> <tr> <td><strong>Preserves Distribution</strong></td> <td>Yes</td> <td>No</td> <td>Partially</td> <td>N/A</td> <td>N/A</td> </tr> <tr> <td><strong>Computational Cost</strong></td> <td>Low</td> <td>Low</td> <td>Medium</td> <td>High</td> <td>Low</td> </tr> <tr> <td><strong>Feature Selection</strong></td> <td>N/A</td> <td>N/A</td> <td>N/A</td> <td>Yes</td> <td>No</td> </tr> <tr> <td><strong>Interpretability</strong></td> <td>N/A</td> <td>N/A</td> <td>N/A</td> <td>High</td> <td>Medium</td> </tr> </tbody> </table> <p><strong>When to use each technique:</strong></p> <p><strong>Normalization:</strong> - <strong>StandardScaler</strong>: Normal distributions, no outliers, most ML algorithms - <strong>MinMaxScaler</strong>: Bounded features needed, neural networks - <strong>RobustScaler</strong>: Data with outliers, non-normal distributions - <strong>QuantileTransformer</strong>: Heavy outliers, need uniform distribution</p> <p><strong>Regularisation:</strong> - <strong>Ridge</strong>: Multicollinearity, want to keep all features - <strong>Lasso</strong>: Feature selection needed, sparse solution desired - <strong>Elastic Net</strong>: Correlated features, balanced selection and shrinkage</p> <h2 id=interview-questions>‚ùì Interview Questions</h2> <details class=question> <summary>Why is feature normalization important in machine learning, and when might you skip it?</summary> <p><strong>Answer:</strong> Feature normalization is crucial for algorithms sensitive to feature scales:</p> <p><strong>Why normalization matters</strong>: 1. <strong>Scale sensitivity</strong>: Algorithms like SVM, k-NN, neural networks use distance metrics 2. <strong>Convergence speed</strong>: Gradient descent converges faster with normalized features 3. <strong>Numerical stability</strong>: Prevents overflow/underflow in computations 4. <strong>Fair feature contribution</strong>: Ensures all features contribute meaningfully</p> <p><strong>Example impact</strong>: <div class=highlight><pre><span></span><code><span class=c1># Without normalization</span>
<span class=n>features</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>50000</span><span class=p>,</span> <span class=mi>25</span><span class=p>],</span> <span class=p>[</span><span class=mi>60000</span><span class=p>,</span> <span class=mi>30</span><span class=p>]]</span>  <span class=c1># [income, age]</span>
<span class=c1># Distance dominated by income differences</span>

<span class=c1># With normalization  </span>
<span class=n>features_norm</span> <span class=o>=</span> <span class=p>[[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.6</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>]]</span>
<span class=c1># Both features contribute to distance</span>
</code></pre></div></p> <p><strong>When to skip normalization</strong>: - <strong>Tree-based models</strong>: Decision trees, Random Forest, XGBoost (scale-invariant) - <strong>Naive Bayes</strong>: Works with original feature distributions - <strong>Linear regression</strong> with interpretability needs: Keep original coefficient meanings - <strong>Count data</strong>: When raw counts are meaningful (e.g., word frequencies)</p> <p><strong>Algorithm sensitivity</strong>: - <strong>Requires normalization</strong>: SVM, k-NN, neural networks, PCA, clustering - <strong>Doesn't require</strong>: Tree-based models, Naive Bayes</p> </details> <details class=question> <summary>Compare StandardScaler, MinMaxScaler, and RobustScaler. When would you use each?</summary> <p><strong>Answer:</strong> Each scaler handles different data characteristics:</p> <p><strong>StandardScaler (Z-score normalization)</strong>: - <strong>Formula</strong>: <code>(x - mean) / std</code> - <strong>Result</strong>: Mean = 0, Std = 1 - <strong>Best for</strong>: Normally distributed data without outliers - <strong>Use cases</strong>: Most ML algorithms, when data follows Gaussian distribution</p> <p><strong>MinMaxScaler</strong>: - <strong>Formula</strong>: <code>(x - min) / (max - min)</code> - <strong>Result</strong>: Range [0, 1] or custom range - <strong>Best for</strong>: Bounded output needed, neural networks - <strong>Use cases</strong>: Image processing, when you need specific value ranges</p> <p><strong>RobustScaler</strong>: - <strong>Formula</strong>: <code>(x - median) / IQR</code> - <strong>Result</strong>: Median = 0, IQR-based scale - <strong>Best for</strong>: Data with outliers, non-normal distributions - <strong>Use cases</strong>: Financial data, medical data with extreme values</p> <p><strong>Comparison with outliers</strong>: <div class=highlight><pre><span></span><code><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>100</span><span class=p>]</span>  <span class=c1># Last value is outlier</span>

<span class=c1># StandardScaler: All values affected by outlier</span>
<span class=c1># MinMaxScaler: Most values compressed near 0</span>
<span class=c1># RobustScaler: Outlier has minimal impact on scaling</span>
</code></pre></div></p> <p><strong>Decision framework</strong>: 1. <strong>Check for outliers</strong> Œª If many, use RobustScaler 2. <strong>Check distribution</strong> Œª If normal, use StandardScaler<br> 3. <strong>Check requirements</strong> Œª If bounded output needed, use MinMaxScaler 4. <strong>Algorithm requirements</strong> Œª Neural networks often prefer MinMax</p> </details> <details class=question> <summary>Explain the difference between L1 and L2 regularization. When would you use each?</summary> <p><strong>Answer:</strong> L1 and L2 regularization differ in penalty function and effects:</p> <p><strong>L1 Regularization (Lasso)</strong>: - <strong>Penalty</strong>: <span class=arithmatex>\(\lambda \sum |w_i|\)</span> (sum of absolute values) - <strong>Effect</strong>: Creates sparse solutions (sets coefficients to exactly zero) - <strong>Gradient</strong>: Constant magnitude, doesn't shrink with coefficient size - <strong>Feature selection</strong>: Automatically selects relevant features</p> <p><strong>L2 Regularization (Ridge)</strong>: - <strong>Penalty</strong>: <span class=arithmatex>\(\lambda \sum w_i^2\)</span> (sum of squared values) - <strong>Effect</strong>: Shrinks coefficients but rarely zeros them - <strong>Gradient</strong>: Proportional to coefficient size - <strong>Multicollinearity</strong>: Handles correlated features by distributing weights</p> <p><strong>Mathematical intuition</strong>: <div class=highlight><pre><span></span><code>L1 gradient: /w (Œª|w|) = ŒªŒªsign(w)    # Constant push toward zero
L2 gradient: /w (ŒªwŒª) = 2Œªw           # Proportional shrinkage
</code></pre></div></p> <p><strong>Visual difference</strong>: - <strong>L1 constraint region</strong>: Diamond shape Œª creates sparsity at corners - <strong>L2 constraint region</strong>: Circle shape Œª shrinks uniformly</p> <p><strong>When to use L1</strong>: -  Feature selection needed -  Interpretable sparse models -  High-dimensional data with irrelevant features -  Storage/computation constraints</p> <p><strong>When to use L2</strong>: -  All features potentially relevant -  Multicollinearity present -  Stability over sparsity -  Better numerical properties</p> <p><strong>Elastic Net</strong> combines both: <span class=arithmatex>\(\alpha \rho ||w||_1 + \alpha(1-\rho)||w||_2^2\)</span></p> </details> <details class=question> <summary>How do you determine the optimal regularization strength (lambda/alpha)?</summary> <p><strong>Answer:</strong> Several approaches for finding optimal regularization strength:</p> <p><strong>1. Cross-Validation (Most common)</strong>: <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>cross_val_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>Ridge</span>

<span class=n>alphas</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=mf>10.0</span><span class=p>,</span> <span class=mf>100.0</span><span class=p>]</span>
<span class=n>best_alpha</span> <span class=o>=</span> <span class=kc>None</span>
<span class=n>best_score</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>inf</span>

<span class=k>for</span> <span class=n>alpha</span> <span class=ow>in</span> <span class=n>alphas</span><span class=p>:</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>)</span>
    <span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span> <span class=o>&gt;</span> <span class=n>best_score</span><span class=p>:</span>
        <span class=n>best_score</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
        <span class=n>best_alpha</span> <span class=o>=</span> <span class=n>alpha</span>
</code></pre></div></p> <p><strong>2. Grid Search with Cross-Validation</strong>: <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>GridSearchCV</span>

<span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;alpha&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>20</span><span class=p>)}</span>
<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>Ridge</span><span class=p>(),</span> <span class=n>param_grid</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
<span class=n>optimal_alpha</span> <span class=o>=</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=p>[</span><span class=s1>&#39;alpha&#39;</span><span class=p>]</span>
</code></pre></div></p> <p><strong>3. Regularization Path Analysis</strong>: - Plot performance vs. regularization strength - Look for "elbow" in validation curve - Balance bias-variance tradeoff</p> <p><strong>4. Information Criteria (AIC/BIC)</strong>: <div class=highlight><pre><span></span><code><span class=c1># For model selection without separate validation set</span>
<span class=c1># AIC = 2k - 2ln(L)  where k=parameters, L=likelihood</span>
</code></pre></div></p> <p><strong>5. Early Stopping</strong>: - Monitor validation loss during training - Stop when validation loss stops improving - Implicit regularization through training time</p> <p><strong>Search strategies</strong>: - <strong>Coarse to fine</strong>: Start with wide range, then narrow down - <strong>Logarithmic spacing</strong>: Use <code>np.logspace</code> for wide range exploration - <strong>Nested CV</strong>: Use inner CV for hyperparameter selection, outer CV for evaluation</p> <p><strong>Practical tips</strong>: - Start with wide range: [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100] - Use stratified CV for classification - Consider computational budget vs. accuracy needs - Validate final choice on completely separate test set</p> </details> <details class=question> <summary>What is the bias-variance tradeoff in the context of regularization?</summary> <p><strong>Answer:</strong> Regularization directly addresses the bias-variance tradeoff:</p> <p><strong>Bias-Variance Decomposition</strong>: <span class=arithmatex>\(<span class=arithmatex>\(E[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Noise}\)</span>\)</span></p> <p><strong>Without Regularization</strong>: - <strong>Low bias</strong>: Model can fit training data well - <strong>High variance</strong>: Model overfits, predictions vary greatly with training data - <strong>Risk</strong>: Poor generalization to new data</p> <p><strong>With Regularization</strong>: - <strong>Higher bias</strong>: Model constrained, can't fit training data perfectly - <strong>Lower variance</strong>: More stable predictions across different training sets - <strong>Goal</strong>: Minimize total error = BiasŒª + Variance + Noise</p> <p><strong>Regularization effects</strong>: <div class=highlight><pre><span></span><code><span class=c1># No regularization (Œª = 0): High variance, low bias</span>
<span class=c1># Strong regularization (Œª &gt;&gt; 1): Low variance, high bias  </span>
<span class=c1># Optimal Œª: Minimizes biasŒª + variance</span>
</code></pre></div></p> <p><strong>Visual intuition</strong>: - <strong>Underfit</strong> (too much regularization): High bias, predictions too simple - <strong>Overfit</strong> (too little regularization): High variance, predictions too complex - <strong>Just right</strong>: Balanced complexity, good generalization</p> <p><strong>Practical example</strong>: <div class=highlight><pre><span></span><code><span class=c1># Polynomial regression with different regularization</span>
<span class=n>Œª</span> <span class=o>=</span> <span class=mi>0</span><span class=p>:</span>    <span class=n>Perfect</span> <span class=n>training</span> <span class=n>fit</span><span class=p>,</span> <span class=n>poor</span> <span class=n>test</span> <span class=n>performance</span> <span class=p>(</span><span class=n>overfit</span><span class=p>)</span>
<span class=n>Œª</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>:</span>  <span class=n>Good</span> <span class=n>training</span> <span class=n>fit</span><span class=p>,</span> <span class=n>good</span> <span class=n>test</span> <span class=n>performance</span> <span class=p>(</span><span class=n>balanced</span><span class=p>)</span>  
<span class=n>Œª</span> <span class=o>=</span> <span class=mi>100</span><span class=p>:</span>  <span class=n>Poor</span> <span class=n>training</span> <span class=n>fit</span><span class=p>,</span> <span class=n>poor</span> <span class=n>test</span> <span class=n>performance</span> <span class=p>(</span><span class=n>underfit</span><span class=p>)</span>
</code></pre></div></p> <p><strong>How to detect</strong>: - <strong>High variance</strong>: Large gap between training and validation performance - <strong>High bias</strong>: Both training and validation performance are poor - <strong>Optimal point</strong>: Minimal validation error</p> <p><strong>Regularization strength effects</strong>: - <strong>Increasing Œª</strong>: Reduces variance, increases bias - <strong>Decreasing Œª</strong>: Reduces bias, increases variance - <strong>Sweet spot</strong>: Cross-validation finds optimal balance</p> </details> <details class=question> <summary>How does regularization help with multicollinearity, and what's the difference between Ridge and Lasso in handling it?</summary> <p><strong>Answer:</strong> Regularization addresses multicollinearity differently depending on the type:</p> <p><strong>Multicollinearity problem</strong>: - <strong>Issue</strong>: When features are highly correlated, ordinary least squares becomes unstable - <strong>Effect</strong>: Small changes in data cause large changes in coefficients - <strong>Math</strong>: <span class=arithmatex>\((X^T X)\)</span> becomes nearly singular, leading to unstable <span class=arithmatex>\((X^T X)^{-1}\)</span></p> <p><strong>Ridge Regression approach</strong>: - <strong>Solution</strong>: Adds <span class=arithmatex>\(\lambda I\)</span> to <span class=arithmatex>\((X^T X)\)</span>, making it invertible - <strong>Formula</strong>: <span class=arithmatex>\((X^T X + \lambda I)^{-1} X^T y\)</span> - <strong>Effect</strong>: Distributes coefficients among correlated features - <strong>Example</strong>: If features A and B are identical, Ridge gives both coefficient = 0.5</p> <p><strong>Lasso Regression approach</strong>: - <strong>Solution</strong>: L1 penalty forces sparsity - <strong>Effect</strong>: Arbitrarily picks one feature from correlated group - <strong>Example</strong>: If features A and B are identical, Lasso gives one coefficient = 1, other = 0 - <strong>Limitation</strong>: Selection among correlated features is somewhat random</p> <p><strong>Practical example</strong>: <div class=highlight><pre><span></span><code><span class=c1># Highly correlated features: house size and number of rooms</span>
<span class=n>X</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>2000</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>2500</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>3000</span><span class=p>,</span> <span class=mi>6</span><span class=p>]]</span>  <span class=c1># [sqft, rooms]</span>

<span class=c1># Ridge: Both features get partial coefficients</span>
<span class=c1># Ridge coefficients: [0.7, 0.6] (both contribute)</span>

<span class=c1># Lasso: One feature dominates  </span>
<span class=c1># Lasso coefficients: [1.2, 0.0] (only sqft matters)</span>
</code></pre></div></p> <p><strong>Elastic Net solution</strong>: - <strong>Combines both</strong>: <span class=arithmatex>\(\alpha \rho ||w||_1 + \alpha(1-\rho)||w||_2^2\)</span> - <strong>Advantage</strong>: Groups correlated features together (like Ridge) but maintains sparsity (like Lasso) - <strong>Best of both</strong>: Handles multicollinearity while doing feature selection</p> <p><strong>Comparison summary</strong>: | Aspect | Ridge | Lasso | Elastic Net | |--------|-------|-------|-------------| | <strong>Multicollinearity</strong> | Distributes weights | Random selection | Groups + selects | | <strong>Stability</strong> | High | Can be unstable | High | | <strong>Feature selection</strong> | No | Yes | Yes | | <strong>Interpretability</strong> | Medium | High | High |</p> <p><strong>When to use each</strong>: - <strong>Ridge</strong>: When you believe all features are relevant - <strong>Lasso</strong>: When you need automatic feature selection - <strong>Elastic Net</strong>: When you have groups of correlated features</p> </details> <details class=question> <summary>Explain data leakage in the context of feature scaling and how to prevent it.</summary> <p><strong>Answer:</strong> Data leakage in feature scaling occurs when test set information influences the scaling parameters:</p> <p><strong>What is scaling data leakage?</strong> - <strong>Problem</strong>: Using entire dataset (including test set) to compute scaling parameters - <strong>Effect</strong>: Model has indirect access to test set information - <strong>Result</strong>: Overly optimistic performance estimates</p> <p><strong>Common mistakes</strong>: <div class=highlight><pre><span></span><code><span class=c1># WRONG: Scaling before train/test split</span>
<span class=n>X_scaled</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>  <span class=c1># Uses ALL data</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=c1># WRONG: Fitting scaler on combined data</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>]))</span>
<span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</code></pre></div></p> <p><strong>Correct approach</strong>: <div class=highlight><pre><span></span><code><span class=c1># CORRECT: Split first, then scale</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>  <span class=c1># Fit only on training</span>
<span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>        <span class=c1># Transform using train params</span>
</code></pre></div></p> <p><strong>Why this matters</strong>: - <strong>Statistical contamination</strong>: Test set statistics influence training - <strong>Optimistic bias</strong>: Model appears better than it actually is - <strong>Production problems</strong>: Real-world performance differs from validation</p> <p><strong>Cross-validation considerations</strong>: <div class=highlight><pre><span></span><code><span class=c1># CORRECT: Scaling inside CV loop</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>

<span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
    <span class=p>(</span><span class=s1>&#39;scaler&#39;</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>()),</span>
    <span class=p>(</span><span class=s1>&#39;model&#39;</span><span class=p>,</span> <span class=n>LogisticRegression</span><span class=p>())</span>
<span class=p>])</span>

<span class=c1># Scaler fitted separately for each CV fold</span>
<span class=n>scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div></p> <p><strong>Time series special case</strong>: - <strong>Problem</strong>: Future information leaking to past predictions - <strong>Solution</strong>: Use forward-chaining validation, fit scaler only on past data</p> <p><strong>Impact magnitude</strong>: - Usually small but can be significant with small datasets - More problematic with MinMaxScaler (uses min/max) - Less problematic with RobustScaler (uses median/IQR)</p> <p><strong>Detection methods</strong>: - Compare performance with/without proper scaling separation - Check if test performance seems unrealistically high - Validate scaling parameters make sense for training data only</p> </details> <details class=question> <summary>How do you handle categorical features when applying normalization?</summary> <p><strong>Answer:</strong> Categorical features require special handling as traditional scaling methods don't apply:</p> <p><strong>Why traditional scaling fails</strong>: - <strong>No inherent order</strong>: Categories like [Red, Blue, Green] have no meaningful distance - <strong>Arbitrary encoding</strong>: Label encoding creates fake ordinal relationships - <strong>Scale meaningless</strong>: Normalizing [1, 2, 3] for categories is nonsensical</p> <p><strong>Proper approaches</strong>:</p> <p><strong>1. One-Hot Encoding</strong> (most common): <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>OneHotEncoder</span>

<span class=c1># Original: [&#39;Red&#39;, &#39;Blue&#39;, &#39;Red&#39;, &#39;Green&#39;]  </span>
<span class=c1># One-hot: [[1,0,0], [0,1,0], [1,0,0], [0,0,1]]</span>

<span class=n>ohe</span> <span class=o>=</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>,</span> <span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>  <span class=c1># Avoid multicollinearity</span>
<span class=n>X_categorical_encoded</span> <span class=o>=</span> <span class=n>ohe</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_categorical</span><span class=p>)</span>

<span class=c1># Then apply scaling to numerical features only</span>
</code></pre></div></p> <p><strong>2. Target Encoding</strong>: <div class=highlight><pre><span></span><code><span class=c1># Replace category with mean target value for that category</span>
<span class=n>category_means</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;category&#39;</span><span class=p>)[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;category_encoded&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;category&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>category_means</span><span class=p>)</span>

<span class=c1># Can then apply scaling to encoded values</span>
</code></pre></div></p> <p><strong>3. Mixed data pipeline</strong>: <div class=highlight><pre><span></span><code><span class=kn>from</span><span class=w> </span><span class=nn>sklearn.compose</span><span class=w> </span><span class=kn>import</span> <span class=n>ColumnTransformer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>OneHotEncoder</span>

<span class=c1># Specify which columns are categorical vs numerical</span>
<span class=n>numerical_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;income&#39;</span><span class=p>,</span> <span class=s1>&#39;credit_score&#39;</span><span class=p>]</span>
<span class=n>categorical_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;city&#39;</span><span class=p>,</span> <span class=s1>&#39;job_type&#39;</span><span class=p>,</span> <span class=s1>&#39;education&#39;</span><span class=p>]</span>

<span class=n>preprocessor</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>(</span>
    <span class=n>transformers</span><span class=o>=</span><span class=p>[</span>
        <span class=p>(</span><span class=s1>&#39;num&#39;</span><span class=p>,</span> <span class=n>StandardScaler</span><span class=p>(),</span> <span class=n>numerical_features</span><span class=p>),</span>
        <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>),</span> <span class=n>categorical_features</span><span class=p>)</span>
    <span class=p>]</span>
<span class=p>)</span>

<span class=n>X_processed</span> <span class=o>=</span> <span class=n>preprocessor</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</code></pre></div></p> <p><strong>4. Ordinal encoding</strong> (only for ordinal categories): <div class=highlight><pre><span></span><code><span class=c1># Only for inherently ordered categories</span>
<span class=n>education_mapping</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;High School&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
    <span class=s1>&#39;Bachelor&#39;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> 
    <span class=s1>&#39;Master&#39;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span>
    <span class=s1>&#39;PhD&#39;</span><span class=p>:</span> <span class=mi>4</span>
<span class=p>}</span>

<span class=n>df</span><span class=p>[</span><span class=s1>&#39;education_ordinal&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;education&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>education_mapping</span><span class=p>)</span>
<span class=c1># Can then apply scaling</span>
</code></pre></div></p> <p><strong>Best practices</strong>: - <strong>Separate preprocessing</strong>: Handle categorical and numerical features separately - <strong>Pipeline usage</strong>: Use sklearn pipelines to prevent data leakage - <strong>High cardinality</strong>: Consider target encoding or embedding for many categories - <strong>Rare categories</strong>: Group infrequent categories into "Other" before encoding - <strong>Validation</strong>: Ensure consistent categories between train/test sets</p> <p><strong>Common pitfalls</strong>: - Scaling label-encoded categorical features - Forgetting to handle new categories in test set - Creating too many dummy variables (curse of dimensionality) - Data leakage in target encoding without proper CV</p> </details> <details class=question> <summary>What are some advanced regularization techniques beyond L1/L2?</summary> <p><strong>Answer:</strong> Several advanced regularization techniques beyond basic L1/L2:</p> <p><strong>1. Elastic Net</strong>: - <strong>Formula</strong>: <span class=arithmatex>\(\alpha \rho ||w||_1 + \alpha(1-\rho)||w||_2^2\)</span> - <strong>Advantage</strong>: Combines L1 sparsity with L2 stability - <strong>Use case</strong>: Correlated features where you want grouping + selection</p> <p><strong>2. Group Lasso</strong>: - <strong>Concept</strong>: Regularizes groups of features together - <strong>Formula</strong>: <span class=arithmatex>\(\lambda \sum_{g} ||w_g||_2\)</span> where <span class=arithmatex>\(g\)</span> represents feature groups - <strong>Effect</strong>: Either selects entire group or zeros out entire group - <strong>Use case</strong>: Gene expression, image pixels, polynomial features</p> <p><strong>3. Fused Lasso (Total Variation)</strong>: - <strong>Formula</strong>: <span class=arithmatex>\(\lambda_1 ||w||_1 + \lambda_2 \sum_{i} |w_i - w_{i+1}|\)</span> - <strong>Effect</strong>: Promotes sparsity + smooth coefficient transitions - <strong>Use case</strong>: Time series, spatial data, signal processing</p> <p><strong>4. Nuclear Norm (Matrix Regularization)</strong>: - <strong>Formula</strong>: <span class=arithmatex>\(\lambda ||W||_*\)</span> (sum of singular values) - <strong>Effect</strong>: Promotes low-rank solutions - <strong>Use case</strong>: Matrix completion, collaborative filtering</p> <p><strong>5. Dropout (Neural Networks)</strong>: <div class=highlight><pre><span></span><code><span class=c1># Randomly set neurons to zero during training</span>
<span class=k>def</span><span class=w> </span><span class=nf>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>rate</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>training</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
    <span class=k>if</span> <span class=n>training</span><span class=p>:</span>
        <span class=n>mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=o>-</span><span class=n>rate</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mask</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>rate</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>x</span>
</code></pre></div></p> <p><strong>6. Batch Normalization</strong>: - <strong>Concept</strong>: Normalize layer inputs during training - <strong>Effect</strong>: Stabilizes training, acts as regularization - <strong>Formula</strong>: <span class=arithmatex>\(\frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} * \gamma + \beta\)</span></p> <p><strong>7. Data Augmentation</strong>: <div class=highlight><pre><span></span><code><span class=c1># Create additional training examples through transformations</span>
<span class=c1># Images: rotation, scaling, flipping</span>
<span class=c1># Text: synonym replacement, back-translation</span>
<span class=c1># Time series: jittering, warping</span>
</code></pre></div></p> <p><strong>8. Early Stopping</strong>: - <strong>Method</strong>: Stop training when validation loss stops improving - <strong>Effect</strong>: Prevents overfitting through limited training time - <strong>Implementation</strong>: Monitor validation loss, stop after patience epochs</p> <p><strong>9. Weight Decay</strong>: - <strong>Concept</strong>: Gradually reduce all weights during training - <strong>Formula</strong>: <span class=arithmatex>\(w_{t+1} = (1-\lambda)w_t - \alpha \nabla L\)</span> - <strong>Effect</strong>: Similar to L2 but applied during optimization</p> <p><strong>10. Spectral Normalization</strong>: - <strong>Method</strong>: Constrain spectral norm of weight matrices - <strong>Effect</strong>: Stabilizes GAN training, improves generalization - <strong>Use case</strong>: Generative models, discriminator regularization</p> <p><strong>Advanced combinations</strong>: <div class=highlight><pre><span></span><code><span class=c1># Multi-task learning with shared regularization</span>
<span class=n>Loss</span> <span class=o>=</span> <span class=n>Œª</span> <span class=n>TaskLoss_i</span> <span class=o>+</span> <span class=n>ŒªŒª</span><span class=o>||</span><span class=n>W_shared</span><span class=o>||</span><span class=n>ŒªŒª</span> <span class=o>+</span> <span class=n>ŒªŒª</span><span class=o>||</span><span class=n>W_specific</span><span class=o>||</span><span class=n>Œª</span>

<span class=c1># Adaptive regularization (learning Œª)</span>
<span class=n>Œª</span> <span class=o>=</span> <span class=n>ŒªŒª</span> <span class=o>*</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>decay</span> <span class=o>*</span> <span class=n>epoch</span><span class=p>)</span>
</code></pre></div></p> <p><strong>Selection criteria</strong>: - <strong>Data structure</strong>: Spatial/temporal data Œª Fused Lasso - <strong>High dimensions</strong>: Group Lasso, Nuclear norm - <strong>Neural networks</strong>: Dropout, Batch norm, Weight decay - <strong>Interpretability needs</strong>: L1, Group Lasso - <strong>Stability needs</strong>: L2, Elastic Net</p> </details> <h2 id=examples>üìù Examples</h2> <h3 id=real-world-example-customer-churn-prediction>Real-world Example: Customer Churn Prediction</h3> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span><span class=p>,</span> <span class=n>cross_val_score</span><span class=p>,</span> <span class=n>GridSearchCV</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>LabelEncoder</span><span class=p>,</span> <span class=n>OneHotEncoder</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LogisticRegression</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>classification_report</span><span class=p>,</span> <span class=n>roc_auc_score</span><span class=p>,</span> <span class=n>roc_curve</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.compose</span><span class=w> </span><span class=kn>import</span> <span class=n>ColumnTransformer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>import</span><span class=w> </span><span class=nn>warnings</span>
<span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>

<span class=c1># Generate realistic customer churn dataset</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>n_customers</span> <span class=o>=</span> <span class=mi>5000</span>

<span class=c1># Create realistic customer features with different scales</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;customer_id&#39;</span><span class=p>:</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_customers</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span>
    <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>40</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>18</span><span class=p>,</span> <span class=mi>80</span><span class=p>),</span>
    <span class=s1>&#39;monthly_charges&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>70</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>200</span><span class=p>),</span>
    <span class=s1>&#39;total_charges&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>2500</span><span class=p>,</span> <span class=mi>1500</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>8000</span><span class=p>),</span>
    <span class=s1>&#39;contract_length&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>24</span><span class=p>],</span> <span class=n>n_customers</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=p>[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>]),</span>
    <span class=s1>&#39;num_services&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>poisson</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>8</span><span class=p>),</span>
    <span class=s1>&#39;support_calls&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>poisson</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>),</span>
    <span class=s1>&#39;payment_method&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=s1>&#39;Credit Card&#39;</span><span class=p>,</span> <span class=s1>&#39;Bank Transfer&#39;</span><span class=p>,</span> <span class=s1>&#39;Cash&#39;</span><span class=p>,</span> <span class=s1>&#39;Check&#39;</span><span class=p>],</span> 
                                      <span class=n>n_customers</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=p>[</span><span class=mf>0.4</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>]),</span>
    <span class=s1>&#39;internet_type&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=s1>&#39;DSL&#39;</span><span class=p>,</span> <span class=s1>&#39;Fiber&#39;</span><span class=p>,</span> <span class=s1>&#39;Cable&#39;</span><span class=p>,</span> <span class=s1>&#39;None&#39;</span><span class=p>],</span> 
                                     <span class=n>n_customers</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=p>[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>]),</span>
    <span class=s1>&#39;senior_citizen&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>n_customers</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=p>[</span><span class=mf>0.85</span><span class=p>,</span> <span class=mf>0.15</span><span class=p>])</span>
<span class=p>}</span>

<span class=c1># Create target variable (churn) with realistic relationships</span>
<span class=n>churn_prob</span> <span class=o>=</span> <span class=p>(</span>
    <span class=mf>0.1</span> <span class=o>+</span>  <span class=c1># Base churn rate</span>
    <span class=mf>0.1</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;monthly_charges&#39;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>100</span><span class=p>)</span> <span class=o>+</span>  <span class=c1># High charges increase churn</span>
    <span class=mf>0.15</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;contract_length&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=o>+</span>  <span class=c1># Month-to-month increases churn</span>
    <span class=mf>0.1</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;support_calls&#39;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>3</span><span class=p>)</span> <span class=o>+</span>      <span class=c1># Many support calls indicate issues</span>
    <span class=mf>0.05</span> <span class=o>*</span> <span class=n>data</span><span class=p>[</span><span class=s1>&#39;senior_citizen&#39;</span><span class=p>]</span> <span class=o>+</span>          <span class=c1># Seniors slightly more likely to churn</span>
    <span class=o>-</span><span class=mf>0.08</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;contract_length&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=mi>24</span><span class=p>)</span>  <span class=c1># Long contracts reduce churn</span>
<span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

<span class=n>data</span><span class=p>[</span><span class=s1>&#39;churn&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>churn_prob</span><span class=p>,</span> <span class=n>n_customers</span><span class=p>)</span>

<span class=c1># Create DataFrame</span>
<span class=n>df_churn</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Customer Churn Prediction - Normalization &amp; Regularization Demo&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Dataset shape: </span><span class=si>{</span><span class=n>df_churn</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Churn rate: </span><span class=si>{</span><span class=n>df_churn</span><span class=p>[</span><span class=s1>&#39;churn&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.1%</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Dataset overview:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df_churn</span><span class=o>.</span><span class=n>describe</span><span class=p>())</span>

<span class=c1># Analyze feature distributions and scales</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>numerical_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;monthly_charges&#39;</span><span class=p>,</span> <span class=s1>&#39;total_charges&#39;</span><span class=p>,</span> <span class=s1>&#39;contract_length&#39;</span><span class=p>,</span> <span class=s1>&#39;num_services&#39;</span><span class=p>,</span> <span class=s1>&#39;support_calls&#39;</span><span class=p>]</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>feature</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>numerical_features</span><span class=p>):</span>
    <span class=n>row</span><span class=p>,</span> <span class=n>col</span> <span class=o>=</span> <span class=n>i</span> <span class=o>//</span> <span class=mi>3</span><span class=p>,</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>3</span>

    <span class=c1># Plot distribution</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>df_churn</span><span class=p>[</span><span class=n>feature</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=se>\n</span><span class=s1>Range: [</span><span class=si>{</span><span class=n>df_churn</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span><span class=si>:</span><span class=s1>.0f</span><span class=si>}</span><span class=s1>, </span><span class=si>{</span><span class=n>df_churn</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s1>.0f</span><span class=si>}</span><span class=s1>]&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Frequency&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=s1>&#39;Feature Distributions (Before Normalization)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Prepare features for modeling</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>df_churn</span><span class=o>.</span><span class=n>drop</span><span class=p>([</span><span class=s1>&#39;customer_id&#39;</span><span class=p>,</span> <span class=s1>&#39;churn&#39;</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>df_churn</span><span class=p>[</span><span class=s1>&#39;churn&#39;</span><span class=p>]</span>

<span class=c1># Split data</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>stratify</span><span class=o>=</span><span class=n>y</span>
<span class=p>)</span>

<span class=c1># Define preprocessing for different column types</span>
<span class=n>numerical_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span> <span class=s1>&#39;monthly_charges&#39;</span><span class=p>,</span> <span class=s1>&#39;total_charges&#39;</span><span class=p>,</span> <span class=s1>&#39;contract_length&#39;</span><span class=p>,</span> <span class=s1>&#39;num_services&#39;</span><span class=p>,</span> <span class=s1>&#39;support_calls&#39;</span><span class=p>]</span>
<span class=n>categorical_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;payment_method&#39;</span><span class=p>,</span> <span class=s1>&#39;internet_type&#39;</span><span class=p>,</span> <span class=s1>&#39;senior_citizen&#39;</span><span class=p>]</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Feature preprocessing:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Numerical features: </span><span class=si>{</span><span class=n>numerical_features</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Categorical features: </span><span class=si>{</span><span class=n>categorical_features</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Test different scaling approaches</span>
<span class=n>scalers_test</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;No Scaling&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
    <span class=s1>&#39;StandardScaler&#39;</span><span class=p>:</span> <span class=n>StandardScaler</span><span class=p>(),</span>
    <span class=s1>&#39;MinMaxScaler&#39;</span><span class=p>:</span> <span class=n>MinMaxScaler</span><span class=p>(),</span>
    <span class=s1>&#39;RobustScaler&#39;</span><span class=p>:</span> <span class=n>RobustScaler</span><span class=p>()</span>
<span class=p>}</span>

<span class=n>preprocessing_results</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>scaler_name</span><span class=p>,</span> <span class=n>scaler</span> <span class=ow>in</span> <span class=n>scalers_test</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Testing </span><span class=si>{</span><span class=n>scaler_name</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>

    <span class=c1># Create preprocessing pipeline</span>
    <span class=k>if</span> <span class=n>scaler</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=c1># No scaling - just handle categorical variables</span>
        <span class=n>preprocessor</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>(</span>
            <span class=n>transformers</span><span class=o>=</span><span class=p>[</span>
                <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>,</span> <span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span> <span class=n>categorical_features</span><span class=p>)</span>
            <span class=p>],</span>
            <span class=n>remainder</span><span class=o>=</span><span class=s1>&#39;passthrough&#39;</span>  <span class=c1># Keep numerical features as-is</span>
        <span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=c1># Apply scaling to numerical features</span>
        <span class=n>preprocessor</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>(</span>
            <span class=n>transformers</span><span class=o>=</span><span class=p>[</span>
                <span class=p>(</span><span class=s1>&#39;num&#39;</span><span class=p>,</span> <span class=n>scaler</span><span class=p>,</span> <span class=n>numerical_features</span><span class=p>),</span>
                <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>,</span> <span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span> <span class=n>categorical_features</span><span class=p>)</span>
            <span class=p>]</span>
        <span class=p>)</span>

    <span class=c1># Create full pipeline with logistic regression</span>
    <span class=n>pipeline</span> <span class=o>=</span> <span class=n>Pipeline</span><span class=p>([</span>
        <span class=p>(</span><span class=s1>&#39;preprocessor&#39;</span><span class=p>,</span> <span class=n>preprocessor</span><span class=p>),</span>
        <span class=p>(</span><span class=s1>&#39;classifier&#39;</span><span class=p>,</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>))</span>
    <span class=p>])</span>

    <span class=c1># Evaluate using cross-validation</span>
    <span class=n>cv_scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>pipeline</span><span class=p>,</span> <span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>)</span>

    <span class=c1># Fit and predict for detailed metrics</span>
    <span class=n>pipeline</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
    <span class=n>y_pred_proba</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
    <span class=n>test_auc</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_proba</span><span class=p>)</span>

    <span class=n>preprocessing_results</span><span class=p>[</span><span class=n>scaler_name</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;cv_auc_mean&#39;</span><span class=p>:</span> <span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span>
        <span class=s1>&#39;cv_auc_std&#39;</span><span class=p>:</span> <span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>(),</span>
        <span class=s1>&#39;test_auc&#39;</span><span class=p>:</span> <span class=n>test_auc</span>
    <span class=p>}</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  CV AUC: </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=o>*</span><span class=mi>2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Test AUC: </span><span class=si>{</span><span class=n>test_auc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Compare preprocessing approaches</span>
<span class=n>preprocessing_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>preprocessing_results</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>

<span class=c1># CV performance comparison</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>methods</span> <span class=o>=</span> <span class=n>preprocessing_df</span><span class=o>.</span><span class=n>index</span>
<span class=n>cv_means</span> <span class=o>=</span> <span class=n>preprocessing_df</span><span class=p>[</span><span class=s1>&#39;cv_auc_mean&#39;</span><span class=p>]</span>
<span class=n>cv_stds</span> <span class=o>=</span> <span class=n>preprocessing_df</span><span class=p>[</span><span class=s1>&#39;cv_auc_std&#39;</span><span class=p>]</span>

<span class=n>bars</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>methods</span><span class=p>,</span> <span class=n>cv_means</span><span class=p>,</span> <span class=n>yerr</span><span class=o>=</span><span class=n>cv_stds</span><span class=p>,</span> <span class=n>capsize</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Cross-Validation Performance by Scaling Method&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;AUC Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Add value labels</span>
<span class=k>for</span> <span class=n>bar</span><span class=p>,</span> <span class=n>mean</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>bars</span><span class=p>,</span> <span class=n>cv_means</span><span class=p>):</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>bar</span><span class=o>.</span><span class=n>get_x</span><span class=p>()</span> <span class=o>+</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_width</span><span class=p>()</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_height</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.005</span><span class=p>,</span> 
             <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>mean</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;bottom&#39;</span><span class=p>)</span>

<span class=c1># Test performance comparison</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>test_aucs</span> <span class=o>=</span> <span class=n>preprocessing_df</span><span class=p>[</span><span class=s1>&#39;test_auc&#39;</span><span class=p>]</span>
<span class=n>bars</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>methods</span><span class=p>,</span> <span class=n>test_aucs</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;orange&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Test Set Performance by Scaling Method&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;AUC Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Add value labels</span>
<span class=k>for</span> <span class=n>bar</span><span class=p>,</span> <span class=n>auc</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>bars</span><span class=p>,</span> <span class=n>test_aucs</span><span class=p>):</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>bar</span><span class=o>.</span><span class=n>get_x</span><span class=p>()</span> <span class=o>+</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_width</span><span class=p>()</span><span class=o>/</span><span class=mi>2</span><span class=p>,</span> <span class=n>bar</span><span class=o>.</span><span class=n>get_height</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.005</span><span class=p>,</span> 
             <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>auc</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;bottom&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Now test different regularization techniques</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;REGULARIZATION COMPARISON&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>

<span class=c1># Use best scaling method</span>
<span class=n>best_scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>  <span class=c1># Typically works well</span>

<span class=n>preprocessor_final</span> <span class=o>=</span> <span class=n>ColumnTransformer</span><span class=p>(</span>
    <span class=n>transformers</span><span class=o>=</span><span class=p>[</span>
        <span class=p>(</span><span class=s1>&#39;num&#39;</span><span class=p>,</span> <span class=n>best_scaler</span><span class=p>,</span> <span class=n>numerical_features</span><span class=p>),</span>
        <span class=p>(</span><span class=s1>&#39;cat&#39;</span><span class=p>,</span> <span class=n>OneHotEncoder</span><span class=p>(</span><span class=n>drop</span><span class=o>=</span><span class=s1>&#39;first&#39;</span><span class=p>,</span> <span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span> <span class=n>categorical_features</span><span class=p>)</span>
    <span class=p>]</span>
<span class=p>)</span>

<span class=c1># Apply preprocessing</span>
<span class=n>X_train_processed</span> <span class=o>=</span> <span class=n>preprocessor_final</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_processed</span> <span class=o>=</span> <span class=n>preprocessor_final</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Processed feature shape: </span><span class=si>{</span><span class=n>X_train_processed</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Test different regularization techniques</span>
<span class=n>regularization_models</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Logistic Regression (No Reg)&#39;</span><span class=p>:</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
    <span class=s1>&#39;Ridge (L2)&#39;</span><span class=p>:</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;l2&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
    <span class=s1>&#39;Lasso (L1)&#39;</span><span class=p>:</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;l1&#39;</span><span class=p>,</span> <span class=n>solver</span><span class=o>=</span><span class=s1>&#39;liblinear&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
    <span class=s1>&#39;ElasticNet&#39;</span><span class=p>:</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;elasticnet&#39;</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>solver</span><span class=o>=</span><span class=s1>&#39;saga&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>regularization_results</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>model_name</span><span class=p>,</span> <span class=n>model</span> <span class=ow>in</span> <span class=n>regularization_models</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Testing </span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>

    <span class=c1># Cross-validation</span>
    <span class=n>cv_scores</span> <span class=o>=</span> <span class=n>cross_val_score</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>X_train_processed</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>)</span>

    <span class=c1># Fit model</span>
    <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_processed</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

    <span class=c1># Predictions</span>
    <span class=n>y_pred_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test_processed</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
    <span class=n>test_auc</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_proba</span><span class=p>)</span>

    <span class=c1># Count non-zero coefficients (sparsity)</span>
    <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s1>&#39;coef_&#39;</span><span class=p>):</span>
        <span class=n>non_zero_coefs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>1e-5</span><span class=p>)</span>
        <span class=n>total_coefs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
        <span class=n>sparsity</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>non_zero_coefs</span> <span class=o>/</span> <span class=n>total_coefs</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>non_zero_coefs</span> <span class=o>=</span> <span class=s2>&quot;N/A&quot;</span>
        <span class=n>sparsity</span> <span class=o>=</span> <span class=s2>&quot;N/A&quot;</span>

    <span class=n>regularization_results</span><span class=p>[</span><span class=n>model_name</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;cv_auc_mean&#39;</span><span class=p>:</span> <span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>(),</span>
        <span class=s1>&#39;cv_auc_std&#39;</span><span class=p>:</span> <span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>(),</span>
        <span class=s1>&#39;test_auc&#39;</span><span class=p>:</span> <span class=n>test_auc</span><span class=p>,</span>
        <span class=s1>&#39;non_zero_coefs&#39;</span><span class=p>:</span> <span class=n>non_zero_coefs</span><span class=p>,</span>
        <span class=s1>&#39;sparsity&#39;</span><span class=p>:</span> <span class=n>sparsity</span><span class=p>,</span>
        <span class=s1>&#39;model&#39;</span><span class=p>:</span> <span class=n>model</span>
    <span class=p>}</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  CV AUC: </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2> (+/- </span><span class=si>{</span><span class=n>cv_scores</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=o>*</span><span class=mi>2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Test AUC: </span><span class=si>{</span><span class=n>test_auc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Non-zero coefficients: </span><span class=si>{</span><span class=n>non_zero_coefs</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>sparsity</span> <span class=o>!=</span> <span class=s2>&quot;N/A&quot;</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Sparsity: </span><span class=si>{</span><span class=n>sparsity</span><span class=si>:</span><span class=s2>.1%</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Hyperparameter tuning for best model</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>40</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;HYPERPARAMETER TUNING&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>40</span><span class=p>)</span>

<span class=c1># Tune regularization strength for Ridge</span>
<span class=n>param_grid</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;C&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>)}</span>

<span class=n>grid_search</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
    <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>penalty</span><span class=o>=</span><span class=s1>&#39;l2&#39;</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>),</span>
    <span class=n>param_grid</span><span class=p>,</span>
    <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
    <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=-</span><span class=mi>1</span>
<span class=p>)</span>

<span class=n>grid_search</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_processed</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best regularization strength (C): </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_params_</span><span class=p>[</span><span class=s1>&#39;C&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Best CV AUC: </span><span class=si>{</span><span class=n>grid_search</span><span class=o>.</span><span class=n>best_score_</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Final model evaluation</span>
<span class=n>best_model</span> <span class=o>=</span> <span class=n>grid_search</span><span class=o>.</span><span class=n>best_estimator_</span>
<span class=n>y_pred_proba_final</span> <span class=o>=</span> <span class=n>best_model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test_processed</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
<span class=n>y_pred_final</span> <span class=o>=</span> <span class=n>best_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test_processed</span><span class=p>)</span>

<span class=n>final_auc</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_proba_final</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Final test AUC: </span><span class=si>{</span><span class=n>final_auc</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># ROC curve comparison</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=c1># Plot ROC curves for different regularization methods</span>
<span class=k>for</span> <span class=n>model_name</span><span class=p>,</span> <span class=n>results</span> <span class=ow>in</span> <span class=n>regularization_results</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>results</span><span class=p>[</span><span class=s1>&#39;model&#39;</span><span class=p>]</span>
    <span class=n>y_proba</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X_test_processed</span><span class=p>)[:,</span> <span class=mi>1</span><span class=p>]</span>
    <span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>roc_curve</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
    <span class=n>auc_score</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_proba</span><span class=p>)</span>
    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s1> (AUC = </span><span class=si>{</span><span class=n>auc_score</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;False Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;True Positive Rate&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;ROC Curves - Regularization Comparison&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Feature importance (coefficients) for Ridge regression</span>
<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>ridge_model</span> <span class=o>=</span> <span class=n>regularization_results</span><span class=p>[</span><span class=s1>&#39;Ridge (L2)&#39;</span><span class=p>][</span><span class=s1>&#39;model&#39;</span><span class=p>]</span>

<span class=c1># Get feature names after preprocessing</span>
<span class=n>feature_names</span> <span class=o>=</span> <span class=p>(</span><span class=n>numerical_features</span> <span class=o>+</span> 
                <span class=nb>list</span><span class=p>(</span><span class=n>preprocessor_final</span><span class=o>.</span><span class=n>named_transformers_</span><span class=p>[</span><span class=s1>&#39;cat&#39;</span><span class=p>]</span>
                    <span class=o>.</span><span class=n>get_feature_names_out</span><span class=p>(</span><span class=n>categorical_features</span><span class=p>)))</span>

<span class=n>coefficients</span> <span class=o>=</span> <span class=n>ridge_model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
<span class=n>feature_importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>feature_names</span><span class=p>,</span>
    <span class=s1>&#39;coefficient&#39;</span><span class=p>:</span> <span class=n>coefficients</span><span class=p>,</span>
    <span class=s1>&#39;abs_coefficient&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>coefficients</span><span class=p>)</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;abs_coefficient&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=c1># Plot top 10 most important features</span>
<span class=n>top_features</span> <span class=o>=</span> <span class=n>feature_importance</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
<span class=n>bars</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>top_features</span><span class=p>)),</span> <span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>top_features</span><span class=p>)),</span> <span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Top 10 Feature Importance (Ridge)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Color bars by sign</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>bar</span><span class=p>,</span> <span class=n>coef</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>bars</span><span class=p>,</span> <span class=n>top_features</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>])):</span>
    <span class=n>bar</span><span class=o>.</span><span class=n>set_color</span><span class=p>(</span><span class=s1>&#39;red&#39;</span> <span class=k>if</span> <span class=n>coef</span> <span class=o>&lt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=s1>&#39;blue&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Top 10 Most Important Features:&quot;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>_</span><span class=p>,</span> <span class=n>row</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>top_features</span><span class=o>.</span><span class=n>iterrows</span><span class=p>()):</span>
    <span class=n>direction</span> <span class=o>=</span> <span class=s2>&quot;increases&quot;</span> <span class=k>if</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=s2>&quot;decreases&quot;</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>:</span><span class=s2>2d</span><span class=si>}</span><span class=s2>. </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>25</span><span class=si>}</span><span class=s2> Œª </span><span class=si>{</span><span class=n>direction</span><span class=si>}</span><span class=s2> churn risk (coef: </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>+.3f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>
</code></pre></div> <h3 id=financial-risk-assessment-example>Financial Risk Assessment Example</h3> <div class=highlight><pre><span></span><code><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
<span class=kn>import</span><span class=w> </span><span class=nn>pandas</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>pd</span>
<span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.preprocessing</span><span class=w> </span><span class=kn>import</span> <span class=n>StandardScaler</span><span class=p>,</span> <span class=n>RobustScaler</span><span class=p>,</span> <span class=n>QuantileTransformer</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.linear_model</span><span class=w> </span><span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>Lasso</span><span class=p>,</span> <span class=n>ElasticNet</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.model_selection</span><span class=w> </span><span class=kn>import</span> <span class=n>train_test_split</span><span class=p>,</span> <span class=n>cross_val_score</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.metrics</span><span class=w> </span><span class=kn>import</span> <span class=n>mean_squared_error</span><span class=p>,</span> <span class=n>r2_score</span><span class=p>,</span> <span class=n>mean_absolute_error</span>
<span class=kn>from</span><span class=w> </span><span class=nn>sklearn.pipeline</span><span class=w> </span><span class=kn>import</span> <span class=n>Pipeline</span>
<span class=kn>import</span><span class=w> </span><span class=nn>seaborn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>sns</span>

<span class=c1># Generate realistic financial dataset with outliers</span>
<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
<span class=n>n_loans</span> <span class=o>=</span> <span class=mi>2000</span>

<span class=c1># Create features with different scales and outlier patterns</span>
<span class=n>data</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;loan_amount&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>lognormal</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># Log-normal (right-skewed)</span>
    <span class=s1>&#39;annual_income&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>lognormal</span><span class=p>(</span><span class=mf>10.5</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># Income distribution</span>
    <span class=s1>&#39;credit_score&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>beta</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>)</span> <span class=o>*</span> <span class=mi>550</span> <span class=o>+</span> <span class=mi>300</span><span class=p>,</span>  <span class=c1># Credit scores 300-850</span>
    <span class=s1>&#39;debt_to_income&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>exponential</span><span class=p>(</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># Debt ratios</span>
    <span class=s1>&#39;employment_years&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>gamma</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># Employment history</span>
    <span class=s1>&#39;num_credit_lines&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>poisson</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># Count of credit lines</span>
    <span class=s1>&#39;loan_to_value&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.95</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>),</span>  <span class=c1># LTV ratio</span>
    <span class=s1>&#39;market_volatility&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mf>0.15</span><span class=p>,</span> <span class=mf>0.05</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mf>0.05</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>)</span>  <span class=c1># Market conditions</span>
<span class=p>}</span>

<span class=c1># Add some extreme outliers (data entry errors, unusual cases)</span>
<span class=n>outlier_indices</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>n_loans</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=n>data</span><span class=p>[</span><span class=s1>&#39;annual_income&#39;</span><span class=p>][</span><span class=n>outlier_indices</span><span class=p>[:</span><span class=mi>25</span><span class=p>]]</span> <span class=o>*=</span> <span class=mi>10</span>  <span class=c1># Very high income outliers</span>
<span class=n>data</span><span class=p>[</span><span class=s1>&#39;debt_to_income&#39;</span><span class=p>][</span><span class=n>outlier_indices</span><span class=p>[</span><span class=mi>25</span><span class=p>:]]</span> <span class=o>*=</span> <span class=mi>5</span>  <span class=c1># Very high debt outliers</span>

<span class=c1># Create target: default risk score (0-1, higher = more risky)</span>
<span class=n>risk_score</span> <span class=o>=</span> <span class=p>(</span>
    <span class=mf>0.1</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;debt_to_income&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;debt_to_income&#39;</span><span class=p>]))</span> <span class=o>+</span>
    <span class=mf>0.2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;credit_score&#39;</span><span class=p>]</span> <span class=o>-</span> <span class=mi>300</span><span class=p>)</span> <span class=o>/</span> <span class=mi>550</span><span class=p>)</span> <span class=o>+</span>
    <span class=mf>0.15</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;loan_to_value&#39;</span><span class=p>])</span> <span class=o>+</span>
    <span class=mf>0.1</span> <span class=o>*</span> <span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;market_volatility&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=mf>0.4</span><span class=p>)</span> <span class=o>+</span>
    <span class=o>-</span><span class=mf>0.05</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;annual_income&#39;</span><span class=p>]</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;annual_income&#39;</span><span class=p>]))</span> <span class=o>+</span>
    <span class=mf>0.1</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>n_loans</span><span class=p>)</span>  <span class=c1># Random noise</span>
<span class=p>)</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>

<span class=n>data</span><span class=p>[</span><span class=s1>&#39;risk_score&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>risk_score</span>

<span class=c1># Create DataFrame</span>
<span class=n>df_risk</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Financial Risk Assessment - Robust Scaling &amp; Regularization&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Dataset shape: </span><span class=si>{</span><span class=n>df_risk</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Dataset summary with outliers:&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>df_risk</span><span class=o>.</span><span class=n>describe</span><span class=p>())</span>

<span class=c1># Identify outliers using IQR method</span>
<span class=k>def</span><span class=w> </span><span class=nf>identify_outliers</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>column</span><span class=p>):</span>
    <span class=n>Q1</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>column</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.25</span><span class=p>)</span>
    <span class=n>Q3</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>column</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.75</span><span class=p>)</span>
    <span class=n>IQR</span> <span class=o>=</span> <span class=n>Q3</span> <span class=o>-</span> <span class=n>Q1</span>
    <span class=n>lower_bound</span> <span class=o>=</span> <span class=n>Q1</span> <span class=o>-</span> <span class=mf>1.5</span> <span class=o>*</span> <span class=n>IQR</span>
    <span class=n>upper_bound</span> <span class=o>=</span> <span class=n>Q3</span> <span class=o>+</span> <span class=mf>1.5</span> <span class=o>*</span> <span class=n>IQR</span>
    <span class=k>return</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=n>column</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>lower_bound</span><span class=p>)</span> <span class=o>|</span> <span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=n>column</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>upper_bound</span><span class=p>)</span>

<span class=c1># Check for outliers in key features</span>
<span class=n>outlier_analysis</span> <span class=o>=</span> <span class=p>{}</span>
<span class=k>for</span> <span class=n>col</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;loan_amount&#39;</span><span class=p>,</span> <span class=s1>&#39;annual_income&#39;</span><span class=p>,</span> <span class=s1>&#39;debt_to_income&#39;</span><span class=p>]:</span>
    <span class=n>outliers</span> <span class=o>=</span> <span class=n>identify_outliers</span><span class=p>(</span><span class=n>df_risk</span><span class=p>,</span> <span class=n>col</span><span class=p>)</span>
    <span class=n>outlier_analysis</span><span class=p>[</span><span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;count&#39;</span><span class=p>:</span> <span class=n>outliers</span><span class=o>.</span><span class=n>sum</span><span class=p>(),</span>
        <span class=s1>&#39;percentage&#39;</span><span class=p>:</span> <span class=p>(</span><span class=n>outliers</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>df_risk</span><span class=p>))</span> <span class=o>*</span> <span class=mi>100</span>
    <span class=p>}</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=si>{</span><span class=n>col</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>outliers</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=si>}</span><span class=s2> outliers (</span><span class=si>{</span><span class=p>(</span><span class=n>outliers</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>df_risk</span><span class=p>)</span><span class=o>*</span><span class=mi>100</span><span class=p>)</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>%)&quot;</span><span class=p>)</span>

<span class=c1># Visualize distributions and outliers</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>12</span><span class=p>))</span>
<span class=n>features</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>df_risk</span><span class=o>.</span><span class=n>columns</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># Exclude target</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>feature</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>features</span><span class=p>):</span>
    <span class=n>row</span><span class=p>,</span> <span class=n>col</span> <span class=o>=</span> <span class=n>i</span> <span class=o>//</span> <span class=mi>3</span><span class=p>,</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>3</span>

    <span class=c1># Histogram</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>feature</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Frequency&#39;</span><span class=p>)</span>

    <span class=c1># Mark outliers if applicable</span>
    <span class=k>if</span> <span class=n>feature</span> <span class=ow>in</span> <span class=n>outlier_analysis</span><span class=p>:</span>
        <span class=n>outliers</span> <span class=o>=</span> <span class=n>identify_outliers</span><span class=p>(</span><span class=n>df_risk</span><span class=p>,</span> <span class=n>feature</span><span class=p>)</span>
        <span class=k>if</span> <span class=n>outliers</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
            <span class=n>outlier_values</span> <span class=o>=</span> <span class=n>df_risk</span><span class=o>.</span><span class=n>loc</span><span class=p>[</span><span class=n>outliers</span><span class=p>,</span> <span class=n>feature</span><span class=p>]</span>
            <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.25</span><span class=p>)</span> <span class=o>-</span> <span class=mf>1.5</span><span class=o>*</span><span class=p>(</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.75</span><span class=p>)</span><span class=o>-</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.25</span><span class=p>)),</span> 
                                  <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Outlier bounds&#39;</span><span class=p>)</span>
            <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.75</span><span class=p>)</span> <span class=o>+</span> <span class=mf>1.5</span><span class=o>*</span><span class=p>(</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.75</span><span class=p>)</span><span class=o>-</span><span class=n>df_risk</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=mf>0.25</span><span class=p>)),</span> 
                                  <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
            <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>

    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=s1>&#39;Feature Distributions with Outliers Highlighted&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Prepare data</span>
<span class=n>X</span> <span class=o>=</span> <span class=n>df_risk</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s1>&#39;risk_score&#39;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>df_risk</span><span class=p>[</span><span class=s1>&#39;risk_score&#39;</span><span class=p>]</span>

<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=c1># Compare different scaling approaches on data with outliers</span>
<span class=n>scalers_robust</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;No Scaling&#39;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>
    <span class=s1>&#39;StandardScaler&#39;</span><span class=p>:</span> <span class=n>StandardScaler</span><span class=p>(),</span>
    <span class=s1>&#39;RobustScaler&#39;</span><span class=p>:</span> <span class=n>RobustScaler</span><span class=p>(),</span>
    <span class=s1>&#39;QuantileTransformer&#39;</span><span class=p>:</span> <span class=n>QuantileTransformer</span><span class=p>(</span><span class=n>output_distribution</span><span class=o>=</span><span class=s1>&#39;normal&#39;</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>scaling_results</span> <span class=o>=</span> <span class=p>{}</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;SCALING COMPARISON ON DATA WITH OUTLIERS&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>

<span class=k>for</span> <span class=n>scaler_name</span><span class=p>,</span> <span class=n>scaler</span> <span class=ow>in</span> <span class=n>scalers_robust</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Testing </span><span class=si>{</span><span class=n>scaler_name</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>

    <span class=k>if</span> <span class=n>scaler</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>X_train</span>
        <span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>X_test</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
        <span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

    <span class=c1># Train simple linear regression</span>
    <span class=n>lr</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
    <span class=n>lr</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_scaled</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

    <span class=c1># Evaluate</span>
    <span class=n>train_score</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_train_scaled</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
    <span class=n>test_score</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test_scaled</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>lr</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test_scaled</span><span class=p>)</span>

    <span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
    <span class=n>mae</span> <span class=o>=</span> <span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

    <span class=n>scaling_results</span><span class=p>[</span><span class=n>scaler_name</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;train_r2&#39;</span><span class=p>:</span> <span class=n>train_score</span><span class=p>,</span>
        <span class=s1>&#39;test_r2&#39;</span><span class=p>:</span> <span class=n>test_score</span><span class=p>,</span>
        <span class=s1>&#39;rmse&#39;</span><span class=p>:</span> <span class=n>rmse</span><span class=p>,</span>
        <span class=s1>&#39;mae&#39;</span><span class=p>:</span> <span class=n>mae</span>
    <span class=p>}</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Train R¬≤: </span><span class=si>{</span><span class=n>train_score</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Test R¬≤: </span><span class=si>{</span><span class=n>test_score</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  RMSE: </span><span class=si>{</span><span class=n>rmse</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  MAE: </span><span class=si>{</span><span class=n>mae</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize scaling effects on first few features</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
<span class=n>sample_features</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;loan_amount&#39;</span><span class=p>,</span> <span class=s1>&#39;annual_income&#39;</span><span class=p>,</span> <span class=s1>&#39;debt_to_income&#39;</span><span class=p>]</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>scaler_name</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>([</span><span class=s1>&#39;StandardScaler&#39;</span><span class=p>,</span> <span class=s1>&#39;RobustScaler&#39;</span><span class=p>,</span> <span class=s1>&#39;QuantileTransformer&#39;</span><span class=p>]):</span>
    <span class=k>if</span> <span class=n>i</span> <span class=o>&gt;=</span> <span class=mi>3</span><span class=p>:</span>
        <span class=k>break</span>

    <span class=n>scaler</span> <span class=o>=</span> <span class=n>scalers_robust</span><span class=p>[</span><span class=n>scaler_name</span><span class=p>]</span>
    <span class=n>X_scaled_sample</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=n>sample_features</span><span class=p>])</span>

    <span class=n>row</span><span class=p>,</span> <span class=n>col</span> <span class=o>=</span> <span class=n>i</span> <span class=o>//</span> <span class=mi>2</span><span class=p>,</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>2</span>

    <span class=c1># Plot first feature</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>X_scaled_sample</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>bins</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>scaler_name</span><span class=si>}</span><span class=se>\n</span><span class=s1>Transformed: </span><span class=si>{</span><span class=n>sample_features</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Frequency&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=c1># Add statistics</span>
    <span class=n>mean_val</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>X_scaled_sample</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>])</span>
    <span class=n>std_val</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>X_scaled_sample</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>])</span>
    <span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=mf>0.02</span><span class=p>,</span> <span class=mf>0.95</span><span class=p>,</span> <span class=sa>f</span><span class=s1>&#39;Mean: </span><span class=si>{</span><span class=n>mean_val</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=se>\n</span><span class=s1>Std: </span><span class=si>{</span><span class=n>std_val</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> 
                       <span class=n>transform</span><span class=o>=</span><span class=n>axes</span><span class=p>[</span><span class=n>row</span><span class=p>,</span> <span class=n>col</span><span class=p>]</span><span class=o>.</span><span class=n>transAxes</span><span class=p>,</span> <span class=n>verticalalignment</span><span class=o>=</span><span class=s1>&#39;top&#39;</span><span class=p>,</span>
                       <span class=n>bbox</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>boxstyle</span><span class=o>=</span><span class=s1>&#39;round&#39;</span><span class=p>,</span> <span class=n>facecolor</span><span class=o>=</span><span class=s1>&#39;white&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.8</span><span class=p>))</span>

<span class=c1># Performance comparison</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>scaling_results</span><span class=o>.</span><span class=n>keys</span><span class=p>(),</span> <span class=p>[</span><span class=n>v</span><span class=p>[</span><span class=s1>&#39;test_r2&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>scaling_results</span><span class=o>.</span><span class=n>values</span><span class=p>()],</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Test R¬≤ by Scaling Method&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;R¬≤ Score&#39;</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>)</span>
<span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Now test regularization with best scaler (RobustScaler typically best for outliers)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;REGULARIZATION WITH ROBUST SCALING&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>60</span><span class=p>)</span>

<span class=n>robust_scaler</span> <span class=o>=</span> <span class=n>RobustScaler</span><span class=p>()</span>
<span class=n>X_train_robust</span> <span class=o>=</span> <span class=n>robust_scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
<span class=n>X_test_robust</span> <span class=o>=</span> <span class=n>robust_scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=c1># Test different regularization strengths</span>
<span class=n>alphas</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>)</span>

<span class=c1># Test Ridge, Lasso, and ElasticNet</span>
<span class=n>regularization_models</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s1>&#39;Ridge&#39;</span><span class=p>:</span> <span class=n>Ridge</span><span class=p>(),</span>
    <span class=s1>&#39;Lasso&#39;</span><span class=p>:</span> <span class=n>Lasso</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>),</span>
    <span class=s1>&#39;ElasticNet&#39;</span><span class=p>:</span> <span class=n>ElasticNet</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
<span class=p>}</span>

<span class=n>regularization_paths</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>model_name</span><span class=p>,</span> <span class=n>base_model</span> <span class=ow>in</span> <span class=n>regularization_models</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Analyzing </span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2> regularization path:&quot;</span><span class=p>)</span>

    <span class=n>train_scores</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>test_scores</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>coefficients</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=n>sparsity_levels</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>alpha</span> <span class=ow>in</span> <span class=n>alphas</span><span class=p>:</span>
        <span class=c1># Set regularization strength</span>
        <span class=k>if</span> <span class=nb>hasattr</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=s1>&#39;alpha&#39;</span><span class=p>):</span>
            <span class=n>model</span> <span class=o>=</span> <span class=n>base_model</span><span class=o>.</span><span class=vm>__class__</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>
            <span class=k>if</span> <span class=n>model_name</span> <span class=o>==</span> <span class=s1>&#39;ElasticNet&#39;</span><span class=p>:</span>
                <span class=n>model</span> <span class=o>=</span> <span class=n>base_model</span><span class=o>.</span><span class=vm>__class__</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>,</span> <span class=n>l1_ratio</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>

        <span class=c1># Fit model</span>
        <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_robust</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

        <span class=c1># Evaluate</span>
        <span class=n>train_score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_train_robust</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
        <span class=n>test_score</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_test_robust</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>

        <span class=n>train_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>train_score</span><span class=p>)</span>
        <span class=n>test_scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>test_score</span><span class=p>)</span>
        <span class=n>coefficients</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=o>.</span><span class=n>copy</span><span class=p>())</span>

        <span class=c1># Calculate sparsity (proportion of near-zero coefficients)</span>
        <span class=n>sparsity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mf>1e-5</span><span class=p>)</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
        <span class=n>sparsity_levels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sparsity</span><span class=p>)</span>

    <span class=n>regularization_paths</span><span class=p>[</span><span class=n>model_name</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s1>&#39;train_scores&#39;</span><span class=p>:</span> <span class=n>train_scores</span><span class=p>,</span>
        <span class=s1>&#39;test_scores&#39;</span><span class=p>:</span> <span class=n>test_scores</span><span class=p>,</span>
        <span class=s1>&#39;coefficients&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>coefficients</span><span class=p>),</span>
        <span class=s1>&#39;sparsity&#39;</span><span class=p>:</span> <span class=n>sparsity_levels</span>
    <span class=p>}</span>

    <span class=c1># Find best alpha</span>
    <span class=n>best_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>test_scores</span><span class=p>)</span>
    <span class=n>best_alpha</span> <span class=o>=</span> <span class=n>alphas</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span>
    <span class=n>best_test_score</span> <span class=o>=</span> <span class=n>test_scores</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span>

    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Best alpha: </span><span class=si>{</span><span class=n>best_alpha</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Best test R¬≤: </span><span class=si>{</span><span class=n>best_test_score</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Sparsity at best alpha: </span><span class=si>{</span><span class=n>sparsity_levels</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span><span class=si>:</span><span class=s2>.1%</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Visualize regularization paths</span>
<span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>18</span><span class=p>,</span> <span class=mi>12</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>results</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>regularization_paths</span><span class=o>.</span><span class=n>items</span><span class=p>()):</span>
    <span class=c1># Performance vs regularization strength</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>results</span><span class=p>[</span><span class=s1>&#39;train_scores&#39;</span><span class=p>],</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>results</span><span class=p>[</span><span class=s1>&#39;test_scores&#39;</span><span class=p>],</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Test&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;R¬≤ Score&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s1>: Performance vs Regularization&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

    <span class=c1># Mark best alpha</span>
    <span class=n>best_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>results</span><span class=p>[</span><span class=s1>&#39;test_scores&#39;</span><span class=p>])</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=n>alphas</span><span class=p>[</span><span class=n>best_idx</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>,</span> 
                      <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;Best Œª=</span><span class=si>{</span><span class=n>alphas</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span><span class=si>:</span><span class=s1>.4f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>

    <span class=c1># Coefficient paths (show first 5 features)</span>
    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>results</span><span class=p>[</span><span class=s1>&#39;coefficients&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>])):</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>results</span><span class=p>[</span><span class=s1>&#39;coefficients&#39;</span><span class=p>][:,</span> <span class=n>j</span><span class=p>],</span> 
                       <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s1>&#39;Feature </span><span class=si>{</span><span class=n>j</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>&#39;</span> <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span> <span class=k>else</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>

    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xscale</span><span class=p>(</span><span class=s1>&#39;log&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Regularization Strength (Œª)&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s1>: Coefficient Paths&#39;</span><span class=p>)</span>
    <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
        <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
    <span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>

<span class=c1># Final model selection and feature importance</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span> <span class=o>+</span> <span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>40</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;FINAL MODEL ANALYSIS&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span><span class=o>*</span><span class=mi>40</span><span class=p>)</span>

<span class=c1># Select best Ridge model (usually most stable)</span>
<span class=n>best_alpha_ridge</span> <span class=o>=</span> <span class=n>alphas</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>regularization_paths</span><span class=p>[</span><span class=s1>&#39;Ridge&#39;</span><span class=p>][</span><span class=s1>&#39;test_scores&#39;</span><span class=p>])]</span>
<span class=n>final_model</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=n>best_alpha_ridge</span><span class=p>)</span>
<span class=n>final_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train_robust</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>

<span class=c1># Final evaluation</span>
<span class=n>y_pred_final</span> <span class=o>=</span> <span class=n>final_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test_robust</span><span class=p>)</span>
<span class=n>final_r2</span> <span class=o>=</span> <span class=n>r2_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_final</span><span class=p>)</span>
<span class=n>final_rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_final</span><span class=p>))</span>
<span class=n>final_mae</span> <span class=o>=</span> <span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_final</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Final Ridge Model (Œª = </span><span class=si>{</span><span class=n>best_alpha_ridge</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>):&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  Test R¬≤: </span><span class=si>{</span><span class=n>final_r2</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  RMSE: </span><span class=si>{</span><span class=n>final_rmse</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;  MAE: </span><span class=si>{</span><span class=n>final_mae</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=c1># Feature importance analysis</span>
<span class=n>feature_importance</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;feature&#39;</span><span class=p>:</span> <span class=n>X</span><span class=o>.</span><span class=n>columns</span><span class=p>,</span>
    <span class=s1>&#39;coefficient&#39;</span><span class=p>:</span> <span class=n>final_model</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span>
    <span class=s1>&#39;abs_coefficient&#39;</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>final_model</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
<span class=p>})</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=s1>&#39;abs_coefficient&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Feature Importance Ranking:&quot;</span><span class=p>)</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>_</span><span class=p>,</span> <span class=n>row</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>feature_importance</span><span class=o>.</span><span class=n>iterrows</span><span class=p>()):</span>
    <span class=n>direction</span> <span class=o>=</span> <span class=s2>&quot;increases&quot;</span> <span class=k>if</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=s2>&quot;decreases&quot;</span>
    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>:</span><span class=s2>2d</span><span class=si>}</span><span class=s2>. </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>20</span><span class=si>}</span><span class=s2> Œª </span><span class=si>{</span><span class=n>direction</span><span class=si>}</span><span class=s2> risk (coef: </span><span class=si>{</span><span class=n>row</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>+.4f</span><span class=si>}</span><span class=s2>)&quot;</span><span class=p>)</span>

<span class=c1># Final visualization</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred_final</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>([</span><span class=n>y_test</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>y_test</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=p>[</span><span class=n>y_test</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>y_test</span><span class=o>.</span><span class=n>max</span><span class=p>()],</span> <span class=s1>&#39;r--&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Actual Risk Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Predicted Risk Score&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Final Model Performance</span><span class=se>\n</span><span class=s1>R¬≤ = </span><span class=si>{</span><span class=n>final_r2</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
<span class=n>bars</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>feature_importance</span><span class=p>)),</span> <span class=n>feature_importance</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>feature_importance</span><span class=p>)),</span> <span class=n>feature_importance</span><span class=p>[</span><span class=s1>&#39;feature&#39;</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Coefficient Value&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Feature Importance (Ridge Coefficients)&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>

<span class=c1># Color bars by sign</span>
<span class=k>for</span> <span class=n>bar</span><span class=p>,</span> <span class=n>coef</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>bars</span><span class=p>,</span> <span class=n>feature_importance</span><span class=p>[</span><span class=s1>&#39;coefficient&#39;</span><span class=p>]):</span>
    <span class=n>bar</span><span class=o>.</span><span class=n>set_color</span><span class=p>(</span><span class=s1>&#39;red&#39;</span> <span class=k>if</span> <span class=n>coef</span> <span class=o>&lt;</span> <span class=mi>0</span> <span class=k>else</span> <span class=s1>&#39;blue&#39;</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></div> <h2 id=references>üìö References</h2> <ul> <li><strong>Books:</strong></li> <li><a href=https://web.stanford.edu/~hastie/ElemStatLearn/ >The Elements of Statistical Learning</a> by Hastie, Tibshirani, and Friedman - Chapters 3, 18</li> <li><a href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>Pattern Recognition and Machine Learning</a> by Christopher Bishop - Chapter 1, 5</li> <li> <p><a href=https://www.deeplearningbook.org/ >Deep Learning</a> by Goodfellow, Bengio, and Courville - Chapter 7 (Regularization)</p> </li> <li> <p><strong>Documentation:</strong></p> </li> <li><a href=https://scikit-learn.org/stable/modules/preprocessing.html>Scikit-learn Preprocessing</a></li> <li><a href=https://scikit-learn.org/stable/modules/linear_model.html>Scikit-learn Linear Models</a></li> <li> <p><a href=https://scikit-learn.org/stable/modules/feature_selection.html>Scikit-learn Feature Selection</a></p> </li> <li> <p><strong>Research Papers:</strong></p> </li> <li><a href=https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf>Regularization and variable selection via the elastic net</a> by Zou &amp; Hastie (2005)</li> <li><a href=https://statweb.stanford.edu/~tibs/lasso/lasso.pdf>Regression Shrinkage and Selection via the Lasso</a> by Tibshirani (1996)</li> <li> <p><a href=https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionHoerlKennard1970.pdf>Ridge Regression: Biased Estimation for Nonorthogonal Problems</a> by Hoerl &amp; Kennard (1970)</p> </li> <li> <p><strong>Tutorials and Guides:</strong></p> </li> <li><a href=https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35>Feature Scaling Techniques</a></li> <li><a href=https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a>Regularization in Machine Learning</a></li> <li> <p><a href=http://scott.fortmann-roe.com/docs/BiasVariance.html>Understanding the Bias-Variance Tradeoff</a></p> </li> <li> <p><strong>Advanced Topics:</strong></p> </li> <li><a href=https://tibshirani.su.domains/ftp/group.pdf>Group Lasso</a> by Yuan &amp; Lin (2006)</li> <li><a href=https://web.stanford.edu/~hastie/Papers/FusedLasso.pdf>The Fused Lasso</a> by Tibshirani et al. (2005)</li> <li> <p><a href=https://jmlr.org/papers/v15/srivastava14a.html>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> by Srivastava et al. (2014)</p> </li> <li> <p><strong>Online Courses:</strong></p> </li> <li><a href=http://cs229.stanford.edu/ >Machine Learning Course - Stanford CS229</a></li> <li><a href=https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning>Statistical Learning - Stanford Online</a></li> <li> <p><a href=https://www.coursera.org/learn/machine-learning>Regularization - Coursera Machine Learning</a></p> </li> <li> <p><strong>Software and Tools:</strong></p> </li> <li><a href=https://scikit-learn.org/stable/ >scikit-learn</a> (Python)</li> <li><a href=https://cran.r-project.org/web/packages/glmnet/index.html>glmnet</a> (R package)</li> <li><a href=https://www.tensorflow.org/ >TensorFlow/Keras</a> (Deep learning regularization)</li> <li><a href=https://pytorch.org/ >PyTorch</a> (Deep learning regularization)</li> </ul> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2020 - <script>document.write(/\d{4}/.exec(Date())[0])</script> ‚Ä¢ <strong>Kuldeep Singh Sidhu</strong> ‚Ä¢ <u><a href=https://choosealicense.com/licenses/agpl-3.0/ target=‚Äù_blank‚Äù>License</a></u> ‚Ä¢ <u><a href=/privacy>Privacy Policy</a></u> ‚Ä¢ <u><a href=/contact>Contact</a></u> ‚Ä¢ </div> </div> <div class=md-social> <a href=https://github.com/singhsidhukuldeep/ target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.01 8.01 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27s-1.36.09-2 .27c-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8"/></svg> </a> <a href=https://linkedin.com/in/singhsidhukuldeep target=_blank rel=noopener title=linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://twitter.com/kuldeep_s_s target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> <a href=https://stackoverflow.com/u/7182350/ target=_blank rel=noopener title=stackoverflow.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 300zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-200v39.7h200zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg> </a> <a href=https://huggingface.co/singhsidhukuldeep target=_blank rel=noopener title=huggingface.co class=md-social__link> <svg width=500 height=463 viewbox="0 0 500 463" fill=none xmlns=http://www.w3.org/2000/svg> <path fill=white d="M496.592 369.699C500.563 381.093 499.61 393.227 494.315 403.778C490.503 411.48 485.05 417.441 478.379 422.769C470.331 429.099 460.324 434.48 448.253 439.65C433.852 445.77 416.274 451.52 408.226 453.63C387.63 458.958 367.829 462.334 347.762 462.493C319.066 462.756 294.34 456.004 276.762 438.753C267.656 439.861 258.443 440.494 249.178 440.494C240.389 440.494 231.706 439.967 223.076 438.912C205.445 456.057 180.825 462.756 152.234 462.493C132.168 462.334 112.366 458.958 91.7177 453.63C83.7229 451.52 66.145 445.77 51.7439 439.65C39.6723 434.48 29.6656 429.099 21.6708 422.769C14.9467 417.441 9.49334 411.48 5.68127 403.778C0.439661 393.227 -0.566304 381.093 3.45755 369.699C-0.248631 360.994 -1.20165 351.024 1.71035 339.998C3.03399 334.987 5.20476 330.344 7.95792 326.229C7.37552 324.067 6.89901 321.851 6.58134 319.424C4.56941 304.97 9.59923 291.781 19.0765 281.547C23.7357 276.43 28.7655 272.895 34.0071 270.627C30.1421 254.273 28.1302 237.445 28.1302 220.247C28.1302 98.5969 127.085 0 249.178 0C291.111 0 330.343 11.6058 363.805 31.8633C369.84 35.5561 375.77 39.5126 381.436 43.7329C384.242 45.8431 387.048 48.006 389.748 50.2744C392.501 52.49 395.201 54.8112 397.796 57.1851C405.632 64.3069 412.991 71.9562 419.715 80.133C421.992 82.8235 424.163 85.6194 426.28 88.4681C430.569 94.1128 434.54 99.9685 438.193 106.035C443.752 115.109 448.623 124.604 452.859 134.469C455.665 141.064 458.101 147.816 460.271 154.727C463.501 165.067 465.99 175.723 467.684 186.696C468.213 190.336 468.69 194.028 469.06 197.721C469.802 205.107 470.225 212.598 470.225 220.247C470.225 237.234 468.213 253.904 464.454 269.994C470.278 272.262 475.784 275.955 480.92 281.547C490.397 291.781 495.427 305.022 493.415 319.477C493.098 321.851 492.621 324.067 492.039 326.229C494.792 330.344 496.963 334.987 498.286 339.998C501.198 351.024 500.245 360.994 496.592 369.699Z"/> <path fill=black d="M433.839 221.75C433.839 120.838 351.531 39.0323 250 39.0323C148.469 39.0323 66.1613 120.838 66.1613 221.75C66.1613 322.662 148.469 404.468 250 404.468C351.531 404.468 433.839 322.662 433.839 221.75ZM45 221.75C45 109.222 136.782 18 250 18C363.218 18 455 109.222 455 221.75C455 334.278 363.218 425.5 250 425.5C136.782 425.5 45 334.278 45 221.75Z"/> <path fill=white d="M250 405.5C352.173 405.5 435 323.232 435 221.75C435 120.268 352.173 38 250 38C147.827 38 65 120.268 65 221.75C65 323.232 147.827 405.5 250 405.5Z"/> <path fill=white d="M202.198 404.174C216.789 383.118 215.755 367.316 195.735 347.627C175.715 327.943 164.062 299.145 164.062 299.145C164.062 299.145 159.709 282.419 149.794 283.958C139.88 285.497 132.6 310.492 153.368 325.783C174.135 341.069 149.232 351.456 141.242 337.099C133.252 322.741 111.435 285.831 100.121 278.772C88.8117 271.713 80.8483 275.668 83.5151 290.218C86.182 304.769 133.48 340.036 128.878 347.668C124.276 355.296 108.058 338.7 108.058 338.7C108.058 338.7 57.3079 293.255 46.2587 305.097C35.2096 316.94 54.641 326.863 82.3328 343.359C110.03 359.85 112.177 364.206 108.248 370.446C104.314 376.685 43.1836 325.971 37.4417 347.47C31.705 368.969 99.8291 375.209 95.6247 390.051C91.4203 404.899 47.6372 361.958 38.6823 378.689C29.7221 395.425 100.465 415.088 101.038 415.234C123.889 421.067 181.924 433.426 202.198 404.174Z"/> <path fill=black d="M90.9935 255C82.4744 255 74.8603 258.477 69.551 264.784C66.2675 268.69 62.8367 274.986 62.5578 284.414C58.985 283.394 55.5489 282.824 52.3391 282.824C44.183 282.824 36.8163 285.93 31.6069 291.573C24.9137 298.815 21.9407 307.715 23.2351 316.62C23.8508 320.861 25.2768 324.663 27.4079 328.182C22.9142 331.795 19.6044 336.826 18.0047 342.876C16.7524 347.619 15.4685 357.497 22.1722 367.673C21.746 368.337 21.3461 369.027 20.9725 369.733C16.9418 377.336 16.684 385.927 20.2411 393.928C25.6346 406.054 39.0368 415.608 65.0625 425.863C81.2536 432.242 96.0661 436.321 96.1976 436.357C117.603 441.874 136.962 444.677 153.721 444.677C184.525 444.677 206.578 435.301 219.27 416.811C239.697 387.036 236.776 359.803 210.346 333.552C195.717 319.026 185.993 297.607 183.967 292.906C179.884 278.986 169.086 263.513 151.138 263.513H151.133C149.622 263.513 148.096 263.633 146.592 263.869C138.73 265.097 131.858 269.595 126.949 276.361C121.65 269.814 116.504 264.606 111.847 261.667C104.827 257.243 97.813 255 90.9935 255ZM90.9935 275.917C93.6771 275.917 96.9553 277.051 100.57 279.331C111.794 286.406 133.452 323.403 141.382 337.793C144.039 342.614 148.581 344.654 152.669 344.654C160.783 344.654 167.118 336.638 153.411 326.451C132.8 311.124 140.03 286.072 149.87 284.529C150.301 284.461 150.727 284.43 151.138 284.43C160.083 284.43 164.03 299.751 164.03 299.751C164.03 299.751 175.595 328.616 195.465 348.346C215.334 368.08 216.36 383.919 201.879 405.024C192.002 419.415 173.096 421.292 153.721 421.292C133.626 421.292 112.99 417.772 101.445 414.796C100.877 414.65 30.7019 396.255 39.5946 379.48C41.089 376.661 43.5516 375.532 46.6509 375.532C59.1744 375.532 81.9535 394.054 91.746 394.054C93.935 394.054 95.5662 392.371 96.1976 390.112C100.555 374.522 32.6646 369.738 38.3633 348.189C39.3683 344.377 42.094 342.829 45.9248 342.834C62.4737 342.834 99.6021 371.756 107.385 371.756C107.979 371.756 108.405 371.584 108.637 371.218C112.536 364.964 110.74 359.872 83.257 343.343C55.7738 326.808 36.1428 317.588 47.114 305.718C48.3768 304.347 50.1659 303.741 52.3391 303.741C69.0248 303.746 108.447 339.398 108.447 339.398C108.447 339.398 119.087 350.395 125.523 350.395C127.001 350.395 128.259 349.815 129.111 348.382C133.673 340.737 86.7366 305.388 84.0898 290.804C82.2955 280.921 85.3474 275.917 90.9935 275.917Z"/> <path fill=white d="M296.9 404.174C282.31 383.118 283.343 367.316 303.363 347.627C323.383 327.943 335.037 299.145 335.037 299.145C335.037 299.145 339.39 282.419 349.304 283.958C359.219 285.497 366.498 310.492 345.731 325.783C324.963 341.069 349.866 351.456 357.856 337.099C365.846 322.741 387.663 285.831 398.978 278.772C410.287 271.713 418.25 275.668 415.583 290.218C412.916 304.769 365.618 340.036 370.22 347.668C374.822 355.296 391.041 338.7 391.041 338.7C391.041 338.7 441.791 293.255 452.84 305.097C463.889 316.94 444.457 326.863 416.766 343.359C389.068 359.85 386.921 364.206 390.85 370.446C394.784 376.685 455.915 325.971 461.657 347.47C467.393 368.969 399.269 375.209 403.474 390.051C407.678 404.899 451.461 361.958 460.416 378.689C469.376 395.425 398.633 415.088 398.06 415.234C375.209 421.067 317.175 433.426 296.9 404.174Z"/> <path fill=black d="M408.105 255C416.624 255 424.238 258.477 429.547 264.784C432.831 268.69 436.262 274.986 436.541 284.414C440.113 283.394 443.549 282.824 446.759 282.824C454.915 282.824 462.282 285.93 467.491 291.573C474.185 298.815 477.158 307.715 475.863 316.62C475.248 320.861 473.822 324.663 471.69 328.182C476.184 331.795 479.494 336.826 481.094 342.876C482.346 347.619 483.63 357.497 476.926 367.673C477.352 368.337 477.752 369.027 478.126 369.733C482.157 377.336 482.414 385.927 478.857 393.928C473.464 406.054 460.062 415.608 434.036 425.863C417.845 432.242 403.032 436.321 402.901 436.357C381.495 441.874 362.136 444.677 345.377 444.677C314.573 444.677 292.52 435.301 279.829 416.811C259.402 387.036 262.322 359.803 288.753 333.552C303.381 319.026 313.105 297.607 315.131 292.906C319.214 278.986 330.012 263.513 347.961 263.513H347.966C349.476 263.513 351.002 263.633 352.507 263.869C360.368 265.097 367.24 269.595 372.15 276.361C377.449 269.814 382.595 264.606 387.252 261.667C394.271 257.243 401.285 255 408.105 255ZM408.105 275.917C405.421 275.917 402.143 277.051 398.528 279.331C387.304 286.406 365.646 323.403 357.716 337.793C355.059 342.614 350.518 344.654 346.429 344.654C338.315 344.654 331.98 336.638 345.687 326.451C366.299 311.124 359.069 286.072 349.229 284.529C348.797 284.461 348.371 284.43 347.961 284.43C339.015 284.43 335.069 299.751 335.069 299.751C335.069 299.751 323.503 328.616 303.634 348.346C283.764 368.08 282.738 383.919 297.219 405.024C307.096 419.415 326.002 421.292 345.377 421.292C365.472 421.292 386.108 417.772 397.653 414.796C398.221 414.65 468.397 396.255 459.504 379.48C458.009 376.661 455.547 375.532 452.447 375.532C439.924 375.532 417.145 394.054 407.352 394.054C405.163 394.054 403.532 392.371 402.901 390.112C398.543 374.522 466.434 369.738 460.735 348.189C459.73 344.377 457.004 342.829 453.174 342.834C436.625 342.834 399.496 371.756 391.714 371.756C391.119 371.756 390.693 371.584 390.461 371.218C386.562 364.964 388.358 359.872 415.841 343.343C443.325 326.808 462.956 317.588 451.984 305.718C450.722 304.347 448.932 303.741 446.759 303.741C430.074 303.746 390.651 339.398 390.651 339.398C390.651 339.398 380.011 350.395 373.576 350.395C372.097 350.395 370.84 349.815 369.987 348.382C365.425 340.737 412.362 305.388 415.009 290.804C416.803 280.921 413.751 275.917 408.105 275.917Z"/> <path fill=#0E1116 d="M319.277 228.901C319.277 205.236 288.585 241.304 250.637 241.465C212.692 241.306 182 205.238 182 228.901C182 244.591 189.507 270.109 209.669 285.591C213.681 271.787 235.726 260.729 238.877 262.317C243.364 264.578 243.112 270.844 250.637 276.365C258.163 270.844 257.911 264.58 262.398 262.317C265.551 260.729 287.594 271.787 291.605 285.591C311.767 270.109 319.275 244.591 319.275 228.903L319.277 228.901Z"/> <path fill=#FF323D d="M262.4 262.315C257.913 264.576 258.165 270.842 250.639 276.363C243.114 270.842 243.366 264.578 238.879 262.315C235.726 260.727 213.683 271.785 209.672 285.589C219.866 293.417 233.297 298.678 250.627 298.806C250.631 298.806 250.635 298.806 250.641 298.806C250.646 298.806 250.65 298.806 250.656 298.806C267.986 298.68 281.417 293.417 291.611 285.589C287.6 271.785 265.555 260.727 262.404 262.315H262.4Z"/> <path fill=black d="M373 196C382.389 196 390 188.389 390 179C390 169.611 382.389 162 373 162C363.611 162 356 169.611 356 179C356 188.389 363.611 196 373 196Z"/> <path fill=black d="M128 196C137.389 196 145 188.389 145 179C145 169.611 137.389 162 128 162C118.611 162 111 169.611 111 179C111 188.389 118.611 196 128 196Z"/> <path fill=#0E1116 d="M313.06 171.596C319.796 173.968 322.476 187.779 329.281 184.171C342.167 177.337 347.06 161.377 340.208 148.524C333.356 135.671 317.354 130.792 304.467 137.626C291.58 144.46 286.688 160.419 293.54 173.272C296.774 179.339 307.039 169.475 313.06 171.596Z"/> <path fill=#0E1116 d="M188.554 171.596C181.818 173.968 179.138 187.779 172.334 184.171C159.447 177.337 154.555 161.377 161.407 148.524C168.259 135.671 184.26 130.792 197.147 137.626C210.034 144.46 214.926 160.419 208.074 173.272C204.84 179.339 194.575 169.475 188.554 171.596Z"/> </svg> </a> <a href=http://kuldeepsinghsidhu.com target=_blank rel=noopener title=kuldeepsinghsidhu.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0M5.78 8.75a9.64 9.64 0 0 0 1.363 4.177q.383.64.857 1.215c.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a10 10 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.51 6.51 0 0 0 4.666 5.5q-.184-.271-.352-.552c-.715-1.192-1.437-2.874-1.581-4.948m-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948q.18-.295.353-.552a6.51 6.51 0 0 0-4.666 5.5m10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948q-.18.296-.353.552a6.51 6.51 0 0 0 4.666-5.5Zm2.733-1.5a6.51 6.51 0 0 0-4.666-5.5q.184.272.353.552c.714 1.192 1.436 2.874 1.58 4.948Z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "search.highlight", "search.share", "search.suggest", "content.tooltips", "navigation.instant.progress", "navigation.path", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../assets/javascripts/bundle.60a45f97.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../javascripts/xfile.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>